{"id": "2505.22704", "pdf": "https://arxiv.org/pdf/2505.22704", "abs": "https://arxiv.org/abs/2505.22704", "authors": ["Feng Yao", "Zilong Wang", "Liyuan Liu", "Junxia Cui", "Li Zhong", "Xiaohan Fu", "Haohui Mai", "Vish Krishnan", "Jianfeng Gao", "Jingbo Shang"], "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.", "AI": {"tldr": "REAL\u662f\u4e00\u79cd\u65b0\u9896\u7684RL\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u5f15\u5bfc\u7684\u53cd\u9988\u63d0\u9ad8LLMs\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\uff0c\u9a8c\u8bc1\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u751f\u6210\u65f6\uff0c\u4ee3\u7801\u8d28\u91cf\u5c24\u5176\u5728\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u52b3\u52a8\u5bc6\u96c6\u578b\u6ce8\u91ca\u6216\u4e0d\u53ef\u9760\u7684\u542f\u53d1\u5f0f\uff0c\u4ece\u800c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86REAL\uff0c\u4e00\u79cd\u4f7f\u7528\u7a0b\u5e8f\u5206\u6790\u5f15\u5bfc\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u6fc0\u52b1LLMs\u751f\u6210\u751f\u4ea7\u8d28\u91cf\u7684\u4ee3\u7801\u3002REAL\u96c6\u6210\u4e86\u4e24\u4e2a\u81ea\u52a8\u5316\u4fe1\u53f7\uff1a\u7a0b\u5e8f\u5206\u6790\u7528\u4e8e\u68c0\u6d4b\u5b89\u5168\u6027\u6216\u53ef\u7ef4\u62a4\u6027\u7f3a\u9677\uff0c\u4ee5\u53ca\u5355\u5143\u6d4b\u8bd5\u7528\u4e8e\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eREAL\u5728\u8bc4\u4f30\u529f\u80fd\u548c\u4ee3\u7801\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "REAL\u80fd\u591f\u7f29\u5c0f\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u4e0e\u751f\u4ea7\u5c31\u7eea\u4ee3\u7801\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7fLLMs\u80fd\u591f\u540c\u65f6\u63d0\u4f9b\u901f\u5ea6\u548c\u8d28\u91cf\u3002"}}
{"id": "2505.22752", "pdf": "https://arxiv.org/pdf/2505.22752", "abs": "https://arxiv.org/abs/2505.22752", "authors": ["Rafik Mankour", "Yassine Chafai", "Hamada Saleh", "Ghassen Ben Hassine", "Thibaud Barreau", "Peter Tankov"], "title": "Climate Finance Bench", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://github.com/Pladifes/climate_finance_bench", "summary": "Climate Finance Bench introduces an open benchmark that targets\nquestion-answering over corporate climate disclosures using Large Language\nModels. We curate 33 recent sustainability reports in English drawn from\ncompanies across all 11 GICS sectors and annotate 330 expert-validated\nquestion-answer pairs that span pure extraction, numerical reasoning, and\nlogical reasoning. Building on this dataset, we propose a comparison of RAG\n(retrieval-augmented generation) approaches. We show that the retriever's\nability to locate passages that actually contain the answer is the chief\nperformance bottleneck. We further argue for transparent carbon reporting in\nAI-for-climate applications, highlighting advantages of techniques such as\nWeight Quantization.", "AI": {"tldr": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9488\u5bf9\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u7684\u5f00\u653e\u57fa\u51c6\uff0c\u63d0\u51fa\u4e86RAG\u65b9\u6cd5\u7684\u6bd4\u8f83\uff0c\u5f3a\u8c03\u68c0\u7d22\u5668\u5b9a\u4f4d\u7b54\u6848\u6bb5\u843d\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u652f\u6301AI\u6c14\u5019\u5e94\u7528\u4e2d\u7684\u900f\u660e\u78b3\u62a5\u544a\u3002", "motivation": "\u5f15\u5165\u4e00\u4e2a\u5f00\u653e\u57fa\u51c6\uff0c\u501f\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u4e2d\u7684\u95ee\u7b54\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u65b9\u6cd5\u7684\u6bd4\u8f83\uff0c\u5229\u752833\u4e2a\u6700\u8fd1\u7684\u53ef\u6301\u7eed\u6027\u62a5\u544a\u548c330\u5bf9\u4e13\u5bb6\u9a8c\u8bc1\u7684\u95ee\u9898\u7b54\u6848\u5bf9\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5c55\u793a\u4e86\u68c0\u7d22\u5668\u5bf9\u4e8e\u5b9a\u4f4d\u5b9e\u9645\u5305\u542b\u7b54\u6848\u7684\u6bb5\u843d\u80fd\u529b\u7684\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u5e76\u6307\u51fa\u6280\u672f\u5982\u6743\u91cd\u91cf\u5316\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u53d6\u5668\u5b9a\u4f4d\u5b9e\u9645\u5305\u542b\u7b54\u6848\u7684\u6bb5\u843d\u7684\u80fd\u529b\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u4e3b\u5f20\u5728AI\u6c14\u5019\u5e94\u7528\u4e2d\u900f\u660e\u7684\u78b3\u62a5\u544a\u3002"}}
{"id": "2505.22757", "pdf": "https://arxiv.org/pdf/2505.22757", "abs": "https://arxiv.org/abs/2505.22757", "authors": ["Ansar Aynetdinov", "Alan Akbik"], "title": "Pre-Training Curriculum for Multi-Token Prediction in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Multi-token prediction (MTP) is a recently proposed pre-training objective\nfor language models. Rather than predicting only the next token (NTP), MTP\npredicts the next $k$ tokens at each prediction step, using multiple prediction\nheads. MTP has shown promise in improving downstream performance, inference\nspeed, and training efficiency, particularly for large models. However, prior\nwork has shown that smaller language models (SLMs) struggle with the MTP\nobjective. To address this, we propose a curriculum learning strategy for MTP\ntraining, exploring two variants: a forward curriculum, which gradually\nincreases the complexity of the pre-training objective from NTP to MTP, and a\nreverse curriculum, which does the opposite. Our experiments show that the\nforward curriculum enables SLMs to better leverage the MTP objective during\npre-training, improving downstream NTP performance and generative output\nquality, while retaining the benefits of self-speculative decoding. The reverse\ncurriculum achieves stronger NTP performance and output quality, but fails to\nprovide any self-speculative decoding benefits.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6765\u4f18\u5316\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u8bb0\u9884\u6d4b\u76ee\u6807\u4e0a\u7684\u8bad\u7ec3\uff0c\u7ed3\u679c\u663e\u793a\u6b63\u5411\u8bfe\u7a0b\u63d0\u5347\u4e86\u4e0b\u6e38\u6027\u80fd\u548c\u751f\u6210\u8d28\u91cf\uff0c\u800c\u53cd\u5411\u8bfe\u7a0b\u5219\u8868\u73b0\u66f4\u5f3a\u4f46\u7f3a\u4e4f\u89e3\u7801\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6807\u8bb0\u9884\u6d4b\u5728\u5927\u578b\u6a21\u578b\u4e2d\u663e\u793a\u51fa\u6539\u5584\u4e0b\u6e38\u6027\u80fd\u3001\u63a8\u7406\u901f\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u7684\u6f5c\u529b\uff0c\u4f46\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u76ee\u6807\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4ee5\u4f18\u5316\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u7528\u4e8e\u591a\u6807\u8bb0\u9884\u6d4b\u8bad\u7ec3\uff0c\u63a2\u7d22\u4e86\u4e24\u79cd\u53d8\u4f53\uff1a\u6b63\u5411\u8bfe\u7a0b\uff0c\u9010\u6b65\u589e\u52a0\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u590d\u6742\u6027\uff0c\u4ece\u5355\u6807\u8bb0\u9884\u6d4b\u5230\u591a\u6807\u8bb0\u9884\u6d4b\uff1b\u53cd\u5411\u8bfe\u7a0b\u5219\u6b63\u597d\u76f8\u53cd\u3002", "result": "\u6b63\u5411\u8bfe\u7a0b\u5b66\u4e60\u4f7f\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u9884\u8bad\u7ec3\u4e2d\u66f4\u597d\u5730\u5229\u7528\u591a\u6807\u8bb0\u9884\u6d4b\u76ee\u6807\uff0c\u63d0\u5347\u4e0b\u6e38\u5355\u6807\u8bb0\u9884\u6d4b\u8868\u73b0\u548c\u751f\u6210\u8f93\u51fa\u8d28\u91cf\uff0c\u5e76\u4fdd\u6301\u81ea\u6211\u63a8\u6d4b\u89e3\u7801\u7684\u4f18\u52bf\uff1b\u800c\u53cd\u5411\u8bfe\u7a0b\u867d\u7136\u5355\u6807\u8bb0\u9884\u6d4b\u8868\u73b0\u548c\u8f93\u51fa\u8d28\u91cf\u66f4\u5f3a\uff0c\u4f46\u6ca1\u6709\u63d0\u4f9b\u81ea\u6211\u63a8\u6d4b\u89e3\u7801\u7684\u4f18\u52bf\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6b63\u5411\u8bfe\u7a0b\u5b66\u4e60\u80fd\u591f\u5e2e\u52a9\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u66f4\u597d\u5730\u5229\u7528\u591a\u6807\u8bb0\u9884\u6d4b\u76ee\u6807\uff0c\u63d0\u5347\u4e0b\u6e38\u7684\u5355\u6807\u8bb0\u9884\u6d4b\u8868\u73b0\u548c\u751f\u6210\u8f93\u51fa\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u81ea\u6211\u63a8\u6d4b\u89e3\u7801\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u53cd\u5411\u8bfe\u7a0b\u867d\u7136\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u5355\u6807\u8bb0\u9884\u6d4b\u8868\u73b0\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u5374\u5728\u81ea\u6211\u63a8\u6d4b\u89e3\u7801\u65b9\u9762\u6ca1\u6709\u4efb\u4f55\u4f18\u52bf\u3002"}}
{"id": "2505.22759", "pdf": "https://arxiv.org/pdf/2505.22759", "abs": "https://arxiv.org/abs/2505.22759", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.", "AI": {"tldr": "FAMA\u662f\u9996\u4e2a\u5f00\u6e90\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u5f02\u4e14\u901f\u5ea6\u5feb\uff0c\u4fc3\u8fdb\u4e86\u8bed\u97f3\u6280\u672f\u7684\u5f00\u653e\u79d1\u5b66\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5c01\u95ed\u6027\u4f7f\u5f97\u53ef\u91cd\u590d\u6027\u548c\u516c\u5e73\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5f00\u6e90\u6a21\u578b\u6765\u4fc3\u8fdb\u5f00\u653e\u79d1\u5b66\u3002", "method": "\u5f00\u53d1\u4e86FAMA\u8fd9\u4e2a\u5f00\u6e90\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b16k\u5c0f\u65f6\u6570\u636e\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "FAMA\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u51fa\u8272\uff0c\u901f\u5ea6\u63d0\u9ad8\u81f3\u591a\u516b\u500d\uff0c\u5e76\u5f00\u653e\u4e86\u6240\u6709\u7814\u53d1\u6210\u679c\u3002", "conclusion": "FAMA\u5728\u6027\u80fd\u548c\u901f\u5ea6\u65b9\u9762\u5177\u6709\u7ade\u4e89\u4f18\u52bf\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f00\u653e\u79d1\u5b66\u7684\u8981\u6c42\u3002"}}
{"id": "2505.22814", "pdf": "https://arxiv.org/pdf/2505.22814", "abs": "https://arxiv.org/abs/2505.22814", "authors": ["Jonghan Lim", "Ilya Kovalenko"], "title": "A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Manufacturing environments are becoming more complex and unpredictable due to\nfactors such as demand variations and shorter product lifespans. This\ncomplexity requires real-time decision-making and adaptation to disruptions.\nTraditional control approaches highlight the need for advanced control\nstrategies capable of overcoming unforeseen challenges, as they demonstrate\nlimitations in responsiveness within dynamic industrial settings. Multi-agent\nsystems address these challenges through decentralization of decision-making,\nenabling systems to respond dynamically to operational changes. However,\ncurrent multi-agent systems encounter challenges related to real-time\nadaptation, context-aware decision-making, and the dynamic exploration of\nresource capabilities. Large language models provide the possibility to\novercome these limitations through context-aware decision-making capabilities.\nThis paper introduces a large language model-enabled control architecture for\nmulti-agent manufacturing systems to dynamically explore resource capabilities\nin response to real-time disruptions. A simulation-based case study\ndemonstrates that the proposed architecture improves system resilience and\nflexibility. The case study findings show improved throughput and efficient\nresource utilization compared to existing approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u652f\u6301\u7684\u591a\u4ee3\u7406\u5236\u9020\u7cfb\u7edf\u63a7\u5236\u67b6\u6784\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u6270\u52a8\uff0c\u63d0\u9ad8\u7cfb\u7edf\u5f39\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5236\u9020\u73af\u5883\u65e5\u76ca\u590d\u6742\u4e14\u96be\u4ee5\u9884\u6d4b\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u52a8\u6001\u5de5\u4e1a\u73af\u5883\u4e2d\u54cd\u5e94\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u63a7\u5236\u7b56\u7565\u6765\u89e3\u51b3\u672a\u9884\u89c1\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\u652f\u6301\u7684\u63a7\u5236\u67b6\u6784\uff0c\u5229\u7528\u591a\u4ee3\u7406\u5236\u9020\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65bd\u57fa\u4e8e\u4eff\u771f\u7684\u6848\u4f8b\u7814\u7a76\u6765\u5c55\u793a\u8be5\u67b6\u6784\u7684\u4f18\u8d8a\u6027\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u63d0\u51fa\u7684\u67b6\u6784\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u7075\u6d3b\u6027\uff1b\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\u67b6\u6784\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u4e14\u5728\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.22698", "pdf": "https://arxiv.org/pdf/2505.22698", "abs": "https://arxiv.org/abs/2505.22698", "authors": ["Luca Fantin", "Marco Antonelli", "Margherita Cesetti", "Daniele Irto", "Bruno Zamengo", "Francesco Silvestri"], "title": "Design and testing of an agent chatbot supporting decision making with public transport data", "categories": ["cs.AI"], "comment": null, "summary": "Assessing the quality of public transportation services requires the analysis\nof large quantities of data on the scheduled and actual trips and documents\nlisting the quality constraints each service needs to meet. Interrogating such\ndatasets with SQL queries, organizing and visualizing the data can be quite\ncomplex for most users. This paper presents a chatbot offering a user-friendly\ntool to interact with these datasets and support decision making. It is based\non an agent architecture, which expands the capabilities of the core Large\nLanguage Model (LLM) by allowing it to interact with a series of tools that can\nexecute several tasks, like performing SQL queries, plotting data and creating\nmaps from the coordinates of a trip and its stops. This paper also tackles one\nof the main open problems of such Generative AI projects: collecting data to\nmeasure the system's performance. Our chatbot has been extensively tested with\na workflow that asks several questions and stores the generated query, the\nretrieved data and the natural language response for each of them. Such\nquestions are drawn from a set of base examples which are then completed with\nactual data from the database. This procedure yields a dataset for the\nevaluation of the chatbot's performance, especially the consistency of its\nanswers and the correctness of the generated queries.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u7b80\u5316\u516c\u5171\u4ea4\u901a\u670d\u52a1\u8d28\u91cf\u6570\u636e\u7684\u4ea4\u4e92\u6d41\u7a0b\u3002\u5b83\u4f7f\u7528\u4ee3\u7406\u67b6\u6784\u6765\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u529f\u80fd\uff0c\u5e76\u5df2\u901a\u8fc7\u591a\u79cd\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e2d\uff0c\u4f7f\u7528SQL\u67e5\u8be2\u5206\u6790\u548c\u53ef\u89c6\u5316\u5927\u89c4\u6a21\u516c\u5171\u4ea4\u901a\u670d\u52a1\u6570\u636e\u5bf9\u5927\u90e8\u5206\u7528\u6237\u6765\u8bf4\u8f83\u4e3a\u590d\u6742\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u804a\u5929\u673a\u5668\u4eba\u7b80\u5316\u7528\u6237\u6570\u636e\u4ea4\u4e92\u548c\u51b3\u7b56\u652f\u6301\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u67b6\u6784\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u80fd\u591f\u4e0e\u4e00\u7cfb\u5217\u5de5\u5177\u4ea4\u4e92\uff0c\u4ee5\u6267\u884cSQL\u67e5\u8be2\u3001\u6570\u636e\u7ed8\u56fe\u4ee5\u53ca\u4ece\u884c\u7a0b\u5750\u6807\u521b\u5efa\u5730\u56fe\u7b49\u4efb\u52a1\u3002", "result": "\u8be5\u804a\u5929\u673a\u5668\u4eba\u7ecf\u8fc7\u4e25\u683c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e00\u4e2a\u5de5\u4f5c\u6d41\u7a0b\u6765\u95ee\u591a\u4e2a\u95ee\u9898\uff0c\u5e76\u5b58\u50a8\u751f\u6210\u7684\u67e5\u8be2\u3001\u68c0\u7d22\u7684\u6570\u636e\u548c\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u3002\u6700\u7ec8\u83b7\u5f97\u7684\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u7279\u522b\u5173\u6ce8\u7b54\u6848\u7684\u4e00\u81f4\u6027\u548c\u751f\u6210\u67e5\u8be2\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u804a\u5929\u673a\u5668\u4eba\u7684\u5de5\u5177\uff0c\u65e8\u5728\u7b80\u5316\u516c\u5171\u4ea4\u901a\u670d\u52a1\u8d28\u91cf\u6570\u636e\u7684\u4ea4\u4e92\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002\u901a\u8fc7\u4ee3\u7406\u67b6\u6784\u7684\u4f7f\u7528\uff0c\u6269\u5c55\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u529f\u80fd\uff0c\u80fd\u591f\u6267\u884cSQL\u67e5\u8be2\u3001\u6570\u636e\u7ed8\u56fe\u4ee5\u53ca\u5730\u56fe\u751f\u6210\u7b49\u4efb\u52a1\uff0c\u5b9e\u73b0\u7528\u6237\u53cb\u597d\u578b\u7684\u6570\u636e\u4ea4\u4e92\u3002"}}
{"id": "2505.22686", "pdf": "https://arxiv.org/pdf/2505.22686", "abs": "https://arxiv.org/abs/2505.22686", "authors": ["Ange-Clement Akazan", "Verlon Roel Mbingui", "Gnankan Landry Regis N'guessan", "Issa Karambal"], "title": "Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs", "categories": ["cs.LG"], "comment": null, "summary": "Weather forecasting is crucial for managing risks and economic planning,\nparticularly in tropical Africa, where extreme events severely impact\nlivelihoods. Yet, existing forecasting methods often struggle with the region's\ncomplex, non-linear weather patterns. This study benchmarks deep recurrent\nneural networks such as $\\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and\nKolmogorov-Arnold-based models $(\\texttt{KAN} and \\texttt{TKAN})$ for daily\nforecasting of temperature, precipitation, and pressure in two tropical cities:\nAbidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce\ntwo customized variants of $ \\texttt{TKAN}$ that replace its original\n$\\texttt{SiLU}$ activation function with $ \\texttt{GeLU}$ and \\texttt{MiSH},\nrespectively. Using station-level meteorological data spanning from 2010 to\n2024, we evaluate all the models on standard regression metrics. $\\texttt{KAN}$\nachieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,\n$\\texttt{MSE} < 0.0014~^\\circ C ^2$), while $\\texttt{TKAN}$ variants minimize\nabsolute errors for precipitation forecasting in low-rainfall regimes. The\ncustomized $\\texttt{TKAN}$ models demonstrate improvements over the standard\n$\\texttt{TKAN}$ across both datasets. Classical \\texttt{RNNs} remain highly\ncompetitive for atmospheric pressure ($R^2 \\approx 0.83{-}0.86$), outperforming\n$\\texttt{KAN}$-based models in this task. These results highlight the potential\nof spline-based neural architectures for efficient and data-efficient\nforecasting.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u70ed\u5e26\u5730\u533a\u5929\u6c14\u9884\u62a5\uff0c\u53d1\u73b0KAN\u548c\u81ea\u5b9a\u4e49\u7684TKAN\u5728\u6e29\u5ea6\u548c\u964d\u6c34\u9884\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u800c\u7ecf\u5178RNN\u5728\u6c14\u538b\u9884\u6d4b\u4e2d\u66f4\u5177\u7ade\u4e89\u529b\uff0c\u663e\u793a\u6837\u6761\u51fd\u6570\u795e\u7ecf\u7ed3\u6784\u5728\u9ad8\u6548\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u7684\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u5728\u5904\u7406\u70ed\u5e26\u975e\u6d32\u590d\u6742\u4e14\u975e\u7ebf\u6027\u7684\u5929\u6c14\u6a21\u5f0f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5929\u6c14\u9884\u62a5\u6a21\u578b\uff0c\u4ee5\u5e2e\u52a9\u7ba1\u7406\u98ce\u9669\u548c\u7ecf\u6d4e\u89c4\u5212\u3002", "method": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08\u5982LSTM\u3001GRU\u3001BiLSTM\u3001BiGRU\uff09\u548cKolmogorov-Arnold\uff08KAN\u548cTKAN\uff09\u57fa\u4e8e\u6837\u6761\u51fd\u6570\u7684\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u4e24\u4e2a\u70ed\u5e26\u57ce\u5e02\uff08\u79d1\u7279\u8fea\u74e6\u963f\u6bd4\u8ba9\u548c\u5362\u65fa\u8fbe\u57fa\u52a0\u5229\uff09\u7684\u6bcf\u65e5\u6e29\u5ea6\u3001\u964d\u6c34\u548c\u6c14\u538b\u3002\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e24\u79cdTKAN\u7684\u81ea\u5b9a\u4e49\u53d8\u4f53\uff0c\u5c06\u5176\u539f\u59cb\u7684SiLU\u6fc0\u6d3b\u51fd\u6570\u66ff\u6362\u4e3aGeLU\u548cMiSH\u3002\u901a\u8fc72010\u5e74\u81f32024\u5e74\u7684\u6c14\u8c61\u6570\u636e\uff0c\u5bf9\u6240\u6709\u6a21\u578b\u8fdb\u884c\u6807\u51c6\u56de\u5f52\u6307\u6807\u7684\u8bc4\u4f30\u3002", "result": "KAN\u6a21\u578b\u5728\u6e29\u5ea6\u9884\u6d4b\u4e0a\u8868\u73b0\u6700\u4f73\uff0cTKAN\u53d8\u4f53\u5728\u4f4e\u964d\u6c34\u73af\u5883\u4e2d\u7684\u964d\u6c34\u9884\u6d4b\u4e2d\u5c06\u7edd\u5bf9\u8bef\u5dee\u6700\u5c0f\u5316\u3002\u6b64\u5916\uff0c\u81ea\u5b9a\u4e49\u7684TKAN\u6a21\u578b\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u6807\u51c6TKAN\u3002\u7ecf\u5178\u7684RNN\u5728\u5927\u6c14\u538b\u529b\u9884\u6d4b\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u6bd4KAN\u57fa\u4e8e\u6a21\u578b\u5728\u8fd9\u4e00\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6837\u6761\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5728\u9ad8\u6548\u4e14\u6570\u636e\u9ad8\u6548\u7684\u5929\u6c14\u9884\u62a5\u9886\u57df\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.22765", "pdf": "https://arxiv.org/pdf/2505.22765", "abs": "https://arxiv.org/abs/2505.22765", "authors": ["Iddo Yosha", "Gallil Maimon", "Yossi Adi"], "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.", "AI": {"tldr": "\u5f15\u5165StressTest\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u53e5\u5b50\u91cd\u97f3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u65b0\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\uff0c\u751f\u6210Stress17k\u8bad\u7ec3\u96c6\uff0c\u4f18\u5316\u540e\u7684StresSLM\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u53e5\u5b50\u91cd\u97f3\u5728\u5851\u9020\u610f\u4e49\u548c\u8bf4\u8bdd\u8005\u610f\u56fe\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u5728SLMs\u7684\u8bc4\u4f30\u548c\u5f00\u53d1\u4e2d\u4ecd\u7136\u88ab\u5ffd\u89c6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5e76\u521b\u5efa\u4e86Stress17k\u8bad\u7ec3\u96c6\uff0c\u901a\u8fc7\u6a21\u62df\u91cd\u97f3\u53d8\u5316\u5f15\u8d77\u7684\u610f\u4e49\u53d8\u5316\u6765\u4f18\u5316\u6a21\u578b\u3002", "result": "\u4f18\u5316\u6a21\u578b\u4e0e\u771f\u5b9e\u4e16\u754c\u5f55\u97f3\u826f\u597d\u5339\u914d\uff0c\u5e76\u80fd\u6709\u6548\u5fae\u8c03SLMs\u3002", "conclusion": "\u6211\u4eec\u7684\u5fae\u8c03\u6a21\u578bStresSLM\u5728\u53e5\u5b50\u91cd\u97f3\u63a8\u7406\u548c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2505.23352", "pdf": "https://arxiv.org/pdf/2505.23352", "abs": "https://arxiv.org/abs/2505.23352", "authors": ["Xu Shen", "Yixin Liu", "Yiwei Dai", "Yili Wang", "Rui Miao", "Yue Tan", "Shirui Pan", "Xin Wang"], "title": "Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The communication topology in large language model-based multi-agent systems\nfundamentally governs inter-agent collaboration patterns, critically shaping\nboth the efficiency and effectiveness of collective decision-making. While\nrecent studies for communication topology automated design tend to construct\nsparse structures for efficiency, they often overlook why and when sparse and\ndense topologies help or hinder collaboration. In this paper, we present a\ncausal framework to analyze how agent outputs, whether correct or erroneous,\npropagate under topologies with varying sparsity. Our empirical studies reveal\nthat moderately sparse topologies, which effectively suppress error propagation\nwhile preserving beneficial information diffusion, typically achieve optimal\ntask performance. Guided by this insight, we propose a novel topology design\napproach, EIB-leanrner, that balances error suppression and beneficial\ninformation propagation by fusing connectivity patterns from both dense and\nsparse graphs. Extensive experiments show the superior effectiveness,\ncommunication cost, and robustness of EIB-leanrner.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86EIB-leanrner\uff0c\u901a\u8fc7\u878d\u5408\u7a00\u758f\u548c\u5bc6\u96c6\u56fe\u7684\u8fde\u63a5\u6a21\u5f0f\u4f18\u5316\u901a\u4fe1\u62d3\u6251\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5177\u5907\u4f18\u8d8a\u7684\u6548\u679c\u3001\u901a\u4fe1\u6210\u672c\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u4fe1\u62d3\u6251\u81ea\u52a8\u5316\u8bbe\u8ba1\u7814\u7a76\u5f80\u5f80\u6784\u5efa\u7a00\u758f\u7ed3\u6784\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5e38\u5e38\u5ffd\u89c6\u4e3a\u4f55\u4ee5\u53ca\u4f55\u65f6\u7a00\u758f\u6216\u5bc6\u96c6\u7684\u62d3\u6251\u6709\u52a9\u6216\u963b\u788d\u534f\u4f5c\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u8bbe\u8ba1\u65b9\u6cd5EIB-leanrner\uff0c\u901a\u8fc7\u878d\u5408\u6765\u81ea\u5bc6\u96c6\u548c\u7a00\u758f\u56fe\u7684\u8fde\u63a5\u6a21\u5f0f\uff0c\u5e73\u8861\u9519\u8bef\u6291\u5236\u4e0e\u6709\u76ca\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u663e\u793aEIB-leanrner\u5728\u6548\u679c\u3001\u901a\u4fe1\u6210\u672c\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u9002\u5ea6\u7a00\u758f\u7684\u62d3\u6251\u7ed3\u6784\u901a\u5e38\u80fd\u8fbe\u5230\u6700\u4f73\u4efb\u52a1\u8868\u73b0\uff0c\u56e0\u4e3a\u5b83\u80fd\u6709\u6548\u6291\u5236\u9519\u8bef\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u76ca\u7684\u4fe1\u606f\u6269\u6563\u3002"}}
{"id": "2505.22753", "pdf": "https://arxiv.org/pdf/2505.22753", "abs": "https://arxiv.org/abs/2505.22753", "authors": ["Arseniy Pertzovsky", "Roni Stern", "Ariel Felner", "Roie Zivan"], "title": "Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": null, "summary": "We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent\nPath Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of\nagents must move to their goal locations without collisions, whereas in LMAPF,\nnew goals are generated upon arrival. We propose methods for incorporating APFs\nin a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and\nPriority Inheritance with Backtracking (PIBT). Experimental results show that\nusing APF is not beneficial for MAPF but yields up to a 7-fold increase in\noverall system throughput for LMAPF.", "AI": {"tldr": "APFs improve LMAPF efficiency, boosting throughput by 7x, but don't help standard MAPF.", "motivation": "To improve the efficiency and performance of both MAPF and LMAPF by using Artificial Potential Fields.", "method": "The paper proposes methods to integrate APFs into several MAPF algorithms such as Prioritized Planning, MAPF-LNS2, and PIBT.", "result": "Incorporating APFs significantly increases overall system throughput by up to seven times for LMAPF, but does not benefit standard MAPF.", "conclusion": "APFs are not beneficial for traditional MAPF but significantly enhance throughput in LMAPF."}}
{"id": "2505.22689", "pdf": "https://arxiv.org/pdf/2505.22689", "abs": "https://arxiv.org/abs/2505.22689", "authors": ["Jialong Guo", "Xinghao Chen", "Yehui Tang", "Yunhe Wang"], "title": "SlimLLM: Accurate Structured Pruning for Large Language Models", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Large language models(LLMs) have garnered significant attention and\ndemonstrated impressive capabilities in a wide range of applications. However,\ndue to their enormous computational costs, the deployment and application of\nLLMs are often severely limited. To address this issue, structured pruning is\nan effective solution to compress the parameters of LLMs. Determining the\nimportance of each sub-module in LLMs and minimizing performance loss are\ncritical issues that need to be carefully addressed in structured pruning. In\nthis paper, we propose an effective and fast structured pruning method named\nSlimLLM for large language models. For channel and attention head pruning, we\nevaluate the importance based on the entire channel or head, rather than merely\naggregating the importance of individual elements within a sub-module. This\napproach enables a more holistic consideration of the interdependence among\nelements within the sub-module. In addition, we design a simple linear\nregression strategy for the output matrix to quickly recover performance. We\nalso propose layer-based importance ratio to determine the pruning ratio for\neach layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other\nmethods and achieves state-of-the-art performance.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5e94\u7528\uff0c\u7ed3\u6784\u5316\u526a\u679d\u53ef\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faSlimLLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u901a\u9053\u548c\u6ce8\u610f\u5934\u526a\u679d\u53ca\u7ebf\u6027\u56de\u5f52\u7b56\u7565\u5b9e\u73b0\u6027\u80fd\u6062\u590d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u90e8\u7f72\u548c\u5e94\u7528\u3002\u7ed3\u6784\u5316\u526a\u679d\u53ef\u6709\u6548\u538b\u7f29\u8fd9\u4e9b\u6a21\u578b\u7684\u53c2\u6570\uff0c\u800c\u786e\u5b9aLLMs\u5b50\u6a21\u5757\u7684\u91cd\u8981\u6027\u53ca\u51cf\u5c11\u6027\u80fd\u635f\u5931\u662f\u5173\u952e\u95ee\u9898\u3002", "method": "\u6574\u7bc7\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u4e00\u79cd\u540d\u4e3aSlimLLM\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u5176\u4e2d\u5305\u62ec\u901a\u9053\u548c\u6ce8\u610f\u5934\u526a\u679d\u53ca\u7528\u4e8e\u5feb\u901f\u6062\u590d\u6027\u80fd\u7684\u7b80\u5355\u7ebf\u6027\u56de\u5f52\u7b56\u7565\uff0c\u8fd8\u63d0\u51fa\u57fa\u4e8e\u5c42\u7684\u91cd\u8981\u6027\u6bd4\u7387\u786e\u5b9a\u526a\u679d\u7387\u3002", "result": "\u6839\u636eLLaMA\u57fa\u51c6\u7ed3\u679c\uff0cSlimLLM\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u76ee\u524d\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5feb\u901f\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5SlimLLM\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u6062\u590d\u548c\u526a\u679d\u7387\u7684\u6700\u4f73\u786e\u5b9a\u3002\u57fa\u4e8eLLaMA\u57fa\u51c6\u6d4b\u8bd5\uff0cSlimLLM\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u8fbe\u5230\u4e86\u76ee\u524d\u7684\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2505.22771", "pdf": "https://arxiv.org/pdf/2505.22771", "abs": "https://arxiv.org/abs/2505.22771", "authors": ["Christopher Ormerod"], "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, AIME-Con Conference Submission", "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell-correction and an encoder-based token classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers.", "AI": {"tldr": "Incorporating annotations for spelling, grammar, and argumentative components enhances AES accuracy, demonstrated using LLMs on the PERSUADE corpus.", "motivation": "To improve the accuracy of automated essay scoring by incorporating feedback-driven annotations.", "method": "The method involves employing two LLMs for generating annotations: a generative language model for spell-correction and an encoder-based token classifier for identifying argumentative elements.", "result": "The approach shows improved performance in AES using encoder-based LLMs fine-tuned as classifiers.", "conclusion": "Integrating feedback-oriented annotations into the scoring pipeline enhances the accuracy of AES."}}
{"id": "2505.23584", "pdf": "https://arxiv.org/pdf/2505.23584", "abs": "https://arxiv.org/abs/2505.23584", "authors": ["Sumbal Malik", "Majid Khonji", "Khaled Elbassioni", "Jorge Dias"], "title": "Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "The rapid growth of e-commerce and the increasing demand for timely,\ncost-effective last-mile delivery have increased interest in collaborative\nlogistics. This research introduces a novel collaborative synchronized\nmulti-platform vehicle routing problem with drones and robots (VRP-DR), where a\nfleet of $\\mathcal{M}$ trucks, $\\mathcal{N}$ drones and $\\mathcal{K}$ robots,\ncooperatively delivers parcels. Trucks serve as mobile platforms, enabling the\nlaunching, retrieving, and en-route charging of drones and robots, thereby\naddressing critical limitations such as restricted payload capacities, limited\nrange, and battery constraints. The VRP-DR incorporates five realistic\nfeatures: (1) multi-visit service per trip, (2) multi-trip operations, (3)\nflexible docking, allowing returns to the same or different trucks (4) cyclic\nand acyclic operations, enabling return to the same or different nodes; and (5)\nen-route charging, enabling drones and robots to recharge while being\ntransported on the truck, maximizing operational efficiency by utilizing idle\ntransit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)\nto minimize both operational costs and makespan. To overcome the computational\nchallenges of solving large-scale instances, a scalable heuristic algorithm,\nFINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to\nprovide efficient, near-optimal solutions. Numerical experiments across various\ninstance sizes evaluate the performance of the MILP and heuristic approaches in\nterms of solution quality and computation time. The results demonstrate\nsignificant time savings of the combined delivery mode over the truck-only mode\nand substantial cost reductions from enabling multi-visits. The study also\nprovides insights into the effects of en-route charging, docking flexibility,\ndrone count, speed, and payload capacity on system performance.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u7ed3\u5408\u4f7f\u7528\u5361\u8f66\u3001\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u7684\u65b0\u65b9\u6cd5\u6765\u4f18\u5316\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u65f6\u95f4\u548c\u6210\u672c\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u7684\u5feb\u901f\u589e\u957f\u548c\u5bf9\u53ca\u65f6\u3001\u6210\u672c\u6548\u76ca\u5f3a\u7684\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\u9700\u6c42\u7684\u589e\u52a0\uff0c\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u5173\u6ce8\u534f\u4f5c\u7269\u6d41\uff0c\u5e76\u5f15\u5165VRP-DR\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7ec4\u5408\u540c\u6b65\u591a\u5e73\u53f0\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP-DR\uff09\uff0c\u5e76\u5c06\u5176\u6784\u5efa\u4e3a\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6a21\u578b\uff0c\u4ee5\u6700\u5c0f\u5316\u64cd\u4f5c\u6210\u672c\u548c\u5b8c\u5de5\u65f6\u95f4\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u63d0\u51fa\u7684MILP\u6a21\u578b\u548cFINDER\u7b97\u6cd5\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4ec5\u4f7f\u7528\u5361\u8f66\u7684\u6a21\u5f0f\u76f8\u6bd4\uff0c\u7ed3\u5408\u4f7f\u7528\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u663e\u8457\u8282\u7701\u65f6\u95f4\uff0c\u5e76\u5728\u591a\u6b21\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u964d\u4f4e\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7ed3\u5408\u4f7f\u7528\u5361\u8f66\u3001\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u8fdb\u884c\u5305\u88f9\u914d\u9001\u5728\u65f6\u95f4\u548c\u6210\u672c\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u6b21\u8bbf\u95ee\u548c\u5728\u9014\u5145\u7535\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.22756", "pdf": "https://arxiv.org/pdf/2505.22756", "abs": "https://arxiv.org/abs/2505.22756", "authors": ["Tian Qin", "Core Francisco Park", "Mujin Kwun", "Aaron Walsman", "Eran Malach", "Nikhil Anand", "Hidenori Tanaka", "David Alvarez-Melis"], "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning tasks have become prominent benchmarks for assessing\nthe reasoning capabilities of LLMs, especially with reinforcement learning (RL)\nmethods such as GRPO showing significant performance gains. However, accuracy\nmetrics alone do not support fine-grained assessment of capabilities and fail\nto reveal which problem-solving skills have been internalized. To better\nunderstand these capabilities, we propose to decompose problem solving into\nfundamental capabilities: Plan (mapping questions to sequences of steps),\nExecute (correctly performing solution steps), and Verify (identifying the\ncorrectness of a solution). Empirically, we find that GRPO mainly enhances the\nexecution skill-improving execution robustness on problems the model already\nknows how to solve-a phenomenon we call temperature distillation. More\nimportantly, we show that RL-trained models struggle with fundamentally new\nproblems, hitting a 'coverage wall' due to insufficient planning skills. To\nexplore RL's impact more deeply, we construct a minimal, synthetic\nsolution-tree navigation task as an analogy for mathematical problem-solving.\nThis controlled setup replicates our empirical findings, confirming RL\nprimarily boosts execution robustness. Importantly, in this setting, we\nidentify conditions under which RL can potentially overcome the coverage wall\nthrough improved exploration and generalization to new solution paths. Our\nfindings provide insights into the role of RL in enhancing LLM reasoning,\nexpose key limitations, and suggest a path toward overcoming these barriers.\nCode is available at https://github.com/cfpark00/RL-Wall.", "AI": {"tldr": "Research shows RL improves execution skills but fails in planning, posing limitations for solving new tasks in LLMs.", "motivation": "The study aims to address limitations in current accuracy metrics by offering a deeper evaluation method for understanding internalized problem-solving skills in LLMs.", "method": "The paper uses a decomposition approach analyzing fundamental capabilities like Plan, Execute, and Verify, along with synthetic task experiments, to explore RL's impact on LLM reasoning.", "result": "FFindings show RL primarily enhances execution robustness and struggles with insufficient planning skills for novel problems, termed 'coverage wall'.", "conclusion": "RL methods mainly boost the execution capability in LLMs, but they struggle with new problem-solving due to weakness in planning skills."}}
{"id": "2505.22694", "pdf": "https://arxiv.org/pdf/2505.22694", "abs": "https://arxiv.org/abs/2505.22694", "authors": ["Dacao Zhang", "Kun Zhang", "Shimao Chu", "Le Wu", "Xin Li", "Si Wei"], "title": "MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning", "categories": ["cs.LG"], "comment": "This paper has been accepted to ACL 2025 Findings", "summary": "With the rapid development of Large Language Models (LLMs),\nParameter-Efficient Fine-Tuning (PEFT) methods have gained significant\nattention, which aims to achieve efficient fine-tuning of LLMs with fewer\nparameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)\nintroduces low-rank matrices to approximate the incremental tuning parameters\nand achieves impressive performance over multiple scenarios. After that, plenty\nof improvements have been proposed for further improvement. However, these\nmethods either focus on single-task scenarios or separately train multiple LoRA\nmodules for multi-task scenarios, limiting the efficiency and effectiveness of\nLoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in\nthis paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for\nmulti-task PEFT. Specifically, instead of using an individual LoRA for each\ntask, we align different ranks of LoRA module with different tasks, which we\nnamed low-rank experts. Moreover, we design a novel adaptive rank selector to\nselect the appropriate expert for each task. By jointly training low-rank\nexperts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task\nscenarios. Finally, we conduct extensive experiments over multiple multi-task\nbenchmarks along with different LLMs to verify model performance. Experimental\nresults demonstrate that compared to traditional LoRA and its variants, MoRE\nsignificantly improves the performance of LLMs in multi-task scenarios and\nincurs no additional inference cost. We also release the model and code to\nfacilitate the community.", "AI": {"tldr": "The paper proposes Mixture of Low-Rank Experts (MoRE) for multi-task PEFT, improving the performance of LLMs in multi-task scenarios without additional inference cost.", "motivation": "Improve the efficiency and effectiveness of Low-Rank Adaptation (LoRA) in multi-task scenarios for LLMs.", "method": "Introduce Mixture of Low-Rank Experts (MoRE) method by aligning different ranks of LoRA modules with different tasks and designing an adaptive rank selector for task-specific expert selection.", "result": "Experimental results show that MoRE significantly enhances the performance of LLMs in multi-task scenarios compared to traditional LoRA and its variants.", "conclusion": "MoRE significantly improves the performance of LLMs in multi-task scenarios without incurring additional inference cost."}}
{"id": "2505.22774", "pdf": "https://arxiv.org/pdf/2505.22774", "abs": "https://arxiv.org/abs/2505.22774", "authors": ["Kaja Dobrovoljc"], "title": "Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel treebank-driven approach to comparing syntactic\nstructures in speech and writing using dependency-parsed corpora. Adopting a\nfully inductive, bottom-up method, we define syntactic structures as\ndelexicalized dependency (sub)trees and extract them from spoken and written\nUniversal Dependencies (UD) treebanks in two syntactically distinct languages,\nEnglish and Slovenian. For each corpus, we analyze the size, diversity, and\ndistribution of syntactic inventories, their overlap across modalities, and the\nstructures most characteristic of speech. Results show that, across both\nlanguages, spoken corpora contain fewer and less diverse syntactic structures\nthan their written counterparts, with consistent cross-linguistic preferences\nfor certain structural types across modalities. Strikingly, the overlap between\nspoken and written syntactic inventories is very limited: most structures\nattested in speech do not occur in writing, pointing to modality-specific\npreferences in syntactic organization that reflect the distinct demands of\nreal-time interaction and elaborated writing. This contrast is further\nsupported by a keyness analysis of the most frequent speech-specific\nstructures, which highlights patterns associated with interactivity,\ncontext-grounding, and economy of expression. We argue that this scalable,\nlanguage-independent framework offers a useful general method for\nsystematically studying syntactic variation across corpora, laying the\ngroundwork for more comprehensive data-driven theories of grammar in use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u4f9d\u5b58\u89e3\u6790\u7684\u6811\u5e93\u6765\u6bd4\u8f83\u82f1\u8bed\u548c\u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed\u4e2d\u8bed\u97f3\u548c\u4e66\u5199\u7684\u53e5\u6cd5\u7ed3\u6784\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8bed\u97f3\u4e0e\u4e66\u5199\u5728\u53e5\u6cd5\u7ed3\u6784\u4e0a\u6709\u663e\u8457\u5dee\u5f02\uff0c\u8bed\u97f3\u66f4\u5c11\u89c1\u3001\u66f4\u5177\u4e92\u52a8\u6027\u3002", "motivation": "\u7814\u7a76\u8bed\u97f3\u548c\u4e66\u5199\u4e2d\u53e5\u6cd5\u7ed3\u6784\u7684\u5dee\u5f02\uff0c\u63a2\u7d22\u4f55\u79cd\u7ed3\u6784\u5728\u67d0\u4e00\u8868\u73b0\u5f62\u5f0f\u4e2d\u66f4\u4e3a\u5178\u578b\u3002", "method": "\u91c7\u7528\u4e86\u6811\u5e93\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f9d\u5b58\u89e3\u6790\u7684\u8bed\u6599\u5e93\u6bd4\u8f83\u8bed\u97f3\u548c\u4e66\u5199\u4e2d\u7684\u53e5\u6cd5\u7ed3\u6784\uff0c\u5177\u4f53\u4ece\u82f1\u8bed\u548c\u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed\u7684UD\u6811\u5e93\u4e2d\u63d0\u53d6\u65e0\u8bcd\u6c47\u5316\u7684\u4f9d\u5b58\u5b50\u6811\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u4e24\u79cd\u8bed\u8a00\u4e2d\uff0c\u8bed\u97f3\u8bed\u6599\u4e2d\u7684\u53e5\u6cd5\u7ed3\u6784\u6570\u91cf\u548c\u591a\u6837\u6027\u5747\u5c11\u4e8e\u4e66\u5199\u8bed\u6599\uff0c\u5176\u4e2d\u7279\u6b8a\u7684\u7ed3\u6784\u7c7b\u578b\u5728\u4e24\u79cd\u8868\u73b0\u5f62\u5f0f\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u8de8\u8bed\u8a00\u504f\u597d\u3002\u8bed\u97f3\u548c\u4e66\u5199\u53e5\u6cd5\u5e93\u5b58\u7684\u91cd\u53e0\u6709\u9650\uff0c\u8868\u660e\u5b58\u5728\u57fa\u4e8e\u8868\u73b0\u5f62\u5f0f\u7684\u53e5\u6cd5\u7ec4\u7ec7\u504f\u597d\u3002\u8fd9\u79cd\u5dee\u5f02\u901a\u8fc7\u5bf9\u8bed\u97f3\u7279\u5b9a\u7ed3\u6784\u7684\u5173\u952e\u6027\u5206\u6790\u5f97\u5230\u652f\u6301\uff0c\u5f3a\u8c03\u4e86\u4e0e\u4e92\u52a8\u6027\u3001\u60c5\u5883\u57fa\u7840\u548c\u8868\u8fbe\u7ecf\u6d4e\u76f8\u5173\u7684\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u6599\u5e93\u4e2d\u7684\u53e5\u6cd5\u53d8\u5f02\uff0c\u4e3a\u6784\u5efa\u66f4\u5168\u9762\u7684\u6570\u636e\u9a71\u52a8\u7684\u8bed\u6cd5\u7406\u8bba\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.22779", "pdf": "https://arxiv.org/pdf/2505.22779", "abs": "https://arxiv.org/abs/2505.22779", "authors": ["Mohammad Helal Uddin", "Sabur Baidya"], "title": "Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds", "categories": ["cs.AI"], "comment": null, "summary": "Mental disorders including depression, anxiety, and other neurological\ndisorders pose a significant global challenge, particularly among individuals\nexhibiting social avoidance tendencies. This study proposes a hybrid approach\nby leveraging smartphone sensor data measuring daily physical activities and\nanalyzing their social media (Twitter) interactions for evaluating an\nindividual's depression level. Using CNN-based deep learning models and Naive\nBayes classification, we identify human physical activities accurately and also\nclassify the user sentiments. A total of 33 participants were recruited for\ndata acquisition, and nine relevant features were extracted from the physical\nactivities and analyzed with their weekly depression scores, evaluated using\nthe Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six\nare derived from physical activities, achieving an activity recognition\naccuracy of 95%, while three features stem from sentiment analysis of Twitter\nactivities, yielding a sentiment analysis accuracy of 95.6%. Notably, several\nphysical activity features exhibited significant correlations with the severity\nof depression symptoms. For classifying the depression severity, a support\nvector machine (SVM)-based algorithm is employed that demonstrated a very high\naccuracy of 94%, outperforming alternative models, e.g., the multilayer\nperceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly\neffective in the long run for monitoring depression without breaching personal\nprivacy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\u548cTwitter\u4e92\u52a8\u5206\u6790\u6765\u8bc4\u4f30\u6291\u90c1\u6c34\u5e73\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548cSVM\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u4ee5\u6b64\u76d1\u63a7\u6291\u90c1\u800c\u4e0d\u4fb5\u72af\u9690\u79c1\u3002", "motivation": "\u5fc3\u7406\u969c\u788d\u5982\u6291\u90c1\u75c7\u3001\u7126\u8651\u75c7\u53ca\u5176\u4ed6\u795e\u7ecf\u969c\u788d\u5728\u5168\u7403\u8303\u56f4\u5185\u9020\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u793e\u4ea4\u56de\u907f\u503e\u5411\u7684\u4e2a\u4eba\u4e2d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86CNN\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u6765\u8bc6\u522b\u4eba\u7c7b\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u7b97\u6cd5\u8fdb\u884c\u6291\u90c1\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528\u516d\u4e2a\u6765\u81ea\u8eab\u4f53\u6d3b\u52a8\u7684\u7279\u5f81\u548c\u4e09\u4e2a\u6765\u81eaTwitter\u6d3b\u52a8\u5206\u6790\u7684\u60c5\u611f\u7279\u5f81\uff0c\u6d3b\u52a8\u8bc6\u522b\u7cbe\u5ea6\u8fbe\u523095%\uff0c\u60c5\u611f\u5206\u6790\u7cbe\u5ea6\u8fbe\u523095.6%\u3002SVM\u7b97\u6cd5\u7528\u4e8e\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe\u523094%\u3002\u51e0\u9879\u8eab\u4f53\u6d3b\u52a8\u7279\u5f81\u4e0e\u6291\u90c1\u75c7\u72b6\u7a0b\u5ea6\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u4fbf\u800c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\u548c\u793e\u4ea4\u5a92\u4f53\u4e92\u52a8\u5206\u6790\u6765\u76d1\u63a7\u6291\u90c1\u75c7\u72b6\uff0c\u540c\u65f6\u4fdd\u62a4\u4e2a\u4eba\u9690\u79c1\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u957f\u671f\u76d1\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.22695", "pdf": "https://arxiv.org/pdf/2505.22695", "abs": "https://arxiv.org/abs/2505.22695", "authors": ["Tengfei Lyu", "Siyuan Feng", "Hao Liu", "Hai Yang"], "title": "LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning", "categories": ["cs.LG"], "comment": null, "summary": "Ride-hailing platforms face significant challenges in optimizing order\ndispatching and driver repositioning operations in dynamic urban environments.\nTraditional approaches based on combinatorial optimization, rule-based\nheuristics, and reinforcement learning often overlook driver income fairness,\ninterpretability, and adaptability to real-world dynamics. To address these\ngaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models\n(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in\nride-hailing services. LLM-ODDR framework comprises three key components: (1)\nMulti-objective-guided Order Value Refinement, which evaluates orders by\nconsidering multiple objectives to determine their overall value; (2)\nFairness-aware Order Dispatching, which balances platform revenue with driver\nincome fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,\nwhich optimizes idle vehicle placement based on historical patterns and\nprojected supply. We also develop JointDR-GPT, a fine-tuned model optimized for\nODDR tasks with domain knowledge. Extensive experiments on real-world datasets\nfrom Manhattan taxi operations demonstrate that our framework significantly\noutperforms traditional methods in terms of effectiveness, adaptability to\nanomalous conditions, and decision interpretability. To our knowledge, this is\nthe first exploration of LLMs as decision-making agents in ride-hailing ODDR\ntasks, establishing foundational insights for integrating advanced language\nmodels within intelligent transportation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM-ODDR\u6846\u67b6\uff0c\u5229\u7528LLMs\u8fdb\u884c\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u548c\u53f8\u673a\u91cd\u5b9a\u4f4d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u548c\u53f8\u673a\u91cd\u5b9a\u4f4d\u64cd\u4f5c\u4e2d\uff0c\u5f80\u5f80\u5ffd\u89c6\u4e86\u53f8\u673a\u6536\u5165\u516c\u5e73\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u771f\u5b9e\u73af\u5883\u52a8\u6001\u7684\u9002\u5e94\u6027\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u5229\u7528LLMs\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "LLM-ODDR\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u591a\u76ee\u6807\u5f15\u5bfc\u7684\u8ba2\u5355\u4ef7\u503c\u7ec6\u5316\uff1b2\uff09\u8003\u8651\u53f8\u673a\u6536\u5165\u516c\u5e73\u6027\u7684\u8ba2\u5355\u8c03\u5ea6\uff1b3\uff09\u7a7a\u95f4\u65f6\u95f4\u9700\u6c42\u611f\u77e5\u7684\u53f8\u673a\u91cd\u5b9a\u4f4d\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aJointDR-GPT\u7684\u6a21\u578b\uff0c\u4ee5\u8fdb\u884cODDR\u4efb\u52a1\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cLLM-ODDR\u6846\u67b6\u5728\u6709\u6548\u6027\u3001\u9002\u5e94\u5f02\u5e38\u6761\u4ef6\u7684\u80fd\u529b\u548c\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aLLM-ODDR\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7f51\u7ea6\u8f66\u670d\u52a1\u4e2d\u8fdb\u884c\u8054\u5408\u8ba2\u5355\u8c03\u5ea6\u548c\u53f8\u673a\u91cd\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u53f8\u673a\u6536\u5165\u516c\u5e73\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u66fc\u54c8\u987f\u51fa\u79df\u8f66\u8fd0\u8425\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2505.22777", "pdf": "https://arxiv.org/pdf/2505.22777", "abs": "https://arxiv.org/abs/2505.22777", "authors": ["John Mendon\u00e7a", "Alon Lavie", "Isabel Trancoso"], "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators", "categories": ["cs.CL"], "comment": "May ARR", "summary": "As the capabilities of chatbots and their underlying LLMs continue to\ndramatically improve, evaluating their performance has increasingly become a\nmajor blocker to their further development. A major challenge is the available\nbenchmarking datasets, which are largely static, outdated, and lacking in\nmultilingual coverage, limiting their ability to capture subtle linguistic and\ncultural variations. This paper introduces MEDAL, an automated multi-agent\nframework for generating, evaluating, and curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. We find that current LLMs struggle to\ndetect nuanced issues, particularly those involving empathy and reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u751f\u6210\u548c\u8bc4\u4f30\u591a\u8bed\u8a00\u5bf9\u8bdd\u7684\u65b0\u6846\u67b6MEDAL\uff0c\u5e76\u53d1\u73b0LLMs\u5728\u8bc4\u4f30\u4e2d\u8bc6\u522b\u7ec6\u5fae\u95ee\u9898\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u6027\u3002", "motivation": "\u6539\u5584\u804a\u5929\u673a\u5668\u4eba\u548cLLMs\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u9759\u6001\u3001\u8fc7\u65f6\u548c\u591a\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165MEDAL\u6846\u67b6\uff0c\u5229\u7528\u5148\u8fdb\u7684LLMs\u751f\u6210\u591a\u8bed\u8a00\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\uff0c\u5e76\u4f7f\u7528GPT-4.1\u8fdb\u884c\u591a\u7ef4\u5ea6\u5206\u6790\u4ee5\u63ed\u793a\u8bed\u8a00\u95f4\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u8c03\u6574\u5e76\u521b\u5efa\u65b0\u7684\u591a\u8bed\u8a00\u5143\u8bc4\u4f30\u57fa\u51c6\uff0c\u53d1\u73b0\u5f53\u524dLLMs\u5728\u68c0\u6d4b\u7ec6\u5fae\u95ee\u9898\u65f6\u6709\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u540c\u7406\u5fc3\u548c\u63a8\u7406\u65b9\u9762\u3002", "conclusion": "\u5f53\u524d\u7684LLMs\u5728\u8bc4\u4f30\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u65f6\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u7ec6\u5fae\u95ee\u9898\u65b9\u9762\u3002MEDAL\u6846\u67b6\u6539\u5584\u4e86\u8bc4\u4f30\u57fa\u51c6\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u3002"}}
{"id": "2505.22804", "pdf": "https://arxiv.org/pdf/2505.22804", "abs": "https://arxiv.org/abs/2505.22804", "authors": ["Jonghan Lim", "Ilya Kovalenko"], "title": "Dynamic Task Adaptation for Multi-Robot Manufacturing Systems with Large Language Models", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Recent manufacturing systems are increasingly adopting multi-robot\ncollaboration to handle complex and dynamic environments. While multi-agent\narchitectures support decentralized coordination among robot agents, they often\nface challenges in enabling real-time adaptability for unexpected disruptions\nwithout predefined rules. Recent advances in large language models offer new\nopportunities for context-aware decision-making to enable adaptive responses to\nunexpected changes. This paper presents an initial exploratory implementation\nof a large language model-enabled control framework for dynamic task\nreassignment in multi-robot manufacturing systems. A central controller agent\nleverages the large language model's ability to interpret structured robot\nconfiguration data and generate valid reassignments in response to robot\nfailures. Experiments in a real-world setup demonstrate high task success rates\nin recovering from failures, highlighting the potential of this approach to\nimprove adaptability in multi-robot manufacturing systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u5236\u9020\u7cfb\u7edf\u7684\u4efb\u52a1\u91cd\u65b0\u5206\u914d\u9002\u5e94\u6027\u4e0e\u6210\u529f\u7387\u3002", "motivation": "\u5e94\u5bf9\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7531\u4e8e\u73af\u5883\u590d\u6742\u548c\u52a8\u6001\u53d8\u5316\u5e26\u6765\u7684\u610f\u5916\u4e2d\u65ad\u4ee5\u53ca\u9002\u5e94\u6027\u6311\u6218\u3002", "method": "\u4e2d\u5fc3\u63a7\u5236\u5668\u4ee3\u7406\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u91ca\u7ed3\u6784\u5316\u7684\u673a\u5668\u4eba\u914d\u7f6e\u6570\u636e\uff0c\u5e76\u5728\u673a\u5668\u4eba\u6545\u969c\u65f6\u751f\u6210\u6709\u6548\u7684\u4efb\u52a1\u91cd\u65b0\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5904\u7406\u673a\u5668\u4eba\u6545\u969c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u8f83\u9ad8\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u9002\u5e94\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u591a\u673a\u5668\u4eba\u5236\u9020\u7cfb\u7edf\u4efb\u52a1\u91cd\u65b0\u5206\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a7\u5236\u6846\u67b6\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2505.22871", "pdf": "https://arxiv.org/pdf/2505.22871", "abs": "https://arxiv.org/abs/2505.22871", "authors": ["Yuval David", "Fabiana Fournier", "Lior Limonad", "Inna Skarbovsky"], "title": "The WHY in Business Processes: Unification of Causal Process Models", "categories": ["cs.AI"], "comment": "28 pages, 6 figures, BPM 2025 Forum", "summary": "Causal reasoning is essential for business process interventions and\nimprovement, requiring a clear understanding of causal relationships among\nactivity execution times in an event log. Recent work introduced a method for\ndiscovering causal process models but lacked the ability to capture alternating\ncausal conditions across multiple variants. This raises the challenges of\nhandling missing values and expressing the alternating conditions among log\nsplits when blending traces with varying activities.\n  We propose a novel method to unify multiple causal process variants into a\nconsistent model that preserves the correctness of the original causal models,\nwhile explicitly representing their causal-flow alternations. The method is\nformally defined, proved, evaluated on three open and two proprietary datasets,\nand released as an open-source implementation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u7edf\u4e00\u591a\u79cd\u56e0\u679c\u8fc7\u7a0b\u53d8\u4f53\uff0c\u89e3\u51b3\u4e86\u4ea4\u66ff\u56e0\u679c\u6761\u4ef6\u7684\u95ee\u9898\uff0c\u5e76\u6709\u6548\u8bc4\u4f30\u4e8e\u591a\u4e2a\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u53d1\u73b0\u56e0\u679c\u8fc7\u7a0b\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u5728\u591a\u4e2a\u53d8\u4f53\u4e2d\u6355\u6349\u4ea4\u66ff\u56e0\u679c\u6761\u4ef6\u7684\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u88ab\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u7ecf\u8fc7\u8bc1\u660e\uff0c\u5e76\u5728\u4e09\u4e2a\u5f00\u653e\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u540c\u65f6\u4f5c\u4e3a\u5f00\u6e90\u5b9e\u73b0\u53d1\u5e03\u3002", "result": "\u65b0\u65b9\u6cd5\u6210\u529f\u5730\u7edf\u4e00\u4e86\u591a\u4e2a\u56e0\u679c\u8fc7\u7a0b\u53d8\u4f53\u4e3a\u4e00\u81f4\u7684\u6a21\u578b\uff0c\u4fdd\u6301\u539f\u59cb\u56e0\u679c\u6a21\u578b\u7684\u6b63\u786e\u6027\uff0c\u5e76\u660e\u786e\u8868\u793a\u5176\u56e0\u679c\u6d41\u66ff\u6362\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u591a\u4e2a\u56e0\u679c\u8fc7\u7a0b\u53d8\u4f53\u7edf\u4e00\u4e3a\u4e00\u81f4\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u56e0\u679c\u6a21\u578b\u7684\u6b63\u786e\u6027\uff0c\u5e76\u660e\u786e\u8868\u793a\u5176\u56e0\u679c\u6d41\u66ff\u6362\u3002"}}
