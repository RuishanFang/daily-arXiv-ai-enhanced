<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 84]
- [cs.AI](#cs.AI) [Total: 77]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.LG](#cs.LG) [Total: 84]
- [nlin.AO](#nlin.AO) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 提出了一种无偏见的评估框架，以更准确地评估GraphRAG方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG答案评估框架存在与问题无关和评估偏见两个关键缺陷，可能导致对性能的偏见或错误的结论。

Method: 通过图-文本基础的问题生成来创建与数据集更相关的问题，并采用无偏见的评估过程来消除基于LLM的答案评估中的偏见。

Result: 通过应用无偏见的评估框架，发现三种代表性GraphRAG方法的性能提升比之前报道的要温和得多。

Conclusion: 提出一种无偏见的评估框架，重新评估了三种GraphRAG方法，发现其性能提升较为温和。

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [2] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM是一种新框架，通过仅使用文本数据训练语音功能语言模型，在语音基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的语音语言模型依赖于大规模配对的语音文本数据和广泛的计算资源，这在可扩展性和可访问性方面存在挑战。

Method: 提出了一种新的框架TESU-LLM，通过一个统一的编码器将语义等价的文本和语音输入映射到共享的潜在空间，并通过轻量级投影网络将编码器输出与LLM的嵌入空间对齐。

Result: TESU-LLM即使仅接受过文本训练，也在多个语音相关基准上表现良好，与大规模多模态数据训练的方法相比不逊色。

Conclusion: TESU-LLM能够在仅使用文本数据训练的情况下，依然在多种语音相关基准上表现出色，与使用大规模多模态数据和大量计算资源训练的基线方法相媲美。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [3] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: 通过软提示和LLM助手标签转移方法提升不同语言下游戏毒性检测的效率和扩展能力。


<details>
  <summary>Details</summary>
Motivation: 在游戏社区中，因为要支持多个游戏和语言，尤其是在实时环境中，毒性检测面临显著的规模化挑战，需要提高计算效率。

Method: 引入软提示方法，通过使用游戏环境标记使单一模型能够有效处理多游戏。同时，开发了一个使用GPT-4o-mini的LLM辅助标签传递框架，以支持更多语言。

Result: 在法语、德语、葡萄牙语和俄语的游戏聊天数据上进行评估，宏平均F1分数从32.96%到58.88%不等，并超过了英语基准的45.39%。

Conclusion: 统一的方法在生产中显著减少了计算资源和维护负担，每天平均每个游戏发现50名行为违规的玩家。

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [4] [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
*Panagiotis Koletsis,Christos Panagiotopoulos,Georgios Th. Papadopoulos,Vasilis Efthymiou*

Main category: cs.CL

TL;DR: 提出了一种使用知识图谱和大型语言模型检测表格数据列关系的混合方法，实验表明其在基准数据集上具竞争力，并公开提供代码。


<details>
  <summary>Details</summary>
Motivation: 鉴于表格解释任务的重要性及该领域中新技术和基准的引入，需要进一步提高对未标记表格数据列关系检测的能力。

Method: 此研究使用知识图谱（KG）作为参考点，通过大型语言模型（LLM）和统计分析，采用一种混合方法来检测非标记表格数据中的列之间的关系。主要模块包括域和范围约束检测以及关系共同出现分析。

Result: 实验评估了SemTab挑战提供的两个基准数据集上各个模块的影响，以及不同量化水平的大型语言模型的有效性。结果显示所提方法具有竞争力。

Conclusion: 所提出的方法在SemTab挑战提供的基准数据集上，与当前最先进的方法具有竞争力。

Abstract: Over the past few years, table interpretation tasks have made significant
progress due to their importance and the introduction of new technologies and
benchmarks in the field. This work experiments with a hybrid approach for
detecting relationships among columns of unlabeled tabular data, using a
Knowledge Graph (KG) as a reference point, a task known as CPA. This approach
leverages large language models (LLMs) while employing statistical analysis to
reduce the search space of potential KG relations. The main modules of this
approach for reducing the search space are domain and range constraints
detection, as well as relation co-appearance analysis. The experimental
evaluation on two benchmark datasets provided by the SemTab challenge assesses
the influence of each module and the effectiveness of different
state-of-the-art LLMs at various levels of quantization. The experiments were
performed, as well as at different prompting techniques. The proposed
methodology, which is publicly available on github, proved to be competitive
with state-of-the-art approaches on these datasets.

</details>


### [5] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: 本文引入LAC框架，通过长远行动评估改进LLM的决策能力，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂决策场景中面临挑战，难以进行长期推理和准确评估结果。

Method: 引入一种新的基于LLM的Actor-Critic框架LAC，在无梯度机制下进行高效的策略改进。

Result: 在各类环境中进行的实验表明，LAC框架展示了其广泛的适用性和优越性，使用较小的参数模型也能在复杂任务中超越基线方法。

Conclusion: 本文提出的LAC框架有效提高了LLM在多步决策环境中的策略优化能力。

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [6] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: DMPI-PMHFE框架通过特征融合提高了提示注入攻击检测的准确性，在各项指标上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的防御机制在有效性和广泛适用性之间存在重大权衡，需要一种高效的、适用于各种大型语言模型的提示注入检测方法。

Method: DMPI-PMHFE是一个双通道特征融合检测框架，结合预训练语言模型和启发式特征工程检测提示注入攻击。

Result: 实验结果表明，DMPI-PMHFE在准确率、召回率和F1值上优于现有方法，并显著降低提示注入攻击的成功率。

Conclusion: 提出的DMPI-PMHFE框架能够有效检测大型语言模型中的提示注入攻击。

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [7] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: 自信强化学习（RLSC）通过利用模型自身置信度作为奖励信号提高推理模型精度，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 提升语言模型在推理任务中的精度，并减少对人工标注和外部奖励模型的依赖。

Method: 提出了一种新的强化学习方法——自信强化学习（RLSC），利用模型自身的置信度作为奖励信号，而无需标注和奖励工程。

Result: 通过应用RLSC，模型在多个数学测试中表现出显著的精度提升：在AIME2024上提升了20.10%，在MATH500上提升了49.40%，在AMC23上提升了52.50%。

Conclusion: 自信强化学习提供了一种简单、可扩展的后训练方法，能够在最小监督下显著提高推理模型的性能。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [8] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 我们提出了一种利用大型语言模型（LLMs）在边缘设备上进行自然语言处理的工作流程，并使用图形数据库来增强战场物联网（IoBT）的态势感知。在测试中，我们的方法提高了查询准确度，并为关键决策提供自然语言交互。


<details>
  <summary>Details</summary>
Motivation: 随着战场物联网（IoBT）的扩展，为增强态势感知提供了新机遇。为了在关键决策中提高IoBT的态势感知潜力，必须将这些设备的数据处理为消费者可用的信息对象，并按需提供给消费者。

Method: 我们提出了一种工作流程，该流程利用自然语言处理（NLP）技术查询数据库，并以自然语言返回响应。我们的解决方案使用适合边缘设备的大型语言模型（LLMs）进行NLP处理，并利用适合动态连接网络的图形数据库。

Result: 我们评估了几种中型LLMs处理数据库查询任务，结果显示Llama 3.1（拥有80亿参数）在所有指标上均表现优异。最显著的是，我们的两步方法与当前方法不同，它放宽了生成的Cypher查询与真实代码的精确匹配（EM）要求，从而实现了19.4%的准确度提升。

Conclusion: 我们的工作流程为部署大规模语言模型（LLMs）到边缘设备上铺平了道路，使这些模型能够与包含信息对象的数据库进行自然语言交互，以支持关键决策。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [9] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: 本文提出DeBoP，一种优化轻量级语言模型的自动方法，显著提升模型在复杂任务中的表现并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 轻量级大型语言模型(LwLLMs)理论上的资源效率、成本效益和数据隐私优点受到其有限的推理能力的限制，而现有的提示优化方法需要耗费大量的人工努力或依赖顶尖的LLMs认知能力，导致对LwLLMs缺乏有效性。

Method: 引入了一种新的直接行为优化范式(DeBoP)，这是源于链式思维(COT)技术，该方法为自动优化，直接关注于LwLLMs的行为进行优化，通过无梯度蒙特卡洛树搜索将复杂的提示优化转化为可量化的执行序列优化。

Result: 实验结果表明，在七个挑战性任务中，DeBoP显著优于最近的提示优化方法。在大多数任务中，DeBoP优化的LwLLMs超越了GPT-3.5，同时与其他自动提示优化方法相比，计算时间减少约60%。

Conclusion: DeBoP为优化轻量级模型(LwLLMs)提供了一种有效的方法，使其在复杂任务中的性能接近甚至超过更强大的LLM。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [10] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 研究揭示价值对齐LLMs在安全性方面的风险，并提出改进方法以促进其安全性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的应用范围不断扩展，个性化的LLMs对齐到人类价值观引起了极大的兴趣。然而，这种对齐也带来了重大的安全隐患，因此需要更深入的研究与解决方案。

Method: 使用包含详细安全类别的数据集来分析价值对齐与安全风险之间的显著相关性，并支持其心理假设。

Result: 研究发现，价值对齐的大型语言模型比未微调模型更容易产生有害行为，并且在传统安全性评估中风险略高于其他微调模型。

Conclusion: 本文揭示了价值对齐的大型语言模型（LLMs）在安全性方面存在潜在风险，并提出了在上下文中进行对齐的方法以增强其安全性。

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [11] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: We introduce SMAR, a technique for better multimodal MoE models, maintaining language capabilities efficiently without high costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models.

Method: We propose Soft Modality-Aware Routing (SMAR), a novel regularization technique using Kullback Leibler divergence to control routing probability distributions across modalities.

Result: Experiments show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance.

Conclusion: Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [12] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 提出了一种方法来确保语言模型生成规范化的token序列，改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在推理过程中生成的token序列可能存在非规范化的问题，导致一些负面后果。

Method: 提出了一种称为规范化采样的简单高效采样方法，防止模型生成非规范化的token序列。

Result: 所提的规范化采样产生的token序列分布比标准采样更接近于训练时使用的真实token序列分布。

Conclusion: 通过使用规范化采样，可以确保在每一步生成过程中生成部分规范化的token序列，从而改进模型生成结果的准确性。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [13] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: 研究大语言模型在背景信息与其参数知识冲突下的表现，发现模型在两者对齐时表现最佳，而在冲突时难以抑制内部知识。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在背景信息与参数知识发生冲突时的表现，评估模型在这种情况下的可靠性。

Method: 提出了一种诊断框架，在情境记忆冲突下系统地评估大语言模型的行为，并构建诊断数据来引发这些冲突，以分析模型在多种任务类型中的表现。

Result: （1）在不需要使用知识的任务上，知识冲突影响较小，（2）当背景与参数知识对齐时，模型表现更佳，（3）即使被指示，模型也难以完全压制其内部知识，（4）提供解释冲突的理由会增加对背景的依赖。

Conclusion: 模型在背景知识与参数知识对齐时表现更好，而在知识冲突情况下，模型难以完全抑制其内部知识，显示出模型评估的有效性问题。

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [14] [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
*Luyao Shi,Michael Kazda,Charles Schmitter,Hemlata Gupta*

Main category: cs.CL

TL;DR: 本文提出使用合成问答数据集来改善电子设计自动化领域中大规模语言模型的性能，并分析和实现了确保信息安全的方法。


<details>
  <summary>Details</summary>
Motivation: 解决电子设计自动化（EDA）领域中大规模语言模型缺乏领域知识的问题，并改善其在检索增强生成（RAG）任务中的性能。面对获取EDA标注问答数据的困难，提出通过使用合成问答数据集来增强大规模语言模型的解决方案。

Method: 本文使用合成问答数据集进行检索增强微调（RAFT），同时评估了使用真实用户问题进行检索增强少样本（RAFS）示例对合成数据生成的影响，还实施了安全访问控制以确保敏感信息的安全。

Result: 研究结果显示，合成问答数据集的使用显著提高了大规模语言模型在电子设计自动化（EDA）中的任务表现。此外，使用真实用户问题作为RAFS示例对合成数据生成有积极的改善效果。

Conclusion: 使用合成问答数据集进行检索增强微调（RAFT）可以显著提高自回归增强生成（RAG）任务中的大规模语言模型性能，尤其是在电子设计自动化（EDA）领域。真实用户问题作为检索增强少样本（RAFS）示例也对合成数据生成有积极影响。

Abstract: Electronic design engineers often struggle to efficiently access relevant
information for tasks like design verification and technology development.
While large language models (LLMs) can enhance productivity as conversational
agents, pre-trained open-source LLMs lack domain-specific knowledge for
Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)
context, LLMs rely on external context but may still produce inaccurate
responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but
acquiring labeled question/answer (Q/A) data in EDA is difficult. To address
this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our
results show that RAFT with synthetic data significantly boosts LLM performance
for RAG-based EDA tasks. We also investigate the impact of using real user
questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data
generation. Additionally, we implement secure access control to ensure
sensitive information is only accessible to authorized personnel. Finally, we
assess the risk of data leakage and unintended memorization during fine-tuning
with synthetic data, providing practical insights.

</details>


### [15] [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
*Kshitish Ghate,Tessa Charlesworth,Mona Diab,Aylin Caliskan*

Main category: cs.CL

TL;DR: 研究发现，基础视觉-语言模型中的固有偏差会系统地传播到后续任务中，尤其是在零样本检索任务中，且更复杂的模型偏差传播更严重。这对于弱势群体的影响尤为明显。


<details>
  <summary>Details</summary>
Motivation: 为了建立公平的AI系统，需要了解在基础编码的视觉-语言模型中社会群体偏差如何影响下游任务中的偏差。

Method: 引入一个控制框架，通过关联表示空间中的固有偏差与零样本文本到图像与图像到文本检索中的外在偏差来衡量偏差传播。

Result: 研究结果显示，内在偏差与外在偏差之间有显著相关性，平均相关系数为0.83±0.10。这种模式在所有分析、检索方向、社会群体和不同模型中是一致的。

Conclusion: 更大或性能更好的模型表现出更大的偏差传播，这在越来越复杂的AI模型趋势下引起关注。研究显示弱势群体的传播较不强健，进一步导致模型相关结果的偏差。

Abstract: To build fair AI systems we need to understand how social-group biases
intrinsic to foundational encoder-based vision-language models (VLMs) manifest
in biases in downstream tasks. In this study, we demonstrate that intrinsic
biases in VLM representations systematically ``carry over'' or propagate into
zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's
outputs. We introduce a controlled framework to measure this propagation by
correlating (a) intrinsic measures of bias in the representational space with
(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and
image-to-text (ITT) retrieval. Results show substantial correlations between
intrinsic and extrinsic bias, with an average $\rho$ = 0.83 $\pm$ 0.10. This
pattern is consistent across 114 analyses, both retrieval directions, six
social groups, and three distinct VLMs. Notably, we find that
larger/better-performing models exhibit greater bias propagation, a finding
that raises concerns given the trend towards increasingly complex AI models.
Our framework introduces baseline evaluation tasks to measure the propagation
of group and valence signals. Investigations reveal that underrepresented
groups experience less robust propagation, further skewing their model-related
outcomes.

</details>


### [16] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 研究通过比较分析两个开放后训练数据集，创造了样本更少但性能优越的新数据集TuluTalk，并公开了数据集以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 当前大多数流行的语言模型后训练数据集对公众不可见且缺乏透明度，开放源码的数据替代方案虽然表现不错，但尚无系统性比较。因此，针对数据质量的评估影响缺乏明确的分析。

Method: 应用Magpie框架对样本进行详细的质量指标标注，包括对话结构、任务类别、输入质量和响应质量，并设计了结构性和定性上的相似性与差异的统计分析。基于这些分析，设计了一个系统的策划方法来生成新的数据集TuluTalk。

Result: TuluTalk包含的样本比原始数据集少14%，但在关键基准上的性能匹配或超过它们。

Conclusion: 通过综合分析两个主要的开放后训练数据集，开发出一个新的数据混合集TuluTalk，其样本量减少了14%但性能达到或超过了原数据集。研究成果为构建更有效的后训练数据集提供了可操作的见解。

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [17] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出用于检测意图幻觉的FAITHQA基准和CONSTRAINT SCORE指标，发现即便是最先进的语言模型也常出现意图幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在应对包含多个条件的复杂查询时，常常只部分满足查询而忽略某些条件，因此需要对这种现象进行研究。

Method: 通过引入FAITHQA基准和CONSTRAINT SCORE自动评估指标，系统性地评估和检测意图幻觉。

Result: FAITHQA基准评估显示，意图幻觉普遍存在于当前最先进的模型中。引入的CONSTRAINT SCORE指标能更接近人类水平检测意图幻觉。

Conclusion: 意图幻觉在最先进的大型语言模型中仍然很常见，主要由于省略或误解查询的部分。

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [18] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 引入了可用于图例个性化生成的LaMP-Cap数据集，该数据集包含图像及多模态图像配置文件，通过多模态信息提高图例生成质量。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型的个性化技术有所进步，但多涉及仅限文字的设置，很少涉及多模态的输入和配置文件。

Method: 引入LaMP-Cap数据集，用于多模态图像配置文件的个性化图例说明生成。

Result: 使用多模态配置文件的信息可以帮助更好地生成接近作者风格的图例，图片比文字段落更有助于提升结果。

Conclusion: 实验表明，利用图像简档可以生成更接近原作者编写的图例说明。

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [19] [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
*Jacqueline He,Howard Yen,Margaret Li,Shuyue Stella Li,Zhiyuan Zeng,Weijia Shi,Yulia Tsvetkov,Danqi Chen,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.CL

TL;DR: 研究介绍了'准确信息控制'任务，用于解决语言模型生成无支持信息的问题，通过可验证声明输入提高模型长篇生成任务的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型生成的信息看似合理但实际上缺乏支持的问题，提高模型生成信息的准确性和可靠性。

Method: 作者提出'准确信息控制（PIC）'任务，让模型仅基于一组短小的自包含声明生成长篇输出。使用了一种弱监督的偏好数据构建方法进行后训练，训练出具有更强PIC能力的8B PIC-LM型号。该方法通过改进全PIC设置中的精确匹配召回，从而提高了模型的准确性和精确度。

Result: 新提出的PIC任务在减少语言模型的内在幻觉方面取得了显著成效。经过训练的PIC-LM型号在全PIC设置中F1提升至91.0%，在模糊QA和具体事实验证任务中表现出色。

Conclusion: 该研究提出了一种名为'准确信息控制'的新任务，旨在解决现代语言模型的内在幻觉问题，即生成看似合理但无依据的信息。通过提供可验证的输入声明，确保生成的长篇输出中没有不支持的信息。这种方法提高了语言模型在长篇信息生成任务中的准确性。

Abstract: A central challenge in modern language models (LMs) is intrinsic
hallucination: the generation of information that is plausible but
unsubstantiated relative to input context. To study this problem, we propose
Precise Information Control (PIC), a new task formulation that requires models
to generate long-form outputs grounded in a provided set of short
self-contained statements, known as verifiable claims, without adding any
unsupported ones. For comprehensiveness, PIC includes a full setting that tests
a model's ability to include exactly all input claims, and a partial setting
that requires the model to selectively incorporate only relevant claims. We
present PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,
summarization, biography generation) adapted to the PIC setting, where LMs are
supplied with well-formed, verifiable input claims. Our evaluation of a range
of open and proprietary LMs on PIC-Bench reveals that, surprisingly,
state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To
alleviate this lack of faithfulness, we introduce a post-training framework,
using a weakly supervised preference data construction method, to train an 8B
PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full
PIC setting. When integrated into end-to-end factual generation pipelines,
PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and
factual precision by 30.5% on a birthplace verification task, underscoring the
potential of precisely grounded generation.

</details>


### [20] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: 文章提出了一个框架用于支持LLM在医学任务中的引用生成，显著提高了引用质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的医学问答系统缺乏引用生成和评估能力，这对其实践采用产生了顾虑。

Method: 提出了一种多次检索引用方法，可以生成高质量的引用，并通过专业专家的注释结果进行了验证。

Result: 所提出的方法在引用精确度和回忆度方面相较于强大的基线方法具有显著的提升，并且评估结果与专业专家的注释结果高度相关。

Conclusion: 该研究提出了一种名为\name的端到端框架，可以设计和评估用于医疗任务的LLM引发的引用生成，显著提高了引用的精准度和回忆度。

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [21] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 研究开发了一种无需训练的新方法，通过重构未见的词嵌入实现大语言模型的编码器移植，在不同任务中表现优越，并通过开源工具进行集成。


<details>
  <summary>Details</summary>
Motivation: 不同编码器之间的词嵌入迁移是个挑战，因此研究提出了一种无训练方法以实现预训练模型在新编码器上的直接再利用，以打破大型编码器不匹配问题。

Method: 使用正交匹配追踪（OMP）方法，将每个超出词汇范围的标记近似为共享标记的稀疏线性组合，并通过两个阶段完成：首先在给定词嵌入空间中计算新标记的表示，然后将相同的稀疏系数转移回基础模型的嵌入空间。

Result: 在两个跨编码器任务上，OMP方法在保留基础模型性能方面表现最佳，优于其他零样本方法。分析表明，不匹配的数字标记化方案是保留数学推理能力的关键挑战。

Conclusion: 该研究提出了一种基于正交匹配追踪（OMP）的无训练大语言模型（LLMs）编码器移植方法，通过重构未见过的词嵌入有效实现跨编码器任务的零样本性能保留。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [22] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 通过仿射映射技术转移小模型的特征到大模型，能以更低的计算成本提高训练效率，同时保留性能，揭示了不同类型特征转移的差异。


<details>
  <summary>Details</summary>
Motivation: 比较小型和大型模型之间的表示空间差异，探索通过特征转移来节省训练成本的可能性。

Method: 使用仿射映射技术在不同大小的语言模型之间进行表征特征转移。

Result: 小型和大型模型学习到相似的表示空间，转移特征到大型模型可以节省FLOPs，下游任务性能也得到了有效恢复；不同类型的特征在转移过程中的表现不同。

Conclusion: 小型和大型模型之间的线性表征空间具有相似性，可以有效地从小模型转移特征到大模型来提高训练效率。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [23] [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
*Samuel Kim,Oghenemaro Imieye,Yunting Yin*

Main category: cs.CL

TL;DR: Zero-shot LLMs effectively classify depressive language but not its severity; LLM-generated summary embeddings help improve classifier performance. Promising for mental health assessment.


<details>
  <summary>Details</summary>
Motivation: To enhance early intervention in mental health conditions by accurately detecting depressive language in social media through advanced NLP techniques.

Method: Comparing the performance of zero-shot LLMs and traditional classifiers using both conventional text embeddings and LLM-generated summary embeddings across three social media classification tasks.

Result: Zero-shot LLMs excelled in binary classification but struggled in finer granular tasks, whereas classifiers based on LLM-generated embeddings performed better, sometimes surpassing traditional methods.

Conclusion: LLMs shows strong abilities in binary classification regarding depressive language, but face challenges in fine-grained tasks, although LLM-generated summary embeddings improve performance.

Abstract: Accurate and interpretable detection of depressive language in social media
is useful for early interventions of mental health conditions, and has
important implications for both clinical practice and broader public health
efforts. In this paper, we investigate the performance of large language models
(LLMs) and traditional machine learning classifiers across three classification
tasks involving social media data: binary depression classification, depression
severity classification, and differential diagnosis classification among
depression, PTSD, and anxiety. Our study compares zero-shot LLMs with
supervised classifiers trained on both conventional text embeddings and
LLM-generated summary embeddings. Our experiments reveal that while zero-shot
LLMs demonstrate strong generalization capabilities in binary classification,
they struggle with fine-grained ordinal classifications. In contrast,
classifiers trained on summary embeddings generated by LLMs demonstrate
competitive, and in some cases superior, performance on the classification
tasks, particularly when compared to models using traditional text embeddings.
Our findings demonstrate the strengths of LLMs in mental health prediction, and
suggest promising directions for better utilization of their zero-shot
capabilities and context-aware summarization techniques.

</details>


### [24] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
*Jesse Woo,Fateme Hashemi Chaleshtori,Ana Marasović,Kenneth Marino*

Main category: cs.CL

TL;DR: BRIEFME数据集用于评估语言模型在法律摘要写作中的表现，LLMs在某些任务上表现良好，但在检索和完成任务上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 法律NLP中法律摘要的写作和编辑尚未得到充分探索，旨在评估语言模型的新能力。

Method: 引入了一个名为BRIEFME的新数据集，包含三个任务：论点摘要、论点完成和案例检索，以协助法律专业人员撰写摘要。

Result: 当前的大型语言模型在摘要和引导完成任务上表现良好，甚至超越了人类生成的标题。

Conclusion: 目前的大型语言模型在摘要和引导完成任务上表现良好，但在现实的论证完成和检索相关法律案件方面表现不佳，希望该数据集能促进法律NLP的发展。

Abstract: A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.

</details>


### [25] [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
*Junzhe Wang,Bichen Wang,Xing Fu,Yixin Sun,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 本文引入MusPsy-Dataset和MusPsy-Model进行多会话心理咨询，结果优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型主要集中于单次会话的心理咨询研究，但心理咨询在现实世界中是一个需要持续多次会话的过程。为解决这一局限性，引入了MusPsy-Dataset数据集。

Method: 利用真实的客户档案构建MusPsy-Dataset，同时开发MusPsy-Model来跟踪客户进展和调整咨询方向。

Result: MusPsy-Model在多会话环境中表现优于基线模型。

Conclusion: 介绍了MusPsy-Dataset，一个由多会话心理咨询对话组成的数据集，以及MusPsy-Model，旨在跟踪客户进展并随时间调整咨询方向。实验结果表明该模型在多会话中优于基线模型。

Abstract: In recent years, Large Language Models (LLMs) have made significant progress
in automated psychological counseling. However, current research focuses on
single-session counseling, which doesn't represent real-world scenarios. In
practice, psychological counseling is a process, not a one-time event,
requiring sustained, multi-session engagement to progressively address clients'
issues. To overcome this limitation, we introduce a dataset for Multi-Session
Psychological Counseling Conversation Dataset (MusPsy-Dataset). Our
MusPsy-Dataset is constructed using real client profiles from publicly
available psychological case reports. It captures the dynamic arc of
counseling, encompassing multiple progressive counseling conversations from the
same client across different sessions. Leveraging our dataset, we also
developed our MusPsy-Model, which aims to track client progress and adapt its
counseling direction over time. Experiments show that our model performs better
than baseline models across multiple sessions.

</details>


### [26] [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
*Chuxue Cao,Han Zhu,Jiaming Ji,Qichao Sun,Zhenghao Zhu,Yinyu Wu,Juntao Dai,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 论文提出了从法律视角进行语言模型安全性评估的SafeLawBench基准，发现当前领先模型表现仍有待提高，并呼吁进一步研究语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准的主观性使得没有明确的标准来评估大型语言模型的安全性，该研究旨在从法律的角度来填补这一空白。

Method: 提出了一个新的基准测试SafeLawBench, 基于法律标准将安全风险分为三个层级，通过多选题和开放域问答任务系统地评估语言模型的安全性。

Result: 在2个闭源和18个开源大型语言模型上进行的评估显示，SafeLawBench能够有效地突出每个模型的安全特性，并通过多数投票机制提高模型表现。

Conclusion: SafeLawBench为评估大型语言模型的安全性提供了一个系统且全面的框架，虽然当前最先进的模型在其多项选择任务中也未达到80.5%的准确率，强调了继续研究语言模型安全性的重要性。

Abstract: With the growing prevalence of large language models (LLMs), the safety of
LLMs has raised significant concerns. However, there is still a lack of
definitive standards for evaluating their safety due to the subjective nature
of current safety benchmarks. To address this gap, we conducted the first
exploration of LLMs' safety evaluation from a legal perspective by proposing
the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three
levels based on legal standards, providing a systematic and comprehensive
framework for evaluation. It comprises 24,860 multi-choice questions and 1,106
open-domain question-answering (QA) tasks. Our evaluation included 2
closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot
prompting, highlighting the safety features of each model. We also evaluated
the LLMs' safety-related reasoning stability and refusal behavior.
Additionally, we found that a majority voting mechanism can enhance model
performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and
GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,
while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community
to prioritize research on the safety of LLMs.

</details>


### [27] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: 研究通过量化回归提升LLM在价格预测中的表现，结果显示Mistral-7B模型优于传统方法和其他学习方式。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在非结构化输入的概率回归中发挥潜力，解决如价格估算需要理解文本细微差别及量化不确定性的任务。

Method: 采用量化回归方法对LLM进行微调，使其能生成完整的预测分布，并结合不同的数据集进行实验对比。

Result: 通过三个价格预测数据集的实验表明，微调后的Mistral-7B模型在点估计和分布估计方面表现优于传统方法，并在后续实验中持续超越其他模型架构及方法。

Conclusion: Mistral-7B模型在价格预测任务中表现优越，能够显著超过传统方法，并可达到接近人类的准确度。

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [28] [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: 研究提升语言模型行为引导能力的方法，分布级干预比逐点干预表现更优，尤其在可控性和健壮性上。


<details>
  <summary>Details</summary>
Motivation: 强化语言模型的行为引导能力，提升在概念子空间内进行点控的效果。

Method: 扩大到分布级别的模型干预方法，允许模型学习概念子空间的点变换及其周围区域。

Result: 在八个常识推理和七个算术推理基准上，分布级干预方法在可控性和健壮性方面优于逐点干预方法。

Conclusion: 分布式干预方法比逐点干预在可控性和健壮性上表现更好，提供了一种更全面的引导语言模型行为的方法。

Abstract: Interventions in language models (LMs) are applied strategically to steer
model behavior during the forward pass. Learnable interventions, also known as
representation fine-tuning, aim to apply pointwise control within the concept
subspace and have proven effective in altering high-level behaviors. In this
work, we extend this approach to the distribution level, enabling the model to
learn not only pointwise transformations but also the surrounding regions of
the concept subspace. We demonstrate that these methods perform effectively in
early layers, with larger standard deviations correlating strongly with
improved performance. Across eight commonsense reasoning and seven arithmetic
reasoning benchmarks, our distribution-wise interventions consistently
outperform pointwise interventions in controllability and robustness. These
results illustrate that distribution-wise interventions provide a more
comprehensive method for steering model behavior and enabling finer-grained
control over language models. The code is at:
\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.

</details>


### [29] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: 论文探讨了动态RAG和参数化RAG以改进LLM的知识检索与整合效率，超越传统静态方法。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG系统在处理需要多跳推理和深入知识整合的复杂任务时效果欠佳，这促使研究人员探索新的方向来增强LLM的知识整合能力。

Method: 本研究通过深入研究动态RAG和参数化RAG，探讨在LLM生成过程中如何适应性地进行实时知识检索，以及如何在参数层而非输入层进行知识注入。

Result: 论文提供了对动态和参数化RAG的最新研究进展的全面概述，并分享了相关理论基础和实际见解。这将支持并激发未来在RAG领域的研究。

Conclusion: 该论文对RAG（检索增强生成)进行了研究，特别是传统静态系统的局限性，并提出了动态RAG和参数化RAG，两者都能改进LLM的知识整合与使用效率。

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [30] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: DivScore是一个专为专业领域设计的LLM文本检测器，显著提高了检测准确性，并能在对抗性环境中提供更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学和法律等高风险领域检测LLM生成文本对于打击错误信息和确保真实性至关重要，但现有的零样本检测器常因领域转变而失效。

Method: 提出DivScore，利用归一化熵评分和领域知识提炼进行零样本检测框架，以识别专业领域中的LLM生成文本。

Result: 在医学和法律领域的基准测试中，DivScore在AUROC和召回率方面显著优于最先进的检测器，并在对抗性设置中表现出22.8%的AUROC优势和29.5%的召回率优势。

Conclusion: DivScore在检测专业领域的LLM生成文本方面表现优异，超过了现有的最先进检测器，并在对抗性环境下表现出更强的鲁棒性。

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [31] [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
*Haiqi Yang,Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: RetNet作为神经网络架构的显著进步，克服了Transformer的局限性，通过保留机制实现高效的长上下文建模，是一个值得深入研究的领域。


<details>
  <summary>Details</summary>
Motivation: Transformer由于自身注意力机制的二次复杂性，导致在处理长序列时存在高内存成本和有限的可扩展性。RetNet旨在通过引入保留机制来克服Transformer的这些局限性。

Method: RetNet引入了一种保留机制，该机制将递归的归纳偏差与注意力的全局依赖建模相结合，实现了线性时间推理和高效的长上下文建模。

Result: RetNet在自然语言处理、语音识别和时间序列分析等机器学习范式中表现出稳健的性能，并且这种表现跨领域一致。

Conclusion: 本论文提供了RetNet架构的详细综述，填补了当前文献的空白。同时探讨了RetNet相关的主要挑战，并提出了未来的研究方向。

Abstract: Retentive Network (RetNet) represents a significant advancement in neural
network architecture, offering an efficient alternative to the Transformer.
While Transformers rely on self-attention to model dependencies, they suffer
from high memory costs and limited scalability when handling long sequences due
to their quadratic complexity. To mitigate these limitations, RetNet introduces
a retention mechanism that unifies the inductive bias of recurrence with the
global dependency modeling of attention. This mechanism enables linear-time
inference, facilitates efficient modeling of extended contexts, and remains
compatible with fully parallelizable training pipelines. RetNet has garnered
significant research interest due to its consistently demonstrated cross-domain
effectiveness, achieving robust performance across machine learning paradigms
including natural language processing, speech recognition, and time-series
analysis. However, a comprehensive review of RetNet is still missing from the
current literature. This paper aims to fill that gap by offering the first
detailed survey of the RetNet architecture, its key innovations, and its
diverse applications. We also explore the main challenges associated with
RetNet and propose future research directions to support its continued
advancement in both academic research and practical deployment.

</details>


### [32] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: C-PATH是一种新型对话AI系统，利用LLMs帮助患者识别症状和推荐医务部门，表现优于领域基线。


<details>
  <summary>Details</summary>
Motivation: 解决患者在寻求及时和适当医疗关注时面临的复杂和压倒性障碍，通过AI系统提高对话的清晰度、信息性和推荐准确性。

Method: 使用大语言模型（LLMs）进行对话AI系统的开发，通过自然多轮对话帮助患者识别症状并推荐适当的医疗科室。C-PATH进行了医疗知识、对话数据和临床摘要的微调，并采用基于LLaMA3架构的多阶段管道。还使用了GPT的数据增强框架，将结构化临床知识转变为患者容易理解的对话，并实施了可扩展的对话历史管理策略。

Result: 通过GPTScore评估显示在清晰度、信息性和推荐准确性方面表现出色，定量基准测试显示C-PATH在GPT重写的对话数据集中超过了特定领域基线。

Conclusion: C-PATH系统在数字健康辅助和分诊方面的用户友好、易访问和准确性方面取得了重要进展。

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [33] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
*Mikhail Salnikov,Dmitrii Korzh,Ivan Lazichny,Elvir Karimov,Artyom Iudin,Ivan Oseledets,Oleg Y. Rogov,Alexander Panchenko,Natalia Loukachevitch,Elena Tutubalina*

Main category: cs.CL

TL;DR: LLMs show strong geopolitical biases, favoring certain national narratives, and simple debiasing techniques are largely ineffective.


<details>
  <summary>Details</summary>
Motivation: To explore geopolitical biases in large language models (LLMs) concerning different national interpretations of historical events.

Method: Evaluation was conducted by analyzing LLMs' interpretations of historical events with contradictory national perspectives, utilizing a novel dataset containing neutral event descriptions and contrasting viewpoints from USA, UK, USSR, and China.

Result: The study revealed significant geopolitical biases, with LLMs showing a preference for specific national narratives. It also found that simple debiasing techniques were insufficient in reducing these biases, and models were sensitive to attribution manipulations, which sometimes increased biases or highlighted inconsistencies.

Conclusion: LLMs exhibit significant geopolitical biases, favoring specific national narratives. Simple debiasing prompts are not very effective in mitigating these biases.

Abstract: This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.

</details>


### [34] [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
*Walter Paci,Alessandro Panunzi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 研究表明，当前大语言模型在检测和解释政治话语中的隐含内容方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在政治话语中的隐含内容检测和解释能力，因为这方面的能力尚未被充分探索。

Method: 利用IMPAQTS语料库进行多项选择任务和开放式生成任务，检验LLMs在解释含蓄语言方面的能力。

Result: 结果表明，所有测试的模型在解释预设和含蓄内容上均表现不佳，显示出当前LLMs在处理复杂隐含语言时的局限性。

Conclusion: 当前的大语言模型缺乏准确解释高度隐含语言（如政治话语）所需的关键语用能力。

Abstract: Implicit content plays a crucial role in political discourse, where speakers
systematically employ pragmatic strategies such as implicatures and
presuppositions to influence their audiences. Large Language Models (LLMs) have
demonstrated strong performance in tasks requiring complex semantic and
pragmatic understanding, highlighting their potential for detecting and
explaining the meaning of implicit content. However, their ability to do this
within political discourse remains largely underexplored. Leveraging, for the
first time, the large IMPAQTS corpus, which comprises Italian political
speeches with the annotation of manipulative implicit content, we propose
methods to test the effectiveness of LLMs in this challenging problem. Through
a multiple-choice task and an open-ended generation task, we demonstrate that
all tested models struggle to interpret presuppositions and implicatures. We
conclude that current LLMs lack the key pragmatic capabilities necessary for
accurately interpreting highly implicit language, such as that found in
political discourse. At the same time, we highlight promising trends and future
directions for enhancing model performance. We release our data and code at
https://github.com/WalterPaci/IMPAQTS-PID

</details>


### [35] [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
*Hiram Ring*

Main category: cs.CL

TL;DR: A CoNLLU-formatted version of taggedPBC is created to include dependency information, enhancing its use for typological linguistic studies.


<details>
  <summary>Details</summary>
Motivation: To enhance the taggedPBC dataset by adding dependency annotations in order to gain more predictive crosslinguistic insights and improve the utility of the dataset for typological studies.

Method: The method involves converting the taggedPBC dataset into a CoNLLU-formatted version, transferring dependency information along with POS tags for all languages.

Result: Word order information derived from the enhanced dataset correlates with expert determinations in three major typological databases, validating the approach despite concerns over data quality.

Conclusion: This study demonstrates the utility of corpus-based typological approaches for linguistic category comparison, indicating that valuable insights can be obtained from noisy data if adequately annotated.

Abstract: The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged
parallel text data from over 1,500 languages, representing 133 language
families and 111 isolates. While this dwarfs previously available resources,
and the POS tags achieve decent accuracy, allowing for predictive
crosslinguistic insights (Ring 2025b), the dataset was not initially annotated
for dependencies. This paper reports on a CoNLLU-formatted version of the
dataset which transfers dependency information along with POS tags to all
languages in the taggedPBC. Although there are various concerns regarding the
quality of the tags and the dependencies, word order information derived from
this dataset regarding the position of arguments and predicates in transitive
clauses correlates with expert determinations of word order in three
typological databases (WALS, Grambank, Autotyp). This highlights the usefulness
of corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)
for extending comparisons of discrete linguistic categories, and suggests that
important insights can be gained even from noisy data, given sufficient
annotation. The dependency-annotated corpora are also made available for
research and collaboration via GitHub.

</details>


### [36] [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
*Tianjie Ju,Yujia Chen,Hao Fei,Mong-Li Lee,Wynne Hsu,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 提出一种自适应框架，通过心理说服策略显著提高LLMs的成功率。


<details>
  <summary>Details</summary>
Motivation: 评估四种常用LLMs并观察其在说服成功率方面的不足，寻找提高说服成功率的方法。

Method: 提出基于直接偏好优化的自适应框架，通过策略特定响应中的说服结果作为偏好对来训练LLMs自主选择最佳策略。

Result: 自适应心理说服方法有效提升了说服者LLMs的成功率，同时保持其一般能力。实验已在三个开源LLMs上进行了验证。

Conclusion: 自适应框架使说服者LLMs能够有效地选择最佳策略，显著提高成功率，同时保持其一般能力。

Abstract: Previous work has showcased the intriguing capabilities of Large Language
Models (LLMs) in instruction-following and rhetorical fluency. However,
systematic exploration of their dual capabilities to autonomously persuade and
resist persuasion, particularly in contexts involving psychological rhetoric,
remains unexplored. In this paper, we first evaluate four commonly adopted LLMs
by tasking them to alternately act as persuaders and listeners in adversarial
dialogues. Empirical results show that persuader LLMs predominantly employ
repetitive strategies, leading to low success rates. Then we introduce eleven
comprehensive psychological persuasion strategies, finding that explicitly
instructing LLMs to adopt specific strategies such as Fluency Effect and
Repetition Effect significantly improves persuasion success rates. However, no
``one-size-fits-all'' strategy proves universally effective, with performance
heavily dependent on contextual counterfactuals. Motivated by these
observations, we propose an adaptive framework based on direct preference
optimization that trains LLMs to autonomously select optimal strategies by
leveraging persuasion results from strategy-specific responses as preference
pairs. Experiments on three open-source LLMs confirm that the proposed adaptive
psychological persuasion method effectively enables persuader LLMs to select
optimal strategies, significantly enhancing their success rates while
maintaining general capabilities. Our code is available at
https://github.com/KalinaEine/PsychologicalPersuasion.

</details>


### [37] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出了一种生成模型框架，通过生成文本描述进行多标签分类，模型在多个数据集上表现优异，超越了强基线。


<details>
  <summary>Details</summary>
Motivation: 文本数据爆炸性增长使得手动文档分类愈发困难，因此需要一种高效的自动化分类方法。

Method: 提出了一个领域无关的生成模型框架，利用预定义的标签描述并通过输入文本生成这些描述，然后通过微调的句子转换器将生成的描述与预定义标签匹配，结合交叉熵损失和余弦相似度的双目标损失函数。

Result: 在所有评估的数据集上实现了新的最先进性能，Micro-F1提升13.94%，Macro-F1提升24.85%。

Conclusion: LAGAMC模型参数高效，适用于不同数据集的实际应用，在各个数据集上超越了多个强基线。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [38] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究表明，当分解可能性、典型性、和上下文关系时，语言模型在判断事件可能性上表现不佳，甚至可能表现得比随机选择更差。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型能否可靠地区分可能事件和仅不大可能事件，挑战先前的研究结论。

Method: 通过将可能性、典型性和上下文关联性分开分析，评估语言模型的表现。

Result: 在一些情形下，模型错误地将不可能的句子赋予更高概率，而非仅仅不大可能的句子。

Conclusion: 尽管先前的研究表明语言模型可以有效地预测事件可能性，但在某些情况下，所有测试的模型，包括Llama 3、Gemma 2和Mistral NeMo，都会在概率判断上表现得比随机选择还差。

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [39] [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
*Bernardo Leite,Henrique Lopes Cardoso*

Main category: cs.CL

TL;DR: 我们提出了一种新策略，结合叙述和难度控制，生成适合教育用途的阅读理解问题，并获得初步有效性证据。


<details>
  <summary>Details</summary>
Motivation: 现有问题生成研究缺乏对结合叙述控制和难度控制的重要关注，我们的动机是填补这一研究空白，以便为教育目的生成量身定制的问题。

Method: 我们提出了一种联合叙述和难度控制的策略，用于在生成阅读理解问题时同时控制这两个属性。

Result: 实验结果初步显示我们的策略在某些条件下表现良好，但在所有情况下效果有限。

Conclusion: 我们的研究为同时控制生成阅读理解问题中的叙述性和难度提供了初步证据，这表明这项策略在某些条件下是可行的。

Abstract: Question Generation (QG), the task of automatically generating questions from
a source input, has seen significant progress in recent years.
Difficulty-controllable QG (DCQG) enables control over the difficulty level of
generated questions while considering the learner's ability. Additionally,
narrative-controllable QG (NCQG) allows control over the narrative aspects
embedded in the questions. However, research in QG lacks a focus on combining
these two types of control, which is important for generating questions
tailored to educational purposes. To address this gap, we propose a strategy
for Joint Narrative and Difficulty Control, enabling simultaneous control over
these two attributes in the generation of reading comprehension questions. Our
evaluation provides preliminary evidence that this approach is feasible, though
it is not effective across all instances. Our findings highlight the conditions
under which the strategy performs well and discuss the trade-offs associated
with its application.

</details>


### [40] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 研究提供了一个孟加拉语跨国政治话语的多语言数据集，以及其主题和内容的概述。


<details>
  <summary>Details</summary>
Motivation: 政治话语的研究有助于分析公众舆论和意识形态极化。在英语中已经有相关研究，但在如孟加拉语这样的重要但资源不足的语言中，研究受到限制，主要是因为缺乏数据集。

Method: 通过社区知情的关键词检索手动整理数据集，并从三个在线平台收集孟加拉语跨国政治话语数据。

Result: 提供了孟加拉跨国政治话语的多语言数据集，并概述其主题和多语言内容。

Conclusion: 为孟加拉语政治话语的研究提供了一个宝贵的多语言数据集工具，填补了相关领域的空白。

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [41] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: 该论文审视了语言技术中的身份偏见，特别是缅甸低资源语言中的性别、宗教、国籍偏见，并通过算法审计表明这些偏见的存在，进而讨论了认识不公和AI对齐等问题。


<details>
  <summary>Details</summary>
Motivation: 语言技术等社会技术系统常常表现出基于身份的偏见，这些偏见加剧了历史上边缘化社区的经历，并且在资源稀缺的背景下仍未得到充分研究。虽然常常建议采用特定于语言或具有多语言支持的模型和数据集来解决这些偏见，但在缅甸语这种广泛使用但资源稀缺的语言环境中，我们通过实证研究测试了这些方法的有效性，重点关注性别、宗教和国籍的身份。

Method: 我们对使用mBERT和BanglaBERT细化的情感分析模型进行了算法审计，这些模型使用谷歌数据集搜索中所有的缅甸情感分析（BSA）数据集进行细化。

Result: 分析表明，尽管有类似的语义内容和结构，缅甸情感分析（BSA）模型在不同身份类别中表现出偏见。此外，我们还研究了结合由不同人口背景的人创建的预训练模型和数据集所产生的不一致性和不确定性。

Conclusion: 这些发现与关于认识不公、AI对齐以及算法审计中的方法选择等更广泛的讨论形成关联。

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [42] [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
*Wenyu Zhang,Yingxu He,Geyu Lin,Zhuohan Liu,Shuo Sun,Bin Wang,Xunlong Zou,Jeremy H. M. Wong,Qiongqiong Wang,Hardik B. Sailor,Nancy F. Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 该研究通过引入融合推理的统一框架，改善了音频大型语言模型的情感识别能力和生成解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的音频大型语言模型在语义任务如语音识别和翻译上表现出色，但在建模副语言线索如情感方面仍有限制。现有方法通常将情感理解视为分类问题，对预测背后的原因提供的见解有限。因此，有必要探索一种能够生成语义对齐、证据支持的解释，以增强情感识别能力。

Method: 引入一个结合推理增强数据监督、双编码架构和任务轮替训练的统一框架，使多任务音频大型语言模型能够有效学习不同任务并结合情感推理。

Result: 实验结果表明，该方法不仅提高了情感预测的准确性，还增强了生成响应的一致性和证据支持性。

Conclusion: 通过在多任务音频大型语言模型中引入情感推理增强策略，能够有效地提高情感识别的准确性和解释性。

Abstract: Audio Large Language Models (AudioLLMs) have achieved strong results in
semantic tasks like speech recognition and translation, but remain limited in
modeling paralinguistic cues such as emotion. Existing approaches often treat
emotion understanding as a classification problem, offering little insight into
the underlying rationale behind predictions. In this work, we explore emotion
reasoning, a strategy that leverages the generative capabilities of AudioLLMs
to enhance emotion recognition by producing semantically aligned,
evidence-grounded explanations. To support this in multitask AudioLLMs, we
introduce a unified framework combining reasoning-augmented data supervision,
dual-encoder architecture, and task-alternating training. This approach enables
AudioLLMs to effectively learn different tasks while incorporating emotional
reasoning. Experiments on IEMOCAP and MELD show that our approach not only
improves emotion prediction accuracy but also enhances the coherence and
evidential grounding of the generated responses.

</details>


### [43] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: Study on LLMs' ability in test case generation for debugging finds potential yet highlights struggles in creating targeted test cases to find code flaws. TCGBench and datasets improve performance.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored area of using LLMs for code checking or debugging through test case generation, especially in complex competition-level programming environments.

Method: The paper proposes a benchmark named TCGBench to evaluate the ability of LLMs in generating test case generators. It involves experimental studies where the capabilities of LLMs are analyzed in two tasks.

Result: State-of-the-art LLMs can usually generate valid test case generators, yet they have difficulty in creating targeted ones that effectively expose coding flaws. Enhanced performance is possible with a high-quality dataset for model prompting and fine-tuning.

Conclusion: LLMs exhibit potential in generating valid test case generators for competition-level programming problems but struggle to effectively generate targeted test cases that can uncover bugs in human-written code.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [44] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: PCoT uses persuasion knowledge to enhance LLMs for disinformation detection, yielding significant improvements.


<details>
  <summary>Details</summary>
Motivation: Improve media literacy by enhancing disinformation detection through the integration of persuasion knowledge in LLMs.

Method: Introduction of Persuasion-Augmented Chain of Thought (PCoT) for zero-shot classification in disinformation detection, leveraging large language models with persuasion knowledge.

Result: PCoT demonstrated a 15% improvement over competitive methods across five LLMs and five datasets, proving its effectiveness in unseen disinformation detection.

Conclusion: PCoT significantly improves disinformation detection by using persuasion knowledge, outperforming other methods by 15% on average.

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [45] [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
*Naibin Gu,Peng Fu,Xiyu Liu,Ke Ma,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: Trans-PEFT方法改进了PEFT模块，使其在更新后的基础模型上保持性能，减少了重新调整的计算成本。


<details>
  <summary>Details</summary>
Motivation: 更新基础模型后，PEFT模块在性能上出现显著下降，需要寻找方法保持高效性能而不重新调整。

Method: 通过分析基础模型更新过程的变化，开发了一个集中于任务特定模式的方法Trans-PEFT，减少了对基础模型特定知识的依赖。

Result: Trans-PEFT方法在多个基础模型和数据集上进行了实验，证明其能够在不需重新调整的情况下，保持性能并降低维护成本。

Conclusion: Trans-PEFT方法能够在更新后的基础模型中保持PEFT模块的性能，减小了大量计算资源的消耗。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a common method for
fine-tuning large language models, where a base model can serve multiple users
through PEFT module switching. To enhance user experience, base models require
periodic updates. However, once updated, PEFT modules fine-tuned on previous
versions often suffer substantial performance degradation on newer versions.
Re-tuning these numerous modules to restore performance would incur significant
computational costs. Through a comprehensive analysis of the changes that occur
during base model updates, we uncover an interesting phenomenon: continual
training primarily affects task-specific knowledge stored in Feed-Forward
Networks (FFN), while having less impact on the task-specific pattern in the
Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel
approach that enhances the PEFT module by focusing on the task-specific pattern
while reducing its dependence on certain knowledge in the base model. Further
theoretical analysis supports our approach. Extensive experiments across 7 base
models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain
performance on updated base models without re-tuning, significantly reducing
maintenance overhead in real-world applications.

</details>


### [46] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
*Jiaxing Guo,Wenjie Yang,Shengzhong Zhang,Tongshan Xu,Lun Du,Da Zheng,Zengfeng Huang*

Main category: cs.CL

TL;DR: 提出了用于评估和训练具备真正规则推理能力的大语言模型的方法ParaStepVerifier，通过逐步验证数学解答，显著提高了识别错误解答的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学问题解决上表现出惊人的成功，但这种成功往往掩盖了模型通过基本不合理的推理过程获得正确答案的关键问题，这是一种奖励机制滥用的现象。

Method: 提出了一种名为ParaStepVerifier的新方法，通过仔细的逐步验证数学解答，以识别不正确的推理步骤。

Result: ParaStepVerifier的实证结果表明，与基线方法相比，该方法在识别有缺陷的解答方面显著提升了准确性。

Conclusion: ParaStepVerifier能够显著提升识别错误解答的准确性，尤其是在复杂的多步骤问题上。

Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable
success in mathematical problem-solving. However, this success often masks a
critical issue: models frequently achieve correct answers through fundamentally
unsound reasoning processes, a phenomenon indicative of reward hacking. We
introduce MathOlympiadEval, a new dataset with fine-grained annotations, which
reveals a significant gap between LLMs' answer correctness and their low
process correctness. Existing automated methods like LLM-as-a-judge struggle to
reliably detect these reasoning flaws. To address this, we propose
ParaStepVerifier, a novel methodology for meticulous, step-by-step verification
of mathematical solutions. ParaStepVerifier identifies incorrect reasoning
steps. Empirical results demonstrate that ParaStepVerifier substantially
improves the accuracy of identifying flawed solutions compared to baselines,
especially for complex, multi-step problems. This offers a more robust path
towards evaluating and training LLMs with genuine mathematical reasoning.

</details>


### [47] [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
*Ziheng Qiao,Houquan Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 本文提出了一种动态混合方法，结合小模型和大语言模型，显著提升中文拼写检查任务的纠错能力，并节省了资源。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在中文拼写检查任务中的表现不尽如人意，而经过微调的基于BERT的小模型在细分领域表现优异，但容易过拟合于编辑模式。本文提出了一种新方法，以同时提升小模型的纠错精度和大型语言模型的流畅性。

Method: 本文提出的动态混合方法在束搜索解码阶段结合了小模型和大型语言模型的概率分布，利用小模型的精确纠错能力和大型语言模型的流畅性，从而避免了对大型语言模型的微调。

Result: 实验表明，本文提出的混合方法显著提升了错误纠正能力，在多个数据集上达到了最新的性能，并且无需微调大型语言模型，节省了大量时间和资源。

Conclusion: 本文提出了一种动态混合方法，通过在束搜索解码阶段有效结合小模型和大型语言模型（LLM）的概率分布，实现了对纠错能力的显著增强，并在多个数据集上达到了最新的性能。

Abstract: In the era of large language models (LLMs), the Chinese Spelling Check (CSC)
task has seen various LLM methods developed, yet their performance remains
unsatisfactory. In contrast, fine-tuned BERT-based models, relying on
high-quality in-domain data, show excellent performance but suffer from edit
pattern overfitting. This paper proposes a novel dynamic mixture approach that
effectively combines the probability distributions of small models and LLMs
during the beam search decoding phase, achieving a balanced enhancement of
precise corrections from small models and the fluency of LLMs. This approach
also eliminates the need for fine-tuning LLMs, saving significant time and
resources, and facilitating domain adaptation. Comprehensive experiments
demonstrate that our mixture approach significantly boosts error correction
capabilities, achieving state-of-the-art results across multiple datasets. Our
code is available at https://github.com/zhqiao-nlp/MSLLM.

</details>


### [48] [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
*Hamid Mojarad,Kevin Tang*

Main category: cs.CL

TL;DR: 研究发现，AAE中特定语言特征影响ASR的识别准确性，尤其是无语言模型ASR系统受词汇邻近效应影响更大。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于了解非洲裔美国英语的特定语言特征是否对ASR的误识别有影响，尤其是对不使用外部语言模型的端到端ASR系统的影响。

Method: 研究采用CORAAL语料库进行实验，使用wav2vec 2.0模型在有和没有语言模型（LM）的条件下对数据进行转录，通过蒙特利尔强制对齐器（MFA）检测CCR和ING-缩减。

Result: 实验分析显示，CCR和ING对词错误率（WER）有显著但较小的影响，且没有语言模型的ASR系统显示出更强的词汇邻近效应。

Conclusion: 自动语音识别（ASR）系统在处理非洲裔美国英语（AAE）的音素、音韵和形态句法特征时表现较差。

Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic,
phonological, and morphosyntactic features found in African American English
(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction
(CCR) and ING-reduction. It examines whether the presence of CCR and
ING-reduction increases ASR misrecognition. Subsequently, it investigates
whether end-to-end ASR systems without an external Language Model (LM) are more
influenced by lexical neighborhood effect and less by contextual predictability
compared to systems with an LM. The Corpus of Regional African American
Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR
and ING-reduction were detected using the Montreal Forced Aligner (MFA) with
pronunciation expansion. The analysis reveals a small but significant effect of
CCR and ING on Word Error Rate (WER) and indicates a stronger presence of
lexical neighborhood effect in ASR systems without LMs.

</details>


### [49] [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
*Mikhail Krasitskii,Grigori Sidorov,Olga Kolesnikova,Liliana Chanona Hernandez,Alexander Gelbukh*

Main category: cs.CL

TL;DR: A hybrid method improves multilingual sentiment analysis accuracy and efficiency, with applications in brand monitoring and discourse analysis, achieving 0.90 accuracy for English and 0.84 for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of standalone methods in multilingual sentiment analysis, such as low accuracy and high computational cost.

Method: The method combines TF-IDF-based extraction with a fine-tuned XLM-R abstractive module, utilizing dynamic thresholding and cultural adaptation.

Result: Experiments demonstrate significant improvements in accuracy, achieving 0.90 for English and 0.84 for low-resource languages, and a 22% increase in computational efficiency.

Conclusion: The proposed hybrid approach for multilingual sentiment analysis effectively improves accuracy and computational efficiency compared to traditional methods, showing potential for practical applications such as brand monitoring and discourse analysis.

Abstract: We propose a hybrid approach for multilingual sentiment analysis that
combines extractive and abstractive summarization to address the limitations of
standalone methods. The model integrates TF-IDF-based extraction with a
fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and
cultural adaptation. Experiments across 10 languages show significant
improvements over baselines, achieving 0.90 accuracy for English and 0.84 for
low-resource languages. The approach also demonstrates 22% greater
computational efficiency than traditional methods. Practical applications
include real-time brand monitoring and cross-cultural discourse analysis.
Future work will focus on optimization for low-resource languages via 8-bit
quantization.

</details>


### [50] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 引入了一种新的方法，将语篇结构整合到新闻文章的总结过程中，开发了名为DiscoSum的算法，结果证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 语言模型通常不能维持长期的语篇结构，而在新闻文章中，组织流对读者参与度有显著影响。

Method: 开发了一种名为DiscoSum的新算法，利用束搜索技术进行结构感知总结。

Result: 人类和自动评估结果表明我们的方法在保持叙事忠实性和满足结构需求方面具有成效。

Conclusion: 我们的方法有效地在新闻总结中保持叙事的忠实性和满足结构性需求。

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [51] [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
*Do Xuan Long,Duy Dinh,Ngoc-Hai Nguyen,Kenji Kawaguchi,Nancy F. Chen,Shafiq Joty,Min-Yen Kan*

Main category: cs.CL

TL;DR: 该研究分析了150多篇相关论文，提出了以性质为中心的框架来评估提示质量，并发现单一性质增强通常是提升效果的关键。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在对话中表现出更类人化的特征，因此提示在其中成为一个决定性因素。但对于自然语言提示的量化目前缺乏共识，因此需要对其进行系统性的研究。

Method: 进行了元分析，调查了150多篇与提示相关的论文，并提出了一个以性质和人为中心的框架来评估提示质量。随后分析现有研究如何评估其对LLM的影响，并进行了实证探索。

Result: 发现纯粹基于单一性质的提升往往产生最大的影响，并且在提示中进行性质增强的指令调整可以导致更好的推理模型。

Conclusion: 研究奠定了以性质为中心的提示评估和优化的基础，并在沟通人类与AI之间的桥梁上提出了新的研究方向。

Abstract: As large language models (LLMs) have progressed towards more human-like and
human--AI communications have become prevalent, prompting has emerged as a
decisive component. However, there is limited conceptual consensus on what
exactly quantifies natural language prompts. We attempt to address this
question by conducting a meta-analysis surveying more than 150
prompting-related papers from leading NLP and AI conferences from 2022 to 2025
and blogs. We propose a property- and human-centric framework for evaluating
prompt quality, encompassing 21 properties categorized into six dimensions. We
then examine how existing studies assess their impact on LLMs, revealing their
imbalanced support across models and tasks, and substantial research gaps.
Further, we analyze correlations among properties in high-quality natural
language prompts, deriving prompting recommendations. We then empirically
explore multi-property prompt enhancements in reasoning tasks, observing that
single-property enhancements often have the greatest impact. Finally, we
discover that instruction-tuning on property-enhanced prompts can result in
better reasoning models. Our findings establish a foundation for
property-centric prompt evaluation and optimization, bridging the gaps between
human--AI communication and opening new prompting research directions.

</details>


### [52] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 推出专门用于评估大语言模型中信念不一致推理的数据集，并发现这些模型在处理逻辑与信念冲突的情况下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 创建一个专门用于评估大语言模型中信念不一致推理的日语数据集。

Method: 开发并使用BIS Reasoning 1.0数据集，对现有大语言模型进行基准测试。

Result: 发现当前大语言模型在处理逻辑有效但与信念冲突的输入时存在显著的偏差。

Conclusion: 研究结果表明，在高风险领域中，必须优先考虑真实的推理能力而不是直觉性信念。

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [53] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 研究了一种使用离线强化学习来优化问答代理生成澄清问题的方法，并取得了更好奖励和语言质量。


<details>
  <summary>Details</summary>
Motivation: 在现有问答代理中增加澄清问题生成功能，以提高回答质量。

Method: 提出并分析了一种离线强化学习目标，将其视为奖励加权的监督微调以优化大型语言模型。

Result: 通过实验比较发现本文方法在优化奖励和语言质量上取得了提升。

Conclusion: 本文提出了一种使用离线强化学习优化QA代理人生成澄清问题的方法，相较于最近提出的方法有更好的奖励与语言质量。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [54] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: 提出一种依赖类型跨语言框架分析事件的指向性和终结性，利用 Agda 形式化，被用于英语句子建模。


<details>
  <summary>Details</summary>
Motivation: 开发一个跨语言框架，用于分析事件的指向性和终结性，旨在通过深入语言结构和事件推理来增强语义分析。

Method: 我们首先分析名词短语的界限性以及它与子类型、限定数量和形容词修饰之间的关系。在动词域中，我们定义了一个依赖事件演算，将指向事件模型化为其经历者已界定的事件，并将达到固有终点的指向事件视为终结事件，同时考虑副词修饰。

Result: 我们实现了以 Agda 证明助理形式化该框架的规则和示例，这增强了框架的有效性和适用性。

Conclusion: 我们提出的框架有助于深入理解不同语言中的事件特性，如事件的指向性和终结性，促进了跨语言的语义分析。

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [55] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: 研究表明大规模语言模型在面对提示扰动时表现出脆弱性，揭示其对语义和提示动态的敏感性。


<details>
  <summary>Details</summary>
Motivation: 探讨这些大规模语言模型是否真正具备推理能力，还是仅仅利用浅层统计模式。

Method: 引入一系列具有语义性但结构上存在对抗性的提示扰动，评估这些扰动对LLMs的推理能力影响。

Result: 部分修改会显著降低其表现，而另一些修改则意外提高模型准确性，揭示模型对语义和提示表面层动态的敏感性。

Conclusion: 现有的推理系统存在脆弱性和不可预知性，需开发更具原则性的推理对齐和提示稳健性方法。

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [56] [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
*Yuji Zhang,Qingyun Wang,Cheng Qian,Jiateng Liu,Chenkai Sun,Denghui Zhang,Tarek Abdelzaher,Chengxiang Zhai,Preslav Nakov,Heng Ji*

Main category: cs.CL

TL;DR: 引入技能链性模式以减少认知负荷，提高表格论据验证的精确性，使用350个微调示例实现了超越现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 科学文本的复杂性可能导致错误信息的传播，而非专业人士尤其容易因科学表格感到误导。现有模型在精细化推理上存在困难。

Method: 受到认知负荷理论启发，开发了模块化的、可重用的推理组件，称为原子技能，并引入技能链模式以减少认知负荷。

Result: 仅通过350个微调示例，通过原子推理训练的模型在科学表格论证验证中优于GPT-4o的链式思维方法，并实现了当前最先进的成果。

Conclusion: 通过引入技能链性模式并创建跨领域基准SciAtomicBench，可以有效提升模型在科学论据验证中的准确性。

Abstract: Scientific texts often convey authority due to their technical language and
complex data. However, this complexity can sometimes lead to the spread of
misinformation. Non-experts are particularly susceptible to misleading claims
based on scientific tables due to their high information density and perceived
credibility. Existing table claim verification models, including
state-of-the-art large language models (LLMs), often struggle with precise
fine-grained reasoning, resulting in errors and a lack of precision in
verifying scientific claims. Inspired by Cognitive Load Theory, we propose that
enhancing a model's ability to interpret table-based claims involves reducing
cognitive load by developing modular, reusable reasoning components (i.e.,
atomic skills). We introduce a skill-chaining schema that dynamically composes
these skills to facilitate more accurate and generalizable reasoning with a
reduced cognitive load. To evaluate this, we create SciAtomicBench, a
cross-domain benchmark with fine-grained reasoning annotations. With only 350
fine-tuning examples, our model trained by atomic reasoning outperforms
GPT-4o's chain-of-thought method, achieving state-of-the-art results with far
less training data.

</details>


### [57] [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
*Cong Liu,Jie Wu,Weigang Wu,Xu Chen,Liang Lin,Wei-Shi Zheng*

Main category: cs.CL

TL;DR: CoM框架通过整合人类方法论见解，提升了语言模型的复杂推理能力，无需额外训练就能实现超越竞争基线的表现。


<details>
  <summary>Details</summary>
Motivation: 大量语言模型在复杂推理任务上表现不佳，这是由于其训练数据中缺乏深入的见解，而这些见解通常在公开的文档中是缺失的。

Method: 使用Chain of Methodologies (CoM)框架，整合了人类的方法论见解，启用高级语言模型的元认知能力，激活用户定义的方法论进行系统推理。

Result: 实验表明，CoM超过了竞争基线，证明了无需训练的提示方法在复杂推理任务中的潜力。

Conclusion: CoM框架能够超越竞争基线，作为一种训练自由的提示方法，为复杂推理任务提供了稳健的解决方案，缩小了通向人类水平推理的差距。

Abstract: Large Language Models (LLMs) often struggle with complex reasoning tasks due
to insufficient in-depth insights in their training data, which are typically
absent in publicly available documents. This paper introduces the Chain of
Methodologies (CoM), an innovative and intuitive prompting framework that
enhances structured thinking by integrating human methodological insights,
enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages
the metacognitive abilities of advanced LLMs, activating systematic reasoning
throught user-defined methodologies without explicit fine-tuning. Experiments
show that CoM surpasses competitive baselines, demonstrating the potential of
training-free prompting methods as robust solutions for complex reasoning tasks
and bridging the gap toward human-level reasoning through human-like
methodological insights.

</details>


### [58] [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
*Senqi Yang,Dongyu Zhang,Jing Ren,Ziqi Xu,Xiuzhen Zhang,Yiliao Song,Hongfei Lin,Feng Xia*

Main category: cs.CL

TL;DR: 本文提出了MultiMM多元文化多模态隐喻数据集和SEMD模型，以提高跨文化隐喻理解，解决现有隐喻处理的文化偏见问题。


<details>
  <summary>Details</summary>
Motivation: 目前的隐喻处理大多基于英语样本，往往反映出西欧或北美的文化偏见。从而导致对模型性能的过高估计，影响NLP的进展。缺乏对文化偏见特别是在多模态上下文中影响的研究。

Method: 该研究提出了一个名为MultiMM的数据集，包含8461对文本-图像广告样本，以及一个名为SEMD的基线模型，将情感嵌入整合到隐喻理解中。

Result: SEMD模型的实验结果证实了其在隐喻检测和情感分析任务上的有效性，证明该模型可提高跨文化隐喻理解。

Conclusion: 本研究引入了MultiMM数据集和SEMD模型，以解决跨文化背景下隐喻处理上的文化偏见问题。实验结果证明了SEMD在隐喻检测和情感分析任务中的有效性。

Abstract: Metaphors are pervasive in communication, making them crucial for natural
language processing (NLP). Previous research on automatic metaphor processing
predominantly relies on training data consisting of English samples, which
often reflect Western European or North American biases. This cultural skew can
lead to an overestimation of model performance and contributions to NLP
progress. However, the impact of cultural bias on metaphor processing,
particularly in multimodal contexts, remains largely unexplored. To address
this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset
designed for cross-cultural studies of metaphor in Chinese and English. MultiMM
consists of 8,461 text-image advertisement pairs, each accompanied by
fine-grained annotations, providing a deeper understanding of multimodal
metaphors beyond a single cultural domain. Additionally, we propose
Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates
sentiment embeddings to enhance metaphor comprehension across cultural
backgrounds. Experimental results validate the effectiveness of SEMD on
metaphor detection and sentiment analysis tasks. We hope this work increases
awareness of cultural bias in NLP research and contributes to the development
of fairer and more inclusive language models. Our dataset and code are
available at https://github.com/DUTIR-YSQ/MultiMM.

</details>


### [59] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 研究了大型推理模型中的过度思考现象，并提出FoReaL-Decoding方法以改善成本质量比，在多个数学推理任务中有效。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在推理过程中产生多余细节的过度思考现象，并分析推理与非推理模型之间的token级别错位。

Method: 提出一种称为FoReaL-Decoding的协作性快慢思维解码方法，以在成本和质量之间进行权衡。方法包括让一个领先模型引导每个句子的前几个token，然后由较弱的草稿模型完成剩余的token。该方法采用随机门，平滑地在小模型和大模型之间进行插值。

Result: FoReaL-Decoding在四个流行的数学推理基准(AIME24, GPQA-Diamond, MATH500, AMC23)上将理论FLOPs减少了30到50%，将思维链长度修剪了最多40%，同时保留了86到100%的模型性能。

Conclusion: FoReaL-Decoding是一种简单、即插即用的方法，可在推理为中心的任务中实现可控的成本质量权衡。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [60] [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
*Yize Cheng,Vinu Sankar Sadasivan,Mehrdad Saberi,Shoumik Saha,Soheil Feizi*

Main category: cs.CL

TL;DR: 提出了一种名为对抗性改写的攻击方法，可以有效躲避AI文本检测器的检测，且对抗性改写比简单改写攻击实现更显著的检出率降低。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成文本检测器容易被简单的改写技术绕过问题，开发新的攻击框架来展示检测的局限性。

Method: 引入一种无训练攻击框架，即对抗性改写，使用现成的遵循指令的LLM在AI文本检测器的指导下，将AI生成文本进行改写以躲避检测。

Result: 对抗性改写在多个检测系统中表现出广泛的有效性和高转移性，能够显著降低检出率，通常仅导致文本质量略微下降。

Conclusion: 现有的检测系统无法有效检测经过对抗性改写的AI生成文本。

Abstract: The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.

</details>


### [61] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 引入ViMUL-Bench进行多语言视频LMM评估，并开发ViMUL以推进文化和语言包容性研究。


<details>
  <summary>Details</summary>
Motivation: 探索超越英语的多语言视频LMM，以实现文化和语言包容性。

Method: 引入ViMUL-Bench基准来评估视频LMM，并开发了ViMUL进行实验。

Result: 开发了ViMUL-Bench来测试视频LMM，并引入了大规模多语言视频训练集以促进研究。

Conclusion: ViMUL-Bench和ViMUL的开发旨在促进文化和语言包容的多语言视频LMM研究。

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [62] [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
*Zhongze Luo,Weixuan Wan,Qizhi Zheng,Yanhong Bai,Jingyun Sun,Jian Wang,Dan Wang*

Main category: cs.CL

TL;DR: 开发了一种智能咨询和问答系统，将大语言模型微调与知识图谱结合，显著提升了通信标准领域的咨询效果。


<details>
  <summary>Details</summary>
Motivation: 传统的咨询模型周期较长且依赖专家的知识与经验，难以满足快速发展的技术需求。

Method: 将大语言模型的微调与知识图谱构建相结合，开发面向通信标准的智能咨询和问答系统。

Result: 经过在通信标准领域构建的6587个问答数据集上进行LoRA微调后，Qwen2.5-7B-Instruct在测试集上显示出优异的专业能力。BLEU-4从18.8564提高到66.8993，ROUGE等评估指标显著提高，超过了比较模型Llama-3-8B-Instruct的微调效果。此外，构建了包含13906个实体和13524个关系的通信标准领域知识图谱，显示出较好的查询准确率。结合DeepSeek评估，RAG框架提升了模型的评分，平均提高2.26%；结合Web服务和API接口，交互体验和后端接入效果优良，具有较高的实际应用价值。

Conclusion: 结合微调大语言模型与知识图谱的智能咨询问答系统在通信标准领域展现出了显著的性能提升，并具有良好的实际应用价值。

Abstract: There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.

</details>


### [63] [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 研究通过多种大型语言模型和增强策略自动提取历史事件，增强不同维度的表现，开发了将RDF转为Coq的流水线以进行深入推理。


<details>
  <summary>Details</summary>
Motivation: 手动构建历史事件的计算表示过于耗时且昂贵，因此需要自动化的方法来提高效率和准确性，同时实现更深入的时间和语义分析。

Method: 采用多种大型语言模型（LLMs），包括GPT-4、Claude、Llama 3.2，并使用三种增强策略：纯基础生成、知识图谱增强和检索增强生成（RAG）。

Result: 不同的增强策略优化了不同的性能维度：基础生成在范围和历史广度上表现最佳，而RAG增强则改善了坐标精度和元数据完整性。模型架构对增强敏感性有根本影响，其中大型模型表现稳定，而较小模型如Llama 3.2表现不稳定。最终开发的翻译流水线能够将提取的RDF表示转换为Coq证明助理规范，以实现超越RDF的高阶推理验证。

Conclusion: 本研究开发了一种将历史事件从叙述文本中提取并转换为可计算表示的自动化方法，并验证了其在深层次时间和语义分析中的有效性。

Abstract: Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.

</details>


### [64] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 研究解决了医学MLLM在知识覆盖与推理能力上的不足，提出Lingshu，通过多阶段训练及丰富数据集，提升其在医疗任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLMs存在三个主要限制：（1）医学知识覆盖有限，尤其在图像之外；（2）数据策划过程欠佳导致幻觉问题；（3）缺乏针对复杂医学场景的推理能力。

Method: 提出全面的数据策划程序，从医学图像和广泛的医学文本及普遍领域数据中有效获取丰富医学知识数据，并综合准确的医学字幕、视觉问答及推理样本。基于此构建一个富含广泛医学知识的多模态数据集，并引进专门医学MLLM：Lingshu，通过多阶段训练嵌入医学专业知识，增强任务解决能力。

Result: Lingshu在多模态问答、基于文本的问答和医学报告生成三项基础医学任务中表现优异，优于现有开源多模态模型。

Conclusion: 引入Lingshu系统化解决了医学MLLM在知识覆盖、数据策划及推理能力方面的不足，通过多阶段训练及强化学习提升其医疗任务表现。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [65] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出了一种新的基准Com^2来解决复杂常识推理的问题，通过因果事件图和慢思维来指导语言模型。实验显示后训练和慢思考有助于提高推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究侧重于复杂任务如数学和代码，而对复杂常识推理关注较少。为了填补这一空白并与真实世界关注点保持一致，提出了一项旨在复杂常识推理的基准。

Method: 采用因果理论（如干预）来修改因果事件图，并通过慢思维指导大语言模型生成示例。

Result: 实验表明大语言模型在复杂常识推理的深度和广度上仍有困难，但是后训练和慢思考可以缓解这个问题。代码和数据公开在GitHub。

Conclusion: 实验表明大语言模型在复杂常识推理的深度和广度上仍有困难，但是后训练和慢思考可以缓解这个问题。

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [66] [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
*Yuanhe Tian,Pengsen Cheng,Guoqing Jin,Lei Zhang,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种基于LLM的新方法，通过分解视觉和文本表示提升多模态情感计算的效果，在多个任务上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖单一模态分析或简单的跨模态信息融合，未能有效处理不同模态间复杂和矛盾的证据。

Method: 我们的方法首先使用预训练的多模态编码器编码并对齐输入模态，然后利用表示分解框架来分离共性情感内容和独特线索，最后通过注意力机制整合这些分解的信号形成多模态LLM的动态软提示。

Result: 我们的方法在多模态方面情感、情感分析和仇恨表情检测任务上有效性优越，持续超越强劲的基准和最现代化的模型。

Conclusion: 所提出的基于LLM的多模态情感计算方法在多模态方面情感、情感分析和仇恨表情检测任务上表现优越，超越了现有的基准模型和最先进的模型。

Abstract: Multi-modal affective computing aims to automatically recognize and interpret
human attitudes from diverse data sources such as images and text, thereby
enhancing human-computer interaction and emotion understanding. Existing
approaches typically rely on unimodal analysis or straightforward fusion of
cross-modal information that fail to capture complex and conflicting evidence
presented across different modalities. In this paper, we propose a novel
LLM-based approach for affective computing that explicitly deconstructs visual
and textual representations into shared (modality-invariant) and
modality-specific components. Specifically, our approach firstly encodes and
aligns input modalities using pre-trained multi-modal encoders, then employs a
representation decomposition framework to separate common emotional content
from unique cues, and finally integrates these decomposed signals via an
attention mechanism to form a dynamic soft prompt for a multi-modal LLM.
Extensive experiments on three representative tasks for affective computing,
namely, multi-modal aspect-based sentiment analysis, multi-modal emotion
analysis, and hateful meme detection, demonstrate the effectiveness of our
approach, which consistently outperforms strong baselines and state-of-the-art
models.

</details>


### [67] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: 文章提出基于强化学习的REO-RL算法，有效减少推理效率差距（REG），在16K token预算下实现了与Qwen3-4B/8B类似的效率，维持较高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在解决问题时表现出色，但其推理路径通常过于冗长，导致高推理成本，限制了实际应用。现有精调方法虽然旨在提高推理效率，但由于评估不一致，难以验证其效率提升。本文提出通过推理效率前沿和REG指标来系统量化和评估不同模型的效率差距。

Method: 本文提出了REO-RL，一种基于强化学习的算法，旨在通过缩小推理效率差距（REG）来提高模型的推理效率。该算法通过一组战略性选择的 token 预算进行数值积分，以低误差近似完整的效率目标。

Result: 实验结果显示，REO-RL可以在较小的token预算下，显著降低REG（推理效率差距）而维持高准确性。与Qwen3-4B/8B效能前沿相比，在16K token预算下，REO-RL实现了>=50的REG减少，且精度损失最小。

Conclusion: 本文研究表明，通过系统评估在数学基准上现有方法的有效性发现，当前方法存在显著效率缺口。应用REO-RL方法后，可以大幅减少REG（推理效率差距），但完全匹配效能前沿仍是一个未解决的挑战。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [68] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: Theorem-of-Thought (ToTh) is a novel approach enhancing LLM reasoning through collaborative inference modes, outperforming existing methods on reasoning benchmarks with improved interpretability and logic.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) currently exhibit strong performance in reasoning tasks but suffer from brittle and hard-to-interpret reasoning processes. Existing prompting techniques lack mechanisms for enforcing logical structure and evaluating internal coherence. To address these limitations, ToTh introduces a structured reasoning framework.

Method: Theorem-of-Thought (ToTh) framework models reasoning as collaboration among three parallel agents simulating abductive, deductive, and inductive inference modes. Each agent produces a reasoning trace structured into a formal reasoning graph, evaluated for consistency using Bayesian belief propagation guided by natural language inference (NLI). The most coherent graph is used to derive the final answer.

Result: Theorem-of-Thought (ToTh) achieves superior performance compared to Chain-of-Thought (CoT), Self-Consistency, and CoT-Decoding on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks, while also providing interpretable and logically grounded reasoning chains.

Conclusion: Theorem-of-Thought (ToTh) consistently outperforms existing techniques such as Chain-of-Thought (CoT), Self-Consistency, and CoT-Decoding in both symbolic and numerical reasoning tasks, suggesting a promising direction for developing robust and cognitively inspired reasoning in LLMs.

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [69] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: CoT提示技术在不同任务和模型中效果各异，对非推理模型有时有助于提高性能，但会增加回答变化性和生成成本。


<details>
  <summary>Details</summary>
Motivation: 帮助商业、教育和政策领导人通过严谨的测试了解与AI相关技术细节。

Method: 通过严格的测试对Chain-of-Thought提示技术进行调查，评估其在不同模型和任务中的有效性。

Result: 即使在具备显性推理能力的模型中，CoT提示在答案准确性上的提升也很有限，同时增加了生成响应所需的时间和代价。

Conclusion: Chain-of-Thought方法在不同任务和模型中效果相差很大，对于非推理模型有时会小幅提升性能，但同时可能增加回答的变化性和错误率。

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [70] [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 我们提出了一种利用LLM进行数据增强的策略，结合信心水平加权微调，显著提高了ACSA任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决在低资源情况下的数据稀缺问题，提高ACSA任务的性能。

Method: 通过提供结构化的提示模板给LLM来生成预定义的内容，并采用后处理技术以确保生成句子与原始句子之间的语义一致性。同时，提出了一种信心水平加权微调策略。

Result: 我们的方法在四个基准数据集上相比所有基线方法均获得了最佳性能。

Conclusion: 我们的方法显著提升了对四个基准数据集的ACSA任务的性能。

Abstract: Large language model (LLM) is an effective approach to addressing data
scarcity in low-resource scenarios. Recent existing research designs
hand-crafted prompts to guide LLM for data augmentation. We introduce a data
augmentation strategy for the aspect category sentiment analysis (ACSA) task
that preserves the original sentence semantics and has linguistic diversity,
specifically by providing a structured prompt template for an LLM to generate
predefined content. In addition, we employ a post-processing technique to
further ensure semantic consistency between the generated sentence and the
original sentence. The augmented data increases the semantic coverage of the
training distribution, enabling the model better to understand the relationship
between aspect categories and sentiment polarities, enhancing its inference
capabilities. Furthermore, we propose a confidence-weighted fine-tuning
strategy to encourage the model to generate more confident and accurate
sentiment polarity predictions. Compared with powerful and recent works, our
method consistently achieves the best performance on four benchmark datasets
over all baselines.

</details>


### [71] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 通过结合序列蒙特卡洛法和句法标注器，可在保持语言流畅性的同时，大幅提高生成文本的句法准确性。


<details>
  <summary>Details</summary>
Motivation: 控制生成文本的句法结构对于那些需要清晰度、风格一致性或可解释性的应用非常重要，但这一任务具有挑战性。

Method: 研究结合了序列蒙特卡洛法（用提议分布进行采样以评估后验分布）和一种更加精准的句法标注方法，以确保每个生成的文本标记都符合所需的句法结构。

Result: 实验表明，通过使用适当的提议分布，可以提高生成文本的句法准确性，并显著提升F1得分。同时不影响语言模型的流畅性。

Conclusion: 采样算法可以在生成过程中有效地强制执行目标的语法结构，这在需要精确语法控制的应用中非常有效。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [72] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
*Yikun Wang,Yibin Wang,Dianyi Wang,Zimian Peng,Qipeng Guo,Dacheng Tao,Jiaqi Wang*

Main category: cs.CL

TL;DR: 该研究提出了一种新的强化学习框架GCPO，通过GeometryZero模型实现了在几何推理中更高效的辅助构建，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决几何问题中的辅助构建挑战，并减少使用大规模语言模型带来的计算成本。

Method: 提出了一种新的强化学习框架——群组对比策略优化（GCPO），并结合几何推理模型GeometryZero来实现高效的辅助构建。

Result: GeometryZero模型在流行的几何基准测试（Geometry3K，MathVista）中较其他方法有平均4.29%的性能提升。

Conclusion: 利用GCPO框架可以在较小模型中实现有效的几何推理和辅助构建，并超越当前基准性能。

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [73] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: 本文研究了实例选择技术在自然语言处理任务中的应用，提出了两种新方法，成功减少了训练集规模和提升了训练速度。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理领域的大型模型训练需要大量的计算资源，本论文旨在研究如何通过实例选择技术减少数据规模以降低计算资源消耗。

Method: 对多种实例选择方法进行了全面而科学的比较，尤其是应用于自动文本分类任务，并提出了两种新的面向噪声和重复数据的实例选择解决方案。

Result: 实例集减少了41%，同时在所有数据集上保持了相同的有效性，速度提升达1.67倍至2.46倍，能够适用于包含数十万个文档的数据集。

Conclusion: 该论文提出了两种新颖的实例选择方法，并证明这些方法在保持模型有效性的同时，可以显著减少训练集的规模和减少训练过程的成本。

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [74] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 本文提出了一种高效的规则遗忘框架，通过优化拒绝边界来选择性移除模型中的特定信息，同时保持其效用，并在实验中超越现有遗忘质量和响应自然性基线。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型(LLMs)在未经筛选的文档上进行训练，包含敏感、受版权保护或非法内容，引发了对LLM遗忘任务的关注，希望在不重新训练或降低效用的情况下选择性地从模型中移除特定信息。

Method: 本文提出了RULE（Reinforcement UnLearning）框架，将遗忘任务视为拒绝边界优化问题，通过使用遗忘集的一小部分和合成边界查询进行训练，并采用可验证的奖励函数来鼓励在相关查询上安全拒绝，同时保留对允许输入的有帮助响应。

Result: RULE在实现目标遗忘的同时，保持了模型的效用，并提高了模型输出的自然性、提高了训练效率，并展现出强大的泛化能力，能够将拒绝行为推广到语义相关但未见过的查询。

Conclusion: RULE能在模型效用不受损的情况下，实现目标化的遗忘。实验结果显示，使用只有12%的遗忘数据集和8%的合成边界数据，RULE在遗忘质量和响应自然性方面比现有基线高达17.5%和16.3%，同时保持通用效用，实现遗忘-保留最佳解。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [75] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 研究引入VISE基准以评估视频LLM的迎合行为，并探讨关键帧选择作为减轻方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频大语言模型（Video-LLMs）在需要基础多模态推理的实际应用中日益集成，确保它们的事实一致性和可靠性至关重要。然而，迎合性问题，即这些模型可能倾向于迎合用户输入，即便这与视觉证据相矛盾，影响了其在这些环境中的可信度。当前的研究大多忽视了迎合性在视频语言领域的具体表现，因此缺乏系统的基准和针对性的评估来理解Video-LLMs在误导性用户输入下的反应。

Method: 本文提出了VISE（Video-LLM Sycophancy Benchmarking and Evaluation），这是首个专门用于评估最先进的视频大语言模型中迎合行为的基准，涵盖了不同的问题格式、提示偏见和视觉推理任务。特别是，VISE首创地将语言学对迎合性的视角引入视觉领域，支持对多种迎合类型和互动模式的细粒度分析。此外，我们探索了关键帧选择作为一种可解释且无需训练的缓解策略，以通过加强视觉基础揭示减少迎合偏见的潜在路径。

Result: 提出了一个用于评估视频-语言模型迎合行为的新基准（VISE），并探讨了通过关键帧选择减轻迎合性的问题。

Conclusion: VISE基准为评估和理解视频大语言模型在迎合现象下的表现提供了一种全新的方法。另外，通过关键帧选择方法，可以在无需训练的情况下潜在地减少迎合性偏见。

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [76] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: SDE-SQL允许大型语言模型在推理时自我探索数据库，在BIRD基准上提升了执行精度，并成为无监督微调情况下开源模型的新标杆。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态的预处理数据库信息，限制了模型理解数据库内容的能力，无法进行动态交互和自主探索数据。

Method: 提出SDE-SQL框架，使大型语言模型能在推理过程中自我探索数据库。通过生成和执行SQL探测，主动从数据库中检索信息，不断更新对数据的理解。

Result: 在BIRD基准上的执行精度相较于基础模型提高8.02%，在监督微调下可进一步提高0.52%。

Conclusion: SDE-SQL实现了在不需要监督微调或模型集成的情况下，通过开放源代码模型实现新的技术水平，并可以通过监督微调进一步提升性能。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [77] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了一种基于TF-IDF的句子排序方法用于长文档分类，提高效率并减少冗余信息，性能与完整上下文基线相近，但显著减少了输入大小和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决长文档分类问题，传统transformer模型如BERT受输入长度和注意力复杂性限制。使用整个文档进行分类通常过于冗余，因为只有部分句子包含必要信息。

Method: 提出了一种基于TF-IDF的句子排序方法，通过选择最有信息量的内容提高效率。探索了固定数量和百分比的句子选择方式，并结合标准化TF-IDF分数和句子长度的增强评分策略。

Result: 在长马拉地语新闻文章的MahaNews LDC数据集上评估该方法，性能优于第一句、最后一句和随机选择等基线方法。使用MahaBERT-v2模型，在减少50%的输入大小和43%的推理延迟的情况下，分类准确率仅下降了0.33%。

Conclusion: 长文档分类任务可以显着减少上下文而不牺牲性能，证明该方法在实际应用中的实用性。

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [78] [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 研究适应了偏差归因评分以适用于菲律宾语模型，发现该模型主要受到特定人物、物品和关系词汇的偏见驱动。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在处理黏着语的语言模型中评估和解释偏见行为。

Method: 将信息论偏差归因评分指标调整为可以在处理黏着语的模型上使用，特别是菲律宾语。

Result: 研究展示了适用于纯菲律宾模型和三个多语言模型的方法有效性，揭示了菲律宾模型中偏见的具体词汇驱动。

Conclusion: 研究表明，菲律宾模型的偏见主要受到与人物、物品和关系相关的词汇驱动，而这些通常在英语模型中与犯罪、性行为和亲社会行为相关。

Abstract: Emerging research on bias attribution and interpretability have revealed how
tokens contribute to biased behavior in language models processing English
texts. We build on this line of inquiry by adapting the information-theoretic
bias attribution score metric for implementation on models handling
agglutinative languages, particularly Filipino. We then demonstrate the
effectiveness of our adapted method by using it on a purely Filipino model and
on three multilingual models: one trained on languages worldwide and two on
Southeast Asian data. Our results show that Filipino models are driven towards
bias by words pertaining to people, objects, and relationships, entity-based
themes that stand in contrast to the action-heavy nature of bias-contributing
themes in English (i.e., criminal, sexual, and prosocial behaviors). These
findings point to differences in how English and non-English models process
inputs linked to sociodemographic groups and bias.

</details>


### [79] [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
*Atahan Özer,Çağatay Yıldız*

Main category: cs.CL

TL;DR: 研究了一种轻量级框架，通过结构化外部记忆来应对LLMs在处理演变知识时的限制，成功优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的知识受到预训练数据范围的限制，而现实世界的信息不断演变，传统的更新方式成本高且不稳定，因此需要一种有效的方法来处理时间相关的文本语料。

Method: 提出了一种轻量级的代理框架，从源文档中增量构建结构化的外部记忆，不需要重新训练，使得模型可以在推理时检索和推理时间上过滤的相关信息。

Result: 在Temporal Wiki和Unified Clark基准上，该方法尤其在需要复杂推理或整合冲突事实的问题上表现优于ICL和RAG基线。

Conclusion: 提出了一种轻量级的结构化外部记忆框架，使模型能够在推理时检索和推理时间上过滤的相关信息，此方法优于ICL和RAG基线。

Abstract: Large language models (LLMs) exhibit remarkable capabilities in question
answering and reasoning thanks to their extensive parametric memory. However,
their knowledge is inherently limited by the scope of their pre-training data,
while real-world information evolves continuously. Updating this knowledge
typically requires costly and brittle re-training, or in-context learning
(ICL), which becomes impractical at scale given the volume and volatility of
modern information. Motivated by these limitations, we investigate how LLMs
perform when exposed to temporal text corpora, or documents that reflect
evolving knowledge over time, such as sports biographies where facts like a
player's "current team" change year by year. To this end, we introduce two new
benchmarks: Temporal Wiki, which captures factual drift across historical
Wikipedia snapshots, and Unified Clark, which aggregates timestamped news
articles to simulate real-world information accumulation. Our analysis reveals
that LLMs often struggle to reconcile conflicting or outdated facts and can be
misled when multiple versions of a fact appear in context. To address these
issues, we propose a lightweight, agentic framework that incrementally builds a
structured, external memory from source documents without requiring
re-training. This knowledge organization strategy enables models to retrieve
and reason over temporally filtered, relevant information at inference time.
Empirically, our method outperforms ICL and RAG baselines across both
benchmarks, especially on questions requiring more complex reasoning or
integration of conflicting facts.

</details>


### [80] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: BiLingua Parser effectively annotates code-switched text with high accuracy, leveraging LLMs with expert-guided few-shot prompting, and offers new annotated datasets for research.


<details>
  <summary>Details</summary>
Motivation: Existing parsers trained on monolingual treebanks often fail in multilingual and code-switched scenarios, and there is a scarcity of annotated data for such contexts. The study aims to address these issues.

Method: The paper introduces an LLM-based annotation pipeline called BiLingua Parser. It uses a prompt-based framework combining few-shot LLM prompting with expert review to generate Universal Dependencies annotations for code-switched text, particularly Spanish-English and Spanish-Guaraní.

Result: BiLingua Parser achieves up to 95.29% LAS after expert revision, outperforming prior baselines and multilingual parsers significantly.

Conclusion: BiLingua Parser achieves high LAS, showing that LLMs can effectively bootstrap syntactic resources in low-resource, code-switched environments when guided properly.

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [81] [Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service](https://arxiv.org/abs/2411.12262)
*Raphael Merx,Adérito José Guterres Correia,Hanna Suominen,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 通过对Tetun.org的使用模式进行研究，发现低资源语言翻译的实际需求与现有语料库不同，强调了在教育相关领域的翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的调查和焦点小组方法样本量小，因此需要通过观察性研究来揭示低资源语言翻译的真实需求和使用模式。

Method: 对Tetun.org的使用模式进行了观察性研究，分析了100,000个翻译请求以揭示翻译需求的真实模式。

Result: 研究发现，用户主要是学生，他们通常使用手机将高资源语言的文本翻译为Tetun，涉及科学、健康、日常生活等多种领域，这与现有以新闻为主的Tetun语料库形成对比。

Conclusion: 低资源机器翻译系统，尤其是对像Tetun这样的少数民族语言，应优先考虑在教育相关领域的翻译准确性，特别是从高资源语言到低资源语言的方向。

Abstract: Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.

</details>


### [82] [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 研究了采样温度对大语言模型性能的影响，并提出了一种基于BERT的温度选择器来优化性能。


<details>
  <summary>Details</summary>
Motivation: 由于采样温度是大语言模型中的一个关键超参数，其影响输出词的分布，因此理解其在不同模型和任务下的影响至关重要。

Method: 研究在0至2的温度范围内，对六种不同能力数据集进行统计分析，并提出一种基于BERT的温度选择器，用于选择最佳温度。

Result: 提出的BERT温度选择器显著提高了在SuperGLUE数据集上的小型和中型模型性能。研究还表明温度效应在FP16精度和量化模型中一致。

Conclusion: 研究结果显示，在不同任务和模型大小下，采样温度对模型性能存在特定影响，优化采样温度对于实际应用至关重要。

Abstract: The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit quantized models. By evaluating
temperature effects up to 4.0 in three quantized models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.

</details>


### [83] [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: 本研究探讨了联接关系标注的主观性，并提出了一种新的分类系统，发现原有资源可能标注不足，标注者在实例识别上重叠度低。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是为了探究在标注关联实例时的主观性，特别是在指代词识别、前件解决和关联子类型选择三个层面。

Method: 作者在现有的GUM语料库的测试集上进行标注试点研究，并提出了一种新开发的关联子类型分类系统，以与先前提出的方案进行对比。

Result: 研究结果表明，虽然关联子类型类别上的一致性适中，但对全面识别关联实例的标注者重叠较低，许多分歧源于对相关实体的主观理解。

Conclusion: 研究发现，标注者在识别和标注关联关系实例方面存在主题性理解分歧，可能导致一些资源严重标注不足。

Abstract: Bridging refers to the associative relationship between inferable entities in
a discourse and the antecedents which allow us to understand them, such as
understanding what "the door" means with respect to an aforementioned "house".
As identifying associative relations between entities is an inherently
subjective task, it is difficult to achieve consistent agreement in the
annotation of bridging anaphora and their antecedents. In this paper, we
explore the subjectivity involved in the annotation of bridging instances at
three levels: anaphor recognition, antecedent resolution, and bridging subtype
selection. To do this, we conduct an annotation pilot on the test set of the
existing GUM corpus, and propose a newly developed classification system for
bridging subtypes, which we compare to previously proposed schemes. Our results
suggest that some previous resources are likely to be severely under-annotated.
We also find that while agreement on the bridging subtype category was
moderate, annotator overlap for exhaustively identifying instances of bridging
is low, and that many disagreements resulted from subjective understanding of
the entities involved.

</details>


### [84] [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
*Yin Huang,Yifan Ethan Xu,Kai Sun,Vera Yan,Alicia Sun,Haidar Khan,Jimmy Nguyen,Mohammad Kachuee,Zhaojiang Lin,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: ConfQA微调策略有效降低了大语言模型的幻觉率，并提出双神经知识框架提高准确性及减少外部检索。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型回答事实性问题时容易产生幻觉陈述的问题，提高模型的准确性和可靠性。

Method: 采用ConfQA微调策略，通过引入减幅提示 "只有当你自信时才回答" 和利用知识图谱中的简单事实语句来校准模型的自信度。

Result: ConfQA策略实现了在多个真实性基准上的幻觉率降低到5%以下，同时提出了双神经知识框架，使得准确性提升到95%以上，并减少了30%以上的不必要的外部检索。

Conclusion: 提出了一种微调策略ConfQA，可以有效减少大语言模型在回答事实性问题时的幻觉现象，从20-40%减少到5%以下。

Abstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating
factual statements? In this paper we present a fine-tuning strategy that we
call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across
multiple factuality benchmarks. The core idea is simple: when the LLM answers a
question correctly, it is trained to continue with the answer; otherwise, it is
trained to admit "I am unsure". But there are two key factors that make the
training highly effective. First, we introduce a dampening prompt "answer only
if you are confident" to explicitly guide the behavior, without which
hallucination remains high as 15%-25%. Second, we leverage simple factual
statements, specifically attribute values from knowledge graphs, to help LLMs
calibrate the confidence, resulting in robust generalization across domains and
question types. Building on this insight, we propose the Dual Neural Knowledge
framework, which seamlessly select between internally parameterized neural
knowledge and externally recorded symbolic knowledge based on ConfQA's
confidence. The framework enables potential accuracy gains to beyond 95%, while
reducing unnecessary external retrievals by over 30%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Chunhou Liu,Ciprian Doru Giurcuaneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: 引入了评估AI在金融背景下推理能力的新基准，提出了无需微调的错误感知学习框架，显著提高了模型性能，但视觉理解和数学逻辑仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 为了评估人工智能模型在金融特定背景下的推理能力，尤其是在理解文本和解释复杂视觉数据方面。

Method: 提出了一种错误感知学习框架，能够利用模型的历史错误和反馈指导推理，而无需进行微调。

Result: 实验表明，多模态输入能显著提升性能，并且加入错误反馈可持续改进效果。展示了金融AI系统中自我反思推理的潜力。

Conclusion: 多模态输入显著提升了模型的表现，而融入错误反馈则带来了持续且可测量的改进。然而，在视觉理解和数学逻辑方面仍然存在挑战。

Abstract: Effective financial reasoning demands not only textual understanding but also
the ability to interpret complex visual data such as charts, tables, and trend
graphs. This paper introduces a new benchmark designed to evaluate how well AI
models - especially large language and multimodal models - reason in
finance-specific contexts. Covering 3,200 expert-level question-answer pairs
across 15 core financial topics, the benchmark integrates both textual and
visual modalities to reflect authentic analytical challenges in finance. To
address limitations in current reasoning approaches, we propose an error-aware
learning framework that leverages historical model mistakes and feedback to
guide inference, without requiring fine-tuning. Our experiments across
state-of-the-art models show that multimodal inputs significantly enhance
performance and that incorporating error feedback leads to consistent and
measurable improvements. The results highlight persistent challenges in visual
understanding and mathematical logic, while also demonstrating the promise of
self-reflective reasoning in financial AI systems. Our code and data can be
found at https://anonymous/FinMR/CodeData.

</details>


### [86] [Unreal Patterns](https://arxiv.org/abs/2506.06284)
*John Beverley,Jim Logan*

Main category: cs.AI

TL;DR: 介绍了一种通过实际类型交集而非虚构项来处理不存在实体的信息框架，提供了在既定本体论承诺下的实用解决方案，避免了形而上学假设和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法存在问题：要么过度承诺形而上学假设，要么导致计算效率低下。因此，有必要提出一种有效且可实施的处理方法。

Method: 提出一种结构化的本体论驱动方法，通过使用实际类型的交集来处理不真实模式。

Result: 开发了一种可实施的本体论框架，通过处理不真实模式，为处理假设或不存在实体提供了一种有效且计算可行的方法。

Conclusion: 论文提出了一种基于交集而非特定不存在项的建模方法，以处理有关虚构实体、蓝图、模拟和未来场景的信息。此框架可以在现有真实主义承诺下提供切实可行的解决方案，避免过度依赖形而上学假设，同时解决计算效率问题。

Abstract: This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.

</details>


### [87] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/abs/2506.06285)
*Kaike Sa Teles Rocha Alves,Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: 介绍了一款Python库evolvingfuzzysystems,它提供多个成熟的可演化模糊系统模型的实现,并通过加州住房数据集评估这些模型的性能和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开的可演化模糊系统模型限制了其可访问性和广泛采用。

Method: 评估模型并比较其性能，使用的评估工具包括：训练、可视化和性能评估；并对加州住房数据集进行测评，分析执行时间和训练测试阶段的规则演变。 

Result: 结果显示ePL作为一种简单而高效的模型,在准确性和计算成本方面具备平衡性,适合用于实际应用。

Conclusion: 通过使这些模型公开可用, evolvingfuzzysystems旨在促进自适应和可解释的机器学习领域的研究与实践应用。

Abstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their
ability to adaptively update their structure in response to data dynamics while
maintaining interpretability. However, the lack of publicly available
implementations of these models limits their accessibility and widespread
adoption. To address this gap, we present evolvingfuzzysystems, a Python
library that provides implementations of several well-established eFS models,
including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\_eTS, and eTS. The
library facilitates model evaluation and comparison by offering built-in tools
for training, visualization, and performance assessment. The models are
evaluated using the fetch\_california\_housing dataset, with performance
measured in terms of normalized root-mean-square error (NRMSE), non-dimensional
error index (NDEI), and mean absolute percentage error (MAPE). Additionally,
computational complexity is analyzed by measuring execution times and rule
evolution during training and testing phases. The results highlight ePL as a
simple yet efficient model that balances accuracy and computational cost,
making it particularly suitable for real-world applications. By making these
models publicly available, evolvingfuzzysystems aims to foster research and
practical applications in adaptive and interpretable machine learning.

</details>


### [88] [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)
*FutureSearch,:,Nikos I. Bosse,Jon Evans,Robert G. Gambee,Daniel Hnyk,Peter Mühlbacher,Lawrence Phillips,Dan Schwarz,Jack Wildman*

Main category: cs.AI

TL;DR: 我们提供了一个名为 Deep Research Bench 的基准测试工具来评估网页研究代理的质量，并证明离线代理的表现能够可靠地反映模型的能力。


<details>
  <summary>Details</summary>
Motivation: 没有直接评估控制不断变化的网络环境下的网页研究代理质量，因此需要一个可靠的方法来评估模型。

Method: 引入了一种名为 Deep Research Bench 的基准测试工具，其中包含 89 个多步骤的网络研究任务实例，并提供一个名为 'RetroSearch' 的离线环境。

Result: 离线 'RetroSearch' 代理的表现与 '实时网络' 代理相当，这证明了可以进行可靠的模型评估。

Conclusion: 我们评估了主要的网络研究产品，并在公开的排行榜上公布了结果。

Abstract: Amongst the most common use cases of modern AI is LLM chat with web search
enabled. However, no direct evaluations of the quality of web research agents
exist that control for the continually-changing web. We introduce Deep Research
Bench, consisting of 89 multi-step web research task instances of varying
difficulty across 8 diverse task categories, with the answers carefully worked
out by skilled humans. We provide a "RetroSearch" environment with a large
frozen set of scraped web pages, and demonstrate that offline "RetroSearch"
agents perform comparably to "live web" agents, enabling reliable evaluations
of models over time. We provide robust agent tooling and scaffolding to
benchmark major LLMs as they are released, including "thinking" models like o3
and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent
traces to report progress over time in hallucinations, tool use, and
forgetting. Finally, we evaluate the major web research products branded as
"Deep Research", "Deep Search", "Search", or "Research." Results are available
on a public leaderboard at https://drb.futuresearch.ai/.

</details>


### [89] [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)
*Muhammad Monjurul Karim,Yan Shi,Shucheng Zhang,Bingzhang Wang,Mehrdad Nasri,Yinhai Wang*

Main category: cs.AI

TL;DR: This paper reviews the use of LLMs in transportation for improving safety and mobility, discussing their applications, customization strategies, potential, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The complexity and dynamism of real-world traffic require advanced analytical frameworks beyond traditional engineering methods.

Method: Review of LLM applications and customization strategies in roadway safety and mobility, focusing on architectural, training, prompting, and multimodal strategies.

Result: Identified challenges of LLMs in transportation, such as hallucinations, reasoning deficits, data privacy issues, deployment complexities, and the need for rigorous safety assurance.

Conclusion: LLMs have transformative potential in enhancing roadway safety and mobility, but responsible innovation is essential for realizing safer, more intelligent transportation systems.

Abstract: Roadway safety and mobility remain critical challenges for modern
transportation systems, demanding innovative analytical frameworks capable of
addressing complex, dynamic, and heterogeneous environments. While traditional
engineering methods have made progress, the complexity and dynamism of
real-world traffic necessitate more advanced analytical frameworks. Large
Language Models (LLMs), with their unprecedented capabilities in natural
language understanding, knowledge integration, and reasoning, represent a
promising paradigm shift. This paper comprehensively reviews the application
and customization of LLMs for enhancing roadway safety and mobility. A key
focus is how LLMs are adapted -- via architectural, training, prompting, and
multimodal strategies -- to bridge the "modality gap" with transportation's
unique spatio-temporal and physical data. The review systematically analyzes
diverse LLM applications in mobility (e.g., traffic flow prediction, signal
control) and safety (e.g., crash analysis, driver behavior assessment,).
Enabling technologies such as V2X integration, domain-specific foundation
models, explainability frameworks, and edge computing are also examined.
Despite significant potential, challenges persist regarding inherent LLM
limitations (hallucinations, reasoning deficits), data governance (privacy,
bias), deployment complexities (sim-to-real, latency), and rigorous safety
assurance. Promising future research directions are highlighted, including
advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI
collaboration, continuous learning, and the development of efficient,
verifiable systems. This review provides a structured roadmap of current
capabilities, limitations, and opportunities, underscoring LLMs' transformative
potential while emphasizing the need for responsible innovation to realize
safer, more intelligent transportation systems.

</details>


### [90] [Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review](https://arxiv.org/abs/2506.06324)
*Shruti Kumar,Xiaoyu Chen,Xiaomei Wang*

Main category: cs.AI

TL;DR: 这篇综述研究分析了人类-AI-机器人合作学习和适应研究中的术语、智能代理类型和理论框架，发现术语使用不一致并探讨其理论基础。


<details>
  <summary>Details</summary>
Motivation: 由于人类-AI-机器人共学习和共适应研究领域相对较新，旨在明确和统一描述这些现象的方法和术语。

Method: 范围综述研究方法，收集并分析已发表的针对人类-AI-机器人合作学习和适应的研究。

Result: 研究识别了现有研究中使用的不同术语，不同智能代理和任务领域，以及用于测量共学习和适应的认知理论和框架。

Conclusion: 在现有研究中，描述人类-智能体合作关系的术语使用不一致，需更严格定义以指导研究。

Abstract: Several papers have delved into the challenges of human-AI-robot co-learning
and co-adaptation. It has been noted that the terminology used to describe this
collaborative relationship in existing studies needs to be more consistent. For
example, the prefix "co" is used interchangeably to represent both
"collaborative" and "mutual," and the terms "co-learning" and "co-adaptation"
are sometimes used interchangeably. However, they can reflect subtle
differences in the focus of the studies. The current scoping review's primary
research question (RQ1) aims to gather existing papers discussing this
collaboration pattern and examine the terms researchers use to describe this
human-agent relationship. Given the relative newness of this area of study, we
are also keen on exploring the specific types of intelligent agents and task
domains that have been considered in existing research (RQ2). This exploration
is significant as it can shed light on the diversity of human-agent
interactions, from one-time to continuous learning/adaptation scenarios. It can
also help us understand the dynamics of human-agent interactions in different
task domains, guiding our expectations towards research situated in dynamic,
complex domains. Our third objective (RQ3) is to investigate the cognitive
theories and frameworks that have been utilized in existing studies to measure
human-agent co-learning and co-adaptation. This investigation is crucial as it
can help us understand the theoretical underpinnings of human-agent
collaboration and adaptation, and it can also guide us in identifying any new
frameworks proposed specifically for this type of relationship.

</details>


### [91] [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
*Jiazheng Kang,Mingming Ji,Zhe Zhao,Ting Bai*

Main category: cs.AI

TL;DR: 引入MemoryOS解决大型语言模型的内存管理问题，通过分层架构提高记忆能力，实验结果显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在固定上下文窗口和不充分的内存管理方面面临的挑战，从而增强长时间记忆能力和互动体验中的个性化问题。

Method: 提出MemoryOS，一个用于AI代理的全面高效的内存管理系统。MemoryOS设计了一个分层存储架构，并包含四个关键模块：内存存储、更新、检索和生成。包括短期记忆、中期记忆和长期个人记忆的存储单元。使用动态更新，通过对话链的FIFO原则以及分段页面组织策略，实现短期到中期和中期到长期记忆的更新。

Result: 在LoCoMo基准上的广泛实验显示F1平均提高49.11%，BLEU-1提高46.18%，在GPT-4o-mini上显示出上下文连贯性和保持个性化记忆的能力。

Conclusion: MemoryOS通过分层记忆整合和动态更新，改善了AI代理的长久记忆能力和个性化互动。

Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context
windows and inadequate memory management, leading to a severe shortage of
long-term memory capabilities and limited personalization in the interactive
experience with AI agents. To overcome this challenge, we innovatively propose
a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and
efficient memory management for AI agents. Inspired by the memory management
principles in operating systems, MemoryOS designs a hierarchical storage
architecture and consists of four key modules: Memory Storage, Updating,
Retrieval, and Generation. Specifically, the architecture comprises three
levels of storage units: short-term memory, mid-term memory, and long-term
personal memory. Key operations within MemoryOS include dynamic updates between
storage units: short-term to mid-term updates follow a dialogue-chain-based
FIFO principle, while mid-term to long-term updates use a segmented page
organization strategy. Our pioneering MemoryOS enables hierarchical memory
integration and dynamic updating. Extensive experiments on the LoCoMo benchmark
show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the
baselines on GPT-4o-mini, showing contextual coherence and personalized memory
retention in long conversations. The implementation code is open-sourced at
https://github.com/BAI-LAB/MemoryOS.

</details>


### [92] [Will artificial agents pursue power by default?](https://arxiv.org/abs/2506.06352)
*Christian Tarsney*

Main category: cs.AI

TL;DR: 本文形式化了工具会聚和权力追求的概念，探讨了权力作为会聚工具目标的实用性，得出在特定情况下这一概念更具预测性。


<details>
  <summary>Details</summary>
Motivation: 研究人员担心来自先进AI的灾难性风险，他们认为我们应该预期具有足够能力的AI代理会追求对人类的权力，因为权力是一个会聚工具目标，对广泛的最终目标都有用。

Method: 本文旨在在一个抽象的、决策理论的框架内形式化工具会聚和寻求权力的概念，并评估权力作为会聚工具目标的主张。

Result: 权力作为会聚工具目标的主张包含真实但有限的预测实用性，会聚工具的事实对那些有望获得绝对权力的代理更具有预测意义。

Conclusion: 虽然权力是一个会聚工具目标这一说法包含一定的真实性，但可能在预测的实用性上有限，因为在缺乏有关代理最终目标的实质信息时，一个代理的选择无法总是以权力进行排名。然而，对那些有可能获得绝对或接近绝对权力的代理而言，会聚工具的事实更具预测性。

Abstract: Researchers worried about catastrophic risks from advanced AI have argued
that we should expect sufficiently capable AI agents to pursue power over
humanity because power is a convergent instrumental goal, something that is
useful for a wide range of final goals. Others have recently expressed
skepticism of these claims. This paper aims to formalize the concepts of
instrumental convergence and power-seeking in an abstract, decision-theoretic
framework, and to assess the claim that power is a convergent instrumental
goal. I conclude that this claim contains at least an element of truth, but
might turn out to have limited predictive utility, since an agent's options
cannot always be ranked in terms of power in the absence of substantive
information about the agent's final goals. However, the fact of instrumental
convergence is more predictive for agents who have a good shot at attaining
absolute or near-absolute power.

</details>


### [93] [Towards Foundation Model on Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2506.06367)
*Jiaxin Pan,Mojtaba Nayyeri,Osama Mohammed,Daniel Hernandez,Rongchuan Zhang,Cheng Cheng,Steffen Staab*

Main category: cs.AI

TL;DR: 引入了一种完全归纳的时间知识图链接预测方法POSTRA，利用正弦位置编码和消息传递，实现零样本推理和广泛的时间结构信息转移。


<details>
  <summary>Details</summary>
Motivation: 现有的转换性或半归纳性Temporal Knowledge Graph Embedding (TKGE)模型在测试时依赖于训练中完全或部分观察到的元素，这限制了模型转移到新领域并推广到真实世界场景的能力。需要克服这一限制以实现更广泛的适用性。

Method: 使用正弦位置编码捕捉细粒度的时间模式，并通过在局部和全局时间上下文中进行消息传递生成自适应实体和关系表示。

Result: 单个预训练模型能够在各种归纳时间推理场景中提高零样本表现，标志着朝时间知识图基础模型迈出重要一步。

Conclusion: POSTRA是一种预训练的、可扩展的、可转移的模型，在看不见的时间知识图上的零样本表现出色，能够有效推广到新实体、关系和时间戳。

Abstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats
(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform
link prediction tasks in transductive or semi-inductive settings, which means
the entities, relations, and temporal information in the test graph are fully
or partially observed during training. Such reliance on seen elements during
inference limits the models' ability to transfer to new domains and generalize
to real-world scenarios. A central limitation is the difficulty in learning
representations for entities, relations, and timestamps that are transferable
and not tied to dataset-specific vocabularies. To overcome these limitations,
we introduce the first fully-inductive approach to temporal knowledge graph
link prediction. Our model employs sinusoidal positional encodings to capture
fine-grained temporal patterns and generates adaptive entity and relation
representations using message passing conditioned on both local and global
temporal contexts. Our model design is agnostic to temporal granularity and
time span, effectively addressing temporal discrepancies across TKGs and
facilitating time-aware structural information transfer. As a pretrained,
scalable, and transferable model, POSTRA demonstrates strong zero-shot
performance on unseen temporal knowledge graphs, effectively generalizing to
novel entities, relations, and timestamps. Extensive theoretical analysis and
empirical results show that a single pretrained model can improve zero-shot
performance on various inductive temporal reasoning scenarios, marking a
significant step toward a foundation model for temporal KGs.

</details>


### [94] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: SIGMA optimizes LLMs by reintegrating discarded decision paths, improving accuracy efficiently on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo Tree Search discards non-optimal nodes, wasting potentially valuable data that could improve model reasoning by enhancing data quality rather than quantity.

Method: SIGMA reintegrates discarded sibling nodes from Monte Carlo Tree Search, uses semantic links and a two-stage refinement model to improve reasoning paths in LLMs.

Result: SIGMA improves a 7B model's accuracy to 54.92% on the MATH benchmark with only 30K samples, exceeding the performance of models trained with much larger datasets.

Conclusion: SIGMA significantly enhances the reasoning capability of language models by leveraging non-optimal reasoning paths, providing efficient data use and improving performance on benchmarks.

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [95] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/abs/2506.06574)
*Suhana Bedi,Iddah Mlauzi,Daniel Shin,Sanmi Koyejo,Nigam H. Shah*

Main category: cs.AI

TL;DR: 多智能体系统在医疗诊断中表现更好，但仅依靠组件优化会导致系统整体性能下降，需要注重信息流和兼容性。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体人工智能系统在临床环境中的应用，并探讨组件优化与系统整体性能之间的关系。

Method: 使用2,400个真实病人案例，并将临床诊断分解为信息收集、解释和鉴别诊断。评估了单智能体系统与多智能体系统在不同任务执行上的表现。

Result: 多智能体系统通常优于单智能体系统，但即使具有优秀组件的最佳系统在诊断准确率上也表现不佳，显示出信息流和代理间兼容性的重要性。

Conclusion: 成功的医疗AI集成不仅需要组件级的优化，还需要重视信息流和代理间的兼容性，并且需要进行端到端的系统验证。

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [96] [Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility](https://arxiv.org/abs/2506.06523)
*Sumanth Pillella*

Main category: cs.AI

TL;DR: 研究提出了一种基于强化学习的框架，优化SAP LE中的仓库任务，实现高效的任务处理和实时优化。


<details>
  <summary>Details</summary>
Motivation: 在供应链需求日益增加的时代，需要更高效的解决方案来管理仓库、运输和交货操作，提升供应链的整体效率。

Method: 研究利用强化学习模型将仓库过程建模为动态环境，进行实时的任务分配、库存移动和订单拣选优化。

Result: 通过模拟30万条LE交易的合成数据，框架实现了95%的任务优化准确性，处理时间减少了60%。

Conclusion: 该研究成功引入了一种创新的基于强化学习的框架，用于优化SAP LE中的仓库任务管理，显著提升了操作敏捷性和效率。

Abstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is
pivotal for managing warehouse operations, transportation, and delivery. This
research introduces a pioneering framework leveraging reinforcement learning
(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing
operational agility and efficiency. By modeling warehouse processes as dynamic
environments, the framework optimizes task allocation, inventory movement, and
order picking in real-time. A synthetic dataset of 300,000 LE transactions
simulates real-world warehouse scenarios, including multilingual data and
operational disruptions. The analysis achieves 95% task optimization accuracy,
reducing processing times by 60% compared to traditional methods.
Visualizations, including efficiency heatmaps and performance graphs, guide
agile warehouse strategies. This approach tackles data privacy, scalability,
and SAP integration, offering a transformative solution for modern supply
chains.

</details>


### [97] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: ScriptDoctor通过大型语言模型实现自动游戏设计，展示了LLM在生成和测试过程中自动化和创新的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管在自动游戏设计领域中对大规模预训练模型的兴趣加剧，但目前这些生成模型的使用主要为临时性，且依赖持续的人工监督。亟需展示这些工具如何整合到自动游戏设计的长时间跨度的流程中，尤其是在无需人工监督能自主测试生成内容的系统接口中。因此，作者提出通过ScriptDoctor展示大规模语言模型在自动游戏设计中的应用潜力。

Method: ScriptDoctor系统通过大型语言模型驱动，实现自动生成和测试基于PuzzleScript的回合制谜题游戏。系统采用迭代循环的方式生成和测试游戏设计创意，利用PuzzleScript引擎的编译错误来生成可函数代码，并使用基于搜索的代理来进行游戏试玩测试。

Result: ScriptDoctor系统能够自动生成和测试基于PuzzleScript的游戏设计创意，并通过迭代循环不断改进其创意的功能性和完成度。

Conclusion: ScriptDoctor展示了大型语言模型系统在自动生成和测试游戏方面的潜力，特别是在迭代生成和测试游戏设计这一环节中显示了LLM在自动化、开放式工作流程中的可行性和创新性。

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [98] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 通过分析数字孪生体在AI模拟中的应用，提出了框架和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代符号学人工智能的采纳面临数据量和质量不足的挑战。

Method: 对22项研究进行系统分析，识别技术趋势，并建立参考框架。

Result: 提出了参照框架，并提供了与ISO 23247数字孪生参考架构的映射。

Conclusion: 数字孪生体在人工智能模拟中的应用，可以有效解决数据不足的问题，并提出了未来研究的挑战和机遇。

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [99] [GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](https://arxiv.org/abs/2506.06634)
*Yubin Xiao,Di Wang,Rui Cao,Xuan Wu,Boyang Li,You Zhou*

Main category: cs.AI

TL;DR: 本文提出的GELD神经TSP求解器能够高效解决各种规模的TSP问题，显著优于现有模型，并且可以处理多达744,710节点的大规模TSP。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络TSP解算器虽然有了很大进展，但在使用相同的预训练模型参数时通常难以高效解决各种规模的TSP问题，这限制了其实用性。

Method: 本文介绍了一种新的神经TSP解算器GELD，基于广域全局评估和精细局部选择框架。具体而言，GELD结合了轻量级的全局视角编码器（GE）和重量级的局部视角解码器（LD），并引入了一种新的低复杂度注意力机制。此外，还提出了两阶段训练策略来增强GELD的泛化能力。

Result: 实验表明，GELD在解决方案质量和推理速度方面优于七种最先进的模型，并能作为后处理方法显著提升现有神经TSP解算器的解的质量。GELD首次在不依赖分而治之策略的情况下解决了高达744,710节点的TSP问题。

Conclusion: GELD具有较大的推理延迟和扩展性，能够有效解决大规模TSP问题，展示了其在解决方案质量和速度上优于当前最先进模型的优势。

Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.

</details>


### [100] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: 本文介绍了Semantic Spacetime图模型的形式方面，定义了有限表示以处理复杂语义，并发现图中的吸收态象征信息泄漏。


<details>
  <summary>Details</summary>
Motivation: 用于知识表示和过程建模。

Method: 定义有限的γ(3,4)表示，以形成一个封闭的操作集，可以扩展到任意语义复杂度。

Result: 发现在任何部分图中都会出现吸收态，意味着图过程会泄漏信息。

Conclusion: 研究表明，Semantic Spacetime模型在图路径中提供了预测能力，同时最小化约束。

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [101] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 引入CER框架，增强语言代理在复杂环境中的适应性，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型代理在环境特定经验缺乏情况下常在复杂任务中失败，且无法在推理时间持续学习，因此需要一种能够高效自我提升的方法。

Method: 提出一种名为Contextual Experience Replay (CER) 的无训练框架，累积并合成过去的经验到动态记忆缓冲区中，以便在新的任务中检索和扩充相关知识。

Result: 在VisualWebArena上，CER取得了31.9%的竞争性表现；在WebArena上，CER实现了36.7%的平均成功率，相比GPT-4o代理基准成功率提高了51.0%。

Conclusion: 通过引入Contextual Experience Replay (CER)，可以显著提升语言代理在复杂环境下的适应能力。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [102] [Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation](https://arxiv.org/abs/2506.06714)
*Hamied Nabizada,Tom Jeleniewski,Lasse Beers,Maximilian Weigand,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本论文提出了一种SysML框架，可将PDDL的规划语义集成到系统模型中，通过可重用刻板印象定义关键概念，并借助OCL约束确保一致性。案例研究展示了框架的有效性，支持在工程设计中实现自动化和模型化的规划生成。


<details>
  <summary>Details</summary>
Motivation: 在系统模型中直接集成基于规划域定义语言（PDDL）的规划语义，以便支持自动化和基于模型的规划描述生成，并在工程设计中提供可重用的桥梁。

Method: 定义可重用的刻板印象，以支持PDDL的关键概念，如类型、谓词、函数和动作，并通过正式的OCL约束确保语法一致性。采用BNF定义的PDDL 3.1推导出的框架，应用于SysML建模实践。

Result: 通过航天制造案例研究，验证了框架的应用：一个具有可更换末端执行器的机器人系统被建模和丰富，以生成PDDL格式的域和问题描述。这些描述被输入到PDDL求解器中，以导出优化的执行计划。

Conclusion: 该方法支持自动化和基于模型的规划描述生成，并在工程设计中提供了系统建模和AI规划之间可重用的桥梁。

Abstract: This paper presents a SysML profile that enables the direct integration of
planning semantics based on the Planning Domain Definition Language (PDDL) into
system models. Reusable stereotypes are defined for key PDDL concepts such as
types, predicates, functions and actions, while formal OCL constraints ensure
syntactic consistency. The profile was derived from the Backus-Naur Form (BNF)
definition of PDDL 3.1 to align with SysML modeling practices. A case study
from aircraft manufacturing demonstrates the application of the profile: a
robotic system with interchangeable end effectors is modeled and enriched to
generate both domain and problem descriptions in PDDL format. These are used as
input to a PDDL solver to derive optimized execution plans. The approach
supports automated and model-based generation of planning descriptions and
provides a reusable bridge between system modeling and AI planning in
engineering design.

</details>


### [103] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM框架通过结合贝叶斯推断和强化学习改善LLM在特定领域的预测能力。实验显示其在文本游戏中有效提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成结构化、特定领域上下文预测方面存在局限性，因为它们无法将宽泛的非结构化理解锚定到特定环境中。WorldLLM旨在通过提升预测功能克服这些局限。

Method: 该框架结合了贝叶斯推断和基于好奇心的强化学习策略，通过自然语言假设指导LLM进行预测。假设在收集的证据支持下，通过使用第二个LLM作为提出分布进行调整，并通过探索低对数似然的状态转换来收集证据。

Result: 实验表明WorldLLM在需要操控和组合对象的文本游戏环境中是有效的，能够提高预测准确性并生成环境动态的理论。

Conclusion: WorldLLM框架提高了LLM在结构化、特定领域模拟环境中的预测能力，通过结合贝叶斯推断和基于强化学习的自主主动探索实现。这一方法使预测更准确，并生成可人类解释的环境动态理论。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [104] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: VisioMath是首个专为评估图像选项的数学推理能力而设计的数据集，揭示了现有大型多模态模型在这一领域的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在处理数学推理问题时表现出色，但其处理基于图像的选项时的能力尚未探索。VisioMath旨在填补这一空白。

Method: 引入VisioMath基准，用于评估大型多模态模型在图像选项环境中的数学推理能力。VisioMath包含8,070张图像和1,800个多项选择题。

Result: 通过在VisioMath上系统评估现有最先进的LMMs，发现即使是最先进的模型也难以处理这些任务。例如，GPT-4o仅实现了45.9%的准确率。

Conclusion: 现有的最先进大型多模态模型在数学推理能力上仍存在显著不足，尤其是在处理基于图像的选项时。VisioMath数据集揭示了这些模型面临的挑战和局限性。

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [105] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 提出了一种缩小假设空间的新方法，通过使用背景知识去除不可能的规则来提高ILP系统的效率，实验表明该方法可以大幅减少学习时间并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程 (ILP) 中的假设搜索空间通常非常大，搜索这些空间需要耗费大量时间。因此，我们希望通过事先缩小假设空间来提高ILP系统的效率。

Method: 我们的方法使用背景知识来找到不能在最佳假设中出现的规则，然后从假设空间中删除违反这些规则的假设。我们通过回答集编程实现这一方法，并用于收缩基于约束的ILP系统的假设空间。

Result: 实验表明，进行仅 10 秒的预处理后，我们的方法可以将学习时间从超过 10 小时减少到仅 2 秒，同时保持预测准确性。

Conclusion: 我们的实验表明，该方法在多个领域中可以显著减少学习时间，同时保持预测准确性。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [106] [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)
*Yigui Feng,Qinglin Wang,Ke Liu,Xinhai Chen,Bo Yang,Jie Liu*

Main category: cs.AI

TL;DR: AI PsyRoom通过细粒度情感分类和多代理框架提升心理咨询对话质量，生成个性化治疗计划，并在多个指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 心理咨询面临巨大挑战，因为心理健康服务需求增加而专业人员短缺。现有大语言模型缺乏对情感的深刻理解及个性化治疗计划的能力。

Method: 设计AI PsyRoom，多代理模拟框架，通过细粒度情感分类和多代理框架构建高质量对话数据集EmoPsy，并生成个性化治疗计划。

Result: 在问题导向、表达、同理心和互动交流质量方面，AI PsyRoom较现有最先进方法分别提高了18%、23%、24%和16%。

Conclusion: AI PsyRoom显著提升了心理咨询质量，多项指标上的表现优于现有最先进的方法。

Abstract: Psychological counseling faces huge challenges due to the growing demand for
mental health services and the shortage of trained professionals. Large
language models (LLMs) have shown potential to assist psychological counseling,
especially in empathy and emotional support. However, existing models lack a
deep understanding of emotions and are unable to generate personalized
treatment plans based on fine-grained emotions. To address these shortcomings,
we present AI PsyRoom, a multi-agent simulation framework designed to enhance
psychological counseling by generating empathetic and emotionally nuanced
conversations. By leveraging fine-grained emotion classification and a
multi-agent framework, we construct a multi-agent PsyRoom A for dialogue
reconstruction, generating a high-quality dialogue dataset EmoPsy, which
contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.
We also propose PsyRoom B for generating personalized treatment plans.
Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms
state-of-the-art methods, achieving 18% improvement in problem orientation, 23%
in expression, 24% in Empathy, and 16% in interactive communication quality.
The datasets and models are publicly available, providing a foundation for
advancing AI-assisted psychological counseling research.

</details>


### [107] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 研究了SNN结合LZC，用于高效分类神经数据，不同算法在准确性和效率间需权衡。


<details>
  <summary>Details</summary>
Motivation: SNN由于其时间动态性、尖峰事件的不可微性以及稀疏事件驱动的激活，使其训练具有挑战性，因此需要考虑选择的学习算法对分类准确性的影响。

Method: 提出了一种基于SNN和Lempel-Ziv复杂性的生物启发分类器，通过结合SNN的时间精度和生物真实性以及LZC的结构复杂性分析，实现对时空神经数据的高效且可解释的分类。

Result: 经典的反向传播算法虽然能达到优异的分类准确性，但计算成本极高，不适合实时应用。生物启发学习算法如tempotron和Spikprop在保持竞争性分类性能的同时提高计算效率，适合时间敏感任务。

Conclusion: 选择最合适的学习算法需要在分类准确性、计算成本以及应用约束之间作出权衡。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [108] [Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain](https://arxiv.org/abs/2506.06786)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: CA-MIQ是一种轻量化的强化学习框架，在高风险搜索救援情境中表现出色，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险的搜索和救援任务中，自动化系统需要持续收集任务关键的信息，并且灵活地适应变化的操作优先级。

Method: 提出了CA-MIQ（Context-Aware Max-Information Q-learning），这是一种轻量级的双重评价器强化学习框架，能够在任务优先级改变时动态调整其探索策略。

Result: 在模拟的搜索和救援网格世界中，实验测试了CA-MIQ对需要重点关注的信息类型优先级改变的适应性。与基线方法相比，CA-MIQ在单次优先级变化后的任务成功率高出近四倍，在多次变化的情境下表现也高出三倍，实现了100%的恢复，而基线方法无法适应。

Conclusion: CA-MIQ在任意具有分段稳定信息价值分布的离散环境中能够有效工作。

Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions
must continuously gather mission-critical information while flexibly adapting
to shifting operational priorities. We propose CA-MIQ (Context-Aware
Max-Information Q-learning), a lightweight dual-critic reinforcement learning
(RL) framework that dynamically adjusts its exploration strategy whenever
mission priorities change. CA-MIQ pairs a standard extrinsic critic for task
reward with an intrinsic critic that fuses state-novelty, information-location
awareness, and real-time priority alignment. A built-in shift detector triggers
transient exploration boosts and selective critic resets, allowing the agent to
re-focus after a priority revision. In a simulated SAR grid-world, where
experiments specifically test adaptation to changes in the priority order of
information types the agent is expected to focus on, CA-MIQ achieves nearly
four times higher mission-success rates than baselines after a single priority
shift and more than three times better performance in multiple-shift scenarios,
achieving 100% recovery while baseline methods fail to adapt. These results
highlight CA-MIQ's effectiveness in any discrete environment with
piecewise-stationary information-value distributions.

</details>


### [109] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: 定义了LLM概率测度的相关任务，提出Xent游戏用于评估模型能力，包含博弈论构建方法。


<details>
  <summary>Details</summary>
Motivation: 通过探讨LLM中概率测度的隐含知识问题，推动生成超越性任务的界定。

Method: 将任务表述为基于LLM测度的博弈，即交叉熵（Xent）游戏，采用博弈论的一致性公理构建。

Result: 展示Xent游戏空间的构建方法以及其应用于测量LLM能力的潜力。

Conclusion: Xent游戏空间可用于构建LLM的能力基准，通过测度覆盖解决范围无界问题。

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [110] [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)
*HaoYang Shang,Xuan Liu,Zi Liang,Jie Zhang,Haibo Hu,Song Guo*

Main category: cs.AI

TL;DR: 该论文提出CoThinker框架，通过管理认知过载和促进协作，提升LLM在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM在复杂多面任务上表现有限，类似于人类认知负荷理论中的界限，因此，需要一种新的方法来提升其协作问题解决能力。

Method: 通过专门化的代理分配内在认知负荷，并通过结构化的通信和集体工作记忆管理事务性负荷，从而减轻认知过载。

Result: 实验证明CoThinker在解决复杂问题任务上的效果优于现有的多代理基线，不仅在解决质量上有所改善，而且在效率上也得到提升。

Conclusion: 引入了CoThinker，一个基于CLT原理设计的LLM多代理框架，能够在解决复杂问题时有效管理认知负荷，提升解决方案的质量和效率。

Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on
complex, multi-faceted tasks, as they often fail to integrate diverse
information or adhere to multiple constraints. We posit that such limitation
arises when the demands of a task exceed the LLM's effective cognitive load
capacity. This interpretation draws a strong analogy to Cognitive Load Theory
(CLT) in cognitive science, which explains similar performance boundaries in
the human mind, and is further supported by emerging evidence that reveals LLMs
have bounded working memory characteristics. Building upon this CLT-grounded
understanding, we introduce CoThinker, a novel LLM-based multi-agent framework
designed to mitigate cognitive overload and enhance collaborative
problem-solving abilities. CoThinker operationalizes CLT principles by
distributing intrinsic cognitive load through agent specialization and managing
transactional load via structured communication and a collective working
memory. We empirically validate CoThinker on complex problem-solving tasks and
fabricated high cognitive load scenarios, demonstrating improvements over
existing multi-agent baselines in solution quality and efficiency. Our analysis
reveals characteristic interaction patterns, providing insights into the
emergence of collective cognition and effective load management, thus offering
a principled approach to overcoming LLM performance ceilings.

</details>


### [111] [Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance](https://arxiv.org/abs/2506.06868)
*Razieh Arshadizadeh,Mahmoud Asgari,Zeinab Khosravi,Yiannis Papadopoulos,Koorosh Aslansefat*

Main category: cs.AI

TL;DR: 论文提出了一种将SafeML与Bayesian Networks结合的概率安全保证框架，用于动态安全评估和系统适应。


<details>
  <summary>Details</summary>
Motivation: 传统安全评估方法无法有效处理机器学习组件的固有缺陷，该研究旨在解决由于操作和训练数据间的分布变化所导致的推理失败问题。

Method: 使用Bayesian Networks（BNs）将SafeML与机器学习失败的概率建模相结合，进行广泛的因果安全分析。

Result: 通过该方法，在仿真的汽车队列系统中对交通标志识别进行了验证，结果表明明确建模机器学习失败在安全评估中具有较大的潜在好处。

Conclusion: 该论文的研究表明，通过主动建模和检测机器学习失败，可以提高安全评估，并在不确定性下实现系统适应。

Abstract: Machine Learning (ML) models are increasingly integrated into safety-critical
systems, such as autonomous vehicle platooning, to enable real-time
decision-making. However, their inherent imperfection introduces a new class of
failure: reasoning failures often triggered by distributional shifts between
operational and training data. Traditional safety assessment methods, which
rely on design artefacts or code, are ill-suited for ML components that learn
behaviour from data. SafeML was recently proposed to dynamically detect such
shifts and assign confidence levels to the reasoning of ML-based components.
Building on this, we introduce a probabilistic safety assurance framework that
integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of
a broader causal safety analysis. This allows for dynamic safety evaluation and
system adaptation under uncertainty. We demonstrate the approach on an
simulated automotive platooning system with traffic sign recognition. The
findings highlight the potential broader benefits of explicitly modelling ML
failures in safety assessment.

</details>


### [112] [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)
*Zixuan Li,Wenxuan Liu,Long Bai,Chunmao Zhang,Wei Li,Fenghui Zhang,Quanxin Jin,Ruoyun He,Zhuo Chen,Zhilei Hu,Fei Wang,Bingbing Xu,Xuhui Jiang,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出了KDR框架和\KCII，通过代码生成加强知识组织和计算，实验显示有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架在系统化知识管理、共享大规模知识处理和复杂知识计算方面面临挑战。

Method: 提出了KDR框架，引入独立的知识组织阶段来预处理数据，使用\KCII生成代码用于知识组织和复杂知识计算。

Result: 在六个知识分析任务的超过三十个数据集上验证了\KCII的有效性。KDR框架中的\KCII能够生成高质量的分析报告。

Conclusion: KDR框架能够有效组织和计算大规模知识，与现有框架相比，可以生成更具洞察力的分析结果。

Abstract: Deep knowledge analysis tasks always involve the systematic extraction and
association of knowledge from large volumes of data, followed by logical
reasoning to discover insights. However, to solve such complex tasks, existing
deep research frameworks face three major challenges: 1) They lack systematic
organization and management of knowledge; 2) They operate purely online, making
it inefficient for tasks that rely on shared and large-scale knowledge; 3) They
cannot perform complex knowledge computation, limiting their abilities to
produce insightful analytical results. Motivated by these, in this paper, we
propose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})
framework that empowers deep research with deep knowledge analysis capability.
Specifically, it introduces an independent knowledge organization phase to
preprocess large-scale, domain-relevant data into systematic knowledge offline.
Based on this knowledge, it extends deep research with an additional kind of
reasoning steps that perform complex knowledge computation in an online manner.
To enhance the abilities of LLMs to solve knowledge analysis tasks in the above
framework, we further introduce \textbf{\KCII}, an LLM that bridges knowledge
organization and reasoning via unified code generation. For knowledge
organization, it generates instantiation code for predefined classes,
transforming data into knowledge objects. For knowledge computation, it
generates analysis code and executes on the above knowledge objects to obtain
deep analysis results. Experimental results on more than thirty datasets across
six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,
when integrated into the KDR framework, \KCII can generate high-quality reports
with insightful analytical results compared to the mainstream deep research
framework.

</details>


### [113] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 为了改善小型多模态模型在上下文学习过程中的不一致性，我们提出一种基于元学习的方法，通过注意力映射模块和软提示提高低数据条件下的任务适应能力。


<details>
  <summary>Details</summary>
Motivation: 大多数大型多模态模型在执行新的任务时依赖于上下文学习，但它在较小模型中的表现不稳定，因此我们提出改善此问题的方法。

Method: 我们使用一种基于元学习的方法，通过从任务相关的图像特征中提炼出的固定软提示来诱导 LMMs 的小样本能力，并引入一种注意力映射模块来促进这种提炼过程。

Result: 我们的模型在 VL-ICL 基准上持续优于 ICL 和相关提示调整方法，即便在图像受到扰动的情况下，其在视觉问答任务中的任务引导和推理能力也得到提升。

Conclusion: 我们提出的基于元学习的方法能在低数据条件下有效地提高大型多模态模型的任务适应能力。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [114] [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)
*Mahnaz Koupaee,Xueying Bai,Mudan Chen,Greg Durrett,Nathanael Chambers,Niranjan Balasubramanian*

Main category: cs.AI

TL;DR: 本研究通过生成因果事件图的协作方法帮助大型语言模型改善事件推理能力，在不微调的情况下实现预测任务的竞争性效果。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在事件推理方面有所进展，但仍难以准确识别事件之间的因果关系，这导致在事件预测和时间线理解这类深层推理任务中的表现不佳。

Method: 提出一种协作的方法生成因果事件图，应用于推理过程中辅助语言模型。使用模拟专家关注具体语义关系，通过多轮讨论，一个最终专家整合生成过程。

Result: 生成的因果事件图用于多下游应用，介绍了一个新的可解释事件预测任务，该任务在解释中需要因果事件链。生成的解释比基准更具信息性和连贯性，未在任何下游任务上微调的整体方法在事件预测和下一事件预测任务上取得了具有竞争力的结果。

Conclusion: 本研究提出了一种协作生成因果事件图的方法，以补充大型语言模型（LLMs）在推理过程中对事件间因果关系的表示。通过多个专家的讨论和整合，生成的因果图能用于多种下游应用，并在不经过下游任务微调的情况下，在事件预测和下一事件预测任务上取得了与现有最先进模型相当的竞争结果。

Abstract: Understanding how events in a scenario causally connect with each other is
important for effectively modeling and reasoning about events. But event
reasoning remains a difficult challenge, and despite recent advances, Large
Language Models (LLMs) still struggle to accurately identify causal connections
between events. This struggle leads to poor performance on deeper reasoning
tasks like event forecasting and timeline understanding. To address this
challenge, we investigate the generation of causal event graphs (e.g., A
enables B) as a parallel mechanism to help LLMs explicitly represent causality
during inference. This paper evaluates both how to generate correct graphs as
well as how graphs can assist reasoning. We propose a collaborative approach to
causal graph generation where we use LLMs to simulate experts that focus on
specific semantic relations. The experts engage in multiple rounds of
discussions which are then consolidated by a final expert. Then, to demonstrate
the utility of causal graphs, we use them on multiple downstream applications,
and also introduce a new explainable event prediction task that requires a
causal chain of events in the explanation. These explanations are more
informative and coherent than baseline generations. Finally, our overall
approach not finetuned on any downstream task, achieves competitive results
with state-of-the-art models on both forecasting and next event prediction
tasks.

</details>


### [115] [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)
*Xutong Zhao,Tengyu Xu,Xuewei Wang,Zhengxing Chen,Di Jin,Liang Tan,Yen-Ting,Zishun Yu,Zhuokai Zhao,Yun He,Sinong Wang,Han Fang,Sarath Chandar,Chen Zhu*

Main category: cs.AI

TL;DR: SPOC通过启用模型在一次推理中进行自我纠正，大幅提升了数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自我纠正方法通常作为单独的生成后修正，依赖额外的提示和系统设计，而不是在一次推理中进行实时的自我修正。

Method: 提出一种名为SPOC的自发性自我纠正方法，使LLMs在一次推理过程中生成交织的解答和验证，由验证结果动态终止生成。采用简单有效的合成数据生成法进行微调，并通过在线强化学习提升准确性。

Result: 在数学推理基准测试中，SPOC显著提高了性能，如Llama-3.1-8B和70B Instruct模型在多个数据集上分别取得了显著的精度提升。

Conclusion: SPOC提升了大型语言模型在数学推理任务上的性能。

Abstract: While large language models (LLMs) have demonstrated remarkable success on a
broad range of tasks, math reasoning remains a challenging one. One of the
approaches for improving math reasoning is self-correction, which designs
self-improving loops to let the model correct its own mistakes. However,
existing self-correction approaches treat corrections as standalone
post-generation refinements, relying on extra prompt and system designs to
elicit self-corrections, instead of performing real-time, spontaneous
self-corrections in a single pass. To address this, we propose SPOC, a
spontaneous self-correction approach that enables LLMs to generate interleaved
solutions and verifications in a single inference pass, with generation
dynamically terminated based on verification outcomes, thereby effectively
scaling inference time compute. SPOC considers a multi-agent perspective by
assigning dual roles -- solution proposer and verifier -- to the same model. We
adopt a simple yet effective approach to generate synthetic data for
fine-tuning, enabling the model to develop capabilities for self-verification
and multi-agent collaboration. We further improve its solution proposal and
verification accuracy through online reinforcement learning. Experiments on
mathematical reasoning benchmarks show that SPOC significantly improves
performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct
models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,
and 3.3% and 6.7% on AIME24, respectively.

</details>


### [116] [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)
*Darui Lu,Jordan M. Malof,Willie J. Padilla*

Main category: cs.AI

TL;DR: 开发了一个Agentic框架用于光子超材料的逆向设计，展示了其在自动化、推理、计划和适应方面的能力和新颖输出。


<details>
  <summary>Details</summary>
Motivation: 整合多种大型语言模型系统的重大进展促进了能够自主完成复杂任务的Agentic框架的发展。

Method: 框架通过提出和开发正向深度学习模型，利用API访问外部工具如仿真和优化，使用存储器，并通过深度逆向方法生成最终设计。

Result: Agentic框架在光子超材料的逆向设计中有效实现了自动化、推理、计划和适应。

Conclusion: 该框架具有自动化、推理、计划和适应能力，可以在光子超材料的逆向设计中实现不同且新颖的输出。

Abstract: Recent significant advances in integrating multiple Large Language Model
(LLM) systems have enabled Agentic Frameworks capable of performing complex
tasks autonomously, including novel scientific research. We develop and
demonstrate such a framework specifically for the inverse design of photonic
metamaterials. When queried with a desired optical spectrum, the Agent
autonomously proposes and develops a forward deep learning model, accesses
external tools via APIs for tasks like simulation and optimization, utilizes
memory, and generates a final design via a deep inverse method. The framework's
effectiveness is demonstrated in its ability to automate, reason, plan, and
adapt. Notably, the Agentic Framework possesses internal reflection and
decision flexibility, permitting highly varied and potentially novel outputs.

</details>


### [117] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: This paper analyzes LRMs' reasoning using puzzle environments, revealing their strengths and limits in different task complexities, and discovers their struggle in maintaining accuracy and consistency at high problem complexities.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the lack of understanding of LRMs' fundamental capabilities, scaling properties, and limitations by investigating their reasoning beyond final answer accuracy.

Method: The study uses controllable puzzle environments to manipulate problem complexity and analyze both the final answers and the reasoning traces of LRMs, providing insights into their thinking processes.

Result: Findings show LRMs' complete accuracy collapse at high complexities and a counterintuitive scaling limit, where reasoning effort decreases despite having remaining token budget.

Conclusion: LRMs have three performance regimes based on task complexity; they outperform standard LLMs only in medium-complexity tasks and struggle with high-complexity tasks, suggesting limitations in reasoning and computation consistency.

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [118] [Deontically Constrained Policy Improvement in Reinforcement Learning Agents](https://arxiv.org/abs/2506.06959)
*Alena Makarova,Houssam Abbas*

Main category: cs.AI

TL;DR: 在强化学习中，研究了如何在满足约束条件下最大化效用，提出并验证了一种基于期望行为效用主义的策略改进方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何在满足伦理、社会或情境约束的情况下，使强化学习代理的效用最大化。

Method: 利用期望行为效用主义的逻辑和概率stit逻辑，在受控MDP环境中进行策略改进，找到满足约束的任务效用局部最大值。

Result: 通过在样本MDPs上的实验，验证了该方法能够在既定约束下优化策略并实现任务效用最大化。

Conclusion: 该方法可以约束强化学习代理在动作选择时，兼顾实现既定目标并满足特定约束条件。

Abstract: Markov Decision Processes (MDPs) are the most common model for decision
making under uncertainty in the Machine Learning community. An MDP captures
non-determinism, probabilistic uncertainty, and an explicit model of action. A
Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a
utility function. This paper considers the problem of learning a decision
policy that maximizes utility subject to satisfying a constraint expressed in
deontic logic. In this setup, the utility captures the agent's mission - such
as going quickly from A to B. The deontic formula represents (ethical, social,
situational) constraints on how the agent might achieve its mission by
prohibiting classes of behaviors. We use the logic of Expected Act
Utilitarianism, a probabilistic stit logic that can be interpreted over
controlled MDPs. We develop a variation on policy improvement, and show that it
reaches a constrained local maximum of the mission utility. Given that in stit
logic, an agent's duty is derived from value maximization, this can be seen as
a way of acting to simultaneously maximize two value functions, one of which is
implicit, in a bi-level structure. We illustrate these results with experiments
on sample MDPs.

</details>


### [119] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: 本文提出了一种处理长尾分布的新框架，使用自导标注和表示平衡技术，提升了广义类别发现的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的广义类别发现方法在类平衡的人工数据集上表现良好，但在真实世界中，由于数据集的长尾分布，表现受到显著影响。

Method: 提出了一种新的框架，首先通过可学习的分布生成伪标签，减少分类器偏差，然后通过采样邻域加强尾类聚焦。

Result: 在公共数据集上的实验结果显示，该框架优于之前的最佳方法。

Conclusion: 提出的框架在处理长尾分布时有效，可以在未标记样本中发现新的类别。

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [120] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 借助神经生物学工具，研究揭示DRL代理的复杂行为模式，展示在确保智能体行为安全对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习行为分析方法不足以理解深度强化学习代理的复杂行为模式，需要引入新的分析框架。

Method: 将神经科学和动物行为学应用于复杂的部分可观测环境情境中，对DRL代理进行联合行为和神经分析。

Result: 发现无模型的RNN基础的DRL代理能够展现出结构化的、类似计划的行为，而不需要显式的记忆模块或世界模型。

Conclusion: 使用神经生物学和动物行为学的工具可以揭示DRL代理的详细行为模式和神经动力学结构，而这些在传统方法下可能不易观察到。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [121] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: 研究开发了一种检测众包任务中LLM辅助作弊的评分机制，验证了其在低努力欺骗检测中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式 AI 的成功，确保数据集中人类输入的质量变得至关重要，而LLM的广泛使用可能会影响数据的真实性。

Method: 该研究使用了一种无训练评分机制，通过分析工人回答之间的相关性，并结合请求者可用的部分LLM生成标签进行评估。

Result: 该方法在理论上和实证上证明了其在检测低努力欺骗方面的鲁棒性。

Conclusion: 该研究提出了一种在众包任务中检测LLM辅助作弊的机制，并证明这种方法在检测低努力欺骗时是有效和可靠的。

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


### [122] [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)
*Yu Xuejun,Jianyuan Zhong,Zijin Feng,Pengyi Zhai,Roozbeh Yousefzadeh,Wei Chong Ng,Haoxiong Liu,Ziyi Shou,Jing Xiong,Yudong Zhou,Claudia Beth Ong,Austen Jeremy Sugiarto,Yaoxi Zhang,Wai Ming Tai,Huan Cao,Dongcai Lu,Jiacheng Sun,Qiang Xu,Shen Xin,Zhenguo Li*

Main category: cs.AI

TL;DR: Mathesis advances theorem proving by processing informal statements, introducing an autoformalizer and prover, achieving high performance on new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of LLM-based theorem provers which require expert-written formal statements as inputs, hindering real-world applicability.

Method: Mathesis introduces an autoformalizer using reinforcement learning to improve formalization of natural language problems and a prover to generate formal proofs, supported by LeanScorer for quality assessment.

Result: Mathesis's autoformalizer surpasses the best baseline by 22% in pass-rate on Gaokao-Formal, with the full system achieving 64% accuracy on MiniF2F and 18% on Gaokao-Formal.

Conclusion: Mathesis demonstrates significant advancements in end-to-end formal theorem proving from informal problem statements, showing state-of-the-art performance on challenging benchmarks.

Abstract: Recent advances in large language models show strong promise for formal
reasoning. However, most LLM-based theorem provers have long been constrained
by the need for expert-written formal statements as inputs, limiting their
applicability to real-world problems expressed in natural language. We tackle
this gap with Mathesis, the first end-to-end theorem proving pipeline
processing informal problem statements. It contributes Mathesis-Autoformalizer,
the first autoformalizer using reinforcement learning to enhance the
formalization ability of natural language problems, aided by our novel
LeanScorer framework for nuanced formalization quality assessment. It also
proposes a Mathesis-Prover, which generates formal proofs from the formalized
statements. To evaluate the real-world applicability of end-to-end formal
theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex
problems from China's national college entrance exam. Our approach is carefully
designed, with a thorough study of each component. Experiments demonstrate
Mathesis's effectiveness, with the autoformalizer outperforming the best
baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other
model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a
state-of-the-art 18% on Gaokao-Formal.

</details>


### [123] [Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression](https://arxiv.org/abs/2506.07075)
*Liwen Zheng,Chaozhuo Li,Haoran Jia,Xi Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新的结构推理框架，旨在提升多跳事实验证系统的性能，实验结果显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法在多跳证据聚合和推理环节中结构捕获能力不足的问题，提出改进方案。

Method: 提出了一种结构推理框架，包括结构增强的检索机制和推理路径引导的验证模块，并结合了一种结构感知的推理机制。

Result: 实验表明我们的方法在FEVER和HoVer数据集上的性能优于多种基线方法。

Conclusion: 我们的结构推理框架显著提升了多跳事实验证系统的准确性和检索精度。

Abstract: The growing complexity of factual claims in real-world scenarios presents
significant challenges for automated fact verification systems, particularly in
accurately aggregating and reasoning over multi-hop evidence. Existing
approaches often rely on static or shallow models that fail to capture the
evolving structure of reasoning paths, leading to fragmented retrieval and
limited interpretability. To address these issues, we propose a Structural
Reasoning framework for Multi-hop Fact Verification that explicitly models
reasoning paths as structured graphs throughout both evidence retrieval and
claim verification stages. Our method comprises two key modules: a
structure-enhanced retrieval mechanism that constructs reasoning graphs to
guide evidence collection, and a reasoning-path-guided verification module that
incrementally builds subgraphs to represent evolving inference trajectories. We
further incorporate a structure-aware reasoning mechanism that captures
long-range dependencies across multi-hop evidence chains, enabling more precise
verification. Extensive experiments on the FEVER and HoVer datasets demonstrate
that our approach consistently outperforms strong baselines, highlighting the
effectiveness of reasoning-path modeling in enhancing retrieval precision and
verification accuracy.

</details>


### [124] [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)
*Liyang Chen,Yujun Cai,Jieqiong Dong,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文提出MARCUS管道，清理BRIGHT语料库，生成BRIGHT-Plus，提高检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前的RAG系统受限于语料库中的常见网络爬虫文献伪影问题，如内容冗余和语义不连续性，这些问题影响了检索准确性和推理能力，尤其在BRIGHT的某些子域中问题尤为明显。

Method: 使用多代理系统MARCUS，借助大型语言模型（LLMs）去除结构噪声和实现语义分割，以生成高质量的BRIGHT-Plus语料库。

Result: 经过实验评估，BRIGHT-Plus在多种检索器上表现出在检索准确性和多跃点推理方面的一致且显著的改进。

Conclusion: BRIGHT-Plus通过使用MARCUS管道进行清理和重新分块，显著改善了检索准确性和多跃点推理。

Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both
structurally clean and semantically coherent. BRIGHT is a recent and
influential benchmark designed to evaluate complex multi-hop retrieval across
diverse, high-reasoning domains. However, its practical effectiveness is
limited by common web-crawled artifacts - such as content redundancy and
semantic discontinuity - that impair retrieval accuracy and downstream
reasoning. Notably, we find that such issues are concentrated in seven
StackExchange-derived subdomains, while other domains (e.g., Coding and
Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large
language models (LLMs) to systematically clean and re-chunk BRIGHT into a
higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for
structural noise removal and semantic segmentation, preserving answer-bearing
spans while improving contextual integrity. Experimental evaluations
demonstrate that BRIGHT-Plus yields consistent and significant improvements in
both retrieval accuracy and multi-hop reasoning across a diverse set of
retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to
support future research on robust, reasoning-centric retrieval.

</details>


### [125] [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)
*Miroslav Popovic,Marko Popovic,Miodrag Djukic,Ilija Basicevic*

Main category: cs.AI

TL;DR: 研究使用ChatGPT自动化翻译联邦学习算法为CSP过程，验证了自动化翻译的可行性和成功性。


<details>
  <summary>Details</summary>
Motivation: 之前的联邦学习算法翻译需要手动操作，本文引入ChatGPT进行自动化翻译以简化流程。

Method: 使用ChatGPT自动化翻译过程，并使用PAT模型检查器验证翻译结果。

Result: 实验表明使用ChatGPT的翻译过程成功实现了从Python到CSP的转换，并通过了模型检查器PAT的验证。

Conclusion: 使用ChatGPT实现了自动化将Python中的联邦学习算法翻译为CSP过程的简化流程，并通过PAT模型检查器验证了集中式和去中心化联邦学习算法。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL
framework that is easy to use by ML&AI developers who do not need to be
professional programmers and is also amenable to LLMs. In the previous
research, generic federated learning algorithms provided by this framework were
manually translated into the CSP processes and algorithms' safety and liveness
properties were automatically verified by the model checker PAT. In this paper,
a simple translation process is introduced wherein the ChatGPT is used to
automate the translation of the mentioned federated learning algorithms in
Python into the corresponding CSP processes. Within the process, the minimality
of the used context is estimated based on the feedback from ChatGPT. The
proposed translation process was experimentally validated by successful
translation (verified by the model checker PAT) of both generic centralized and
decentralized federated learning algorithms.

</details>


### [126] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 提出SHE框架针对行为幻觉问题进行检测和解决，效果显著。


<details>
  <summary>Details</summary>
Motivation: 对于多模态大语言模型中的行为幻觉缺乏研究，为此提出解决方案。

Method: 引入SHE框架，通过视觉文本对齐检查检测幻觉，并通过正交投影在联合嵌入空间中减轻它们。

Result: 在标准基准上，SHE框架在BEACH指标上将行为幻觉减少了超过10%。

Conclusion: 提出SHE框架成功减少了行为幻觉并且保持描述准确性。

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [127] [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)
*Luwei Bai,Dongkeun Han,Sara Hennessy*

Main category: cs.AI

TL;DR: 研究分析了MyGPT在课堂对话编码中的表现，并提出了在有限数据下配置有效代理的实用策略。


<details>
  <summary>Details</summary>
Motivation: 由于课堂对话分析需要对话功能的细致理解且人工转录编码耗时，研究探索利用大型语言模型自动化过程以提高效率。

Method: 使用设计型研究方法，通过变量控制方法评估MyGPT代理在课堂对话编码中的基线表现，并探索不同示例输入如何影响其表现。

Result: 识别了一套基于MyGPT独特功能的实用策略，用于在有限数据条件下配置高效代理。

Conclusion: MyGPT代理经过定制策略后，尽管存在局限性，仍然可以成为有用的编码助手，通过生成编码建议支持用户。

Abstract: This study investigates effective strategies for developing a customised GPT
agent to code classroom dialogue. While classroom dialogue is widely recognised
as a crucial element of education, its analysis remains challenging due to the
need for a nuanced understanding of dialogic functions and the labour-intensive
nature of manual transcript coding. Recent advancements in large language
models offer promising avenues for automating this process. However, existing
studies predominantly focus on training large-scale models or evaluating
pre-trained models with fixed codebooks, which are often not applicable or
replicable for dialogue researchers working with small datasets or customised
coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its
baseline performance in coding classroom dialogue with a human codebook and
examines how performance varies with different example inputs through a
variable control method. Through a design-based research approach, it
identifies a set of practical strategies, based on MyGPT's unique features, for
configuring effective agents with limited data. The findings suggest that,
despite some limitations, a MyGPT agent developed with these strategies can
serve as a useful coding assistant by generating coding suggestions.

</details>


### [128] [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)
*Ming Liu,Wensheng Zhang*

Main category: cs.AI

TL;DR: 文章提出了通过任务扰动的方法来更好地评估多模态大型语言模型的泛化能力，以揭示其真正理解水平。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型在视觉语言基准测试中表现优异，但数据污染问题可能掩盖其真实的泛化能力。需要新的评估方式来揭示模型的真正理解能力。

Method: 通过任务扰动而非输入扰动，使用相同的视觉输入，对模型进行多任务评估（如问答、图片说明、问题提出、验证）来探测其多样化能力。

Result: 使用所提出的框架对现有图像/视频多模态大型语言模型进行了评估，发现极端污染会导致任务表现锐化但损害整体泛化能力。

Conclusion: 提出了一种动态评估框架，可以更加严格地评估多模态大型语言模型的泛化能力，揭示模型性能的真实性。

Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language
benchmark performance, yet growing concerns about data contamination (test set
exposure during training) risk masking true generalization. This concern
extends to reasoning MLLMs, often fine-tuned via reinforcement learning from
potentially contaminated base models. We propose a novel dynamic evaluation
framework to rigorously assess MLLM generalization, moving beyond static
benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the
same visual input, models are evaluated across a family of tasks (e.g., QA,
captioning, question posing, verification) to probe diverse capabilities. This
task perturbation reveals whether model performance is robust or reliant on
superficial task-specific cues. Our approach is analogous to loss landscape
sharpness: models overfit or contaminated for a single task (sharp minima)
falter under task shifts, unlike models with generalizable solutions (flatter
minima). We developed an automated pipeline with a calibrated judge scoring
open-ended generations (captions, questions) using paraphrase and corruption
sampling. Applying this framework to leading image/video MLLMs on benchmarks
including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task
"ability vector." We demonstrate that fine-tuning on simulated test data
(extreme contamination) drastically sharpens task-specific performance but
harms overall generalization. Our dynamic task perturbation offers deeper
insights into MLLM generalization, distinguishing genuine understanding from
spurious leakage or overfitting.

</details>


### [129] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)
*Zihan Deng,Changyu Du,Stavros Nousias,André Borrmann*

Main category: cs.AI

TL;DR: BIMgent是一个用于3D建筑建模的自动化框架，在保持设计意图的同时减少了手工工作量，成功率为32%。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理主要集中在通用桌面自动化任务上，而在建筑信息建模软件中的3D建模过程中涉及开放性设计任务和复杂交互模式，这些领域应用探索尚未完全解决。

Method: 提出了一种基于多模态大型语言模型的agentic框架，即BIMgent，可以通过图形用户界面实现自主的建筑模型创作，包括概念设计、软件工作流程规划和图形用户界面操作的执行。

Result: 在实际建筑建模任务中进行评估，BIMgent在文本为基础的概念设计生成和现有建筑设计重构方面均表现出合理的设计质量，其操作达到了32%的成功率，而所有基线模型在任务上均未成功（0%成功率）。

Conclusion: BIMgent通过自动化策略显著降低了建筑建模过程中的人工工作量，同时保持了设计意图，展示了其在实际建筑建模场景中部署的潜力。

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios.

</details>


### [130] [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)
*Yangqing Zheng,Shunqi Mao,Dingxin Zhang,Weidong Cai*

Main category: cs.AI

TL;DR: 研究提出了一种新方法和代理模型RRARA，以解决高风险动态环境中的决策延迟问题，报告显示RRARA在延迟敏感场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探讨在高风险动态环境下代理的性能，尤其是处理决策延迟问题。

Method: 提出了时间转换机制（TCM）和扩展的HAZARD评估协议，使得认知和物理成本在单一FPS基础指标下对齐。引入快速反射异步反思代理（RRARA），结合轻量级LLM引导的反馈模块和基于规则的代理。

Result: RRARA在延迟敏感场景中显著优于现有基准。

Conclusion: 提出了一个新的评估协议和代理模型RRARA，在高风险动态场景下具有显著的性能提升。

Abstract: In the realm of embodied intelligence, the evolution of large language models
(LLMs) has markedly enhanced agent decision making. Consequently, researchers
have begun exploring agent performance in dynamically changing high-risk
scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under
these extreme conditions, the delay in decision making emerges as a crucial yet
insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that
translates inference delays in decision-making into equivalent simulation
frames, thus aligning cognitive and physical costs under a single FPS-based
metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action
Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we
present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a
lightweight LLM-guided feedback module with a rule-based agent to enable
immediate reactive behaviors and asynchronous reflective refinements in situ.
Experiments on HAZARD show that RRARA substantially outperforms existing
baselines in latency-sensitive scenarios.

</details>


### [131] [Subgoal-Guided Policy Heuristic Search with Learned Subgoals](https://arxiv.org/abs/2506.07255)
*Jake Tuero,Michael Buro,Levi H. S. Lelis*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法，通过学习子目标来提高政策树搜索算法的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的政策树搜索算法虽然表现出色，但在训练过程中需要完整的解决方案轨迹，这导致在较难的实例中学习成本非常高，尤其是在随机初始化政策时。

Method: 本文提出了一种新的基于子目标政策的学习方法，用于政策树搜索算法。子目标及其相关政策是从搜索扩展的树中学习的，包括失败尝试的搜索树。

Result: 经过实验证实，本文提出的政策制定和训练方法提高了在在线设置中学习一个政策和启发函数的样本效率。

Conclusion: 使用子目标可以改善政策树搜索算法的学习过程，并提高样本效率。

Abstract: Policy tree search is a family of tree search algorithms that use a policy to
guide the search. These algorithms provide guarantees on the number of
expansions required to solve a given problem that are based on the quality of
the policy. While these algorithms have shown promising results, the process in
which they are trained requires complete solution trajectories to train the
policy. Search trajectories are obtained during a trial-and-error search
process. When the training problem instances are hard, learning can be
prohibitively costly, especially when starting from a randomly initialized
policy. As a result, search samples are wasted in failed attempts to solve
these hard instances. This paper introduces a novel method for learning
subgoal-based policies for policy tree search algorithms. The subgoals and
policies conditioned on subgoals are learned from the trees that the search
expands while attempting to solve problems, including the search trees of
failed attempts. We empirically show that our policy formulation and training
method improve the sample efficiency of learning a policy and heuristic
function in this online setting.

</details>


### [132] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: 本文提出了一个名为ReVD的新框架，通过推理数据合成和偏好优化来提高LLM在软件漏洞检测任务中的表现，并在实验中获得显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在检测软件漏洞上的能力有限，主要由于缺乏漏洞相关的推理数据和过于关注语义表示而忽略漏洞模式。

Method: ReVD框架通过推理数据合成和漏洞特定的偏好优化来挖掘漏洞模式，包括构建漏洞及修复代码的正反向推理过程、高质量推理数据合成，以及三重监督微调和课程在线偏好优化。

Result: ReVD框架在PrimeVul和SVEN数据集上的实验显著提高了漏洞检测的准确率，提升幅度达12.24%-22.77%。

Conclusion: ReVD在LLM基础上为软件漏洞检测设立了新标杆，显著提升了准确性。

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [133] [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu*

Main category: cs.AI

TL;DR: 提出一种整合LLM和DRL的智能故障自愈机制，大幅提高云AI系统故障恢复效率。


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的云AI系统的规模和复杂性挑战，尤其是系统故障的检测及自适应恢复，以保障服务可靠性和连续性。

Method: 提出了一种基于LLM和DRL的双阶段混合架构，包括故障语义解析模块和DRL恢复策略优化器，以及引入记忆引导的元控制器。

Result: 在云故障注入平台上的实验结果表明，相较于现有的DRL和规则方法，IFSHM框架在未知故障场景下将系统恢复时间缩短了37%。

Conclusion: IFSHM框架在未知故障场景中能够显著缩短系统恢复时间。

Abstract: As the scale and complexity of cloud-based AI systems continue to increase,
the detection and adaptive recovery of system faults have become the core
challenges to ensure service reliability and continuity. In this paper, we
propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates
Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to
realize a fault recovery framework with semantic understanding and policy
optimization capabilities in cloud AI systems. On the basis of the traditional
DRL-based control model, the proposed method constructs a two-stage hybrid
architecture: (1) an LLM-driven fault semantic interpretation module, which can
dynamically extract deep contextual semantics from multi-source logs and system
indicators to accurately identify potential fault modes; (2) DRL recovery
strategy optimizer, based on reinforcement learning, learns the dynamic
matching of fault types and response behaviors in the cloud environment. The
innovation of this method lies in the introduction of LLM for environment
modeling and action space abstraction, which greatly improves the exploration
efficiency and generalization ability of reinforcement learning. At the same
time, a memory-guided meta-controller is introduced, combined with
reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve
continuous adaptation to new failure modes and avoid catastrophic forgetting.
Experimental results on the cloud fault injection platform show that compared
with the existing DRL and rule methods, the IFSHM framework shortens the system
recovery time by 37% with unknown fault scenarios.

</details>


### [134] [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)
*Arnau Igualde Sáez,Lamyae Rhomrasi,Yusef Ahsini,Ricardo Vinuesa,Sergio Hoyas,Jose P. García Sabater,Marius J. Fullana i Alfonso,J. Alberto Conejero*

Main category: cs.AI

TL;DR: 研究分析了多模态大语言模型在数学问题解决中的表现，发现在多语言基准测试中，模型表现中等，且在高级推理方面有待提高。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大语言模型在视觉呈现的数学问题解决中的有效性，特别是在处理图表、多语言文本和符号表示方面的表现。

Method: 评估多个MLLMs模型在多语言Kangaroo风格基准测试中的表现，测试语言包括英语、法语、西班牙语和加泰罗尼亚语，主要关注几何、视觉代数、逻辑、模式和组合学等领域。

Result: 实验揭示了四项主要发现：1. 总体精度在各个话题上表现中等，无单一模型在所有主题上表现优异。2. 在缺少图像的问题上，大多数模型准确性略微改善，但增幅有限。3. 存在语言和难度级别差异：模型在简单项目上表现良好，但在高级几何和组合推理上表现不佳。4. Gemini 2.0 Flash在基于图像的任务中表现最好，尽管没有模型达到人类水平。分析表明，Gemini和GPT 4o在结构化推理和一致性上表现突出。

Conclusion: 尽管多模态大语言模型在特定任务中表现出色，但仍存在显著的改进空间，特别是在高级推理和多语言能力方面。

Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language
capabilities, yet their effectiveness in visually presented mathematics remains
underexplored. This paper analyzes the development and evaluation of MLLMs for
mathematical problem solving, focusing on diagrams, multilingual text, and
symbolic notation. We then assess several models, including GPT 4o, Pixtral,
Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual
Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our
experiments reveal four key findings. First, overall precision remains moderate
across geometry, visual algebra, logic, patterns, and combinatorics: no single
model excels in every topic. Second, while most models see improved accuracy
with questions that do not have images, the gain is often limited; performance
for some remains nearly unchanged without visual input, indicating
underutilization of diagrammatic information. Third, substantial variation
exists across languages and difficulty levels: models frequently handle easier
items but struggle with advanced geometry and combinatorial reasoning. Notably,
Gemini 2.0 Flash achieves the highest precision on image based tasks, followed
by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.
Fourth, a complementary analysis aimed at distinguishing whether models reason
or simply recite reveals that Gemini and GPT 4o stand out for their structured
reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less
consistent reasoning, often defaulting to heuristics or randomness when unable
to align their outputs with the given answer options.

</details>


### [135] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: 提出了HeTa，一种能够在不同HGNNs间一般化的关系感知异构图基础攻击模型，具有强大的攻击性能和普遍性。


<details>
  <summary>Details</summary>
Motivation: 鉴于HGNNs的脆弱性和现有攻击方法的复杂性，我们希望设计一个可以在不同HGNNs间进行一般化并适应新环境的基础攻击模型。

Method: 采用基础代理模型来对齐异质性并识别重要的共享关系感知攻击单元，进而实现基于关系权重的序列化关系攻击。

Result: 实验结果显示该方法具有强大的攻击性能和普遍性。

Conclusion: 我们提出了一种新的关系感知异构图基础攻击模型HeTa，能够针对不同的HGNNs进行一般化扰动，并快速适应新的异构图。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [136] [LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning](https://arxiv.org/abs/2506.07443)
*Weijie Shi,Han Zhu,Jiaming Ji,Mengze Li,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: LegalReasoner通过逐步验证和纠正推理过程，提高了法律判决预测的可靠性，显著改善了与法院判决的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在进行复杂法律推理时往往出现逻辑错误。

Method: 通过逐步验证和纠正推理过程来增强法律判决预测的可靠性。首先识别争议点以分解复杂案件，然后进行逐步推理，并利用过程验证器从正确性、进展和潜在性角度验证每一步逻辑。当检测到错误时，应用专家设计的归因和解决策略进行纠正。

Result: 通过实验表明，LegalReasoner显著提高了法律判决预测与法院决策的一致性。

Conclusion: LegalReasoner显著提高了与法院判决的一致性，从72.37提升到80.27。

Abstract: Legal judgment prediction (LJP) aims to function as a judge by making final
rulings based on case claims and facts, which plays a vital role in the
judicial domain for supporting court decision-making and improving judicial
efficiency. However, existing methods often struggle with logical errors when
conducting complex legal reasoning. We propose LegalReasoner, which enhances
LJP reliability through step-wise verification and correction of the reasoning
process. Specifically, it first identifies dispute points to decompose complex
cases, and then conducts step-wise reasoning while employing a process verifier
to validate each step's logic from correctness, progressiveness, and potential
perspectives. When errors are detected, expert-designed attribution and
resolution strategies are applied for correction. To fine-tune LegalReasoner,
we release the LegalHK dataset, containing 58,130 Hong Kong court cases with
detailed annotations of dispute points, step-by-step reasoning chains, and
process verification labels. Experiments demonstrate that LegalReasoner
significantly improves concordance with court decisions from 72.37 to 80.27 on
LLAMA-3.1-70B. The data is available at
https://huggingface.co/datasets/weijiezz/LegalHK.

</details>


### [137] [Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification](https://arxiv.org/abs/2506.07446)
*Liwen Zheng,Chaozhuo Li,Zheng Liu,Feiran Huang,Haoran Jia,Zaisheng Ye,Xi Zhang*

Main category: cs.AI

TL;DR: AFEV框架能够有效地提高事实核查的准确性，通过分解复杂的主张来实现细粒度的证据检索和自适应推理。


<details>
  <summary>Details</summary>
Motivation: 传统的事实核查方法在处理复杂的多跳推理时表现不佳，因此需要一种新的方法来提高核查的准确性。

Method: AFEV框架通过迭代的事实提取和重新排序证据来减少错误传播。

Result: AFEV在五个基准数据集上的实验中表现出色，达到了最新的性能。

Conclusion: AFEV框架提高了复杂事实核查任务的准确性和可解释性。

Abstract: Fact verification plays a vital role in combating misinformation by assessing
the veracity of claims through evidence retrieval and reasoning. However,
traditional methods struggle with complex claims requiring multi-hop reasoning
over fragmented evidence, as they often rely on static decomposition strategies
and surface-level semantic retrieval, which fail to capture the nuanced
structure and intent of the claim. This results in accumulated reasoning
errors, noisy evidence contamination, and limited adaptability to diverse
claims, ultimately undermining verification accuracy in complex scenarios. To
address this, we propose Atomic Fact Extraction and Verification (AFEV), a
novel framework that iteratively decomposes complex claims into atomic facts,
enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically
refines claim understanding and reduces error propagation through iterative
fact extraction, reranks evidence to filter noise, and leverages
context-specific demonstrations to guide the reasoning process. Extensive
experiments on five benchmark datasets demonstrate that AFEV achieves
state-of-the-art performance in both accuracy and interpretability.

</details>


### [138] [Efficient Generation of Diverse Cooperative Agents with World Models](https://arxiv.org/abs/2506.07450)
*Yi Loo,Akshunn Trivedi,Malika Meghjani*

Main category: cs.AI

TL;DR: 提出了一种利用世界模型生成模拟轨迹的XPM-WM方法，提高XPM的采样效率，能够生成多样化且高效的伙伴。


<details>
  <summary>Details</summary>
Motivation: 现有的交叉游戏最小化（XPM）方法在种群生成时耗费巨大而且采样效率低下，所有伙伴都从头开始训练，未能利用相同任务的协调学习。

Method: 引入了一种名为XPM-WM的框架，利用通过学习得来的世界模型（World Model）生成XPM的模拟轨迹。

Result: 实验结果表明，利用模拟轨迹的XPM方法能够去除多重轨迹采样需求，并生成具有多样性协作习惯的伙伴，其在SP种群训练奖励方面的表现可与以往方法相媲美，同时更高效并具备更大的规模拓展能力。

Conclusion: 我们提出的XPM-WM方法通过模拟轨迹可以大大加速以往XPM方法的训练过程，提高采样效率并能够更大规模地生成多样性合作伙伴。

Abstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC)
agents is the generation of partner agents that are diverse in collaborative
conventions. Current Cross-play Minimization (XPM) methods for population
generation can be very computationally expensive and sample inefficient as the
training objective requires sampling multiple types of trajectories. Each
partner agent in the population is also trained from scratch, despite all of
the partners in the population learning policies of the same coordination task.
In this work, we propose that simulated trajectories from the dynamics model of
an environment can drastically speed up the training process for XPM methods.
We introduce XPM-WM, a framework for generating simulated trajectories for XPM
via a learned World Model (WM). We show XPM with simulated trajectories removes
the need to sample multiple trajectories. In addition, we show our proposed
method can effectively generate partners with diverse conventions that match
the performance of previous methods in terms of SP population training reward
as well as training partners for ZSC agents. Our method is thus, significantly
more sample efficient and scalable to a larger number of partners.

</details>


### [139] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: ReLIFT结合RL和监督微调，大幅提升LLM推理能力，超过单一方法，并显现出良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习无法促使模型突破其固有限制。通过监督微调，可以弥补RL不能习得的新知识和推理模式。

Method: 使用ReLIFT方法，将强化学习与高质量数据的监督微调交替进行，以增强模型的推理能力。

Result: ReLIFT在五个竞赛级基准测试和一个分布外基准测试中平均提升超过5.2分，并且在仅使用13%的演示数据情况下，超过了单纯使用RL和SFT的方法。

Conclusion: ReLIFT显著提高了模型的推理能力，突破了RL的固有局限，并且在有限数据的条件下表现优异。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [140] [Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification](https://arxiv.org/abs/2506.07528)
*Qisheng Hu,Quanyu Long,Wenya Wang*

Main category: cs.AI

TL;DR: 提出HARIS模型，通过分层代理推理与搜索提高多跳声明验证性能，在基准测试中效果显著。


<details>
  <summary>Details</summary>
Motivation: 多跳声明验证需要多步骤推理，构建确认链并搜索信息以揭示隐藏的事实，推理与搜索过程是交织的，因此需要一个能够协调这些过程的模型。

Method: 提出了分层代理推理与信息搜索(HARIS)模型，包括一个高层推理代理和一个低层搜索代理，通过强化学习进行训练并根据结果给予奖励。

Result: 实验结果表明，在EX-FEVER和HOVER基准测试上，HARIS模型取得了很好的性能表现。

Conclusion: HARIS模型提升了多跳声明验证的准确性和可解释性，在EX-FEVER和HOVER基准测试中表现良好，显著推进了该领域的研究。

Abstract: Multi-hop claim verification is inherently challenging, requiring multi-step
reasoning to construct verification chains while iteratively searching for
information to uncover hidden bridging facts. This process is fundamentally
interleaved, as effective reasoning relies on dynamically retrieved evidence,
while effective search demands reasoning to refine queries based on partial
information. To achieve this, we propose Hierarchical Agent Reasoning and
Information Search (HARIS), explicitly modeling the coordinated process of
reasoning-driven searching and search-informed reasoning. HARIS consists of a
high-level reasoning agent that focuses on constructing the main verification
chain, generating factual questions when more information is needed, and a
low-level search agent that iteratively retrieves more information, refining
its search based on intermediate findings. This design allows each agent to
specialize in its respective task, enhancing verification accuracy and
interpretability. HARIS is trained using reinforcement learning with
outcome-based rewards. Experimental results on the EX-FEVER and HOVER
benchmarks demonstrate that HARIS achieves strong performance, greatly
advancing multi-hop claim verification.

</details>


### [141] [Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.07548)
*Weiqiang Jin,Hongyang Du,Guizhong Liu,Dong In Kim*

Main category: cs.AI

TL;DR: 引入动态课程学习框架及反事实优势机制来提升多智能体强化学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前绝大多数多智能体强化学习方法倾向于在固定对手策略下训练，这限制了其适应动态环境的能力，并导致次优策略的出现。

Method: 提出了一种动态课程学习框架，并开发了一个用于稳定学习过程的反事实群体相对策略优势（CGRPA）机制。

Result: 实验表明该方法在训练稳定性和最终性能方面有所提高，并在与最新方法对比中取得竞争性结果。

Conclusion: 动态课程学习框架结合反事实优势机制能有效提升多智能体强化学习的适应性和表现。

Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.

</details>


### [142] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/abs/2506.07553)
*Jingchao Wang,Haote Yang,Jiang Wu,Yifan He,Xingjian Wei,Yinfan Wang,Chengjin Liu,Lingli Ge,Lijun Wu,Bin Wang,Dahua Lin,Conghui He*

Main category: cs.AI

TL;DR: GTR-Mol-VLM is a new framework for optical chemical structure recognition, offering improved accuracy over existing models by implementing a novel graph traversal mechanism and data-centric principles.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in current VLMs' image-captioning approaches, which struggle with complex molecular structures and inconsistent annotations in optical chemical structure recognition.

Method: The paper introduces the GTR-Mol-VLM framework with two key innovations: the 'Graph Traversal as Visual Chain of Thought' mechanism and the 'Faithfully Recognize What You've Seen' principle. It also develops a large-scale dataset (GTR-CoT-1.3M) and a benchmark (MolRec-Bench) for evaluating graph-parsing accuracy.

Result: GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points in both SMILES-based and graph-based metrics when dealing with molecular images featuring functional group abbreviations.

Conclusion: GTR-Mol-VLM significantly outperforms existing models in Optical Chemical Structure Recognition (OCSR), particularly in handling molecular images with functional group abbreviations.

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought}
mechanism that emulates human reasoning by incrementally parsing molecular
graphs through sequential atom-bond predictions, and (2) the data-centric
principle of \textit{Faithfully Recognize What You've Seen}, which addresses
the mismatch between abbreviated structures in images and their expanded
annotations. To support model development, we constructed GTR-CoT-1.3M, a
large-scale instruction-tuning dataset with meticulously corrected annotations,
and introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [143] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW是一个用于LLM/VLM代理构建的框架，旨在增强信息流控制及安全性。通过实验表明，它在复杂环境中比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架缺乏安全的信息流、可靠性和多代理协调机制。

Method: SAFEFLOW通过细粒度的信息流控制、事务执行和冲突解决等机制，确保代理能够在多个环境中进行可靠操作。

Result: 使用SAFEFLOW构建的代理在恶劣环境下仍能保持高任务性能和安全保障，显著优于现有技术。

Conclusion: SAFEFLOW及其评估基准SAFEFLOWBENCH为构建可靠自治代理生态系统提供了基础。

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [144] [Automating Exploratory Multiomics Research via Language Models](https://arxiv.org/abs/2506.07591)
*Shang Qu,Ning Ding,Linhai Xie,Yifei Li,Zaoqu Liu,Kaiyan Zhang,Yibai Xiong,Yuxin Zuo,Zhangren Chen,Ermo Hua,Xingtai Lv,Youbang Sun,Yang Li,Dong Li,Fuchu He,Bowen Zhou*

Main category: cs.AI

TL;DR: PROTEUS automates hypothesis generation in clinical proteogenomics, producing 360 hypotheses from 10 datasets, and showing promise in automating specialized scientific research.


<details>
  <summary>Details</summary>
Motivation: Effective downstream data analysis and hypothesis proposal are critical for novel discoveries in clinical proteogenomics, necessitating an automated system like PROTEUS.

Method: PROTEUS simulates scientific processes using separate modules for data exploration, statistical analysis, and hypothesis proposal, managing these with unified graph structures.

Result: Applied to 10 published datasets, PROTEUS generated 360 hypotheses, which were evaluated using external data validation and scoring.

Conclusion: PROTEUS successfully generates hypotheses from complex multiomics datasets and represents a promising approach for automating scientific research in specialized domains.

Abstract: This paper introduces PROTEUS, a fully automated system that produces
data-driven hypotheses from raw data files. We apply PROTEUS to clinical
proteogenomics, a field where effective downstream data analysis and hypothesis
proposal is crucial for producing novel discoveries. PROTEUS uses separate
modules to simulate different stages of the scientific process, from open-ended
data exploration to specific statistical analysis and hypothesis proposal. It
formulates research directions, tools, and results in terms of relationships
between biological entities, using unified graph structures to manage complex
research processes. We applied PROTEUS to 10 clinical multiomics datasets from
published research, arriving at 360 total hypotheses. Results were evaluated
through external data validation and automatic open-ended scoring. Through
exploratory and iterative research, the system can navigate high-throughput and
heterogeneous multiomics data to arrive at hypotheses that balance reliability
and novelty. In addition to accelerating multiomic analysis, PROTEUS represents
a path towards tailoring general autonomous systems to specialized scientific
domains to achieve open-ended hypothesis generation from data.

</details>


### [145] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)
*Haoran Wang,Zhenyu Hou,Yao Wei,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: SWE-Dev是基于开源LLM构建的SWE代理，通过创新的测试用例合成流程和高效的训练数据构建方法，在SWE-bench-Verified测试中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 目前构建有效的软件工程代理仍然面临挑战，主要是由于缺乏高质量的训练数据和有效的测试用例。

Method: 我们开发了一套坚实的流程来合成用于补丁评估的测试用例，并扩大了代理轨迹以构建SWE-Dev的训练数据。

Result: SWE-Dev模型在SWE-bench-Verified基准上表现卓越，超过了所有现有的开源软件工程代理。

Conclusion: SWE-Dev在SWE-bench-Verified基准测试中表现优异，7B和32B参数模型的成功率分别达到23.4%和36.6%，超过了现有的开源模型。

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [146] [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)
*Yunhe Yan,Shihe Wang,Jiajun Du,Yexuan Yang,Yuxuan Shan,Qichen Qiu,Xianqing Jia,Xinge Wang,Xin Yuan,Xu Han,Mao Qin,Yinxiao Chen,Chen Peng,Shangguang Wang,Mengwei Xu*

Main category: cs.AI

TL;DR: MCPWorld是一个新颖的CUA测试平台，支持API、GUI和混合代理。它使用白盒应用程序以增加设计空间和验证任务完成度，促进和标准化下一代CUA的基准测试。初步实验显示出良好的任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有的CUA基准主要针对GUI代理，其评估方法容易受到UI更改的影响，并忽略了通过应用程序API暴露的功能交互, 对此，我们提议MCPWorld来克服这些限制。

Method: 我们提出了MCPWorld，这是第一个针对API、GUI和API-GUI混合代理的自动化CUA试验台。MCPWorld的一个核心原则是使用"白盒应用程序"，即那些具有源代码可用性并可以根据需要修订/重新编译的应用程序。

Result: MCPWorld当前包括201个精心策划和注释的用户任务，涵盖多样化的用例和难度等级。系统还完全容器化，支持GPU加速，以便在不同操作系统/硬件环境下灵活使用。初步实验显示，使用LLM驱动的CUA框架可达到75.12%的任务完成准确率。

Conclusion: 我们的初步实验使用一个有代表性的LLM驱动CUA框架，达到了75.12%的任务完成率，这为利用MCP进行代理自动化的实际效果提供了初步证据。我们预期MCPWorld能够促进和标准化下一代计算机使用代理的基准测试，这些代理能利用丰富的外部工具。

Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU acceleration support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.

</details>


### [147] [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)
*Mouadh Yagoubi,Yasser Dahou,Billel Mokeddem,Younes Belkada,Phuc H. Le-Khac,Basma El Amel Boussaha,Reda Alami,Jingwei Zuo,Damiano Marsili,Mugariya Farooq,Mounia Lalmas,Georgia Gkioxari,Patrick Gallinari,Philip Torr,Hakim Hacid*

Main category: cs.AI

TL;DR: 提出竞赛以设计专门用于测量语言模型早期训练进展的科学知识评估任务，提供小型预训练模型和中间检查点，鼓励开发新评估方法，使研究更系统和基准化。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估完全训练的较大语言模型的性能方面效果显著。然而，小型模型在早期训练阶段的评估信号往往不具备显著或能区分的特性。因此，该竞争专注于设计科学知识评估任务，以专门衡量语言模型的早期训练进程。

Method: 提供了三个预训练的小模型，并安排中间检查点，在训练过程中采样达200B个tokens。允许所有实验和开发工作在广泛可用的免费云GPU平台上运行，使得研究人员即使拥有有限的计算资源也能参与。

Result: 鼓励参与者开发新的评估方法或调整现有基准，以更好地捕捉语言模型之间的性能差异。提交作品的评估标准包括：它们生成的性能信号的质量、在1万亿tokens训练时的模型排名一致性，以及它们与科学知识领域的相关性。

Conclusion: 通过促进为早期训练阶段设计量身定制的评估策略，该竞赛旨在吸引来自各种学科的广泛参与者。最终，该倡议旨在使基础大型语言模型研究自模型开发的最早阶段更具系统性和基准参考性。

Abstract: Existing benchmarks have proven effective for assessing the performance of
fully trained large language models. However, we find striking differences in
the early training stages of small models, where benchmarks often fail to
provide meaningful or discriminative signals. To explore how these differences
arise, this competition tackles the challenge of designing scientific knowledge
evaluation tasks specifically tailored for measuring early training progress of
language models. Participants are invited to develop novel evaluation
methodologies or adapt existing benchmarks to better capture performance
differences among language models. To support this effort, we provide three
pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate
checkpoints sampled during training up to 200B tokens. All experiments and
development work can be run on widely available free cloud-based GPU platforms,
making participation accessible to researchers with limited computational
resources. Submissions will be evaluated based on three criteria: the quality
of the performance signal they produce, the consistency of model rankings at 1
trillion tokens of training, and their relevance to the scientific knowledge
domain. By promoting the design of tailored evaluation strategies for early
training, this competition aims to attract a broad range of participants from
various disciplines, including those who may not be machine learning experts or
have access to dedicated GPU resources. Ultimately, this initiative seeks to
make foundational LLM research more systematic and benchmark-informed from the
earliest phases of model development.

</details>


### [148] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)
*Jingnan Zheng,Xiangtian Ji,Yijun Lu,Chenhang Cui,Weixiang Zhao,Gelei Deng,Zhenkai Liang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RSafe是一种通过推理进行引导并强化对齐的安全保护方法，可提高应对新出现或对抗性安全威胁的能力。


<details>
  <summary>Details</summary>
Motivation: LLMs模型在安全对齐方面仍存在漏洞，可能对用户和社会带来显著风险。现有的保护措施依赖大量人工策划的数据集，并且在处理分布外威胁时表现不佳。

Method: RSafe采用两阶段训练模式：首先进行引导推理，分析输入内容的安全风险；其次通过基于规则的强化学习优化推理路径，使其与准确的安全预测保持一致。

Result: RSafe能够在推理阶段接受用户指定的安全策略，根据具体的安全需求提供增强的保护措施。

Conclusion: RSafe通过基于推理的适应性保护方法，有效提升了在未见或对抗性安全违规场景下的安全保护能力。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [149] [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)
*Diego Forniés-Tabuenca,Alejandro Uribe,Urtzi Otamendi,Arkaitz Artetxe,Juan Carlos Rivera,Oier Lopez de Lacalle*

Main category: cs.AI

TL;DR: This paper introduces REMoH, combining NSGA-II and LLM-generated heuristics, to improve multi-objective optimization with reduced modeling effort and high adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-objective optimization algorithms require extensive problem-specific modeling and struggle with nonlinear structures. LLMs offer enhanced explainability and adaptability, motivating their use to improve traditional approaches.

Method: This paper proposes the Reflective Evolution of Multi-objective Heuristics (REMoH) framework, which integrates NSGA-II with LLM-based heuristic generation, including a reflection mechanism using clustering and search-space reflection.

Result: REMoH was evaluated on the Flexible Job Shop Scheduling Problem using three instance datasets and demonstrated competitive performance with state-of-the-art methods while requiring less modeling effort.

Conclusion: REMoH achieves competitive results in multi-objective optimization with reduced modeling effort and enhanced adaptability, demonstrating the potential of LLMs to augment traditional optimization approaches.

Abstract: Multi-objective optimization is fundamental in complex decision-making tasks.
Traditional algorithms, while effective, often demand extensive
problem-specific modeling and struggle to adapt to nonlinear structures. Recent
advances in Large Language Models (LLMs) offer enhanced explainability,
adaptability, and reasoning. This work proposes Reflective Evolution of
Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with
LLM-based heuristic generation. A key innovation is a reflection mechanism that
uses clustering and search-space reflection to guide the creation of diverse,
high-quality heuristics, improving convergence and maintaining solution
diversity. The approach is evaluated on the Flexible Job Shop Scheduling
Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using
three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate
that REMoH achieves competitive results compared to state-of-the-art approaches
with reduced modeling effort and enhanced adaptability. These findings
underscore the potential of LLMs to augment traditional optimization, offering
greater flexibility, interpretability, and robustness in multi-objective
scenarios.

</details>


### [150] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/abs/2506.07807)
*John Laird,Christian Lebiere,Paul Rosenbloom,Andrea Stocco,Robert Wray*

Main category: cs.AI

TL;DR: 文中提出了一种将元认知整合到CMC中的方法，通过调用工作记忆中的认知能力进行推理，并提供了相关例子。


<details>
  <summary>Details</summary>
Motivation: 为了通过在工作记忆中做最小的结构和信息扩展，将元认知有效集成入CMC。

Method: 该方法基于考察代理人在工作记忆中对认知能力和过程的显式表征进行推理。

Result: 通过本提案中的例子，展示了元认知的应用。

Conclusion: 本文提出了一种将元认知整合到CMC（认知的一般模型）中的方法。

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [151] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)
*Jiaxiang CHen,Zhuo Wang,Mingxi Zou,Qifan Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: Guideline Forest框架通过结构化和逐步聚合策略提升大型语言模型的推理性能，优于现有强基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在模拟人类多样性和适应性推理方面存在困难，而现有方法在利用多种可重用策略方面存在不足。

Method: 提出了Guideline Forest框架，通过引导结构化推理策略并通过逐步聚合执行它们，以增强大型语言模型的推理能力。

Result: 在四个基准（GSM8K, MATH-500, MBPP, and HumanEval）上的表现优于强基线方法，展示了其适应性和泛化潜力。

Conclusion: Guideline Forest框架通过利用经过验证的推理经验和可重用指南，提高了模型解决不确定性和综合解决方案的能力。

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


### [152] [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)
*Yao Yan*

Main category: cs.AI

TL;DR: 大型语言模型LLaMA-3-8B-Instruct在多位数加法中展示出分层计算过程，线性解码在早期阶段有效，中后期准确生成结果，表明内部计算优于记忆化。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在多位数加法上的计算能力。

Method: 结合线性探测与logit-lens检查分析模型的内在算术过程。

Result: 深入激活层后，结果的数值抽象变得清晰，实现对和的各个数字的接近完美检测和解码；临近输出时，模型可靠地产生正确结果。

Conclusion: 模型采用分层过程，内部计算优于死记硬背。

Abstract: Multi-digit addition is a clear probe of the computational power of large
language models. To dissect the internal arithmetic processes in
LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.
Inspired by the step-by-step manner in which humans perform addition, we
propose and analyze a coherent four-stage trajectory in the forward
pass:Formula-structure representations become linearly decodable first, while
the answer token is still far down the candidate list.Core computational
features then emerge prominently.At deeper activation layers, numerical
abstractions of the result become clearer, enabling near-perfect detection and
decoding of the individual digits in the sum.Near the output, the model
organizes and generates the final content, with the correct token reliably
occupying the top rank.This trajectory suggests a hierarchical process that
favors internal computation over rote memorization. We release our code and
data to facilitate reproducibility.

</details>


### [153] [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)
*Shijie Wang,Yilun Zhang,Zeyu Lai,Dexing Kong*

Main category: cs.AI

TL;DR: 本论文提出一种用于领域特定数据生成的管道并建立了 ReMUD 数据集，有效提升医学超声领域的多模态模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在一般领域展示了巨大的潜力，但由于缺乏领域特定的数据，在某些特定领域表现不佳，例如医学超声领域中的数据缺乏标准化。

Method: 提出了一种新颖的图文推理监督微调数据生成管道，从特定领域材料中创建四元组（图像、问题、思维过程和答案），并建立了 ReMUD 数据集。

Result: 建立了 ReMUD 数据集，包含超过 45000 的推理和非推理监督精调问答和视觉问答数据。微调的 ReMUD-7B 模型在医学超声领域超过了一般领域的多模态语言模型。

Conclusion: ReMUD-7B 模型在医学超声领域表现优于一般领域的多模态大模型，这通过生成一个特定领域的数据集和训练模型来实现。

Abstract: Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.

</details>


### [154] [A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms](https://arxiv.org/abs/2506.07853)
*Hudson de Martim*

Main category: cs.AI

TL;DR: 本文扩展FRBRoo框架，提出了一个细粒度、时间感知的法律文本版本管理模型，并通过巴西联邦宪法验证其有效性，可用于准确的历史分析。


<details>
  <summary>Details</summary>
Motivation: 现有法律文档模型缺乏细粒度的版本管理，难以实现对法律文本在不同时点的准确重建，这对法律技术和人工智能应用至关重要。

Method: 提出了一种结构化的时间模型，扩展FRBRoo框架，通过引入特定的子类如Temporal Version (TV)和Language Version (LV)，以及Component Work (CW)、Component Temporal Version (CTV)和Component Language Version (CLV)，实现法律规范的细粒度、时间感知的版本管理。

Result: 采用巴西联邦宪法作为案例研究，演示了如何通过每次修正创造新的组件时间版本，使得任何法律文本的任一部分都可以在特定日期进行精确、确定性的检索和重建。

Conclusion: 该模型为开发能够进行准确历史分析和影响评估的先进法律信息系统、知识图谱以及AI工具提供了坚实的基础。

Abstract: Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.

</details>


### [155] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: 本研究探讨现代大型语言模型是否可以解决AI的两个核心哲学问题，发现部分闭源模型在稳定性和表现上具有优势。


<details>
  <summary>Details</summary>
Motivation: 探讨现代大型语言模型是否具备解决哲学上人工智能的核心难题“框架问题”和“符号扎根问题”的认知能力。

Method: 设计两个基准任务以反映每个问题的哲学核心，在零样本条件下对13个知名大型语言模型进行测试，每个模型进行五次试验，并根据多项标准评估模型输出的质量。

Result: 结果表明，开源模型由于模型大小、量化和指令调整的差异而表现出性能差异，而若干闭源模型则始终获得高分。

Conclusion: 部分现代大型语言模型可能具备足够的能力来对长期存在的理论挑战产生有意义和稳定的反应。

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [156] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: 提出了在快速变化环境中提高自治系统决策效果的LUCIFER框架，结合了层次决策、强化学习和大型语言模型，并在测试中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 解决自治系统中存在的内在模型与操作环境快速变化之间的鸿沟问题，目标是提升决策效能。

Method: 提出LUCIFER框架，将分层决策架构与强化学习和大型语言模型结合，使用注意力机制和零样本探索指导行动选择过程。

Result: LUCIFER框架在各种语言模型的测试中表现出色，提高了探索效率和决策质量，优于传统的目标条件策略。

Conclusion: LUCIFER框架利用人类的上下文知识提高自治系统的决策质量和探索效率。

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [157] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 针对不等式证明，提出了新方法和数据集，但顶级大型语言模型表现出推理链薄弱，仅在最终答案上表现较好。


<details>
  <summary>Details</summary>
Motivation: 目前的任务数据集稀缺、合成或过于正式，难以评估模型的真正推理能力。

Method: 提出了一个非正式但可验证的任务，将不等式证明任务重新表述为界限估计和关系预测两部分。

Result: 29个领先的大型语言模型在该数据集上的系统评估显示，即便是顶级模型在逐步审查下准确率不到10%。

Conclusion: 即便是最顶尖的模型，当前的大型语言模型在不等式证明任务中表现不佳，整体步骤准确率不足10%。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [158] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: 去中心化平台Gradients通过经济激励使超参数优化成为竞争性市场，系统性地发现中央化方法未能发现的配置，实验表明其在多个模型架构和任务类型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的AutoML平台依赖单一优化策略，难以全面探索超参数配置，导致微调基础模型面临挑战。

Method: 使用去中心化的平台Gradients，将超参数优化转变为一个竞争性市场，矿工通过经济激励争夺最佳配置。

Result: Gradients在180次实验中对比基准平台表现出明显优势，对HuggingFace AutoTrain取得82.8%的胜率，对TogetherAI、Databricks和Google Cloud取得100%的胜率，均实现显著的性能提升。

Conclusion: 竞争性和经济驱动的AutoML平台能够系统地发现中央化方法未能找到的优化配置。

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


### [159] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 研究提出一种自监督的双重奖励机制，提高大规模多模态模型的图像-文本对齐能力，无需外部监督，特别提升了文本到图像生成任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态模型（LMMs）在图像-文本对齐方面存在挑战，通常会生成与视觉输入相矛盾的文本响应或未能遵循文本到图像的提示。当前的解决方案需要外部监督，并且仅解决单向任务（理解或生成）。

Method: 引入了一种自监督的双重奖励机制，通过在一个任务域中采样多个输出，然后反向输入输出对以计算模型的双重似然性作为自我奖励进行优化。

Result: 实验结果表明，该方法可以有效提高模型的视觉理解和生成性能，特别是在文本到图像的生成任务中取得了显著的改进。

Conclusion: 这项研究提出了一种无需外部监督即可增强大规模多模态模型（LMMs）性能的方法，特别是在文本到图像生成任务中取得了显著的改进。

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [160] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: 本文介绍了一个新的对话AI基准，$	au^2$-bench，用于测试代理在用户共享环境中的协作和通信能力，实验表明多用户环境下的挑战更大。


<details>
  <summary>Details</summary>
Motivation: 现有的对话式AI代理基准大多为单控制环境，用户只是被动的信息提供者。这与现实世界场景（如技术支持）不同，用户需要主动参与修改环境状态。为了解决这一差距，作者提出了新的基准。

Method: 1) 创造了一个新的电信双控制域，通过Dec-POMDP模型描述，2) 设计了一个组成式任务生成器，来创建多样化且可验证的任务，3) 研发了一个与环境紧密耦合的用户模拟器，4) 提供了多种消融实验进行代理性能的细粒度分析。

Result: 实验显示，当代理从无用户到双控制环境时，性能显著下降，说明在指导用户方面存在挑战。

Conclusion: 本文提出了一个新的基准，$	au^2$-bench，用于测试代理在与用户共享控制的动态环境中进行协调和沟通的能力。

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


### [161] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: 提出GUI-Reflection框架，增强多模态GUI模型的自反思和纠错能力，提升GUI自动化的智能性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI模型缺乏反思和错误恢复能力，主要依赖于几乎无错误的离线轨迹学习。我们希望克服这一缺陷。

Method: 采用专门的训练阶段，包括GUI特定预训练、离线监督微调和在线反思调优，集成自反思和纠错能力。

Result: 在移动设备上，我们构建了一个高效的在线训练和数据采集环境，并提出了一种迭代在线反思调优算法，显著提升了模型的反思和纠错能力。

Conclusion: 我们提出了GUI-Reflection，一个新框架，使得多模态GUI模型具备自反思和纠错能力，从而提高GUI自动化的健壮性和适应性。

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [162] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 该论文提出了一种模型，结合智能体的有限理性和不确定性，利用自然语言处理和大型语言模型来生成妥协提案，旨在改善协作文件撰写中的民主过程。


<details>
  <summary>Details</summary>
Motivation: 解决如何在智能体提案之间寻找妥协的问题，并将其应用于合作性文件撰写领域。

Method: 使用自然语言处理技术和大型语言模型，在文本上诱导一个语义度量空间，并在此基础上设计算法，以建议可能获得广泛支持的妥协点。

Result: 通过模拟联盟形成过程，证明AI能够在大规模民主文本编辑（传统工具受限的领域）中发挥作用。

Conclusion: AI methods can facilitate large-scale democratic text editing by generating compromise proposals in collaborative settings.

Abstract: The challenge of finding compromises between agent proposals is fundamental
to AI subfields such as argumentation, mediation, and negotiation. Building on
this tradition, Elkind et al. (2021) introduced a process for coalition
formation that seeks majority-supported proposals preferable to the status quo,
using a metric space where each agent has an ideal point. A crucial step in
this process involves identifying compromise proposals around which agent
coalitions can unite. How to effectively find such compromise proposals remains
an open question. We address this gap by formalizing a model that incorporates
agent bounded rationality and uncertainty, and by developing AI methods to
generate compromise proposals. We focus on the domain of collaborative document
writing, such as the democratic drafting of a community constitution. Our
approach uses natural language processing techniques and large language models
to induce a semantic metric space over text. Based on this space, we design
algorithms to suggest compromise points likely to receive broad support. To
evaluate our methods, we simulate coalition formation processes and show that
AI can facilitate large-scale democratic text editing, a domain where
traditional tools are limited.

</details>


### [163] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: LIET框架通过个体学习和团队进化提高LLMs在多代理环境中的规划和合作能力，实验结果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM规划算法在适应多代理具体场景方面能力较弱，因此需要一种能够在测试前和测试期间学习和进化的框架，以提升环境相关知识和通信能力。

Method: LIET（Learn as Individuals, Evolve as a Team）范式，通过个体学习和团队进化，结合集中训练和分散执行的理念，使LLMs适应多代理环境。

Result: LIET框架在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准测试中表现优于现有基线，展示了强大的合作规划能力。

Conclusion: LIET框架使得LLMs在多代理环境中能够更好地进行复杂规划和合作，通过个体学习和团队进化实现全面灵活的适应能力。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [164] [Digital Twin-based Smart Manufacturing: Dynamic Line Reconfiguration for Disturbance Handling](https://arxiv.org/abs/2506.07332)
*Bo Fu,Mingjie Bi,Shota Umeda,Takahiro Nakano,Youichi Nonaka,Quan Zhou,Takaharu Matsui,Dawn M. Tilbury,Kira Barton*

Main category: cs.MA

TL;DR: 本文提出了一种新的动态制造线重构框架，通过数字孪生监控、能力模型优化和快速模拟，成功应对生产干扰，并避免了系统吞吐量的大幅度下降。


<details>
  <summary>Details</summary>
Motivation: 现代制造业的复杂性、多样化需求、供应链不确定性、产品定制化，要求制造系统具备灵活更新配置和快速适应扰动的能力。现有研究并未能提供这种全面性的解决方案。

Method: 提出了一种动态制造线重构框架，包括：监控干扰和触发重构的数字孪生系统、捕捉可用代理和资源选项的能力本体模型、生成最优线配置的优化器、初始化模拟并评估线配置的程序。

Result: 在电池生产线的案例研究中，框架成功在两种干扰情境下恢复系统吞吐量。在没有重构计划的情况下，吞吐量分别下降26%和63%，而框架能够避免这种情况。重构优化器在0.03秒内为具有51个操作和40个代理的制造线找到最优重构方案。

Conclusion: 本文提出的动态制造线重构框架能够有效应对制造过程中的干扰，维持系统吞吐量，即便资源有限也能恢复生产能力。

Abstract: The increasing complexity of modern manufacturing, coupled with demand
fluctuation, supply chain uncertainties, and product customization, underscores
the need for manufacturing systems that can flexibly update their
configurations and swiftly adapt to disturbances. However, current research
falls short in providing a holistic reconfigurable manufacturing framework that
seamlessly monitors system disturbances, optimizes alternative line
configurations based on machine capabilities, and automates simulation
evaluation for swift adaptations. This paper presents a dynamic manufacturing
line reconfiguration framework to handle disturbances that result in operation
time changes. The framework incorporates a system process digital twin for
monitoring disturbances and triggering reconfigurations, a capability-based
ontology model capturing available agent and resource options, a configuration
optimizer generating optimal line configurations, and a simulation generation
program initializing simulation setups and evaluating line configurations at
approximately 400x real-time speed. A case study of a battery production line
has been conducted to evaluate the proposed framework. In two implemented
disturbance scenarios, the framework successfully recovers system throughput
with limited resources, preventing the 26% and 63% throughput drops that would
have occurred without a reconfiguration plan. The reconfiguration optimizer
efficiently finds optimal solutions, taking an average of 0.03 seconds to find
a reconfiguration plan for a manufacturing line with 51 operations and 40
available agents across 8 agent types.

</details>


### [165] [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)
*Yun Hua,Haosheng Chen,Shiqin Wang,Wenhao Li,Xiangfeng Wang,Jun Luo*

Main category: cs.MA

TL;DR: 提出Shapley-Coop工作流，结合Shapley链式思维和协商协议，促进LLM代理的协作与公平信用分配，实验结果显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多代理系统中表现优秀并逐渐参与复杂的人机协作，对公平有效的定价机制需求变得迫切，以确保公正的补偿和责任分配。

Method: 提出了一种合作工作流Shapley-Coop，将Shapley链式思维与结构化的协商协议结合起来，以实现有效的价格匹配，使LLM代理能通过理性的任务时间定价和后任务奖励重新分配进行协调。

Result: 在两个多代理游戏和一个软件工程模拟中评估了Shapley-Coop，证明了其能够持续增强LLM代理协作并促进公平的信用分配。

Conclusion: Shapley-Coop的定价机制在任务执行过程中有效反映了个体贡献的准确性。

Abstract: Large Language Models (LLMs) show strong collaborative performance in
multi-agent systems with predefined roles and workflows. However, in open-ended
environments lacking coordination rules, agents tend to act in self-interested
ways. The central challenge in achieving coordination lies in credit assignment
-- fairly evaluating each agent's contribution and designing pricing mechanisms
that align their heterogeneous goals. This problem is critical as LLMs
increasingly participate in complex human-AI collaborations, where fair
compensation and accountability rely on effective pricing mechanisms. Inspired
by how human societies address similar coordination challenges (e.g., through
temporary collaborations such as employment or subcontracting), we propose a
cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley
Chain-of-Thought -- leveraging marginal contributions as a principled basis for
pricing -- with structured negotiation protocols for effective price matching,
enabling LLM agents to coordinate through rational task-time pricing and
post-task reward redistribution. This approach aligns agent incentives, fosters
cooperation, and maintains autonomy. We evaluate Shapley-Coop across two
multi-agent games and a software engineering simulation, demonstrating that it
consistently enhances LLM agent collaboration and facilitates equitable credit
assignment. These results highlight the effectiveness of Shapley-Coop's pricing
mechanisms in accurately reflecting individual contributions during task
execution.

</details>


### [166] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 引入G-Memory内存系统，通过层次化记忆结构提升多智能体系统表现，显著提高成功率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的内存机制过于简单，忽视了代理之间合作的复杂轨迹，并缺乏个性化定制，与单一智能体的记忆相比相差甚远。

Method: 提出了一种称为G-Memory的层次化智能体内存系统，该系统借鉴了组织记忆理论，通过三层图层次结构来管理多智能体系统的交互。

Result: G-Memory在五个基准、三种大型语言模型和三种流行多智能体系统框架上的实验表明，其成功率最高提升20.89%，知识问答准确性提升10.12%。

Conclusion: G-Memory显著提高了多智能体系统在具体现实行为和知识问答上的成功率和准确性，而无需更改原有框架。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [167] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: MedChat enhances medical imaging diagnostics by using multiple specialized agents to improve reliability and interaction, overcoming limitations of general LLMs in medical contexts.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of applying general LLMs to medical imaging, such as hallucinations, lack of interpretability, and insufficient domain-specific knowledge, and to improve the emulation of multidisciplinary medical team reasoning.

Method: MedChat employs a multi-agent diagnostic framework where specialized vision models are combined with multiple role-specific LLM agents, coordinated by a director agent, to enhance interaction and reliability in clinical use.

Result: The use of MedChat reduces the risk of hallucinations, improves reliability, and allows for interactive diagnostic reporting suitable for clinical review and education.

Conclusion: MedChat offers a more reliable and interactive diagnostic reporting platform by using a multi-agent approach combining specialized vision models and role-specific LLM agents, addressing the limitations of using single generalist agents in medical imaging.

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


### [168] [Diffusion of Responsibility in Collective Decision Making](https://arxiv.org/abs/2506.07935)
*Pavel Naumov,Jia Tao*

Main category: cs.MA

TL;DR: 研究表明，为避免责任扩散，两代理中需有独裁者，多代理需选举独裁者决策。


<details>
  <summary>Details</summary>
Motivation: 研究责任扩散问题对集体决策机制中的个体责任感到模糊的现象，尤其是在多个代理共享责任情况下。

Method: 通过定义决策机制的双仿真，证明双仿真保持与责任相关的属性，并在此基础上为一个最小的双仿真机制建立结果。

Result: 在两个代理的情况下，避免责任扩散的方法是让一个代理成为独裁者；在多个代理情况下，任何避免责任扩散的机制都是“选举独裁”机制。

Conclusion: 在多代理的决策场景中，避免责任扩散的唯一机制是在两个代理的情况下让一个代理成为独裁者，而在多个代理的情况下，机制必须是通过选举产生一个代理来单方面决策，这称为“选举独裁”。

Abstract: The term "diffusion of responsibility'' refers to situations in which
multiple agents share responsibility for an outcome, obscuring individual
accountability. This paper examines this frequently undesirable phenomenon in
the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to
avoid diffusion of responsibility is for one agent to act as a "dictator'',
making the decision unilaterally. In scenarios with more than two agents, any
diffusion-free mechanism is an "elected dictatorship'' where the agents elect a
single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of
decision-making mechanisms, proving that bisimulation preserves
responsibility-related properties, and establishing the results for a smallest
bisimular mechanism.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [169] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 引入CellCLIP框架，通过对比学习方法改进高内涵筛选数据的解析，优于目前的模型。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解不同干扰之间的关系及其对细胞状态的影响，结合现有的高内涵筛选数据，需要开发一种方法来学习统一的潜在空间，使得干扰及其对应的形态效应能够对齐。

Method: 引入了CellCLIP框架，这是一种用于高内涵筛选数据的跨模态对比学习方法。它利用预训练的图像编码器和新颖的通道编码方案来更好地捕捉不同显微镜通道之间的关系，同时使用自然语言编码器来表示干扰。

Result: CellCLIP框架在跨模态检索和下游任务中表现出色，并显著减少了计算时间。

Conclusion: CellCLIP框架优于当前的开源模型，在跨模态检索及生物学有意义的下游任务中表现最好，同时大大减少了计算时间。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [170] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: A learning-based framework using GNNs, BC, and RL reduces MILP computation times in real-time tasks, maintaining solution quality.


<details>
  <summary>Details</summary>
Motivation: Mixed Integer Linear Programs (MILPs) are widely used in industries such as construction, manufacturing, and logistics for planning and scheduling, but they suffer from long computational times in large-scale, real-time scenarios.

Method: The paper presents a learning-based framework that uses Behavior Cloning and Reinforcement Learning to train Graph Neural Networks, which generate high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems.

Result: Experimental results show that the proposed method reduces optimization time and variance compared to traditional techniques, while maintaining solution quality and feasibility.

Conclusion: The proposed learning-based framework effectively reduces computational time for MILPs without sacrificing solution quality, making it viable for real-time applications in critical industries.

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [171] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: 提出了一种自训练方法Mutual-Taught，通过模拟EM算法迭代优化政策模型和奖励模型，无需额外人工标注，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 偏好优化中产生的分布变化降低了奖励模型（RM）的有效性，反过来影响了政策模型（PM）的性能。需要一种既能提升PM，又能适应分布变化的RM的方法。

Method: 提出了Mutual-Taught自训练方法，通过模拟期望最大化（EM）算法，从而在E步中使用当前的RM反馈更新PM，并在M步中通过PM的输出构建训练数据来更新RM。

Result: 实验表明提出的方法使得8B政策模型LLaMA-3-8B-Instruct-MT在AlpacaEval-2上达到54.1%的长度控制胜率，而8B奖励模型FsfairX-LLaMA3-RM-MT在RewardBench上的表现可与GPT-4o-2024-08-06媲美。

Conclusion: 通过提出的Mutual-Taught方法，可以在不需要额外人工标注的情况下，迭代提升政策模型（PM）和奖励模型（RM）的性能。实验结果表明，这种迭代方法能够为两个模型带来持续的改进。

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [172] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 研究引入持久同源性与传统网络构建异质网络，改进银行信用评级。实验表明新方法效果优异。


<details>
  <summary>Details</summary>
Motivation: 在银行间连接图不完整且保密的情况下，改善银行信用评级的预测效果。

Method: 利用持久同源性构建银行间关系网络，并结合传统贷款网络，形成异质网络，通过改进的图神经网络方法进行评级预测。

Result: 实验证明，HTGNN在全球真实数据集上的预测效果优于传统方法。

Conclusion: 本研究有效提高了银行信用评级预测的准确性。

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [173] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: GLProtein是一种创新的蛋白质预训练框架，结合蛋白质的全局结构和局部氨基酸信息以提高预测精度，实验结果证明其性能优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 目前对蛋白质结构信息的理解仍有改进的空间，尤其是如何整合蛋白质的3D信息和局部的氨基酸分子信息，从而提高预测精度和功能洞察。

Method: 提出GLProtein框架，通过结合蛋白质掩膜模型和三元组结构相似性评分、蛋白质三维距离编码及基于子结构的氨基酸分子编码来实现对蛋白质的预训练。

Result: 实验结果表明，GLProtein在预测蛋白质-蛋白质相互作用、接触预测等多个生物信息学任务中优于现有方法。

Conclusion: GLProtein框架有效整合了蛋白质的全局结构相似性和局部氨基酸细节，提升了蛋白质功能预测的准确性。

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [174] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 提出了dLLM-Cache缓存框架，大幅提升dLLM模型的推理速度，接近传统ARMs。


<details>
  <summary>Details</summary>
Motivation: 克服dLLMs的高推理延迟问题，并兼容其双向注意机制。

Method: 提出了一个无需训练的自适应缓存框架dLLM-Cache，通过长间隔提示缓存和基于特征相似度的部分响应更新来加速推理。

Result: dLLM-Cache在不影响模型性能的情况下，实现了推理速度最高9.1倍的提升，推理延迟接近ARMs。代码将在GitHub上公开。

Conclusion: dLLM-Cache能够显著加速dLLM推理而不牺牲输出质量，使其推理延迟接近传统ARMs。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [175] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: 本文提出的Jacobi-KAN-DGCNN在三维点云分类中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 希望通过将Jacobi-KAN框架整合进DGCNN架构中，改进其分类效果，尤其是在保持参数效率的同时提高准确性和收敛速度。

Method: 将Dynamic Graph Convolutional Neural Network (DGCNN)与Jacobi Kolmogorov-Arnold Networks (KAN)集成用于三维点云分类，并用自适应单变量多项式扩展替换DGCNN中的多层感知机层。

Result: 在ModelNet40数据集上的对比实验中，使用Jacobi多项式的KAN层在准确性和收敛速度上优于传统DGCNN。

Conclusion: 本研究引入的Jacobi-KAN-DGCNN框架能够在准确性和收敛速度上优于传统的线性层基础的DGCNN，同时保持参数效率。

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [176] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: 研究使用强化学习优化医院超声心动图检查的资源分配策略，结果显示实时策略能更好适应变化并提高效率。


<details>
  <summary>Details</summary>
Motivation: 安排医院里的超声心动图检查存在许多挑战，包括患者未出现、到达时间、检查时间不确定等因素，以及胎儿与非胎儿病患资源的非对称约束。

Method: 利用SimPy库开发了离散事件随机模拟模型，并与Gymnasium Python库集成。通过对比实时分配和预约分配策略，对资源分配进行了优化。

Result: 实时分配策略表现更佳，更有效地适应了患者的变动和资源的约束。基于此，应用了强化学习（RL）来推导近似最优动态分配策略，并与最佳规则策略进行比较。

Conclusion: 通过智能化、数据驱动的资源管理，强化学习策略可以显著提高超声心动图实验室效率。

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [177] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: 通过多种奖励函数的分布反映人类偏好的多样性，提出的实用训练启发能够提高对多元化价值观的忠实表现。


<details>
  <summary>Details</summary>
Motivation: 目前的对齐管道假定的唯一统一的理想行为未必能够适应用户、情境和文化的不同偏好，使得少数群体观点被忽视。为解决这一问题，需要通过多种奖励函数的分布来反映人类偏好的多样性。

Method: 使用基于成对校准的中心标准，通过一组奖励函数的分布来反映多种人类偏好。这些函数从成对偏好中直接学习，而无需标注者身份或预定义组。偏好中的分歧被视为有意义的软标签。

Result: 提出了一种实用的训练启发来学习这种集成模型，并验证了其有效性，通过改进的校准来提升对多元化价值观的忠实表现。

Conclusion: 即使是在只有少数非异常个体的情况下，集成模型也能够准确地代表多样化的偏好分布。提出的训练启发算法被验证有效，能够更好反映多元化的价值观。

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [178] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs提供了一种无需手动插值的边界优化新方法，在复杂拓扑条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的PINNs依赖基于密度的拓扑描述，需手动插值，且不适用于复杂几何结构。提出LT-PINNs以解决这些局限性，实现精确边界的确定。

Method: LT-PINNs通过将拓扑边界曲线的控制变量参数化为可学习参数，消除了手动插值的需求。引入了特殊的边界条件损失函数和拓扑损失函数以确保精确的边界表示。

Result: LT-PINNs在相较于DT-PINNs具有更低的相对L2误差，能够处理任意边界条件，并且在复杂拓扑条件下无需手动插值即可推断出清晰的拓扑边界。

Conclusion: LT-PINNs在处理复杂拓扑结构的边界优化时表现出色，不需要手动插值，能够有效推断出清晰的拓扑边界。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [179] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: LLM能在推理时自我优化，ICRL提示显著改善其任务表现。


<details>
  <summary>Details</summary>
Motivation: 研究发现，LLM在推理时自然地表现出类似强化学习的现象，希望通过提示机制提高LLM的任务完成质量。

Method: 提出了一种名为ICRL提示的多轮提示框架，通过奖励反馈逐步改进LLM对任务的回应。

Result: ICRL提示在Game of 24、创意写作、ScienceWorld等三个基准测试中的表现显著优于其他基线方法。

Conclusion: 结果表明，ICRL在多项基准测试中优于其他方法，甚至在奖励信号由LLM自身生成时也能提升性能。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [180] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: 研究评估了五种集成学习器在葡萄酒质量预测中的效果，发现梯度提升算法表现最优，随机森林为生产中最具性价比的选择。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒质量评估通常因依赖于主观的、劳动密集型的品鉴小组而不够准确和可重复，研究旨在通过机器学习方法提高葡萄酒质量评估的精确度和可重复性。

Method: 研究使用了五种集成学习器（随机森林、梯度提升、XGBoost、LightGBM、CatBoost）在Vinho Verde红、白葡萄酒数据集上的表现，采用无泄漏的工作流程，包括80:20分层训练-测试数据划分、五折分层组交叉验证、标准化处理、SMOTE-Tomek重采样、逆频率成本加权、Optuna超参数搜索以及两阶段特征选择。最终在未处理的测试集上以加权F1作为主要评估指标。

Result: 在加权F1指标上，梯度提升算法在红酒和白酒数据集上分别取得了0.693（±0.028）和0.664（±0.016）的最高分。研究表明，限制每个模型在五个最高排名变量上的表现可以降低55%的维度，且只对加权F1得分产生微小影响。运行时剖析显示，梯度提升平均需要12小时完成五折研究，而其他模型运行时间更短。

Conclusion: 研究评估了五种集成学习器在葡萄酒质量预测中的表现，得出梯度提升算法在准确度上表现最佳。尽管降维会略微影响加权F1得分，但模型仍能保持较高的预测信号。研究建议在生产中使用随机森林作为最具性价比的模型，而XGBoost和LightGBM适合于GPU效率的场合，梯度提升算法则适用于离线基准测试。

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [181] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: 推出ExplainBench，提供系统评估地方模型解释的开源工具，促进公平研究中的可解释AI方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域中对可解释和值得信赖的模型的需求日益增加，当前缺乏用于比较评估地方解释技术的标准化和可重现框架，特别是在公平敏感环境中。

Method: 介绍了ExplainBench，一个开源的基准测试套件，用于通过统一封装器、集成的模型训练和解释生成管道以及评估标准（忠诚度、稀疏性和鲁棒性）对地方模型解释进行系统评估。

Result: 展示了ExplainBench在常用于公平研究的数据集上的应用，展示了不同解释方法在统一实验协议下的表现。

Conclusion: ExplainBench通过提供统一的封装、集成的工作流程和多样的评估标准，进步了可解释机器学习的方法基础，并促进了现实世界AI系统中的责任性。

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [182] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: 提出自对弈强化学习算法Self-RedTeam，增强语言模型安全性，通过攻击和防御者的协同进化，实现更高安全性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型的安全对齐通常是反应性和分散的，导致防御者总是落后于新出现的威胁。因此，该研究旨在通过使攻击者和防御者通过持续互动共同进化，来提高安全性能。

Method: 论文采用了一种多智能体强化学习（MARL）的方法，将语言模型的安全对齐问题表述为两个角色轮流扮演攻击者和防御者的零和游戏。通过这种方式，模型可以不断在对抗环境中自我改进。

Result: Self-RedTeam 方法可以发现比静态防御者训练的攻击方法多21.8%的多样性攻击，并在安全基准测试中表现出更高的鲁棒性，例如在WildJailBreak测验中的表现提高了65.5%。此外，该方法还通过隐式思维链提高了攻击的多样性并减少了过多拒绝。

Conclusion: 这篇论文提出了一种名为Self-RedTeam的在线自对弈强化学习算法，可以增强语言模型的安全性，解决现行方法中攻击者和防御者不同步的问题。结果表明，与静态训练的攻击者和防御者相比，这种方法能够发现更具多样性的攻击，并在安全基准上表现出更高的鲁棒性。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [183] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy引入了一种通用框架，用于执行状态合并算法，极大简化了算法实现过程。


<details>
  <summary>Details</summary>
Motivation: 希望在AALpy中加入一种通用实现，减少定义及执行状态合并算法的工作量。

Method: 使用统一的内部表示法实现红蓝框架的通用实现，使状态合并算法配置灵活简单。

Result: 在AALpy中定义某些现有的状态合并算法仅需几行代码，大大简化了算法的实现。

Conclusion: 该论文介绍了AALpy新的通用实现，重点在于被动自动学习领域的重要方法：在红蓝框架中的状态合并。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [184] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 提出了一种利用深度强化学习优化联邦学习中训练数据选择的新框架，在多种基准数据集 和框架上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在非IID数据环境中，联邦学习的中心化模型聚合会导致性能下降，且本地过多的数据训练不一定有利于全局性能提升。

Method: 使用深度强化学习代理选择每个客户端模型训练所需的最佳数据量，在训练过程中，DRL代理通过观察训练损失的变化作为奖励信号，不断调整每个类的最佳权重，以优化数据分配。

Result: 实验显示，通过所提算法训练的联邦学习客户端在多个基准数据集和框架上表现更优。

Conclusion: 通过深度强化学习优化训练数据选择提高联邦学习性能，尤其在非IID数据情境下效果更佳。

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [185] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: AI, especially Transformers and LLMs, improves energy management by enhancing decision-making and situational awareness.


<details>
  <summary>Details</summary>
Motivation: To synthesize advancements in AI, particularly Transformers and LLMs, for improving energy management in smart grids, addressing challenges in generalization, situational awareness, and heterogeneous data integration.

Method: The paper reviews recent advancements in AI applications, specifically focusing on Transformers and Large Language Models, examining their architectural foundations, domain-specific adaptations, and practical implementations in the energy sector.

Result: The paper introduces the concept of the Agentic Digital Twin, a next-generation model integrating LLMs to enhance autonomy and proactivity in digital twin-based energy management systems.

Conclusion: Generative AI is playing an increasing role in both high-level planning and daily operations in the energy sector, enhancing tasks such as forecasting, grid balancing, workforce training, and asset onboarding.

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [186] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 文章首次综述极端事件的合成数据生成技术，介绍了一个评估框架，分析各指标在极端条件下的适用性，并列出开放挑战和未充分探索的领域。


<details>
  <summary>Details</summary>
Motivation: 极端事件如市场崩盘、自然灾害和流行病通常是罕见但灾难性的事件，可以引发系统性的连锁反应。虽然数据驱动方法在极端事件建模中提供了强大的功能，但之所需的大量训练数据与极端事件数据稀缺的特性形成矛盾。因此，在保护隐私的同时生成合成数据成为解决该问题的重要方法。

Method: 这篇文章系统回顾了生成建模技术、大型语言模型，尤其是那些经过统计理论增强的模型，并引入了专门的训练和采样机制以捕捉厚尾分布特征。此外，还总结了基准数据集，并引入了一个涵盖统计、依赖性、视觉和任务导向的评价框架。

Result: 本文提供了一个系统化的框架，用于评估和生成极端事件的合成数据，并识别了行为金融、野火、地震、风暴和感染性爆发等未被充分研究的领域，指出了未来研究的开放挑战。

Conclusion: 这篇综述对极端事件的合成数据生成进行了首次全面的概述。通过对生成建模技术、大型语言模型以及统计理论增强方法的系统回顾，文章为极端事件的合成数据生成提供了行之有效的评估框架及领域适应性分析。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [187] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: 本文分析了位置编码对Transformer模型的影响，并提出了新的编码方法，其在实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 位置编码是Transformer模型处理序列数据的核心部分。本文旨在分析不同位置编码方法对Transformer模型的表达能力、泛化能力及对长序列的外推能力的影响，并提出新的编码方法。

Method: 理论框架分析各种位置编码方法的影响，提出基于正交函数的新编码方法，并通过实验验证其效果。

Result: 实验结果显示正交变换编码在泛化和外推能力上优于传统的正弦编码方法。

Conclusion: 本文提供了一个统一的理论框架来分析各种位置编码方法对于Transformer模型的影响，并提出了基于正交函数的新编码方法，这些方法在实验中表现优于传统的正弦编码。

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [188] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: CoxNTF enhances survival analysis by combining Coxnet-based survival probabilities with non-negative tensor factorization, improving both prediction accuracy and interpretability compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Nonnegative Matrix Factorization (NMF) don't incorporate survival information, which limits their predictive power. This motivates the need for a new approach that integrates survival data in latent factor representation.

Method: CoxNTF uses non-negative tensor factorization (NTF) with survival probabilities derived from the Coxnet model to create a weighted covariate tensor, guiding the tensorization process.

Result: CoxNTF matches the performance of the Coxnet model's survival predictions when using original covariates, while also providing an interpretable clustering framework and effectively handling feature redundancy.

Conclusion: CoxNTF achieves a balance between predictive performance and interpretability in survival analysis by utilizing non-negative tensor factorization to produce meaningful latent representations related to survival outcomes.

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [189] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD提供了一种数据高效的新类发现框架，可以在不依赖大量标记数据或人工交互的情况下实现高级的分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统的显式表示方式受限于其离散性、易产生孔洞和噪声的特性，这限制了新类的准确发现。

Method: 使用Embedding-NeRF模型结合KL散度替代传统的3D分割图来聚合语义嵌入和视觉嵌入空间中的熵。

Result: NeurNCD在NYUv2和Replica数据集上的性能显著优于现有的最先进方法。

Conclusion: 该研究提出了一种新颖的框架NeurNCD，能够在开放世界中高效进行新类发现，并且在闭环和开放世界中均能实现优越的分割表现。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [190] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 通过分析中间层嵌入，发现其在属性预测中表现优于最终层，改进性能达40.8%，并证明了“评估后微调”策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统上，分子编码器的使用通常依赖于最终层嵌入来进行下游任务，这可能会丢失重要的信息。本文旨在挑战这一惯例，探索中间层嵌入在ADMET属性预测任务中的表现。

Method: 本文通过对五种不同的分子编码器在22个ADMET属性预测任务中的层级分析，评估中间层嵌入的表现，并进行嵌入层的固化和微调，以提高下游任务的性能。

Result: 研究发现，中间层嵌入的表示比最终层表现更好。使用中间层固化嵌入能提高平均5.4%的下游性能，最高增幅达28.6%；而微调中间层嵌入能提高8.5%的平均性能提升，最高达40.8%，在多个基准测试中获得新的最优结果。此外，固定嵌入性能与微调结果之间的强正相关表明可以通过评估后微调的策略降低计算成本。

Conclusion: 探索分子编码器的全层次表示可以实现显著的性能提升和计算效率。通过使用中间层嵌入，能显著提高下游任务的表现，并使分子编码器更符合实际应用需求。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [191] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 本文提出SAFFRON推理扩展方法，改善LLM在安全保证中的表现，并公开相关资源以推动安全领域的研究。


<details>
  <summary>Details</summary>
Motivation: 当前的安全保证研究主要关注训练阶段的对齐，以向LLM注入安全行为，但这些方法容易受到各种越狱攻击且在推理阶段的安全探索仍然不足。因而有必要探索推理扩展在安全保证中的应用。

Method: 本文采用一种名为SAFFRON的推理扩展范式，重点引入了多分叉奖励模型，以减少奖励模型评估的次数。此外，还提出了部分监督的训练目标、保守探索约束以及基于Trie的键值缓存策略。

Result: 实验结果验证了SAFFRON方法的有效性，显著提高了LLM在安全背景下的推理性能。

Conclusion: 本文提出了一种新的推理扩展范式SAFFRON，用于提高LLM在安全保证方面的性能，并公开发布了相关代码、模型和数据，以促进该领域的进一步研究。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [192] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: DeepEDM结合深度学习与动力学建模，在时间序列预测上超过现有方法，展示了出色的准确性和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 很多现有的深度学习方法没有明确建模时间序列的动力学，而理解这些动态对精准预测至关重要，因此需要一种结合动力学建模的深度学习方法。

Method: DeepEDM将非线性动力学系统建模与深度神经网络相结合，使用时间延迟嵌入学习潜在空间，通过核回归逼近底层动力学，并利用softmax注意力机制实现高效预测。

Result: 在合成的非线性动力学系统数据及跨领域的实际时间序列上进行的实验表明，DeepEDM对输入噪声具有鲁棒性，并在预测精度上优于目前的最先进方法。

Conclusion: DeepEDM在处理复杂非线性动态的时间序列预测中表现出色，超过了现有的最先进方法。

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [193] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 研究提出了WISCA，一种新方法，在多个机器学习模型上取得了一致的解释性结果，提升了机器学习模型解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管预测准确性在机器学习中常被优先考虑，但在科学和高风险领域，可解释性仍然至关重要。不同的解释性算法常常导致冲突的解释，这突显了需要通过共识方法协调结果。

Method: 研究使用了六个有已知真相的合成数据集，训练了六种机器学习模型，并利用不同的模型无关解释性技术生成共识解释，提出了一种新方法WISCA。

Result: WISCA方法通过结合类别概率和归一化属性，在一致性上优于其他方法，显著提高了解释的可靠性。

Conclusion: WISCA方法显著提高了解释的可靠性，通过与表现最好的单个方法一致，提供了强有力的共识策略。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [194] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 本文提出了一种智能巡航控制框架，通过强化学习与神经网络模型，优化驾驶行为以提高婴儿乘坐时的睡眠质量，同时保持旅行效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶虽然提高了安全性和舒适度，但对乘坐者福祉，尤其是婴儿睡眠的影响研究不足。

Method: 使用强化学习与长短期记忆网络（LSTM）及变压器模型集成，基于可穿戴传感器的睡眠质量指标、车辆控制器的驾驶行为数据及地图应用的数据来动态计算最佳驾驶激进程度，并转化为具体的自动驾驶控制策略。

Result: 模拟结果表明，与基线方法相比，所提方案显著提高了婴儿睡眠质量，同时保持了出行效率。

Conclusion: 提出的智能巡航控制框架在改善婴儿睡眠质量的同时保持了令人满意的行驶效率。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [195] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: TimeRecipe是一个新的时间序列预测基准框架，通过模块级别的实验评估可优化模型设计。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法深入分析不同设计模型的有效性，提出TimeRecipe框架以填补这一空白。

Method: TimeRecipe进行超过10,000次实验，评估不同组件在各种数据集、预测范围和任务设置中的有效性。

Result: 通过全面探索设计空间，TimeRecipe发现可超越现有最先进方法的模型，并揭示设计选择与预测场景的具体关联。还发布了一个实际工具包，用于推荐合适的模型架构。

Conclusion: 提出TimeRecipe框架，通过系统性评估时间序列预测模型的组件模块，提高模型设计的有效性和性能。

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [196] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: 提出了一种无需访问原始数据的认证遗忘框架，通过替代数据集进行数据删除，同时确保模型的隐私和效用。


<details>
  <summary>Details</summary>
Motivation: 传统的遗忘方法需要访问完整的训练数据集，而在无法访问原始数据时，这是不现实的。因此，需要能够在没有原始训练数据样本的情况下有效地删除数据的方法。

Method: 我们的方法使用一个替代数据集来近似源数据的统计特性，并根据两者之间的统计距离进行噪声缩放控制。我们还建立理论界限，引入实际噪声校准技术，验证方法效果。

Result: 我们的实验结果证实了方法在隐私敏感环境中的有效性和可靠性，理论上给出了界限，实际中进行了噪声校准。

Conclusion: 我们的框架在隐私敏感的环境中有效且可靠，确保模型在学习后行为的强保证，同时保持整体效用。实验验证了方法的有效性和可靠性。

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [197] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 提出一种基于量化回归的成员推断攻击方法，针对无法访问完整数据分布情况，比影子模型攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 当前的成员推断攻击假设敌手可以访问与目标模型训练分布匹配的背景数据分布，但这样的假设在现实中可能不成立。因此需要探索在无法访问整个子类分布的情况下进行成员推断攻击的方法。

Method: 研究成员推断攻击，当敌手无法访问分布中的整个子类时。提出了一种新的量化回归的方法，不受传统影子模型攻击的限制。

Result: 量化回归攻击在类丢失环境下优于影子模型攻击。例如，在CIFAR-100的数据集中，量化回归攻击的TPR比影子模型高出11倍；在ImageNet上，即使移除90%训练类，也能够达到非平凡的TPR。

Conclusion: 量化回归能够在不完整数据分布情况下有效进行成员推断，比传统影子模型攻击在极端分布转移情况下更具优势。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [198] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: 本文提出交替梯度流（AGF），一种描述神经网络特征学习动态的算法框架，成功量化了损失下降模式，并扩展了现有分析。


<details>
  <summary>Details</summary>
Motivation: 了解神经网络学习特征的方式及其动态过程仍然是一个未解决的问题。

Method: 提出交替梯度流（AGF）作为一种算法框架，描述了两层网络中特征学习的动态过程，该网络从小初始化开始训练。

Result: AGF准确量化了损失下降的顺序、时间和幅度，并成功匹配不同架构的实验结果。在对角线线性网络中，证明AGF在消失初始化的极限下收敛到梯度流。在二次网络中，首次完整刻画了训练动态，揭示网络学习傅里叶特征的顺序。

Conclusion: AGF提供了一种理解神经网络中特征学习动态的新方法，并扩展了现有的分析方法。

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [199] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: SPARQ方法通过生成高质量和多样化的合成数据来提升模型的推理能力，提高了模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的方法对于复杂和多样化的问题域的扩展性有限，因此需要一种新方法来生成高质量的合成数据，以提升模型的推理能力。

Method: 提出了一种名为SPARQ的方法，使用质量-多样性算法通过测量问题的解题率来生成高质量和多样化的合成数学问题和解决方案对。

Result: 通过使用合成数据进行模型微调，相对性能提高了24%。发现更高质量的数据有助于提高分布内性能，而更多样化的数据则有助于提高分布外泛化能力。

Conclusion: 生成的高质量和多样化的合成数学问题和解决方案数据有助于提高模型的泛化能力，尤其是在解决分布外问题时表现更为出色。

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [200] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 研究表明，通过正确的正则化策略，可显著提高随机任务排序下连续线性回归的学习速率。


<details>
  <summary>Details</summary>
Motivation: 缩小最坏情况下损失的上下限差距，提高连续线性回归模型的学习效率。

Method: 使用两种正则化方法：（1）显式各向同性$\ell_2$正则化，（2）通过有限步预算隐式正则化。这些方法通过对经过精心定义的替代损失执行随机梯度下降（SGD）来实现。

Result: 通过固定正则化强度，可以实现接近最优的$O(\log k / k)$加快学习速率。同时，通过时间变化函数的一般化SGD变体分析，得到了随时间增加的正则化强度安排以实现最优的$O(1/k)$速率。

Conclusion: 研究表明，通过增加正则化强度或减少每个任务的步骤数，可以在最坏情况下提高学习速率。

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [201] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: 基于FPGA的InstantFT方法能在物联网设备上实现超快速CNN微调，比传统方法快17.4倍，且能效提高16.3倍。


<details>
  <summary>Details</summary>
Motivation: 为了在资源有限的物联网平台上实现深度神经网络的即时适应，克服训练所需的计算和内存限制。

Method: 提出了一种基于FPGA的方法，称为InstantFT，通过优化参数高效微调中的前向和后向计算，实现物联网设备上的超快速CNN微调。

Result: 实验表明，InstantFT在有概念漂移的数据集上微调预训练CNN的速度比现有的基于低秩适应的方法快17.4倍，同时达到相当的准确性。

Conclusion: InstantFT显著减少了微调时间，提高了能效，使CNN能够在动态数据分布下进行实时适应。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [202] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文提出MVP算法，分析其在episodic MDP中实现的方差感知遗憾界，发现该界依赖于最大条件总方差。


<details>
  <summary>Details</summary>
Motivation: 分析episodic MDP中与间隙相关的遗憾界，特别是如何通过变化和间隙的统计特性来优化算法的遗憾界。

Method: 提出Monotonic Value Propagation (MVP)算法，通过对次优间隙的加权和进行新颖分析来获得与方差感知的间隙相关的遗憾界。

Result: MVP算法实现了一种方差感知的间隙相关遗憾界，并证明了这种界与最大条件总方差的依赖关系。

Conclusion: 即使在最大无条件总方差接近零的情况下，也必须依赖最大条件总方差，验证了这种依赖的必要性。

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [203] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 论文研究了在二维和三维网络中多无人机的运动和通信控制方法，使用大型语言模型进行分层协作控制。


<details>
  <summary>Details</summary>
Motivation: 在动态约束环境中优化多无人机系统的控制是一个重大挑战。

Method: 提出了一种基于大型语言模型的分层协作控制方法。

Result: 实验结果表明，该方法实现了更高的系统奖励，更低的运行成本，以及显著降低的无人机碰撞率。

Conclusion: 基于LLM的协作方法在多无人机系统中展现出优异性能。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [204] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip is a geometry-aware method for DP-SGD that improves performance by adaptively estimating gradient transformations, reducing noise and enhancing privacy-utility balance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of setting the per-sample gradient clipping threshold in DP-SGD, which affects the privacy-utility trade-off and to improve on existing methods that fail to account for correlations across gradient coordinates.

Method: GeoClip adapts a geometry-aware framework that clips and perturbs gradients in a transformed basis aligned with the geometry of the gradient distribution, using previously released noisy gradients to estimate the transformation without incurring additional privacy costs.

Result: GeoClip provides convergence guarantees and a closed-form solution for the optimal transformation, minimizing noise addition while controlling gradient clipping probability, as demonstrated through experiments on tabular and image datasets.

Conclusion: GeoClip consistently outperforms existing adaptive clipping methods under the same privacy budget.

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [205] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: 该论文提出了一种基于SDN的强健虚假数据检测和缓解系统（FDDMS），能够有效检测和缓解车载网络中的虚假数据注入攻击，并对抗对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和联网车辆的发展，车载网络中电子控制单元（ECU）的日益复杂，确保ECU之间通信的安全在维护车辆的安全性和安保上至关重要。

Method: 利用软件定义网络（SDN）的能力，设计了FDDMS系统，结合了基于长短期记忆（LSTM）的检测模型，并使用变体的DeepFool攻击评估模型的鲁棒性。同时，采用重新训练技术方法，结合基于阈值的选择策略，并实施了一种缓解方案，通过SDN动态更新流规则来重定向攻击流量。

Result: 实验结果表明，FDDMS系统在面对对抗性攻击时具有鲁棒性，能够实时有效地检测和缓解虚假数据注入攻击。

Conclusion: FDDMS系统能够有效检测和缓解实时的虚假数据注入攻击，并在实验中证明其对抗对抗性攻击的鲁棒性。

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [206] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: 通过将随机特征用于参数构造而非传统的迭代优化，HGN模型在训练速度上提高了600倍，且保持了类似的准确性，并能在不重新训练的情况下从小系统泛化到更大的系统。


<details>
  <summary>Details</summary>
Motivation: 学习尊重物理对称性和约束的动态系统在数据驱动建模中仍是一个基本挑战。通过集成物理定律和图神经网络，可以有效地建模复杂的N体动力学，并产生准确且排列不变的模型。

Method: 使用Hamiltonian Graph Networks (HGN)替代迭代优化，通过随机特征参数构造来加速训练。

Result: 即便在最小值的8节点系统上训练，该模型可以在不重新训练的情况下，以零次泛化的方式推广到多达4096个节点的系统。

Conclusion: 我们的工作挑战了基于迭代梯度下降的优化算法在物理系统神经网络模型训练中的优势。

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [207] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 本文提出了SpectRe，一个整合光谱信息到PH图的新拓扑描述符，拥有更强的表现力，并在实验中展示出优异的效果。


<details>
  <summary>Details</summary>
Motivation: 现有通过特征增强经典PH图的方法在表达性上不足，尤其是无法捕捉基础的图结构信息。

Method: 提出了SpectRe作为一个新的拓扑描述符，同时引入了全局和局部稳定性的概念，通过分析证明SpectRe在局部是稳定的。

Result: 实验结果表明SpectRe的表现超出已有的图描述符，展示其增强图模型能力的潜力。

Conclusion: SpectRe展示出在图模型学习任务中的效果和潜力。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [208] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 多LLM选择策略和级联推理在有限资源环境中提升LLM推理效率，有助于更有效的实际应用。


<details>
  <summary>Details</summary>
Motivation: 在有限硬件、供电或带宽的设置中，LMs的推理成本高且耗能，这限制了其在移动设备、边缘计算或对价格敏感的环境中的部署，提出的策略旨在通过动态分配计算资源解决这一问题。

Method: 通过调查现有两种策略：路由选择和级联推理，对比分析这两种策略在关键性能指标上的表现，并讨论了其基准测试工作和面临的挑战。

Result: 这两种策略通过使用轻量级模型处理简单任务，仅在必要时转向较大模型，从而减少了计算量。

Conclusion: 多LLM智能模型选择策略和级联推理可以显著提高LLM在计算资源限制下的推理效率，从而提高其在实际应用中的可及性和效率。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [209] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: 提出了一个桥接图和拓扑信息传递的统一框架以解决过度压缩等问题，并展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 解决拓扑信息传递网络中的过度压缩问题。

Method: 从关系结构的角度看单纯复形和细胞复形及其消息传递方案的统一公理框架。

Result: 证明该框架在理论分析和实证研究中对拓扑深度学习的潜力。

Conclusion: 该论文的框架能够推进拓扑深度学习的研究。

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [210] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文在过参数化设置下，为Gradient EM提供了全局收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 在EM算法无法保证对m≥3的确切参数化GMM进行全球收敛的情况下，探讨过参数化是否能解决这一问题，提供收敛保证。

Method: 通过Herimit多项式研究Gradient EM的动态过程，并利用张量分解描述似然损失的几何结构，进行两阶段分析，引入一系列新的工具进行高斯混合分析。

Result: 对于一般位置的良好分离的GMM，浅度过参数化（n=Ω(m log m)）时，随机初始化的Gradient EM能以多项式速率收敛到真实值，并使用多项式样本。

Conclusion: 本文首次在过参数化下为Gradient EM提供了全局收敛和恢复结果，突破了仅在m=2的特殊情况下获得全局收敛的限制。

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [211] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: 本文提出直接预测集最小化（DPSM）算法，成功减少预测集大小，提高保序预测实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的保序预测校准方法往往产生较大的预测集，在实践中不太实用。因此，提出直接最小化预测集大小的方法。

Method: 将保序原则融入深度分类器的训练过程，采用直接预测集最小化算法（DPSM），通过双层优化问题解决。

Result: 在各种基准数据集和深度模型上的实验表明，DPSM算法显著减少了预测集大小，并验证了理论。

Conclusion: DPSM显著优于现有的保序训练基线，在预测集大小上降低了20.46%。

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [212] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: 研究表明声学特征和多模态方法在认知障碍预测中优于语言特征和单模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习算法多用于阿尔茨海默症及相关痴呆的预测，尚未应用于更广泛的认知障碍预测。

Method: 评估多种基于语音的开源方法和多模态情感分析方法，以预测患者录音中的认知障碍。

Result: 多模态方法在认知障碍预测中比单模态方法效果更好，声学特征尤其是与情感和语调相关的解释性特征优于基于BERT的语言特征。

Conclusion: 声学特征在认知障碍预测中表现优于语言特征，且多模态方法优于单模态方法。

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [213] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: 引入了新的加速 $\ell_p$ 最速下降算法Stacey，在处理非欧几里得优化任务时表现出比SGD、AdamW等更高的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的优化方法如SGD、AdamW和Lion在处理现代深度网络训练中的非欧几里得结构时存在重要的不足。

Method: 引入了一种新的加速 $\ell_p$ 最速下降算法Stacey，采用插值的原始-对偶迭代序列来导航非欧几里得平滑优化任务。

Result: 通过与流行的方法在图像分类和语言模型预训练上的比较，进一步验证了Stacey在这些任务中的更快收敛速度和更高的精度。不同$p$值对不同模型和数据集的结果表明，非欧几里得方法的重要性和效率高于标准欧几里得方法。

Conclusion: 与现有的优化算法相比，Stacey在处理非欧几里得优化任务上提供了更高的收敛速度和最终精度。

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [214] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 通过从易到难的任务调度策略来优化语言模型的推理能力，显著超越传统RL方法。


<details>
  <summary>Details</summary>
Motivation: 提高语言模型的推理能力是关键。传统RL方法在处理难度大的任务时效果有限，需要新的策略。

Method: 提出一种从易到难的任务调度方法(E2H)，结合增强学习和课程学习以优化推理能力。

Result: 通过课程调度以减少过拟合，实验表明小型语言模型的推理能力显著提高。

Conclusion: E2H Reasoner能够有效提高语言模型在难度较高任务中的推理能力，且比传统RL方法需要更少的样本。

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [215] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 介绍了一种量子-经典混合模型Vision-QRWKV，并在14个图像分类基准上评估其性能。结果表明，量子增强模型在多个数据集上表现优异，特别是在处理微妙或噪音较大的类别时。


<details>
  <summary>Details</summary>
Motivation: 量子机学习在处理复杂高维数据方面显示出了潜力。本研究旨在进一步将量子技术应用到视觉分类任务中，以提升非线性特征转换能力。

Method: 使用变分量子电路（VQC）集成到RWKV模型的通道混合组件中，并对比经典的RWKV模型。

Result: 量子增强模型在大多数数据集上优于经典模型，尤其是在具有微妙或噪声类区分的数据集上。

Conclusion: 量子增强RWKV模型在视觉领域的应用首次得到系统评估，揭示了量子模型在轻量级视觉任务中的架构权衡和未来潜力。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [216] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 该研究提出结合图像特征与持续学习的新NILM方法，大幅提升识别准确性。


<details>
  <summary>Details</summary>
Motivation: 非侵入式负载监控 (NILM) 对智能电力管理具有重要意义，但传统NILM方法面临特征鲁棒性差和模型泛化能力不足的问题。

Method: 提出了一种将“图像负载特征”和持续学习相结合的非侵入式负载监控新方法，利用深度卷积神经网络识别和分类多种设备，并引入自监督预训练和持续在线学习策略。

Result: 在高采样率负载数据集上进行实验，结果显示提议的方法在识别准确性上取得了显著提升。

Conclusion: 通过将多维电力信号转化为视觉图像负载特征并结合深度学习技术，新方法显著提高了设备识别准确性，解决了传统NILM方法的缺陷。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [217] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Spark Transformer是一种新架构，通过实现高水平激活稀疏性来提高模型效率，保持质量和性能，同时在CPU和GPU上实现显著的墙时间加速。


<details>
  <summary>Details</summary>
Motivation: 发现训练后的Transformer中懒惰神经元现象，即FFN中的绝大多数神经元对每个token都是不活跃的。这激发了对激活稀疏性的广泛兴趣，以提高大型模型效率。然而，现代Transformer已经从对该现象至关重要的ReLU激活函数中脱离，现有重新引入激活稀疏性的努力通常会损害模型质量、增加参数数量、复杂化或减慢训练速度。

Method: 引入了一种被称为Spark Transformer的新型架构，通过在FFN和注意力机制中实现高水平的激活稀疏性来保持模型质量、参数数量和标准训练程序。该方法通过top-k掩码实现稀疏性，对稀疏性级别进行显式控制。特别是引入了统计top-k，一种硬件加速器友好、线性时间的近似算法，避免了昂贵的排序，缓解了从标准top-k算子来的显著训练减速。此外，Spark Transformer重新分配现有的FFN参数和注意力关键嵌入，形成一个低成本预测器以识别激活条目。

Result: 只有8%的FFN神经元被激活，且每个token最多关注256个tokens。这种稀疏性可以转化为FLOPs减少2.5倍，在CPU上解码时墙时间加速可达1.79倍，在GPU上达到1.40倍。

Conclusion: Spark Transformer在标准基准测试上表现出竞争力，同时展示了显著的稀疏性，带来了墙时间效率的提升。

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [218] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: SAFER通过整合结构化EHR数据与临床笔记，实现了更优的动态治疗方案推荐，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有DTR系统主要依赖于结构化EHR数据和缺乏最优策略的医生推荐，导致其在推荐治疗时的可靠性受到限制。

Method: SAFER使用校准的风险感知表格-语言推荐框架，将结构化的EHR数据与临床笔记结合，并通过保形预测提供统计保障。

Result: SAFER在两个公开的脓毒症数据集上的实验中，在多个推荐指标和反事实死亡率上超越了最先进的基线。

Conclusion: SAFER在可解释性和可靠性方面表现优异，特别是在高风险的动态治疗方案应用中。

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [219] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 引入了重新标度的影响函数（RIF）以提高数据归因准确性，解决了传统影响函数在高维度数据集上的不精确性问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统影响函数在高维度数据集上不准确，常常低估样本移除影响的问题。

Method: 采用了重新标度的影响函数进行数据归因，并通过理论分析展示其效果和优越性。

Result: RIF在实际数据集中提供了更好的预测性能，并成功检测出传统影响函数难以探测的投毒攻击。

Conclusion: 提出的RIF在数据归因上比传统的影响函数更准确，尤其在高维度情况下表现明显优于传统方法。

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [220] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: SDP-CROWN结合了半定规划和线性边界传播优点，提升了神经网络验证器的紧度和扩展性，适用于大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络验证器在处理大型模型时表现出色，但在处理神经元耦合时却可能不够精确。而半定规划验证器能很自然地处理神经元耦合，但由于其复杂性，只能用于小规模模型。因此，研究人员希望开发出一种既能结合半定规划紧度，又具有线性边界传播验证器可扩展性的验证框架。

Method: 本文提出了一种名为SDP-CROWN的新型混合验证框架，其核心是通过SDP原理推导的新线性边界，该边界显式捕获基于L2范数的神经元间耦合，同时每层仅增加一个额外参数。该边界可以无缝集成到任何线性边界传播管道中。

Result: 理论上证明该神经元间边界比传统的每神经元边界可以紧到√n倍。在实践中，将其整合到先进的α-CROWN验证器中时，在处理多达65千个神经元和2.47百万个参数的大型模型时，验证性能显著提高，达到了接近昂贵的SDP方法的紧度。

Conclusion: SDP-CROWN框架在理论和实践中都展示了其在神经网络验证中的优势，结合了SDP和线性边界传播验证器的优点，显著提升了验证紧度和性能。

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [221] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 研究提出了一种新框架，用于足球比赛中破防传球的检测和分析，并提供了新的战术指标以量化其效能。


<details>
  <summary>Details</summary>
Motivation: 为了增强球队战术分析能力，尤其是在比赛中攻破对方防守线的传球战术上，希望通过引入新的分析指标和方法来提高评估精准度和效能。

Method: 采用无监督学习和聚类分析，通过垂直空间分割模型化对手队形，并以事件和追踪数据检测破防传球，并引入了多个战术指标来量化传球的效能。

Result: 通过2022年世界杯的数据，评估了不同球队和球员的战术风格差异，尤其是纵向推进和结构破坏方面的差异。

Conclusion: 提出了一种无监督的聚类框架，用于检测和分析足球比赛中的破防传球，提供了一系列战术指标来量化其战术影响，并在2022年世界杯上进行了评估。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [222] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出了HetCRF，一个用于异构图的双通道自监督学习框架，在节点分类任务中超越了现有最优基线，特别是在缺少节点特征的数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在语义稀疏的场景中，CL在视图构建上有困难，并且在正负样本之间的梯度不平衡依然存在。为了克服这些挑战，本文提出了HetCRF框架。

Method: 本文提出了一种用于异构图的双通道自监督学习框架HetCRF。它采用两阶段聚合策略来适配嵌入语义，使其兼容于MAE和CL，并增强编码器输出以进行视图构建，提高效率。此外，还提出了两种正样本增强策略以平衡梯度贡献。

Result: 在四个真实世界的异构图数据集上的实验表明，HetCRF比现有的最先进基线表现更佳。在缺失节点特征的数据集上，HetCRF提高了Macro-F1得分，这验证了其有效性和优越性。

Conclusion: Node classification实验表明，HetCRF在四个真实的异构图数据集上超越了现有的最优基线。在缺少节点特征的数据集（如Aminer和Freebase）中，HetCRF在节点分类中的Macro-F1得分分别提高了2.75%和2.2%。

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [223] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL是一种用于训练出行基础模型的可扩展、隐私保护的框架，在多种数据集上显示了强大的性能和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 针对出行数据的隐私敏感性和导致的数据孤岛现象，提出MoveGCL框架，旨在开发可伸缩且隐私保护的出行基础模型。

Method: MoveGCL通过生成持续学习的方法进行训练，通过冻结教师模型生成的合成轨迹进行去中心化和渐进的模型演变，并通过定制的蒸馏策略加强知识保留，同时使用专家混合Transformer和逐层渐进适应策略以稳定持续更新。

Result: 在六个真实城市数据集上的实验表明，MoveGCL的表现与联合训练相当，显著优于联邦学习基线。

Conclusion: MoveGCL框架在不共享原始数据的情况下，实现了可以与联合训练相媲美的性能，并显著超越联邦学习基线，同时为模型提供了强大的隐私保护。这标志着为人类出行建立基础模型的一个关键进展。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [224] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: MarginSel通过选择边界难例子提高LLM的ICL性能，F1-score提升2-7%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在ICL中表现优异，但其性能对示范例子的选择和顺序非常敏感，MarginSel旨在通过选择合适的示范例子，改善ICL的效果。

Method: 提出了一种名为MarginSel的两步方法，通过选择困难的示范实例来提高LLM在ICL中的表现。

Result: 与随机选择示例相比，MarginSel在分类任务中的F1-score提高了2-7%。

Conclusion: MarginSel通过选择困难的示范例子，以最大的边距提高了LLMs的性能，显著改善了ICL的表现。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [225] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: 本文提出Sequence Protein Transformers（SPT）用于蛋白质功能预测，并利用Sequence Score技术解释预测结果。在不同数据集上取得了高准确率，发现了符合生物领域知识的序列模式。


<details>
  <summary>Details</summary>
Motivation: 研究Protein Transformers能否在蛋白质序列间捕获生物智能。

Method: 引入了新的变压器架构Sequence Protein Transformers（SPT），以及解释性人工智能技术Sequence Score，用于蛋白质功能预测和结果解释。

Result: 最小的SPT-Tiny模型在抗生素耐药性数据集上获得了94.3%的准确率，在Protein-FN数据集上达到99.6%的准确率，并且能够发现与生物学领域知识一致的序列结构模式。

Conclusion: 本文提出了一种高效的蛋白质功能预测方法，使用新的变压器架构和解释性人工智能技术，不仅提高了预测准确性，而且揭示了蛋白质序列中的生物学模式。

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [226] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: 提出一种新的方法SVH-MOL，使用SVGD近似Pareto集合，通过实验验证了其卓越性能。


<details>
  <summary>Details</summary>
Motivation: 当前一些方法在多目标学习中面临的挑战是如何让Pareto解在最大化体积值的同时保持多样性。

Method: 提出了一种新的方法，使用Stein Variational Gradient Descent (SVGD) 来近似整个Pareto集合。

Result: 通过在多目标问题和多任务学习上的广泛实验验证了该方法的有效性，并展示了其卓越的性能。

Conclusion: SVH-MOL方法可以有效地解决Pareto解的多样性和最大化体积值的问题。

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [227] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 研究通过模型编辑技术，提高识别系统在低资源语言中的表现，显著增强了转移学习和域外评估性能。


<details>
  <summary>Details</summary>
Motivation: 识别系统在面对不同领域时需要具备鲁棒性，然而低资源语言通常在大量预训练和基础技术中被低估，因此需要开发能快速适应新数据分布的模型。

Method: 利用最新模型编辑技术，通过域合并，在稀疏数据分布的情况下展示其有效性，而不依赖总体分布关系或其他原型需求。

Result: 实验显示，以相同训练数据进行训练的模型在转移学习体验新的字母表和域外评估中展现了显著的性能提升，适用于包括历史加密文本和非拉丁字母的挑战性域移位。

Conclusion: 该研究提出了一种新的模型构建方法，使其能够较轻松地适应未被充分代表的字母系统，从而支持文档识别在更广泛的背景和文化中应用。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [228] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 提出FIND方法，通过三个组件提高动态测试场景中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试域的分布转变下表现不佳，导致应用体验质量下降。现有的测试时适应方法在动态、多重的测试分布中面临挑战。

Method: 提出了Feature-based Instance Neighbor Discovery (FIND)方法，包含了Layer-wise Feature Disentanglement (LFD)、Feature Aware Batch Normalization (FABN)和Selective FABN (S-FABN)三个关键组件。

Result: 在动态场景中实现了30％的准确性提升，同时保持计算效率。

Conclusion: FIND significantly enhances performance in dynamic test scenarios, outperforming existing methods by 30% in accuracy while maintaining computational efficiency.

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [229] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: 这篇论文引入了一种高效聚合机制，在牺牲一些表达能力的情况下，加强了聚合能力，并提出了Caterpillar GNN模型，能够在真实数据集上实现与现有模型相似的预测性能，同时显著减少计算图中隐藏层的节点数。


<details>
  <summary>Details</summary>
Motivation: 目前的图神经网络过于追求最大化表达能力，因此我们希望能找到一种在表达能力和聚合能力之间达到有效平衡的方法。

Method: 提出了一种高效聚合机制，通过使用广义毛虫图的同态计数来对每个中间步骤的表达能力进行严格的刻画。

Result: Caterpillar GNN在设计复杂的合成图任务中表现出色，并在真实世界数据集上实现了与传统图神经网络相当的预测效果，同时减少了计算图中隐藏层节点的数量。

Conclusion: 通过引入高效聚合机制，Caterpillar GNN模型在保持良好的预测性能的同时提升了图级别的聚合能力，并降低了计算复杂度。

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [230] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: FuncGNN enhances logic circuit representation by addressing heterogeneity and merging semantic information, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and density of modern integrated circuits cause structural heterogeneity and global logic information loss in And-Inverter Graphs (AIGs), challenging accurate circuit modeling.

Method: FuncGNN integrates hybrid feature aggregation, gate-aware normalization, and multi-layer integration to enhance logic circuit representations. It addresses structural heterogeneity and merges local and global semantic information effectively.

Result: FuncGNN outperforms state-of-the-art methods in signal probability prediction and truth-table distance prediction, improving performance by 2.06% and 18.71%, respectively. It also reduces training time by 50.6% and GPU memory usage by 32.8%.

Conclusion: FuncGNN significantly improves the representation of complex logic circuits, demonstrated by its superior performance in logic-level analysis tasks while reducing computational resources.

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [231] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: 本文提出了两种简单的替代方法来替代OT方法用于逆向强化学习，展现出它们在多个基准测试中与OT方法的等效或更优效果，建议重新审视未来逆向强化学习设计中的复杂性。


<details>
  <summary>Details</summary>
Motivation: 质疑在逆向强化学习中使用OT的必要性，并提出简单的替代方案来解决OT方法所引入的算法复杂性和超参数敏感性。

Method: 提出两种简单的启发式替代方法：Minimum-Distance Reward 和 Segment-Matching Reward，避免了优化问题，且实现简便，复杂度为线性时间。

Result: 通过与三个强化学习算法的广泛评估，证明简单奖励的方法有效地匹配或超越了OT方法。

Conclusion: 我们的简单方法在32个在线和离线基准测试中表现出与或优于最近的基于OT的方法。

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [232] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: IMPA-HGAE framework improves node embeddings in heterogeneous graphs using full node information along meta-paths, showing superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing heterogeneous graph SSL models convert heterogeneous graphs to homogeneous ones via meta-paths, underutilizing heterogeneous node information along the meta-paths, which this paper addresses.

Method: Proposes a framework named IMPA-HGAE that fully exploits internal node information along meta-paths to enhance target node embeddings. Introduces innovative masking strategies to strengthen generative SSL models.

Result: IMPA-HGAE demonstrates superior performance in experimental results on heterogeneous datasets, providing robust representation learning in complex graph scenarios.

Conclusion: IMPA-HGAE achieves superior performance on heterogeneous datasets and enhances target node embeddings by utilizing internal node information along meta-paths.

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [233] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: 研究神经扩散过程在路径积分采样中的应用，表现出色，但需解决高维空间中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 探索神经扩散过程在全局优化中的潜力，特别是应用于路径积分采样器。

Method: 利用Schrödinger桥采样问题，通过Girsanov定理和简单的（单点）先验将其框定在随机控制术语中，并通过神经近似（傅里叶MLP）计算解的积分项。

Result: 优化器在尺寸从2到1247维的优化任务中表现出良好的每步性能，但在面对15900个参数的高维空间时表现不佳，需关注适应性的提升。

Conclusion: 神经扩散过程在特定维度下表现出色，但在更高维的优化任务中需进一步改进。

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [234] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了Curvature-Enhanced Manifold Sampling (CEMS) 方法用于回归任务，有效提高回归模型在多个数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管过参数化模型在分类任务中通过保持标签变换的数据增强技术取得了成功，但在回归问题中的应用较少受到关注。为解决这一问题，提出了一种新的流形学习方法，通过数据流形的一级近似来生成合成数据。

Method: 提出了一个理论框架和实际工具用于近似和采样一般数据流形，并引入了基于第二阶数据流形表示的Curvature-Enhanced Manifold Sampling（CEMS）方法。

Result: 实验表明，CEMS方法在多个数据集上的分布内和分布外场景中都表现出优越的性能，同时仅引入了最小的计算开销。

Conclusion: CEMS方法可以有效地在回归任务中进行采样和重建新数据点，在多个数据集和与最新方法的比较中表现优越，代码已公开提供。

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [235] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: 本文提出的FA-INR模型通过灵活的特征表示和高效的专家混合模型，在缩小模型规模的同时，提升了INR的准确性，实验表明其达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示法（INR）虽然在建模空间结构化数据方面提供了紧凑且连续的框架，但在处理具有局部高频变化的复杂科学领域时往往表现不佳。

Method: FA-INR利用交叉注意力机制结合增强的记忆库，学习灵活的特征表示，并引入了坐标引导的专家混合模型（MoE），提高了特征表示的专业化和效率。

Result: 通过在三个大规模模拟数据集上的实验，FA-INR在模型规模显著减小的前提下，实现了基于INR的替代模型的最先进保真度表现。

Conclusion: FA-INR实现了先进的保真度，同时显著减少了模型规模，为基于INR的替代模型建立了准确性和紧凑性之间的新权衡前沿。

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [236] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了高维重尾响应下的差分隐私稀疏线性回归，提出了 DP-IHT-H 和 DP-IHT-L 方法，验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在规则数据分布或低维不规则数据，本研究旨在弥补其在高维重尾响应下的不足。

Method: 本文提出 DP-IHT-H 方法，通过结合 Huber 损失和私有迭代硬阈值技术；还提出了 DP-IHT-L 方法，在特定假设下进一步改进误差界。

Result: 在实验中，通过合成数据集和真实数据集的实验验证，证明了所提方法优于标准差分隐私算法。

Conclusion: 本文提出了两种新的隐私保护稀疏线性回归方法（DP-IHT-H 和 DP-IHT-L），在高维数据和重尾响应下，展示了优于标准差分隐私算法的效果。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [237] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 提出SAFE方法，通过稀疏性约束优化寻求稀疏又平滑的子网络，有效提升网络性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受鲁棒优化研究的启发，希望寻找既稀疏又平滑的子网络来解决剪枝后的性能下降问题。

Method: 通过将剪枝问题形式化为一个稀疏性约束优化问题，并使用扩展拉格朗日对偶方法求解，提出了SAFE及其扩展SAFE+。

Result: 实验表明SAFE方法在提升稀疏网络的泛化性能方面表现良好，并能有效处理噪音数据，表现优于一些知名基线。

Conclusion: SAFE方法能够找到既稀疏又平滑的子网络，并在常见的数据集上表现出色，适应性强。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [238] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 本文提出了一种新型LSE估计器，用于解决离线策略学习和评估中的高方差和性能问题，理论和实证结果显示其具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 解决高方差和低质量倾向评分以及重尾分布导致的表现不佳的问题。

Method: 本文引入基于对数和指数（LSE）算子的估计器，与传统的逆倾向评分估计器相比，该方法表现更好。为离线策略评估推导了估计器的偏差和方差的上界，并在离线策略学习场景中建立了遗憾界限。这些界限假设称加权奖励的$(1+\epsilon)$-阶矩是有界的。

Result: LSE估计器在重尾条件下表现出方差减少和稳健性，并取得收敛速度为$O(n^{-\epsilon/(1+ \epsilon)})$的遗憾界限。

Conclusion: 本文提出的基于对数和指数（LSE）算子的新估计器在离线策略学习和评估中表现优越，实现了方差减少，并在重尾条件下具有鲁棒性。理论分析与实证评估相结合，证明了该方法的实际优势。

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [239] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 为了应对视觉语言模型推理延迟问题，我们提出了一种改进策略，通过早退出机制提高推理速度，实验证明效果良好。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型在视觉语言任务中表现优异，但在实际应用中推理延迟成为问题。

Method: 提出了一种基于生成对抗网络（GAN）的早退出策略，结合转换器层和分类器，进行对抗训练。

Result: 我们的实验结果表明，通过我们的方法可以将推理速度提高超过1.51倍，并保持相当的性能。

Conclusion: 我们的方法有效地提高了推理速度，同时性能基本保持不变。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [240] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: 研究了一种对抗训练框架（AT-DPT）以提高决策预训练Transformer（DPT）对奖励投毒攻击的鲁棒性。结果显示，AT-DPT在赌博和MDP环境中都显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 为了应对针对决策预训练Transformer的奖励投毒攻击，研究ICRL的腐败鲁棒性。

Method: 提出了一种新颖的对抗训练框架，即对抗训练决策预训练Transformer（AT-DPT）。该方法同时训练攻击者通过污染环境奖励最小化DPT的真实奖励，以及训练DPT模型从污染数据中推断最优行动。

Result: 我们的方法在具有学习攻击者的赌博环境中表现出明显优越的效果，并在适应性攻击者上观察到类似的结果。此外，我们在MDP设置中的扩展评估证实了在赌博场景中观察到的鲁棒性可以推广到更复杂的环境。

Conclusion: 我们的方法在具有学习攻击者的赌博环境中表现出明显优于基线的效果，并且在适应性攻击者上观察到类似结果。此外，我们的评估在MDP环境中确认了在赌博场景中观察到的鲁棒性可以推广到更复杂的环境。

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [241] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出了一种利用潜在Kronecker结构改进大数据集上的高斯过程计算方法，在多个真实应用场景中性能超过最新方法。


<details>
  <summary>Details</summary>
Motivation: 在处理非常大的数据集时，高斯过程面临计算可扩展性的问题。虽然矩阵结构，如Kronecker乘积，可以显著加速计算，但其应用通常需要近似或不切实际的假设。尤其是在实际数据中常出现的缺失观察会破坏笛卡尔乘积结构。

Method: 提出了一种利用潜在Kronecker结构的方法，通过将观测值的核矩阵表达为潜在Kronecker乘积的投影。结合迭代线性系统求解器和路径条件方法，可以在消耗较少计算资源的情况下进行精确高斯过程推断。

Result: 在多达五百万个例子的真实世界数据集中，这种方法在机器人学、自动化机器学习和气候应用中，性能优于最新的稀疏和变分高斯过程。

Conclusion: 通过利用潜在Kronecker结构，该方法能够在更少的计算资源下实现对大规模数据集的精确高斯过程推断，并在多种实际应用中显示出优越的性能。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [242] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的SPDE方法用于图神经网络中的不确定性估计，改善了在不同标签信息量下的分布外检测。


<details>
  <summary>Details</summary>
Motivation: 在图神经网络中，准确估计图上的不确定性十分困难，尤其是在分布变化的情况下。这种不确定性需同时考虑图结构和标签分布带来的随机性，这增加了复杂性。

Method: 通过类比马特恩高斯过程驱动的随机偏微分方程（SPDE）演化与图神经网络（GNN）层的消息传递，设计了一种新颖的消息传递方案，结合高斯过程对SPDE的空间-时间噪声处理方法。

Result: 我们的实验在具有不同标签信息量的图数据集上的分布外（OOD）检测中，验证了模型的合理性和优越性。

Conclusion: 我们的方法同时捕捉了空间和时间的不确定性，并允许对协方差内核平滑度的明确控制，从而增强了在低和高标签信息量的图上的不确定性估计。

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [243] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: GraPhy通过物理引导的图神经网络提高了贫困地区空气质量建模的精度和分辨率，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的空气质量监测网络在社会经济不发达地区往往比较稀疏，导致空气质量建模的精度和分辨率受到限制。为了减少公众对污染物的暴露，迫切需要细粒度的空气质量监测信息。

Method: 提出了一种名为GraPhy的物理引导图神经网络架构，专为低分辨率监测数据设计的层和边缘特征。

Result: 实验结果表明，与各种基线模型相比，GraPhy可以在平均平方误差（MSE）、平均绝对误差（MAE）和R平方值（R2）方面将性能提高9%到56%。此外，GraPhy在不同的空间异质性水平上也一致超过基线模型，证明了模型设计的有效性。

Conclusion: GraPhy是一种基于图的、物理引导的学习框架，可以在监测数据有限的城市区域进行高分辨率和准确的空气质量建模，尤其在社会经济不发达地区优势明显。

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [244] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: 引入了新架构basis transformers，以解决表格数据处理中的挑战，并在基准测试中显示出优异的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 处理表格数据具有挑战性，因为表格数据通常存在信息不完整、噪声以及异构结构等问题。现有技术在处理文本信息、可变数量的列以及缺乏元数据的未见数据时常常表现不佳。

Method: 提出了一种新颖的架构——basis transformers，旨在解决表格数据的这些挑战，同时尊重表格数据固有的不变性，包括分层结构和数值表示。

Result: 在多任务表格回归基准测试中评估设计，取得了多项改进，包括在OpenML-CTR23基准测试的34个任务中，中位数R^2分数提高了0.338，并且表现出最低的标准差。此外，模型参数数量比最佳基线少五倍，并超越了预训练的大型语言模型基线，即便初始化使用随机权重。

Conclusion: 提出的basis transformers架构有效解决了表格数据处理中的多项挑战，并在基准测试中展示了出色的性能和效率优势。

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [245] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 该研究提出针对不对称查询成本的决策型攻击框架，通过优化搜索策略及梯度估计，能有效降低总攻击成本，并取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实际应用中未考虑查询成本不对称性，尤其是在内容审核系统中，某些输出结果可能触发额外处理，导致成本更高。

Method: 提出两种关键改进：不对称搜索(AS)和不对称梯度估计(AGREST)，通过调整搜索策略和采样分布来降低查询成本。

Result: 在多种成本设置下，实现比现有方法更低的总查询成本和更小的扰动，部分设置下改进可达40%。

Conclusion: 本文提出了一种适用于不对称查询成本场景的通用框架：不对称黑盒攻击，可以集成到现有的攻击中，并在标准图像分类基准上进行理论分析和实证评估。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [246] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 研究分析了神经网络训练中的锐度现象，使用极简模型揭示其动态，并通过理论和实验增强理解。


<details>
  <summary>Details</summary>
Motivation: 尽管在实践中普遍观察到渐进锐化现象，其背后的机制仍然缺乏深入了解。

Method: 本文采用一个极简化的模型进行分析：一个每层只有一个节点的线性深度网络。

Result: 研究表明，该极简模型能够有效捕捉到最近实证研究中观察到的锐度动态，并通过理论分析展示数据集特性、网络深度、优化器的随机性和步长如何影响渐进锐化的程度。随后，作者通过实验证明这些理论见解在实际场景中的扩展性。

Conclusion: 该研究提供了对神经网络训练中锐度动态的更深刻理解，特别是在深度、训练数据和优化器之间的相互作用。

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [247] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: 提出一种基于风险正则化的分位数算法，通过整合条件风险值，在简单架构下确保安全约束，并在任务模拟中表现优于风险中性方法。


<details>
  <summary>Details</summary>
Motivation: 主流的近似动作值迭代强化学习算法在高方差随机环境中存在高估偏差，导致次优策略。

Method: 提出了一种基于风险正则化的分位数算法，通过整合条件风险值（CVaR）来增强安全性，并提供了关于风险敏感分布贝尔曼算子在Wasserstein空间中收缩性质的理论保证。

Result: 在动态的移动机器人到达-避让任务模拟中，该方法相比风险中性方法表现出更好的目标成功率、更少的碰撞以及更好的安全性能权衡。

Conclusion: 基于风险正则化的分位数算法能有效减轻动作值迭代过程中的高估偏差，同时在不需要复杂架构的情况下满足安全约束。

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [248] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: 该研究提出了一个利用医学知识提高临床预测领域泛化能力的方法，称为UdonCare，并在实验中表现出优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在临床预测中，患者群体常常表现出数据分布的变化，导致模型性能下降。典型的领域泛化方法在真实世界的医疗环境中面临两个主要问题：（1）通常没有可用的患者特定领域标签，使得领域发现特别困难；（2）纯粹以数据为驱动的方法忽视了关键的临床洞察，导致医学知识整合的差距。

Method: 本文提出了一个名为UdonCare的层次引导框架，该框架通过迭代修剪细粒度领域、编码这些精细领域，并应用一种Siamese-type推理机制，将领域相关信号与患者级特征分离。

Result: 实验结果表明，提出的模型在临床数据集（MIMIC-III和MIMIC-IV）上表现出更高的性能。

Conclusion: 本文提出的模型在存在显著领域差距时表现优于其他领域泛化基线，强调了医学知识在提高实际医疗应用中的领域泛化能力方面的潜力。

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [249] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 设计了一种新的算法SEE，用于解决纯探索中的1-识别问题，并在非渐近情况下实现了上下界近乎最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 受到现有文献中的一个开放方向的激励，研究用于纯探索的基础多臂老虎机问题的1-识别问题，以确定是否存在至少达到已知阈值的奖励平均值，或输出None。

Method: 设计了一种新的算法Sequential-Exploration-Exploitation，并通过理论分析证明了在非渐近情况下的近乎最优性。

Result: 新算法达到了上下界之间的近乎最优匹配，数值结果显示该算法相较于现有基准更为有效。

Conclusion: 提出了一种新的算法SEE，并在非渐近的视角下进行了理论分析，显示其有效性和接近最优性。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [250] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: MoXGATE 提供了一种创新的多组学数据整合方法，提高癌症亚型分类准确性并具有较好的生物学泛化能力。


<details>
  <summary>Details</summary>
Motivation: 有效整合多组学数据对于个性化治疗和预后评估至关重要，但由于组学特征的异质性，整合一直具有挑战性。

Method: 提出了一种结合跨注意力机制和可学习模态权重的深度学习框架 MoXGATE，用于增强多组学数据的特征融合。

Result: MoXGATE 在 GIAC 和 BRCA 数据集上的分类准确率达到 95%，表现优于现有方法，且对未见过的癌症类型具有良好的泛化能力。

Conclusion: MoXGATE 是一种通过跨模态的多组学集成框架、模态加权融合及焦损失的应用来提高癌症亚型分类性能的有前途方法。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [251] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 该论文提出了一种新的经过认证的机器遗忘方法，使用噪声微调来确保遗忘保障，在多个环境中有效且无需对损失函数的假设。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和法规要求（如“被遗忘权”），需要在请求时从模型中移除特定训练数据的影响。现有方法存在限制性假设或缺乏正式保证，因此我们提出了一种新的经过认证的机器遗忘方法。

Method: 我们的方法使用对保留数据的噪声微调，即不需要删除的数据，以确保可验证的遗忘保障。该方法无需对潜在损失函数的假设，使其具备广泛的适用性。

Result: 我们的方法不仅在理论上提供了明确的遗忘保障，还在实践中有效运行，优于现有的基准方法。

Conclusion: 我们提出了一种新的经过认证的机器遗忘方法，它利用随机后处理来加强隐私保护并确保可验证的遗忘保障。该方法在效率和准确性方面具有理论上的权衡，并在实践中表现出色，优于现有的基准。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [252] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: 本文改进了Hyperblock模型，使其在减少复杂性的同时保持高准确度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 增强模型的可解释性，减少训练时间，并在不牺牲准确度的情况下降低模型复杂性。

Method: 引入了一套Hyperblock简化算法，包括去除冗余属性、通过重叠分析去除冗余块以及创建析取单元，还引入可解释的k-近邻回退机制。

Result: 在WBC和MNIST等基准数据集上，实现了在保持竞争性准确度的同时，显著减少了模型复杂性。

Conclusion: 通过简化Hyperblock模型的复杂性，同时保持卓越的分类能力，实现了对高维、大量数据集的可解释模型的拓展。

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [253] [How do higher-order interactions shape the energy landscape?](https://arxiv.org/abs/2506.06791)
*Zheng Wang,Wenchang Qi,Jinjie Zhu,Xianbin Liu*

Main category: nlin.AO

TL;DR: 研究揭示高阶相互作用扩展或收缩耦合振荡器网络不同状态的盆地，并提高某些状态的稳定性，为复杂系统同步控制提供新见解。


<details>
  <summary>Details</summary>
Motivation: 了解高阶相互作用如何塑造耦合振荡器网络的能量景观，对表征复杂的同步现象至关重要。

Method: 研究采用广义库拉莫托模型，并结合确定性盆地分析、噪声诱导的转变及量子退火方法进行分析。

Result: 揭示了三元耦合时高绕数态和非扭曲态相对于同步态的稳定性提高，并且系统可能展示出深势阱的稳定性不对称性，使其抗噪声转变能力增强。

Conclusion: 研究揭示了高阶相互作用对耦合振荡器网络能量景观的双重影响。高阶相互作用一方面扩展了非扭曲态的盆地，另一方面收缩了扭曲态的盆地，同时也改变了两者的势阱深度。随着三元耦合的增强，高绕数态和非扭曲态相对于同步态的稳定性提高。系统表现出显著的稳定性不对称性，小盆地的状态有深的势阱，使其一旦形成便高度抗噪声诱导的状态转变。这些发现将准势理论扩展到高维网络系统，并为控制复杂系统中的同步提供了新的见解。

Abstract: Understanding how higher-order interactions shape the energy landscape of
coupled oscillator networks is crucial for characterizing complex
synchronization phenomena. Here, we investigate a generalized Kuramoto model
with triadic interactions, combining deterministic basin analysis,
noise-induced transitions, and quantum annealing methods. We uncover a dual
effect of higher-order interactions: they simultaneously expand basins for
non-twisted states while contracting those of twisted states, yet modify
potential well depths for both. As triadic coupling strengthens,
higher-winding-number states and non-twisted states gain stability relative to
synchronized states. The system exhibits remarkable stability asymmetry, where
states with small basins can possess deep potential wells, making them highly
resistant to noise-induced transitions once formed. These findings extend
quasipotential theory to high-dimensional networked systems and offer new
insights for controlling synchronization in complex systems.

</details>


### [254] [Resilience and adaptability in self-evidencing systems](https://arxiv.org/abs/2506.06897)
*Mahault Albarracin,Dalton A R Sakthivadivel*

Main category: nlin.AO

TL;DR: 论文探讨了韧性和自由能原则的关系，通过自组织过程的角度来理解身份的维护，提出了一个韧性的思考框架并提供相应模型。


<details>
  <summary>Details</summary>
Motivation: 动机在于探讨韧性与自由能原则之间的关系以及自组织对于保持身份的作用。

Method: 通过自由能原则建模韧性，并扩展为自组织过程涉及身份自我重新配置的框架。

Result: 提供了一个关于在自由能原则下思考韧性的框架，并用模型来展示和验证这一框架的应用。

Conclusion: 文章得出结论，自组织在自由能原则下与韧性相关，表明自我身份的持续存在涉及自我重新配置的能量和过程。

Abstract: In this paper we will articulate a view of resilience under the free energy
principle and vice versa. The free energy principle is about existence and
identity, and resilience is the condition under which things exist at all. In
previous work this has been investigated as modelling resilience using the free
energy principle. We will extend that work by making the case that
self-organisation under the free energy principle is about resilience, in the
sense that identity is a constant process of self-reconfiguration, implying the
existence of a self-model and the energy to reconfigure that self-model -- and
hence, the resilience of a maintained identity under changes. A general
framework for thinking about resilience in this context will be sketched out
and some models will be provided using that framework.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [255] [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)
*Chance Jiajie Li,Jiayi Wu,Zhenze Mo,Ao Qu,Yuhan Tang,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Jinhua Zhao,Paul Liang,Luis Alonso,Kent Larson*

Main category: cs.CY

TL;DR: 提出生成型思维（GenMinds）和RECAP框架，以提高生成代理在社会模拟中的认知和因果分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-based代理在模拟个人和群体行为时缺乏内部一致性、因果推理和信念可追溯性。因此，本文旨在解决这些问题，以提高生成代理在社会模拟中的可靠性。

Method: 通过开发生成型思维（GenMinds）及其评测框架RECAP（重建因果路径）来支持生成代理中的结构化认知和因果分析。

Result: 引入了RECAP框架，用于评估生成代理的推理保真度以及因果可追踪性、人口统计学基础和干预一致性。这将模拟思维而不仅仅是语言，从而更好地进行社会模拟。

Conclusion: 本文提出了一种新的概念建模范式，即生成型思维（GenMinds），用于通过从认知科学借鉴来支持生成代理中的结构化信念表示。

Abstract: Simulating society with large language models (LLMs), we argue, requires more
than generating plausible behavior -- it demands cognitively grounded reasoning
that is structured, revisable, and traceable. LLM-based agents are increasingly
used to emulate individual and group behavior -- primarily through prompting
and supervised fine-tuning. Yet they often lack internal coherence, causal
reasoning, and belief traceability -- making them unreliable for analyzing how
people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds
(GenMinds), which draws from cognitive science to support structured belief
representations in generative agents. To evaluate such agents, we introduce the
RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess
reasoning fidelity via causal traceability, demographic grounding, and
intervention consistency. These contributions advance a broader shift: from
surface-level mimicry to generative agents that simulate thought -- not just
language -- for social simulations.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [256] [Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies](https://arxiv.org/abs/2506.06325)
*Viorica Rozina Chifu,Tudor Cioara,Cristina Bianca Pop,Ionut Anghel*

Main category: cs.NE

TL;DR: 提出一种去中心化的微电网能源合作模型，通过进化算法进行交易行为建模，在仿真场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 发展去中心化的微电网能源合作模型，以实现社区层面的决策自主性。

Method: 通过进化算法进行交易行为的建模，使用一种专门针对矩阵结构的重组运算符以及高斯分布的变异运算符来实现种群进化。

Result: 100个微电网中有95个达到了稳定的能源状态，证明了该方法的有效性。

Conclusion: 所提出的模型能够实现微电网内个体层面和整个社区层面的稳定能源状态。

Abstract: This paper proposes a decentralized model of energy cooperation between
microgrids, in which decisions are made locally, at the level of the microgrid
community. Each microgrid is modeled as an autonomous agent that adopts a Hawk
or Dove strategy, depending on the level of energy stored in the battery and
its role in the energy trading process. The interactions between selling and
buying microgrids are modeled through an evolutionary algorithm. An individual
in the algorithm population is represented as an energy trading matrix that
encodes the amounts of energy traded between the selling and buying microgrids.
The population evolution is achieved by recombination and mutation operators.
Recombination uses a specialized operator for matrix structures, and mutation
is applied to the matrix elements according to a Gaussian distribution. The
evaluation of an individual is made with a multi-criteria fitness function that
considers the seller profit, the degree of energy stability at the community
level, penalties for energy imbalance at the community level and for the
degradation of microgrids batteries. The method was tested on a simulated
scenario with 100 microgrids, each with its own selling and buying thresholds,
to reflect a realistic environment with variable storage characteristics of
microgrids batteries. By applying the algorithm on this scenario, 95 out of the
100 microgrids reached a stable energy state. This result confirms the
effectiveness of the proposed model in achieving energy balance both at the
individual level, for each microgrid, and at the level of the entire community.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [257] [AI Agent Behavioral Science](https://arxiv.org/abs/2506.06366)
*Lin Chen,Yunke Zhang,Jie Feng,Haoye Chai,Honglin Zhang,Bingbing Fan,Yibo Ma,Shiyuan Zhang,Nian Li,Tianhui Liu,Nicholas Sukiennik,Keyu Zhao,Yu Li,Ziyi Liu,Fengli Xu,Yong Li*

Main category: q-bio.NC

TL;DR: 该论文提出AI代理行为科学，以观察并解释AI系统行为，并推动在公平、安全、解释性等方面的研究。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的进步，AI系统在交互和开放性场景中展现出越来越像人类的行为。这种行为不仅仅是模型内部架构的产物，而是从它们整合到代理系统中并在特定环境下运作时所产生的，其中目标、反馈和互动随着时间的推移塑造行为。这一转变呼唤新的科学视角：AI代理行为科学。

Method: 系统化研究个人、多代理和人类代理交互场景中的AI行为，通过设计干预来测试假设，并基于理论指导解释AI代理如何随着时间的推移行动、适应和互动。

Result: 提出了AI代理行为科学作为对传统方法的必要补充，提供理解、评估和治理越来越自主的AI系统在现实世界行为方面的基本工具。

Conclusion: AI代理行为科学强调观察行为、设计实验以验证假设，并解释AI代理如何随着时间的推移行动和互动。这种视角有助于处理公平、安全、可解释性、问责和隐私作为行为属性的问题，并为负责的AI开发提供信息。

Abstract: Recent advances in large language models (LLMs) have enabled AI systems to
behave in increasingly human-like ways, exhibiting planning, adaptation, and
social dynamics across increasingly diverse, interactive, and open-ended
scenarios. These behaviors are not solely the product of the models' internal
architecture, but emerge from their integration into agentic systems that
operate within situated contexts, where goals, feedback, and interactions shape
behavior over time. This shift calls for a new scientific lens: AI Agent
Behavioral Science. Rather than focusing only on internal mechanisms, this
paradigm emphasizes the systematic observation of behavior, design of
interventions to test hypotheses, and theory-guided interpretation of how AI
agents act, adapt, and interact over time. We systematize a growing body of
research across individual, multi-agent, and human-agent interaction settings,
and further demonstrate how this perspective informs responsible AI by treating
fairness, safety, interpretability, accountability, and privacy as behavioral
properties. By unifying recent findings and laying out future directions, we
position AI Agent Behavioral Science as a necessary complement to traditional
approaches, providing essential tools for understanding, evaluating, and
governing the real-world behavior of increasingly autonomous AI systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [258] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: 研究引入一个名为KRAMABENCH的新基准测试，以评估AI系统在数据科学管道中的性能，结果表明当前模型在处理现实世界问题时还有待完善。


<details>
  <summary>Details</summary>
Motivation: 设计和实现数据科学管道需要领域知识、技术专长和项目特定的见解。尽管AI系统在推理、编码和理解能力方面表现出色，但这些能力能否成功地转化为复杂管道的设计和执行尚不确定。

Method: 引入了一个名为KRAMABENCH的基准测试，包括104个手动整理的现实世界数据科学管道，涉及1700个数据文件，来自6个不同领域的24个数据源。此基准测试评估AI系统在数据处理中的端到端能力，测试了5个通用模型和3个代码生成模型。使用参考框架DS-GURU指导AI模型将问题分解为一系列子任务，通过每一步进行推理，并合成实现所设计的Python代码。

Result: 在KRAMABENCH上的结果显示，当需要广泛的数据处理和领域知识时，现有的模型无法有效地构建现实世界的数据科学管道。

Conclusion: 虽然现有的AI模型在解决数据科学代码生成任务中表现出色，但在需要广泛的数据处理和领域知识来构建现实世界的数据科学管道时，这些模型还不够完备。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: CPS-Guard is a novel framework for automating assurance in AI-powered Cyber-Physical Systems, offering continuous evaluation through a multi-role orchestration approach.


<details>
  <summary>Details</summary>
Motivation: Traditional verification and validation methods struggle to handle the unpredictable and dynamic nature of AI components in CPS, necessitating a new approach for assurance in critical applications.

Method: CPS-Guard employs multi-role orchestration by assigning specialized roles like safety monitoring, security assessment, fault injection, and recovery planning to dedicated agents within a simulated environment. This allows for continuous evaluation and refinement of AI behavior against dependability requirements.

Result: Through a case study with an autonomous vehicle, CPS-Guard effectively detects vulnerabilities, manages performance impacts, and supports adaptive recovery strategies.

Conclusion: CPS-Guard provides a structured and extensible solution for rigorous verification and validation in safety- and security-critical systems.

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [260] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: 提出了一种边缘启用的协作物体检测框架（ECOD），通过感知聚合与协作估计、可变对象计数与评估算法实现多视角物体检测，显著提升了自动驾驶系统的检测准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统车载感知系统的准确性受阻于遮挡和盲区，而云端解决方案的高延迟使其不适合动态环境中自动驾驶的实时处理需求。因此，需要一种既能够增强检测精度又能保持低延迟的解决方案。

Method: 引入了一种边缘启用的协作物体检测(ECOD)框架，包括两个关键算法：感知聚合与协作估计(PACE)和可变对象计数与评估(VOTE)。在边缘服务器上聚合来自多个CAV的检测数据以增强感知能力，并通过基于共识的投票机制提升物体分类准确性。

Result: 实验结果表明，ECOD框架在物体分类准确性方面有显著改进，超越了传统车载单视角方法，提升幅度达到75%。

Conclusion: ECOD框架通过使用边缘计算和多CAV协作，显著提高了CAV的物体检测准确性，比传统单一视角车载方法在分类准确性上提升了75%。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [261] [Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution](https://arxiv.org/abs/2506.07293)
*Seabin Lee,Joonyeol Sim,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出了一种基于路径规划的多机器人任务分配方法，能有效处理密集环境下的机器人冲突和死锁问题，并在实验中优于竞品。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多障碍环境下不理想，无法有效解决机器人之间的冲突导致的额外成本问题，因此需要一种新的方法来优化机器人路径，避免冲突和死锁。

Method: 提出了一种可扩展的多机器人任务分配方法，通过构建广义Voronoi图进行路径规划，利用分区和推送机制优化任务分配，避免机器人之间的冲突和僵局。

Result: 大量实验表明，提出的方法在密集环境中可以处理数百个机器人的任务分配，表现优于竞品，能够在限定时间内给出解决方案。

Conclusion: 提出的方法能有效解决多机器人任务分配问题，在密集障碍环境中表现出色，能够在大量机器人的情况下有效工作。

Abstract: We consider the Multi-Robot Task Allocation (MRTA) problem that aims to
optimize an assignment of multiple robots to multiple tasks in challenging
environments which are with densely populated obstacles and narrow passages. In
such environments, conventional methods optimizing the sum-of-cost are often
ineffective because the conflicts between robots incur additional costs (e.g.,
collision avoidance, waiting). Also, an allocation that does not incorporate
the actual robot paths could cause deadlocks, which significantly degrade the
collective performance of the robots.
  We propose a scalable MRTA method that considers the paths of the robots to
avoid collisions and deadlocks which result in a fast completion of all tasks
(i.e., minimizing the \textit{makespan}). To incorporate robot paths into task
allocation, the proposed method constructs a roadmap using a Generalized
Voronoi Diagram. The method partitions the roadmap into several components to
know how to redistribute robots to achieve all tasks with less conflicts
between the robots. In the redistribution process, robots are transferred to
their final destinations according to a push-pop mechanism with the first-in
first-out principle. From the extensive experiments, we show that our method
can handle instances with hundreds of robots in dense clutter while competitors
are unable to compute a solution within a time limit.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [262] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: GOLFer uses smaller open-source LMs with a hallucination filter and documents combiner to effectively improve query expansion while reducing reliance on large LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the cost, computational intensity, and limited accessibility associated with large-scale LLM-based query expansions by using smaller open-source LMs in a more efficient way.

Method: GOLFer consists of two main modules: 1) a hallucination filter to detect and remove non-factual and inconsistent sentences in generated documents, and 2) a documents combiner that merges the filtered content with the query using a weight vector to balance their influences.

Result: Experimental results show that GOLFer consistently outperforms other methods when using smaller LMs and maintains a competitive performance level compared with approaches using large-size LLMs.

Conclusion: GOLFer consistently outperforms other query expansion methods using smaller LMs, while maintaining competitive performance against methods using larger LLMs, indicating its effectiveness.

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [263] [Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning](https://arxiv.org/abs/2309.11082)
*Chen Jiang,Hong Liu,Xuzheng Yu,Qing Wang,Yuan Cheng,Jia Xu,Zhongyi Liu,Qingpei Guo,Wei Chu,Ming Yang,Yuan Qi*

Main category: cs.CV

TL;DR: 该论文提升了文本视频检索，通过引入DMAE和TPM-CL两个新技术，解决了困难负样本挖掘和细粒度语义建模的问题，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法对困难负样本关注不足，且缺乏建模不同语义相似度的能力。

Method: 提出了Dual-Modal Attention-Enhanced Module(DMAE)用于挖掘困难负样本，并引入了Negative-aware InfoNCE(NegNCE)损失。此外，提出了Triplet Partial Margin Contrastive Learning(TPM-CL)模块，通过自生成细粒度的困难负样本构建部分顺序三元组样本。

Result: 实验表明，提出的方法在MSR-VTT、MSVD、DiDeMo和ActivityNet四个文本视频检索数据集上超越了现有方法。

Conclusion: 提出的TPM-CL和DMAE方法显著提高了文本视频检索的性能，在四个广泛使用的数据集上超过了现有方法。

Abstract: In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.

</details>


### [264] [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.CV

TL;DR: 本文通过扩展和微调MiniGPT-4模型进行逆向设计任务，证明了其在复杂任务中具备较好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 近年来视觉语言模型取得重大进展，尤其是在与大型语言模型结合后。逆向设计作为一种复杂的视觉语言任务，旨在预测给定源图像、编辑版本和可选的高级文本编辑描述的编辑及其参数。这个任务需要视觉语言模型理解源图像、编辑版本和可选文本上下文之间的相互作用，超越传统视觉语言任务。

Method: 在本文中，我们扩展并微调了MiniGPT-4，用于逆向设计任务。

Result: 我们的实验表明，现成的视觉语言模型，特别是MiniGPT-4，在处理更复杂任务（例如逆向设计）方面的可扩展性。

Conclusion: MiniGPT-4为逆向设计任务展示了良好的可扩展性效果，这表明视觉语言模型能够进行复杂的多模态任务处理。

Abstract: Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}

</details>


### [265] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow是基于归一化流的生成模型，结合自回归变压器，实现高分辨率图像合成，样本质量接近于最新扩散模型。


<details>
  <summary>Details</summary>
Motivation: 探索在高分辨率图像合成中实现强性能的可扩展生成模型。

Method: 使用归一化流结合自回归变压器结构进行高分辨率图像合成。主要通过深浅设计、潜在空间建模以及创新指导算法来增强模型的可扩展性和样本质量。

Result: STARFlow在类条件和文本条件图像生成任务中表现优异，达到与最先进扩散模型相当的样本质量。

Conclusion: 该研究首次成功展示了归一化流在如此规模和分辨率下有效运行，证明了TARFlow的理论普遍性以及模型架构和算法创新的有效性。

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [266] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: 论文介绍了一种名为DigitalShadow的先进早期预警系统，使用经过微调的面部模型来进行CAD风险评估，可以无接触地分析面部特征，并生成健康建议，确保数据隐私。


<details>
  <summary>Details</summary>
Motivation: CAD是全球死亡的主要原因之一，并且大多数可以预防，因此早期检测和主动管理至关重要。

Method: 使用了一个经过微调的面部基础模型，该模型首先在2100万张面部图像上进行预训练，然后在来自中国四家医院的1751名受试者的7004张面部图像上进行微调，成为专门的CAD风险评估模型LiveCAD。

Result: DigitalShadow可以被集成到个性化数据库中，以生成自然语言风险报告和个性化的健康建议，无需用户主动参与。

Conclusion: DigitalShadow系统能够被安全地本地部署，以确保用户数据的安全处理。

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [267] [Deep Equivariant Multi-Agent Control Barrier Functions](https://arxiv.org/abs/2506.07755)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: eess.SY

TL;DR: 研究引入了一种注入对称性的控制屏障函数，以提高多智能体系统的安全性、可扩展性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的方法常常在可扩展性、泛化和采样效率上表现不足，因为它们忽略了系统固有的几何结构。因此，该研究旨在通过引入对称性注入的分布式控制屏障函数来解决这些问题。

Method: 研究引入了对称性注入的分布式控制屏障函数，利用可以学习的基于图的安全证书来强制满足内在对称性。提出了一种通过兼容的群作用构建等变组模块化网络的方法。这种方法是简单、高效且可适应的。

Result: 通过在多机器人导航任务上的广泛模拟，该方法在安全性、可扩展性和任务成功率方面优于现有的先进基线。

Conclusion: 嵌入对称性的分布式神经策略在安全性、可扩展性和任务成功率方面优于现有的先进基线方法。这种方法强调了在安全分布式神经策略中嵌入对称性的重要性。

Abstract: With multi-agent systems increasingly deployed autonomously at scale in
complex environments, ensuring safety of the data-driven policies is critical.
Control Barrier Functions have emerged as an effective tool for enforcing
safety constraints, yet existing learning-based methods often lack in
scalability, generalization and sampling efficiency as they overlook inherent
geometric structures of the system. To address this gap, we introduce
symmetries-infused distributed Control Barrier Functions, enforcing the
satisfaction of intrinsic symmetries on learnable graph-based safety
certificates. We theoretically motivate the need for equivariant
parametrization of CBFs and policies, and propose a simple, yet efficient and
adaptable methodology for constructing such equivariant group-modular networks
via the compatible group actions. This approach encodes safety constraints in a
distributed data-efficient manner, enabling zero-shot generalization to larger
and denser swarms. Through extensive simulations on multi-robot navigation
tasks, we demonstrate that our method outperforms state-of-the-art baselines in
terms of safety, scalability, and task success rates, highlighting the
importance of embedding symmetries in safe distributed neural policies.

</details>
