<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.LG](#cs.LG) [Total: 34]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes](https://arxiv.org/abs/2506.05380)
*Yiliang Zhou,Abigail M. Newbury,Gongbo Zhang,Betina Ross Idnay,Hao Liu,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: This paper introduces EvidenceOutcomes, a corpus for clinically meaningful outcome extraction in medicine, showing high annotation quality and effective model performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks often neglect or oversimplify outcomes in evidence-based medicine, necessitating a robust corpus focused on clinically meaningful outcome extraction.

Method: Development of annotation guidelines through clinician and NLP expert collaboration, annotation of biomedical literature by independent annotators, and fine-tuning of the PubMedBERT model.

Result: EvidenceOutcomes achieved high inter-rater agreement (0.76) and good performance of the PubMedBERT model (F1-score of 0.69 at the entity level and 0.76 at the token level).

Conclusion: EvidenceOutcomes provides a high-quality benchmark for extracting clinically meaningful outcomes from biomedical literature and can aid the development of future machine learning models.

Abstract: The fundamental process of evidence extraction and synthesis in
evidence-based medicine involves extracting PICO (Population, Intervention,
Comparison, and Outcome) elements from biomedical literature. However,
Outcomes, being the most complex elements, are often neglected or
oversimplified in existing benchmarks. To address this issue, we present
EvidenceOutcomes, a novel, large, annotated corpus of clinically meaningful
outcomes extracted from biomedical literature. We first developed a robust
annotation guideline for extracting clinically meaningful outcomes from text
through iteration and discussion with clinicians and Natural Language
Processing experts. Then, three independent annotators annotated the Results
and Conclusions sections of a randomly selected sample of 500 PubMed abstracts
and 140 PubMed abstracts from the existing EBM-NLP corpus. This resulted in
EvidenceOutcomes with high-quality annotations of an inter-rater agreement of
0.76. Additionally, our fine-tuned PubMedBERT model, applied to these 500
PubMed abstracts, achieved an F1-score of 0.69 at the entity level and 0.76 at
the token level on the subset of 140 PubMed abstracts from the EBM-NLP corpus.
EvidenceOutcomes can serve as a shared benchmark to develop and test future
machine learning algorithms to extract clinically meaningful outcomes from
biomedical abstracts.

</details>


### [2] [LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models](https://arxiv.org/abs/2506.05385)
*Xinxin Li,Huiyao Chen,Chengjun Liu,Jing Li,Meishan Zhang,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 将检索增强和自我纠正机制应用于大型语言模型，在语义角色标注任务上取得了最新成绩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种自然语言处理任务中表现突出，但在语义角色标注任务上仍不如最新的编码器-解码器模型。

Method: 本文将通过引入两种机制来增强大型语言模型：一是检索增强生成，二是自我纠正。前者利用外部语言知识，后者识别并纠正不一致的输出。

Result: 在三大广泛使用的基准上进行了实验，显示该方法在中文和英文中均达到了最新水平。

Conclusion: 本文提出的方法首次使大型语言模型在语义角色标注任务上超越了编码器-解码器模型。

Abstract: Semantic role labeling (SRL) is a crucial task of natural language processing
(NLP). Although generative decoder-based large language models (LLMs) have
achieved remarkable success across various NLP tasks, they still lag behind
state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we
seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a)
retrieval-augmented generation and (b) self-correction. The first mechanism
enables LLMs to leverage external linguistic knowledge such as predicate and
argument structure descriptions, while the second allows LLMs to identify and
correct inconsistent SRL outputs. We conduct extensive experiments on three
widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results
demonstrate that our method achieves state-of-the-art performance in both
Chinese and English, marking the first successful application of LLMs to
surpass encoder-decoder approaches in SRL.

</details>


### [3] [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)
*Lo Pang-Yun Ting,Chengshuai Zhao,Yu-Hua Zeng,Yuan Jee Lim,Kun-Ta Chuang*

Main category: cs.CL

TL;DR: R2AG使用强化学习和医学知识图谱来改善长篇临床笔记生成，被证明在临床有效性和语言生成方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的方法在生成长篇临床文本时，由于患者信息有限而表现不佳，因此需要一种能够提供明确语义指导的系统以改进生成结果。

Method: 使用强化学习来训练R2AG以从医学知识图谱中检索推理路径，并提出了一种名为Group-Based Retriever Optimization (GRO) 的优化方法，以提高检索质量。

Result: 实验结果显示，R2AG在MIMIC-IV-Note数据集上的表现优于基线，能够帮助大型语言模型通过关注关键证据和遵循连贯推理来避免临床误解。

Conclusion: R2AG通过引入医学知识图谱中的推理路径和使用强化学习来填补输入信息稀疏的语义空白，以提高生成长篇临床笔记的效果。在实验中显示其在临床有效性和自然语言生成方面优于基线方法。

Abstract: Clinical note generation aims to automatically produce free-text summaries of
a patient's condition and diagnostic process, with discharge instructions being
a representative long-form example. While recent large language model
(LLM)-based methods pre-trained on general clinical corpora show promise in
clinical text generation, they fall short in producing long-form notes from
limited patient information. In this paper, we propose R2AG, the first
reinforced retriever for long-form discharge instruction generation based on
pre-admission data. R2AG is trained with reinforcement learning to retrieve
reasoning paths from a medical knowledge graph, providing explicit semantic
guidance to the LLM. To bridge the information gap, we propose Group-Based
Retriever Optimization (GRO) which improves retrieval quality with
group-relative rewards, encouraging reasoning leaps for deeper inference by the
LLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG
outperforms baselines in both clinical efficacy and natural language generation
metrics. Further analysis reveals that R2AG fills semantic gaps in sparse input
scenarios, and retrieved reasoning paths help LLMs avoid clinical
misinterpretation by focusing on key evidence and following coherent reasoning.

</details>


### [4] [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/abs/2506.05387)
*Jaydip Sen,Saptarshi Sengupta. Subhasis Dasgupta*

Main category: cs.CL

TL;DR: The chapter proposes ASTS to enhance text generation in LLMs, outperforming existing methods in fluency and coherence while maintaining diversity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional decoding methods face challenges in balancing fluency, diversity, and coherence in text generation. The paper aims to address these challenges by enhancing the LTS algorithm.

Method: The paper proposes Adaptive Semantic-Aware Typicality Sampling (ASTS) which incorporates dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments to enhance LTS.

Result: ASTS ensures contextually coherent and diverse text generation while maintaining computational efficiency. It demonstrates superior performance in benchmarks like story generation and abstractive summarization, evaluated using metrics such as perplexity, MAUVE, and diversity scores.

Conclusion: ASTS outperforms existing sampling techniques by reducing repetition, enhancing semantic alignment, and improving fluency.

Abstract: This chapter explores advancements in decoding strategies for large language
models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)
algorithm. Traditional decoding methods, such as top-k and nucleus sampling,
often struggle to balance fluency, diversity, and coherence in text generation.
To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)
is proposed as an improved version of LTS, incorporating dynamic entropy
thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS
ensures contextually coherent and diverse text generation while maintaining
computational efficiency. Its performance is evaluated across multiple
benchmarks, including story generation and abstractive summarization, using
metrics such as perplexity, MAUVE, and diversity scores. Experimental results
demonstrate that ASTS outperforms existing sampling techniques by reducing
repetition, enhancing semantic alignment, and improving fluency.

</details>


### [5] [taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades](https://arxiv.org/abs/2506.05388)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: taz2024full是目前最大的德语报纸文章公开语料库，有助于研究性别代表性等语言和社会问题。


<details>
  <summary>Details</summary>
Motivation: 由于德语的大规模资源有限，限制了对语言趋势和社会问题（如性别偏见）的研究，因此需要创建一个庞大的德语语料库。

Method: 使用可扩展的结构化分析程序，研究报导中的演员提及、情感以及语言框架。

Result: 分析发现，报道中男性被过度代表，但近年来逐渐趋向于更加平衡的报道。

Conclusion: taz2024full语料库为研究德语语境中的偏见和歧视提供了重要资源，支持各种应用并促进包容性和可重复性研究。

Abstract: Open-access corpora are essential for advancing natural language processing
(NLP) and computational social science (CSS). However, large-scale resources
for German remain limited, restricting research on linguistic trends and
societal issues such as gender bias. We present taz2024full, the largest
publicly available corpus of German newspaper articles to date, comprising over
1.8 million texts from taz, spanning 1980 to 2024.
  As a demonstration of the corpus's utility for bias and discrimination
research, we analyse gender representation across four decades of reporting. We
find a consistent overrepresentation of men, but also a gradual shift toward
more balanced coverage in recent years. Using a scalable, structured analysis
pipeline, we provide a foundation for studying actor mentions, sentiment, and
linguistic framing in German journalistic texts.
  The corpus supports a wide range of applications, from diachronic language
analysis to critical media studies, and is freely available to foster inclusive
and reproducible research in German-language NLP.

</details>


### [6] [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
*Markelle Kelly,Mohammad Tahaei,Padhraic Smyth,Lauren Wilcox*

Main category: cs.CL

TL;DR: 研究探讨了在电子商务领域大语言模型中的性别偏见，开发了分类法并进行了量化分析，揭示了产品描述中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型（LLMs）中的性别偏见已被广泛研究，但在电子商务中的应用仍缺乏研究。电子商务可能揭示新的算法偏见和危害形式。

Method: 开发数据驱动的性别偏见分类法，分析AI生成的产品描述中的性别偏见，并在此过程中将其与现有的危害分类法联系起来。

Result: 揭示了性别偏见的一些独特而未充分探索的维度，如对服装尺寸的假设、产品特征广告中的刻板偏见以及劝说语言使用的差异。这些发现加深了我们对电子商务背景下AI危害如排斥性规范、刻板印象及性能差异的理解。

Conclusion: 研究成果表明，在电子商务领域使用的大规模语言模型中，性别偏见普遍存在，需特别的检测和缓解方法。

Abstract: While gender bias in large language models (LLMs) has been extensively
studied in many domains, uses of LLMs in e-commerce remain largely unexamined
and may reveal novel forms of algorithmic bias and harm. Our work investigates
this space, developing data-driven taxonomic categories of gender bias in the
context of product description generation, which we situate with respect to
existing general purpose harms taxonomies. We illustrate how AI-generated
product descriptions can uniquely surface gender biases in ways that require
specialized detection and mitigation approaches. Further, we quantitatively
analyze issues corresponding to our taxonomic categories in two models used for
this task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these
forms of bias commonly occur in practice. Our results illuminate unique,
under-explored dimensions of gender bias, such as assumptions about clothing
size, stereotypical bias in which features of a product are advertised, and
differences in the use of persuasive language. These insights contribute to our
understanding of three types of AI harms identified by current frameworks:
exclusionary norms, stereotyping, and performance disparities, particularly for
the context of e-commerce.

</details>


### [7] [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
*Shenyang Huang,Ali Parviz,Emma Kondrup,Zachary Yang,Zifeng Ding,Michael Bronstein,Reihaneh Rabbany,Guillaume Rabusseau*

Main category: cs.CL

TL;DR: 该研究提出了TGTalker，一种为大型语言模型设计的新型时间图学习框架，展示了强大的链接预测能力和解释能力。


<details>
  <summary>Details</summary>
Motivation: 在动态图上应用大型语言模型的能力尚未被充分探索，涉及真实世界的变化网络，需要填补这一领域的空白。

Method: TGTalker利用时间图中的最近偏置来提取结构信息，并将其转换为自然语言供大型语言模型使用，同时利用时间邻居作为预测的附加信息。

Result: TGTalker在链接预测能力上与现有的时间图神经网络模型相比具有竞争力，且在多个模型上表现优于流行模型如TGN和HTGN。

Conclusion: TGTalker在五个真实世界网络中表现良好，并且在解释性和可解释性方面提供了新的方向。

Abstract: Large Language Models (LLMs) have recently driven significant advancements in
Natural Language Processing and various other applications. While a broad range
of literature has explored the graph-reasoning capabilities of LLMs, including
their use of predictors on graphs, the application of LLMs to dynamic graphs --
real world evolving networks -- remains relatively unexplored. Recent work
studies synthetic temporal graphs generated by random graph models, but
applying LLMs to real-world temporal graphs remains an open question. To
address this gap, we introduce Temporal Graph Talker (TGTalker), a novel
temporal graph learning framework designed for LLMs. TGTalker utilizes the
recency bias in temporal graphs to extract relevant structural information,
converted to natural language for LLMs, while leveraging temporal neighbors as
additional information for prediction. TGTalker demonstrates competitive link
prediction capabilities compared to existing Temporal Graph Neural Network
(TGNN) models. Across five real-world networks, TGTalker performs competitively
with state-of-the-art temporal graph methods while consistently outperforming
popular models such as TGN and HTGN. Furthermore, TGTalker generates textual
explanations for each prediction, thus opening up exciting new directions in
explainability and interpretability for temporal link prediction. The code is
publicly available at https://github.com/shenyangHuang/TGTalker.

</details>


### [8] [Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations](https://arxiv.org/abs/2506.05400)
*Ayesha Qamar,Arushi Raghuvanshi,Conal Sathi,Youngseo Son*

Main category: cs.CL

TL;DR: 本研究提出Auto Review系统，旨在通过改善电话记录的准确性来减少人工审核工作量，并改善医疗服务效率。


<details>
  <summary>Details</summary>
Motivation: 在电话记录中噪声较大，为了减少人工审核工作量，同时保持高准确性，需要自动化后续处理阶段。

Method: 引入一个二阶段后处理流程，利用多自动语音识别（ASR）替代方案和一种不需要手动校正的伪标签方法来进行信息提取。

Result: 通过实验，利用通用大语言模型和基于特征的模型管道，显著提高了电话记录校正的质量，提升了Auto Review的效率。

Conclusion: Auto Review系统通过使用多种ASR替代方案和伪标签方法，在无需人工校正的情况下提高了电话记录的准确性，从而提升了系统的效率。

Abstract: Automating benefit verification phone calls saves time in healthcare and
helps patients receive treatment faster. It is critical to obtain highly
accurate information in these phone calls, as it can affect a patient's
healthcare journey. Given the noise in phone call transcripts, we have a
two-stage system that involves a post-call review phase for potentially noisy
fields, where human reviewers manually verify the extracted
data$\unicode{x2013}$a labor-intensive task. To automate this stage, we
introduce Auto Review, which significantly reduces manual effort while
maintaining a high bar for accuracy. This system, being highly reliant on call
transcripts, suffers a performance bottleneck due to automatic speech
recognition (ASR) issues. This problem is further exacerbated by the use of
domain-specific jargon in the calls. In this work, we propose a second-stage
postprocessing pipeline for accurate information extraction. We improve
accuracy by using multiple ASR alternatives and a pseudo-labeling approach that
does not require manually corrected transcripts. Experiments with
general-purpose large language models and feature-based model pipelines
demonstrate substantial improvements in the quality of corrected call
transcripts, thereby enhancing the efficiency of Auto Review.

</details>


### [9] [Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs](https://arxiv.org/abs/2506.05410)
*Wanyun Cui,Mingwei Xu*

Main category: cs.CL

TL;DR: 提出了一种无训练的压缩框架解决长上下文注意力机制的效率问题，并在实际任务中取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决注意力机制的二次复杂性所带来的长上下文建模效率问题。

Method: 提出了一个无训练的压缩框架（AsymKV），结合基于同质性的关键合并和经过数学证明的无损值压缩。

Result: AsymKV在LLaMA3.1-8B上的平均得分为43.95，显著超过当前最先进的方法如H₂O（38.89）。

Conclusion: AsymKV在长上下文建模方面优于现有压缩方法，在多个任务和基础模型上表现出色。

Abstract: Recent advances in Large Language Models (LLMs) have highlighted the critical
importance of extending context length, yet the quadratic complexity of
attention mechanisms poses significant challenges for efficient long-context
modeling. KV cache compression has emerged as a key approach to address this
challenge. Through extensive empirical analysis, we reveal a fundamental yet
previously overlooked asymmetry in KV caches: while adjacent keys receive
similar attention weights (local homogeneity), adjacent values demonstrate
distinct heterogeneous distributions. This key-value asymmetry reveals a
critical limitation in existing compression methods that treat keys and values
uniformly. To address the limitation, we propose a training-free compression
framework (AsymKV) that combines homogeneity-based key merging with a
mathematically proven lossless value compression. Extensive experiments
demonstrate that AsymKV consistently outperforms existing long-context methods
across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV
achieves an average score of 43.95 on LongBench, surpassing SOTA methods like
H$_2$O (38.89) by a large margin.

</details>


### [10] [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.CL

TL;DR: SmoothRot是一种新颖的后训练量化技术，通过融合通道级缩放与Hadamard变换改善激活异常问题，提高4位量化的效率。实验表明其可减少10-30%的性能差距，无额外推断延迟。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型中4位量化的效率。

Method: 将在通道级缩放与Hadamard变换相融合来解决大量激活异常问题，从而实现量化友好的激活。

Result: 实验表明，在流行的LLMs（如LLaMA2 7B、LLaMA3.1 8B和Mistral 7B）上进行的测试，SmoothRot能够在语言生成和零次推理任务中，将量化模型与FP16模型之间的性能差距减少约10-30%。

Conclusion: SmoothRot能够有效减少量化模型与FP16模型之间的性能差距，而不会增加推断延迟。

Abstract: We present SmoothRot, a novel post-training quantization technique to enhance
the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot
addresses the critical challenge of massive activation outliers, by integrating
channel-wise scaling with Hadamard transformations. Our technique effectively
transforms extreme outliers into quantization-friendly activations,
significantly improving quantization accuracy. Experiments conducted on popular
LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot
consistently reduces the performance gap between quantized and FP16 models by
approximately 10-30\% across language generation and zero-shot reasoning tasks,
without introducing additional inference latency. Code is available at
https://github.com/czakop/smoothrot.

</details>


### [11] [Automatically Detecting Amusing Games in Wordle](https://arxiv.org/abs/2506.05415)
*Ronaldo Luo,Gary Liang,Cindy Liu,Adam Kabbara,Minahil Bakhtawar,Kina Kim,Michael Guerzhoy*

Main category: cs.CL

TL;DR: 该论文研究了如何通过计算方式预测Wordle游戏对Reddit用户的娱乐效果，使用GPT-3.5对反应进行分类，并探讨游戏特征对娱乐的影响。


<details>
  <summary>Details</summary>
Motivation: 探索能否自动预测哪些Wordle游戏能让Reddit用户感到娱乐。

Method: 我们使用OpenAI的GPT-3.5进行少样本提示，以分类Reddit用户对Wordle游戏的反应是否表达了娱乐，并验证GPT-3.5的标签与人工标签大致对应。

Result: 结果表明，用户对Wordle游戏的娱乐性可以在一定程度上通过计算方式预测。我们探讨了游戏中的哪些特征对用户娱乐有贡献。

Conclusion: 用户的娱乐性是可预测的，这表明通过幽默，Wordle游戏中融入了一种可衡量的创造力。

Abstract: We explore automatically predicting which Wordle games Reddit users find
amusing.
  We scrape approximately 80k reactions by Reddit users to Wordle games from
Reddit, classify the reactions as expressing amusement or not using OpenAI's
GPT-3.5 using few-shot prompting, and verify that GPT-3.5's labels roughly
correspond to human labels.
  We then extract features from Wordle games that can predict user amusement.
We demonstrate that the features indeed provide a (weak) signal that predicts
user amusement as predicted by GPT-3.5.
  Our results indicate that user amusement at Wordle games can be predicted
computationally to some extent. We explore which features of the game
contribute to user amusement.
  We find that user amusement is predictable, indicating a measurable aspect of
creativity infused into Wordle games through humor.

</details>


### [12] [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)
*Hongbo Zhao,Fei Zhu,Rundong Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.CL

TL;DR: 提出了一种新颖的MLLM-CL基准和方法，通过参数隔离和MLLM路由机制解决灾难性干扰问题，实验结果表明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MMML在处理需要持续整合新知识和技能的动态现实场景中面临挑战，本文旨在通过持续学习解决这一问题。

Method: 我们通过参数隔离来防止灾难性干扰，并引入了基于MLLM的路由机制。

Result: 我们的实验表明，我们的方法能够有效地整合领域知识和功能能力，同时随着最小的遗忘，性能显著优于现有方法。

Conclusion: 我们的方法能够在最小遗忘的情况下整合特定领域的知识和功能能力，显著优于现有方法。

Abstract: Recent Multimodal Large Language Models (MLLMs) excel in vision-language
understanding but face challenges in adapting to dynamic real-world scenarios
that require continuous integration of new knowledge and skills. While
continual learning (CL) offers a potential solution, existing benchmarks and
methods suffer from critical limitations. In this paper, we introduce MLLM-CL,
a novel benchmark encompassing domain and ability continual learning, where the
former focuses on independently and identically distributed (IID) evaluation
across evolving mainstream domains, whereas the latter evaluates on non-IID
scenarios with emerging model ability. Methodologically, we propose preventing
catastrophic interference through parameter isolation, along with an MLLM-based
routing mechanism. Extensive experiments demonstrate that our approach can
integrate domain-specific knowledge and functional abilities with minimal
forgetting, significantly outperforming existing methods.

</details>


### [13] [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
*Niruthiha Selvanayagam*

Main category: cs.CL

TL;DR: 本研究通过PCA和聚类分析，揭示特定语言障碍主要表现为产出能力的减少，提出使用无监督学习技术改进诊断标准的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于传统诊断方法可能忽略微妙的发育模式，该研究旨在通过识别有或没有SLI儿童的自然语言发展轨迹，为早期识别和针对性干预提供洞察。

Method: 研究使用无监督机器学习技术，通过PCA和聚类分析，分析来自1,163名儿童的叙述样本。评估了64种语言特征，以揭示发展轨迹并区分语言特征。

Result: 分析发现，语言能力主要分为两个集群：高语言产出且SLI患病率低，以及有限产出但句法复杂性高且SLI患病率高。此外，边界案例表现出中间特征，支持语言能力连续体模型。

Conclusion: 研究表明，特定语言障碍主要表现为产出能力的减少，而不是句法复杂性不足。结果对传统诊断框架提出了挑战，并强调使用无监督学习技术在诊断标准和干预策略上的潜力。

Abstract: Specific Language Impairment (SLI) affects approximately 7 percent of
children, presenting as isolated language deficits despite normal cognitive
abilities, sensory systems, and supportive environments. Traditional diagnostic
approaches often rely on standardized assessments, which may overlook subtle
developmental patterns. This study aims to identify natural language
development trajectories in children with and without SLI using unsupervised
machine learning techniques, providing insights for early identification and
targeted interventions. Narrative samples from 1,163 children aged 4-16 years
across three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using
Principal Component Analysis (PCA) and clustering. A total of 64 linguistic
features were evaluated to uncover developmental trajectories and distinguish
linguistic profiles. Two primary clusters emerged: (1) high language production
with low SLI prevalence, and (2) limited production but higher syntactic
complexity with higher SLI prevalence. Additionally, boundary cases exhibited
intermediate traits, supporting a continuum model of language abilities.
Findings suggest SLI manifests primarily through reduced production capacity
rather than syntactic complexity deficits. The results challenge categorical
diagnostic frameworks and highlight the potential of unsupervised learning
techniques for refining diagnostic criteria and intervention strategies.

</details>


### [14] [Improving LLMs with a knowledge from databases](https://arxiv.org/abs/2506.05560)
*Petr Máša*

Main category: cs.CL

TL;DR: 论文提出了一种通过增强关联规则改善LLMs回答问题的新方法，显著提升了基于数据集的问答能力，并指出了未来改进的方向。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在回答问题方面的潜力和挑战，尤其是在涉及数据集和数据库时，这篇论文的动机是找到一种新方法来提高回答准确性和可解释性。

Method: 提出一种基于知识模式生成规则集的方法，将规则通过规则转文本转换器转换为文本形式，并作为RAG技术的一部分融入LLM中。

Result: 与ChatGPT（包括使用agents）进行比较后，发现新方法在基于数据集回答问题时表现出显著的改进。

Conclusion: 所提出的方法提供了显著的回答改进，并且未来可以通过结合其他模式和将规则挖掘用作代理等多种方式进行改进。

Abstract: Large language models (LLMs) are achieving significant progress almost every
moment now. Many advanced techniques have been introduced and widely accepted,
like retrieval-augmentation generation (RAG), agents, and tools. Tools can
query the database to answer questions from structured data files or perform
groupings or other statistics. This unlocks huge opportunities, such as it can
answer any question, but also poses threats, such as safety, because there is
no control over the commands that are created. We would like to discuss whether
we can create a new method that improves answers based on dataset/database via
some interpretable ML methods, namely enhanced association rules. The advantage
would be if the method can be also used in some safe technique like RAG.
Association rules have a sound history. Since the introduction of CN2 and
aproiri, many enhancements have been made. In parallel, enhanced association
rules have been introduced and evolved over the last 40 years. The general
problem is typically that there are too many rules. There are some techniques
for handling it, but when LLM emerged, it turned out to be the best use case
for the RAG technique for LLMs. We proposed a method that generates a ruleset
based on defined knowledge patterns, then converts rules into text form via a
rule-to-text converter, and includes the result as an RAG into LLM. We compared
this method with ChatGPT (even with using agents) and we have discovered a
significant improvement in answering questions based on the dataset. We have
also tried several strategies how much rules to generate. We found this
improvement interesting. Moreover, it can also be improved in many ways as
future work, like incorporating other patterns, the use of rule mining as an
agent, and many others.

</details>


### [15] [Combating Misinformation in the Arab World: Challenges & Opportunities](https://arxiv.org/abs/2506.05582)
*Azza Abouzied,Firoj Alam,Raian Ali,Paolo Papotti*

Main category: cs.CL

TL;DR: 该研究探讨了阿拉伯地区在地缘政治不稳定性下的错误信息的检测、追踪与缓解策略，强调与基层组织合作及文化认知。


<details>
  <summary>Details</summary>
Motivation: 由于地缘政治不稳定性、语言多样性和文化差异，阿拉伯地区在对抗全球错误资讯方面面临独特的脆弱性。

Method: 研究通过检测、追踪、缓解和社区参与等方面应对错误资讯。

Result: 通过连接基层事实核查组织、理解文化规范、促进社会更正，及创建强大的协作信息网络，可以在阿拉伯世界构建一个更具韧性的资讯生态系统。

Conclusion: 在阿拉伯地区，通过与基层事实核查组织联系、理解文化规范、促进社会更正、建立强有力的信息协作网络，能够创造更具韧性的资讯生态系统。

Abstract: Misinformation and disinformation pose significant risks globally, with the
Arab region facing unique vulnerabilities due to geopolitical instabilities,
linguistic diversity, and cultural nuances. We explore these challenges through
the key facets of combating misinformation: detection, tracking, mitigation and
community-engagement. We shed light on how connecting with grass-roots
fact-checking organizations, understanding cultural norms, promoting social
correction, and creating strong collaborative information networks can create
opportunities for a more resilient information ecosystem in the Arab world.

</details>


### [16] [UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting](https://arxiv.org/abs/2506.05589)
*Sara Shields-Menard,Zach Reimers,Joshua Gardner,David Perry,Anthony Rios*

Main category: cs.CL

TL;DR: 该系统通过选择相关句子并生成支持引用的回答，将大语言模型应用于EHRs，发现8B模型效果优于70B模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决如何使用电子健康记录（EHRs）回答临床问题的挑战。

Method: 使用大语言模型分两步进行：首先寻找与临床问题相关的EHR句子，其次基于这些句子生成简短的、引用支持的响应。使用少样本提示、自一致性和阈值处理来改进句子分类步骤。

Result: 较小的8B模型在识别相关信息方面优于较大的70B模型。

Conclusion: 准确的句子选择对于生成高质量的响应至关重要，自一致性和阈值处理有助于提高这些决策的可靠性。

Abstract: We describe our system for the ArchEHR-QA Shared Task on answering clinical
questions using electronic health records (EHRs). Our approach uses large
language models in two steps: first, to find sentences in the EHR relevant to a
clinician's question, and second, to generate a short, citation-supported
response based on those sentences. We use few-shot prompting, self-consistency,
and thresholding to improve the sentence classification step to decide which
sentences are essential. We compare several models and find that a smaller 8B
model performs better than a larger 70B model for identifying relevant
information. Our results show that accurate sentence selection is critical for
generating high-quality responses and that self-consistency with thresholding
helps make these decisions more reliable.

</details>


### [17] [SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs](https://arxiv.org/abs/2506.05598)
*Michael J Ryan,Omar Shaikh,Aditri Bhagirath,Daniel Frees,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: SynthesizeMe通过用户交互生成合成角色，提高了个性化LLM性能，且在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型个人化方法依赖额外的身份信息，而SynthesizeMe旨在通过用户交互诱导合成用户角色，以克服这一限制。

Method: SynthesizeMe通过生成和验证推理来解释用户偏好，诱导合成用户角色，并过滤具有信息价值的过往用户交互，以构建个性化提示。

Result: SynthesizeMe诱导的提示提高了Chatbot Arena上的个性化LLM判断准确性4.4%，结合奖励模型在PersonalRewardBench上取得了最佳表现。

Conclusion: SynthesizeMe通过合成用户角色和个性化提示提高了LLM的准确性。

Abstract: Recent calls for pluralistic alignment of Large Language Models (LLMs)
encourage adapting models to diverse user preferences. However, most prior work
on personalized reward models heavily rely on additional identity information,
such as demographic details or a predefined set of preference categories. To
this end, we introduce SynthesizeMe, an approach to inducing synthetic user
personas from user interactions for personalized reward modeling. SynthesizeMe
first generates and verifies reasoning to explain user preferences, then
induces synthetic user personas from that reasoning, and finally filters to
informative prior user interactions in order to build personalized prompts for
a particular user. We show that using SynthesizeMe induced prompts improves
personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining
SynthesizeMe derived prompts with a reward model achieves top performance on
PersonalRewardBench: a new curation of user-stratified interactions with
chatbots collected from 854 users of Chatbot Arena and PRISM.

</details>


### [18] [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
*Ziyi Wang,Yuxuan Lu,Wenbo Li,Amirali Amini,Bo Sun,Yakov Bart,Weimin Lyu,Jiri Gesi,Tian Wang,Jing Huang,Yu Su,Upol Ehsan,Malihe Alikhani,Toby Jia-Jun Li,Lydia Chilton,Dakuo Wang*

Main category: cs.CL

TL;DR: 引入OPERA数据集，以评估大型语言模型模拟用户行为的能力，为个性化数字助理的研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）模拟用户下一步网络行动的能力和挑战，特别是在缺乏既包含可观察行为又包含用户内在推理的高质量公开数据集时。

Method: 介绍OPERA数据集，包含从真人在线购物会话中收集的观察、人格、推理和行动。利用在线问卷和定制浏览器插件高保真收集数据。以此数据集建立了评估当前LLMs预测特定用户下一步行动及推理的基准。

Result: 开发了OPERA数据集，能够捕获用户角色、浏览观察、细粒度网络行动和即时自述推理，为进一步研究以个性化数字孪生体为目标的LLM代理奠定基础。

Conclusion: OPERA数据集的引入为评估和改进LLMs在模拟真实用户行为方面的能力提供了标准，推动个性化数字助理的研究。

Abstract: Can large language models (LLMs) accurately simulate the next web action of a
specific user? While LLMs have shown promising capabilities in generating
``believable'' human behaviors, evaluating their ability to mimic real user
behaviors remains an open challenge, largely due to the lack of high-quality,
publicly available datasets that capture both the observable actions and the
internal reasoning of an actual human user. To address this gap, we introduce
OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected
from real human participants during online shopping sessions. OPERA is the
first public dataset that comprehensively captures: user personas, browser
observations, fine-grained web actions, and self-reported just-in-time
rationales. We developed both an online questionnaire and a custom browser
plugin to gather this dataset with high fidelity. Using OPERA, we establish the
first benchmark to evaluate how well current LLMs can predict a specific user's
next action and rationale with a given persona and <observation, action,
rationale> history. This dataset lays the groundwork for future research into
LLM agents that aim to act as personalized digital twins for human.

</details>


### [19] [Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking](https://arxiv.org/abs/2506.05610)
*Zhecheng Sheng,Xiruo Ding,Brian Hur,Changye Li,Trevor Cohen,Serguei Pakhomov*

Main category: cs.CL

TL;DR: 研究使用深度变压器模型进行阿尔茨海默症早期检测，提出方法解决性别混淆问题，结果表明去性别权重可降低过拟合但性能略减。


<details>
  <summary>Details</summary>
Motivation: 探索说话者性别对基于深度变压器模型的早期阿尔茨海默症检测中的影响，以解决痴呆检测中的性别混淆问题。

Method: 提出了两种方法：扩展混淆过滤器和双重过滤器，用于隔离和消除与性别相关的权重。使用来自认知障碍患者和健康对照者的第一人称叙述的痴呆数据集进行评估。

Result: 变压器模型在训练数据分布中往往容易过拟合。通过干扰与性别相关的权重，可以得到去混淆的痴呆分类器，但这会稍微降低痴呆检测的性能。

Conclusion: 该研究表明，通过干扰与性别相关的权重可以使痴呆分类器去混淆，但同时会稍微降低痴呆检测的性能。

Abstract: Deep transformer models have been used to detect linguistic anomalies in
patient transcripts for early Alzheimer's disease (AD) screening. While
pre-trained neural language models (LMs) fine-tuned on AD transcripts perform
well, little research has explored the effects of the gender of the speakers
represented by these transcripts. This work addresses gender confounding in
dementia detection and proposes two methods: the $\textit{Extended Confounding
Filter}$ and the $\textit{Dual Filter}$, which isolate and ablate weights
associated with gender. We evaluate these methods on dementia datasets with
first-person narratives from patients with cognitive impairment and healthy
controls. Our results show transformer models tend to overfit to training data
distributions. Disrupting gender-related weights results in a deconfounded
dementia classifier, with the trade-off of slightly reduced dementia detection
performance.

</details>


### [20] [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/abs/2506.05629)
*Ananth Muppidi,Abhilash Nandy,Sambaran Bandyopadhyay*

Main category: cs.CL

TL;DR: 本文提出了一种高效的ID-SPAM软提示微调技术，通过减少训练参数量，提高了模型的领域迁移能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域任务中的表现需要进行微调，这种微调计算代价高并且技术上具有挑战性。因此，寻找参数高效的微调方法显得尤为重要。

Method: 使用输入依赖的软提示技术与自注意力机制相结合，开发出ID-SPAM技术，该方法通过调整少量参数适应下游任务。

Result: 与最先进的技术相比，提出的方法在多项任务上表现出色，并提升了零样本领域迁移能力。

Conclusion: 提出了一种名为ID-SPAM的输入依赖软提示技术，通过自注意力机制生成基于输入的软提示，能够在保持少量可训练参数的情况下，提高领域迁移能力。

Abstract: The performance of large language models in domain-specific tasks
necessitates fine-tuning, which is computationally expensive and technically
challenging. This paper focuses on parameter-efficient fine-tuning using soft
prompting, a promising approach that adapts pre-trained models to downstream
tasks by learning a small set of parameters. We propose a novel Input Dependent
Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that
generates soft prompts based on the input tokens and attends different tokens
with varying importance. Our method is simple and efficient, keeping the number
of trainable parameters small. We show the merits of the proposed approach
compared to state-of-the-art techniques on various tasks and show the improved
zero shot domain transfer capability.

</details>


### [21] [IYKYK: Using language models to decode extremist cryptolects](https://arxiv.org/abs/2506.05635)
*Christine de Kock,Arij Riabi,Zeerak Talat,Michael Sejr Schlichtkrull,Pranava Madhyastha,Ed Hovy*

Main category: cs.CL

TL;DR: 研究表明，通用语言模型难以检测极端主义语言，领域适配和专门提示技术可提高性能，提供重要洞察以开发自动化审核技术。


<details>
  <summary>Details</summary>
Motivation: 探讨当前语言技术是否能检测和解释极端主义平台的加密语言。

Method: 评估八种模型在六项任务中的表现，开发新的标注和未标注数据集。

Result: 通过领域适配和专门提示技术，语言模型的检测和解码性能得到显著提高。

Conclusion: 通用语言模型无法一致地检测或解码极端主义语言。通过领域适配和专门提示技术可以显著提高性能。

Abstract: Extremist groups develop complex in-group language, also referred to as
cryptolects, to exclude or mislead outsiders. We investigate the ability of
current language technologies to detect and interpret the cryptolects of two
online extremist platforms. Evaluating eight models across six tasks, our
results indicate that general purpose LLMs cannot consistently detect or decode
extremist language. However, performance can be significantly improved by
domain adaptation and specialised prompting techniques. These results provide
important insights to inform the development and deployment of automated
moderation technologies. We further develop and release novel labelled and
unlabelled datasets, including 19.4M posts from extremist platforms and
lexicons validated by human experts.

</details>


### [22] [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/abs/2506.05639)
*John Kirchenbauer,Janny Mongkolsupawan,Yuxin Wen,Tom Goldstein,Daphne Ippolito*

Main category: cs.CL

TL;DR: 提出一个数据集用于研究语言模型的记忆过程，包括事实记忆和逐字序列记忆，并展示合成数据的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何在训练中记住事实和逐字序列，以提高对语言模型的理解。

Method: 提出一个新的数据集，包括合成生成的关于虚构事件的类网络文本文档以及关于事件的问题-答案对，并进行训练实验。

Result: 展示合成数据可以有效地区分语言模型记忆事实和记忆逐字序列的形式。

Conclusion: 语言模型可以通过合成数据有效地区分事实记忆和逐字序列记忆的不同形式，同时面临构建现实的虚构合成数据的挑战。

Abstract: When language models are trained on textual data, they acquire both knowledge
about the structure of language as well as knowledge of facts about the world.
At inference time, their knowledge of facts can be leveraged to solve
interesting problems and perform useful knowledge work for users. It is well
known that language models can verbatim memorize long sequences from their
training data. However, it is much less well understood how language models
memorize facts seen during training. In this work, we propose a new dataset to
specifically empower researchers to study the dual processes of fact
memorization and verbatim sequence memorization. The dataset consists of
synthetically-generated, webtext-like documents about fictional events, as well
as question-answer pairs about the events. We conduct training experiments
showing how synthetic data about fictional events can be effective in teasing
apart different forms of memorization. We also document the challenges in
effectively building realistic, fictional synthetic data.

</details>


### [23] [Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment](https://arxiv.org/abs/2506.05670)
*Priyanka Dey,Yugal Khanter,Aayush Bothra,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: CulturalPersonas benchmark evaluates LLMs' personality in culturally rich contexts, improving alignment with human personality through expressive outputs.


<details>
  <summary>Details</summary>
Motivation: The growing importance of LLMs in interactive applications such as tutoring and mental health necessitates the ability to express culturally appropriate personalities.

Method: Introducing CulturalPersonas, a large-scale benchmark validated by humans, for evaluating LLMs' personality expression in culturally grounded contexts with a dataset of 3,000 scenarios across six countries.

Result: The CulturalPersonas benchmark improves alignment with country-specific human personality distributions, showing a significant reduction in Wasserstein distance and eliciting expressive, culturally coherent outputs.

Conclusion: CulturalPersonas paves the way for creating LLMs that are both socially intelligent and globally adaptive, by bridging personality expression with cultural nuances.

Abstract: As LLMs become central to interactive applications, ranging from tutoring to
mental health, the ability to express personality in culturally appropriate
ways is increasingly important. While recent works have explored personality
evaluation of LLMs, they largely overlook the interplay between culture and
personality. To address this, we introduce CulturalPersonas, the first
large-scale benchmark with human validation for evaluating LLMs' personality
expression in culturally grounded, behaviorally rich contexts. Our dataset
spans 3,000 scenario-based questions across six diverse countries, designed to
elicit personality through everyday scenarios rooted in local values. We
evaluate three LLMs, using both multiple-choice and open-ended response
formats. Our results show that CulturalPersonas improves alignment with
country-specific human personality distributions (over a 20% reduction in
Wasserstein distance across models and countries) and elicits more expressive,
culturally coherent outputs compared to existing benchmarks. CulturalPersonas
surfaces meaningful modulated trait outputs in response to culturally grounded
prompts, offering new directions for aligning LLMs to global norms of behavior.
By bridging personality expression and cultural nuance, we envision that
CulturalPersonas will pave the way for more socially intelligent and globally
adaptive LLMs.

</details>


### [24] [Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation with Large Language Models](https://arxiv.org/abs/2506.05675)
*Zefan Zeng,Xingchen Hu,Qing Cheng,Weiping Ding,Wentao Li,Zhong Liu*

Main category: cs.CL

TL;DR: MEFA is a novel zero-shot framework for Event Causality Identification that improves performance and reduces errors compared to unsupervised baselines.


<details>
  <summary>Details</summary>
Motivation: To address the dependence of ECI models on large-scale annotated data and the causal hallucination issues in zero-shot ECI by LLMs.

Method: MEFA uses Multi-source Evidence Fuzzy Aggregation by decomposing causality reasoning into primary and auxiliary tasks, guiding LLMs through prompts, and using fuzzy aggregation for causality scoring.

Result: MEFA improves performance by 6.2% in F1-score and 9.3% in precision on three benchmarks compared to other unsupervised baselines.

Conclusion: MEFA significantly reduces hallucination-induced errors and outperforms second-best unsupervised baselines.

Abstract: Event Causality Identification (ECI) aims to detect causal relationships
between events in textual contexts. Existing ECI models predominantly rely on
supervised methodologies, suffering from dependence on large-scale annotated
data. Although Large Language Models (LLMs) enable zero-shot ECI, they are
prone to causal hallucination-erroneously establishing spurious causal links.
To address these challenges, we propose MEFA, a novel zero-shot framework based
on Multi-source Evidence Fuzzy Aggregation. First, we decompose causality
reasoning into three main tasks (temporality determination, necessity analysis,
and sufficiency verification) complemented by three auxiliary tasks. Second,
leveraging meticulously designed prompts, we guide LLMs to generate uncertain
responses and deterministic outputs. Finally, we quantify LLM's responses of
sub-tasks and employ fuzzy aggregation to integrate these evidence for
causality scoring and causality determination. Extensive experiments on three
benchmarks demonstrate that MEFA outperforms second-best unsupervised baselines
by 6.2% in F1-score and 9.3% in precision, while significantly reducing
hallucination-induced errors. In-depth analysis verify the effectiveness of
task decomposition and the superiority of fuzzy aggregation.

</details>


### [25] [A Unified Representation for Continuity and Discontinuity: Syntactic and Computational Motivations](https://arxiv.org/abs/2506.05686)
*Ratna Kandala,Prakash Mondal*

Main category: cs.CL

TL;DR: 论文提出了PSG、DG和CG的统一表示，以简化句法分析和计算复杂性，尤其是在处理不连续句子时。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于从句法和计算复杂性考虑出发，提出一种对语言结构统一表示的方法，从而处理PSG、DG和CG中的不连续性问题。

Method: 该论文提出一个对应原则，实现对PSG、DG和CG的统一表示，具体通过展示一系列步骤来说明如何在土耳其语中实现不连续从句的统一表示。

Result: 研究表明，统一表示能够简化关于PSG、DG和CG基本原则的连续和不连续句子的神经认知表示和处理的计算复杂性。

Conclusion: 本文得出结论，通过对PSG、DG和CG的统一表示，不仅可以从理论上处理自然语言中的不连续性，还可以简化连续和不连续句子的计算复杂性。

Abstract: This paper advances a unified representation of linguistic structure for
three grammar formalisms, namely, Phrase Structure Grammar (PSG), Dependency
Grammar (DG) and Categorial Grammar (CG) from the perspective of syntactic and
computational complexity considerations. The correspondence principle is
proposed to enable a unified representation of the representational principles
from PSG, DG, and CG. To that end, the paper first illustrates a series of
steps in achieving a unified representation for a discontinuous subordinate
clause from Turkish as an illustrative case. This affords a new way of
approaching discontinuity in natural language from a theoretical point of view
that unites and integrates the basic tenets of PSG, DG, and CG, with
significant consequences for syntactic analysis. Then this paper demonstrates
that a unified representation can simplify computational complexity with
regards to the neurocognitive representation and processing of both continuous
and discontinuous sentences vis-\`a-vis the basic principles of PSG, DG, and
CG.

</details>


### [26] [When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05690)
*Zhishang Xiang,Chuanjie Wu,Qinggang Zhang,Shengyuan Chen,Zijin Hong,Xiao Huang,Jinsong Su*

Main category: cs.CL

TL;DR: GraphRAG-Bench is a benchmark for evaluating the effectiveness of GraphRAG in comparison to traditional RAG, identifying scenarios where GraphRAG excels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine the effectiveness of GraphRAG compared to traditional RAG systems, identifying scenarios where graph structures enhance knowledge retrieval and generation.

Method: GraphRAG-Bench is developed with a comprehensive dataset for hierarchical knowledge retrieval and deep contextual reasoning, including tasks of fact retrieval, complex reasoning, contextual summarization, and creative generation, to systematically evaluate GraphRAG models.

Result: The benchmark uncovers specific conditions under which GraphRAG outperforms traditional RAG models and elucidates the underlying reasons, providing a resource and guidelines for the application of GraphRAG systems.

Conclusion: GraphRAG-Bench provides a systematic benchmark for evaluating the scenarios in which GraphRAG models surpass traditional RAG systems, offering insights and guidelines for practical applications.

Abstract: Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful
paradigm for enhancing large language models (LLMs) with external knowledge. It
leverages graphs to model the hierarchical structure between specific concepts,
enabling more coherent and effective knowledge retrieval for accurate
reasoning.Despite its conceptual promise, recent studies report that GraphRAG
frequently underperforms vanilla RAG on many real-world tasks. This raises a
critical question: Is GraphRAG really effective, and in which scenarios do
graph structures provide measurable benefits for RAG systems? To address this,
we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate
GraphRAG models onboth hierarchical knowledge retrieval and deep contextual
reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of
increasing difficulty, coveringfact retrieval, complex reasoning, contextual
summarization, and creative generation, and a systematic evaluation across the
entire pipeline, from graph constructionand knowledge retrieval to final
generation. Leveraging this novel benchmark, we systematically investigate the
conditions when GraphRAG surpasses traditional RAG and the underlying reasons
for its success, offering guidelines for its practical application. All related
resources and analyses are collected for the community at
https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.

</details>


### [27] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: Introduces a novel curriculum learning framework, POCL, that improves KD by progressively introducing ranked training samples, enhancing stability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods often cause distribution shifts in student models, leading to issues like catastrophic forgetting and mode collapse. The motivation is to address these challenges with a new learning framework.

Method: The novel plug-in curriculum learning framework inspired by progressive overload, incorporating two components: difficulty measurer and training scheduler, integrating into existing KD methods with minimal overhead.

Result: POCL consistently enhances the performance of student models in various KD methods as shown in extensive experiments in instruction-following settings.

Conclusion: POCL effectively improves the performance and stability of distilled student models by structuring training data and introducing them progressively based on difficulty.

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [28] [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/abs/2506.05700)
*Yan Wang,Yueru He,Ruoyu Xiang,Jeff Zhao*

Main category: cs.CL

TL;DR: RKEFino1通过增强知识和推理能力，在金融合规任务中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融应用中有很大潜力，但在数字监管报告中引发了准确性和合规性方面的挑战。为解决这些问题，提出了加强型金融推理模型RKEFino1。

Method: 通过微调Fino1并结合来自XBRL、CDM和MOF的领域知识来构建增强型金融推理模型RKEFino1，并设计两个QA任务：知识型推理和数学推理。此外，引入了覆盖句子和表格的金融实体的新增EEP任务。

Result: 实验结果表明RKEFino1在合规性关键的金融任务中具备有效性，并已在Hugging Face平台发布。

Conclusion: RKEFino1在合规性关键的金融任务中表现出有效性和泛化能力。

Abstract: Recent advances in large language models (LLMs) hold great promise for
financial applications but introduce critical accuracy and compliance
challenges in Digital Regulatory Reporting (DRR). To address these issues, we
propose RKEFino1, a regulation knowledge-enhanced financial reasoning model
built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We
formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce
a novel Numerical NER task covering financial entities in both sentences and
tables. Experimental results demonstrate the effectiveness and generalization
capacity of RKEFino1 in compliance-critical financial tasks. We have released
our model on Hugging Face.

</details>


### [29] [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)
*Fang Wu,Vijay Prakash Dwivedi,Jure Leskovec*

Main category: cs.CL

TL;DR: Rel-LLM uses a GNN-based encoder to preserve relational structures in databases, outperforming current methods in processing structured data with LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing approaches that disregard critical relational structures in databases and introduce redundancy, while often exceeding standard LLM context lengths.

Method: The method utilizes a graph neural network (GNN)-based encoder within a retrieval-augmented generation (RAG) framework to preserve relational database structures and generate structured relational prompts for LLMs.

Result: Extensive experiments demonstrate that Rel-LLM surpasses existing methods in relational deep learning tasks.

Conclusion: Rel-LLM outperforms existing methods on key relational deep learning tasks, presenting a scalable and efficient solution for integrating large language models with structured data sources.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, yet their application to relational deep learning (RDL)
remains underexplored. Existing approaches adapt LLMs by traversing relational
links between entities in a database and converting the structured data into
flat text documents. Still, this text-based serialization disregards critical
relational structures, introduces redundancy, and often exceeds standard LLM
context lengths. We introduce Rel-LLM, a novel architecture that utilizes a
graph neural network (GNN)- based encoder to generate structured relational
prompts for LLMs within a retrieval-augmented generation (RAG) framework.
Unlike traditional text-based serialization approaches, our method preserves
the inherent relational structure of databases while enabling LLMs to
effectively process and reason over complex entity relationships. Specifically,
the GNN encoder extracts a local subgraph around an entity to build feature
representations that contain relevant entity relationships and temporal
dependencies. These representations are transformed into structured prompts
using a denormalization process, effectively allowing the LLM to reason over
relational structures. Through extensive experiments, we demonstrate that
Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and
efficient approach to integrating LLMs with structured data sources. Code is
available at https://github.com/smiles724/Rel-LLM.

</details>


### [30] [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
*Rongzhe Wei,Peizhi Niu,Hans Hao-Hsun Hsu,Ruihan Wu,Haoteng Yin,Mohsen Ghassemi,Yifan Li,Vamsi K. Potluru,Eli Chien,Kamalika Chaudhuri,Olgica Milenkovic,Pan Li*

Main category: cs.CL

TL;DR: 提出了一个新的知识遗忘评估框架，通过知识图谱和推理协议来更真实地评估大型语言模型的遗忘效果，发现现有策略对遗忘效果的评估往往过高。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术主要关注于孤立事实的显式删除，忽略了推理依赖关系及知识的非确定性。这导致被认为已遗忘的事实可能仍通过关联信息隐性存在。

Method: 提出了一种基于知识图谱和推理的评估方法，利用知识图谱模型生成相关事实环境，同时通过推理协议来评估遗忘效果。评估协议利用大型语言模型作为评估者，通过精心设计的提示词并进行校准与人类评价对比以确保评估者的可靠性和稳定性。

Result: 实验结果表明，该框架能更真实、严格地评估遗忘效果。研究发现当前的评估策略往往高估了遗忘的有效性。

Conclusion: 现有的机器遗忘评估方法对遗忘效果的评估不够真实，往往高估了遗忘效果。本文提出了一种新的评估框架，可以更有效地评估大型语言模型的知识遗忘效果。

Abstract: Machine unlearning techniques aim to mitigate unintended memorization in
large language models (LLMs). However, existing approaches predominantly focus
on the explicit removal of isolated facts, often overlooking latent inferential
dependencies and the non-deterministic nature of knowledge within LLMs.
Consequently, facts presumed forgotten may persist implicitly through
correlated information. To address these challenges, we propose a knowledge
unlearning evaluation framework that more accurately captures the implicit
structure of real-world knowledge by representing relevant factual contexts as
knowledge graphs with associated confidence scores. We further develop an
inference-based evaluation protocol leveraging powerful LLMs as judges; these
judges reason over the extracted knowledge subgraph to determine unlearning
success. Our LLM judges utilize carefully designed prompts and are calibrated
against human evaluations to ensure their trustworthiness and stability.
Extensive experiments on our newly constructed benchmark demonstrate that our
framework provides a more realistic and rigorous assessment of unlearning
performance. Moreover, our findings reveal that current evaluation strategies
tend to overestimate unlearning effectiveness. Our code is publicly available
at https://github.com/Graph-COM/Knowledge_Unlearning.git.

</details>


### [31] [LLM-Symbolic Integration for Robust Temporal Tabular Reasoning](https://arxiv.org/abs/2506.05746)
*Atharv Kulkarni,Kushagra Dixit,Vivek Srikumar,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: The paper introduces a method that transforms tables into database schemas for SQL query generation and utilizes adaptive few-shot prompting, leading to enhanced performance in temporal tabular question answering.


<details>
  <summary>Details</summary>
Motivation: Traditional prompting methods in Temporal Tabular Question Answering are often inadequate, suffering from issues like memorization, sensitivity to table size, and poor performance on complex queries. There is a need for a robust approach to overcome these limitations.

Method: We introduce a symbolic intermediate representation that transforms tables into database schemas for SQL query generation. Furthermore, adaptive few-shot prompting with contextually tailored examples is applied to improve model performance.

Result: The proposed method consistently shows improvements across key challenges in the field, offering superior robustness, scalability, and performance in temporal reasoning with LLMs.

Conclusion: Our method enhances generalization and reduces biases in Temporal Tabular Question Answering by transforming tables into database schemas for SQL query generation and execution, incorporating adaptive few-shot prompting, ultimately setting a new benchmark in this field.

Abstract: Temporal tabular question answering presents a significant challenge for
Large Language Models (LLMs), requiring robust reasoning over structured data,
which is a task where traditional prompting methods often fall short. These
methods face challenges such as memorization, sensitivity to table size, and
reduced performance on complex queries. To overcome these limitations, we
introduce TempTabQA-C, a synthetic dataset designed for systematic and
controlled evaluations, alongside a symbolic intermediate representation that
transforms tables into database schemas. This structured approach allows LLMs
to generate and execute SQL queries, enhancing generalization and mitigating
biases. By incorporating adaptive few-shot prompting with contextually tailored
examples, our method achieves superior robustness, scalability, and
performance. Experimental results consistently highlight improvements across
key challenges, setting a new benchmark for robust temporal reasoning with
LLMs.

</details>


### [32] [Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning](https://arxiv.org/abs/2506.05760)
*Xuanyu Lei,Chenliang Li,Yuning Wu,Kaiming Liu,Weizhou Shen,Peng Li,Ming Yan,Ji Zhang,Fei Huang,Yang Liu*

Main category: cs.CL

TL;DR: 提出Writing-RL框架，通过强化学习改进大模型长文写作能力，且对长输入推理任务有良好泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调方法在数据饱和和教师信号的限制下，学习能力有限，因此需要新的方法来提升长文写作的能力。

Method: 提出了一个自适应课程强化学习框架——Writing-RL，包括边际感知的数据选择策略、成对比较奖励机制以及动态参考调度方法。

Result: 在7B规模的写作模型实验中，Writing-RL框架大大提升了长文写作的性能，并且长输出RL训练的模型在长输入推理任务中表现良好。

Conclusion: 通过使用Writing-RL框架，可以大幅提升大规模模型在长文写作中的表现，并且这些经过长输出RL训练的模型在长输入推理任务中表现出色，展现出对重新思考长上下文训练的潜力。

Abstract: Recent advances in Large Language Models (LLMs) have enabled strong
performance in long-form writing, yet existing supervised fine-tuning (SFT)
approaches suffer from limitations such as data saturation and restricted
learning capacity bounded by teacher signals. In this work, we present
Writing-RL: an Adaptive Curriculum Reinforcement Learning framework to advance
long-form writing capabilities beyond SFT. The framework consists of three key
components: Margin-aware Data Selection strategy that prioritizes samples with
high learning potential, Pairwise Comparison Reward mechanism that provides
discriminative learning signals in the absence of verifiable rewards, and
Dynamic Reference Scheduling approach, which plays a particularly critical role
by adaptively adjusting task difficulty based on evolving model performance.
Experiments on 7B-scale writer models show that our RL framework largely
improves long-form writing performance over strong SFT baselines. Furthermore,
we observe that models trained with long-output RL generalize surprisingly well
to long-input reasoning tasks, potentially offering a promising perspective for
rethinking long-context training.

</details>


### [33] [BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions](https://arxiv.org/abs/2506.05766)
*Saptarshi Sengupta,Shuhua Yang,Paul Kwong Yu,Fali Wang,Suhang Wang*

Main category: cs.CL

TL;DR: 提出了BioMol-MQA数据集，用于测试LLM在多模态下检索和推理能力。结果显示LLM需要更强的RAG支持以处理复杂多模态问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RAG的大型语言模型主要进行单一模态的信息检索，而现实世界中如医疗领域的问题涉及多种模态的信息，因此需要能够检索多模态特定领域信息并进行推理和合成以生成精确答案的能力。

Method: 提出了一种新的多模态问题回答数据集BioMol-MQA，包含一个多模态知识图谱和用于检验LLM能力的挑战性问题。

Result: 我们的基准测试表明，目前的LLM在回答多模态知识图谱问题时仅在提供必要背景数据时表现良好，显示出对强大RAG架构的必要性。

Conclusion: 现有的大型语言模型在回答多模态问题上表现不佳，表明需要更强大的检索增强生成（RAG）框架来支持。

Abstract: Retrieval augmented generation (RAG) has shown great power in improving Large
Language Models (LLMs). However, most existing RAG-based LLMs are dedicated to
retrieving single modality information, mainly text; while for many real-world
problems, such as healthcare, information relevant to queries can manifest in
various modalities such as knowledge graph, text (clinical notes), and complex
molecular structure. Thus, being able to retrieve relevant multi-modality
domain-specific information, and reason and synthesize diverse knowledge to
generate an accurate response is important. To address the gap, we present
BioMol-MQA, a new question-answering (QA) dataset on polypharmacy, which is
composed of two parts (i) a multimodal knowledge graph (KG) with text and
molecular structure for information retrieval; and (ii) challenging questions
that designed to test LLM capabilities in retrieving and reasoning over
multimodal KG to answer questions. Our benchmarks indicate that existing LLMs
struggle to answer these questions and do well only when given the necessary
background data, signaling the necessity for strong RAG frameworks.

</details>


### [34] [dots.llm1 Technical Report](https://arxiv.org/abs/2506.05767)
*Bi Huo,Bin Tu,Cheng Qin,Da Zheng,Debing Zhang,Dongjie Zhang,En Li,Fu Guo,Jian Yao,Jie Lou,Junfeng Tian,Li Hu,Ran Zhu,Shengdong Chen,Shuo Liu,Su Guang,Te Wo,Weijun Zhang,Xiaoming Shi,Xinxin Peng,Xing Wu,Yawen Liu,Yuqiu Ji,Ze Wen,Zhenhai Liu,Zichao Li,Zilong Liao*

Main category: cs.CL

TL;DR: dots.llm1 是一个混合专家模型，通过激活少量参数实现高性能，同时降低成本，并开放训练检查点。


<details>
  <summary>Details</summary>
Motivation: 为了高效扩展语言模型，通过激活参数子集实现高性能，同时降低开销。

Method: 混合专家（MoE）模型通过仅激活部分参数进行语言模型扩展。使用高质量的海量数据进行预训练，并开放中间训练检查点以促进研究。

Result: 激活1420亿参数中的140亿参数，实现与Qwen2.5-72B模型相当的性能，并提供训练检查点。

Conclusion: dots.llm1 模型能够在不使用合成数据的情况下，实现与当前最先进模型表现相当的性能，同时降低训练和推理成本。

Abstract: Mixture of Experts (MoE) models have emerged as a promising paradigm for
scaling language models efficiently by activating only a subset of parameters
for each input token. In this report, we present dots.llm1, a large-scale MoE
model that activates 14B parameters out of a total of 142B parameters,
delivering performance on par with state-of-the-art models while reducing
training and inference costs. Leveraging our meticulously crafted and efficient
data processing pipeline, dots.llm1 achieves performance comparable to
Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to
fully unlock its capabilities. Notably, no synthetic data is used during
pretraining. To foster further research, we open-source intermediate training
checkpoints at every one trillion tokens, providing valuable insights into the
learning dynamics of large language models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [A Path to Loving](https://arxiv.org/abs/2506.05352)
*John Beverley,Regina Hurley*

Main category: cs.AI

TL;DR: 本研究通过本体论方法，将爱情解析为被动感受和主动评判的串联，为人工智能等领域的应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解析爱情的哲学复杂性和科学相关性，特别是在心理学和社会学领域，并探索这种表征如何增强相关的人工智能应用。

Method: 本研究依赖于基础形式本体（BFO）和其他应用的本体论方法，以区分爱情的不同感知。

Result: 提出了一个因果关联模型，确保情感和认知成分的关联性，为将来跨学科应用奠定基础。

Conclusion: 本研究为爱情的本体论表征奠定了基础，认为爱情可以被理解为被动感受和主动评判的串联。

Abstract: This work lays the foundations for a rigorous ontological characterization of
love, addressing its philosophical complexity and scientific relevance, with
particular emphasis on psychology and sociology, as well as highlighting ways
in which such characterization enhances relevant AI based applications. The
position defended here is that love is best understood as a concatenation of
passive sensations (e.g., emotional arousal) and active evaluative judgments
(e.g., perceiving the beloved as valuable), in the interest of balancing the
involuntary aspects of love with its rational accountability. To provide a
structured foundation, the paper draws on Basic Formal Ontology (BFO) and other
applied ontological methods to differentiate various senses of love. This work
engages with objections to the understanding of love as concatenation,
particularly concerning the relationship between sensation and judgment. A
causal correlation model is defended, ensuring that the affective and cognitive
components are linked. By offering a precise and scalable ontological account,
this work lays the foundation for future interdisciplinary applications, making
love a subject of formal inquiry in ontology engineering, artificial
intelligence, and the sciences.

</details>


### [36] [Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems](https://arxiv.org/abs/2506.05370)
*Kristy Wedel*

Main category: cs.AI

TL;DR: 本论文提出CMI作为构建智能系统的新范式，解决了生成AI内存局限，并通过Insight Layer整合上下文记忆，增强系统的连贯性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成性AI系统在应用于各种组织环境时，内存组件仍存在重大局限，导致决策时无法存储或反思完整上下文，进而导致重复错误和缺乏清晰性。提出CMI为解决此问题的新范式。

Method: 引入Insight Layer这一模块化架构，利用人机协作的反思机制、漂移检测和理由保留功能，将上下文记忆融入系统。

Result: 通过CMI系统，能够利用数据、历史、判断和变化的上下文进行推理，从而增强了人机协作、生成性AI设计和机构的韧性。

Conclusion: CMI提供了一个构建智能系统的新范式，通过将上下文存储和反思融入系统，解决了当前生成性AI架构中的基础性盲点。

Abstract: A critical challenge remains unresolved as generative AI systems are quickly
implemented in various organizational settings. Despite significant advances in
memory components such as RAG, vector stores, and LLM agents, these systems
still have substantial memory limitations. Gen AI workflows rarely store or
reflect on the full context in which decisions are made. This leads to repeated
errors and a general lack of clarity. This paper introduces Contextual Memory
Intelligence (CMI) as a new foundational paradigm for building intelligent
systems. It repositions memory as an adaptive infrastructure necessary for
longitudinal coherence, explainability, and responsible decision-making rather
than passive data. Drawing on cognitive science, organizational theory,
human-computer interaction, and AI governance, CMI formalizes the structured
capture, inference, and regeneration of context as a fundamental system
capability. The Insight Layer is presented in this paper to operationalize this
vision. This modular architecture uses human-in-the-loop reflection, drift
detection, and rationale preservation to incorporate contextual memory into
systems. The paper argues that CMI allows systems to reason with data, history,
judgment, and changing context, thereby addressing a foundational blind spot in
current AI architectures and governance efforts. A framework for creating
intelligent systems that are effective, reflective, auditable, and socially
responsible is presented through CMI. This enhances human-AI collaboration,
generative AI design, and the resilience of the institutions.

</details>


### [37] [Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference](https://arxiv.org/abs/2506.05422)
*Andrei T. Patrascu*

Main category: cs.AI

TL;DR: 提出了一种替代传统奖励优化的新框架，通过逻辑推理实现安全规划和符号认知。与Q学习相比，具备完美安全性和高效收敛性。


<details>
  <summary>Details</summary>
Motivation: 为了替代传统的基于奖励的优化方法，我们提出了一种新的学习和规划框架，旨在通过逻辑推理保证状态转变和策略的合理性，避免概率试错。

Method: 我们的模型采取构造性逻辑推理来进行决策，基于直觉主义逻辑进行构造性证明，并在一个结构化的网格世界中运行，其中达到目标需要满足一系列中间子目标，每个子目标由逻辑约束管理。

Result: 我们的构造性代理通过目标链、条件跟踪和知识积累，构建一个可证明正确的计划。在与Q学习的实证对比中，我们的方法实现了完美的安全性、可解释的行为和高效的收敛性，并且没有无效动作。

Conclusion: 我们的方法与Q学习方法相比，实现了完美的安全性、可解释性和高效收敛性，且无无效动作，展示了在安全规划、符号认知和可信AI方面的潜力。

Abstract: We introduce a novel learning and planning framework that replaces
traditional reward-based optimisation with constructive logical inference. In
our model, actions, transitions, and goals are represented as logical
propositions, and decision-making proceeds by building constructive proofs
under intuitionistic logic. This method ensures that state transitions and
policies are accepted only when supported by verifiable preconditions --
eschewing probabilistic trial-and-error in favour of guaranteed logical
validity. We implement a symbolic agent operating in a structured gridworld,
where reaching a goal requires satisfying a chain of intermediate subgoals
(e.g., collecting keys to open doors), each governed by logical constraints.
Unlike conventional reinforcement learning agents, which require extensive
exploration and suffer from unsafe or invalid transitions, our constructive
agent builds a provably correct plan through goal chaining, condition tracking,
and knowledge accumulation. Empirical comparison with Q-learning demonstrates
that our method achieves perfect safety, interpretable behaviour, and efficient
convergence with no invalid actions, highlighting its potential for safe
planning, symbolic cognition, and trustworthy AI. This work presents a new
direction for reinforcement learning grounded not in numeric optimisation, but
in constructive logic and proof theory.

</details>


### [38] [Towards Data Systems That Are Business Semantic-Centric and AI Agents-Assisted](https://arxiv.org/abs/2506.05520)
*Cecil Pang*

Main category: cs.AI

TL;DR: The paper proposes BSDS, a system to align data systems with business needs using AI agents, improving efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies and delays caused by existing data platforms that emphasize tools over alignment with business needs, by proposing a system that integrates architecture, workflows, and team organization to ensure data systems align with business priorities.

Method: BSDS has a modular architecture with curated data linked to business entities, a knowledge base for context-aware AI agents, efficient data pipelines, optimized workflows, and incorporates human factor alignment with business semantics.

Result: Validated through real-world implementation, BSDS enables faster time-to-market, enhances cross-functional collaboration, and offers a scalable solution for businesses.

Conclusion: BSDS accelerates time-to-market for data-driven initiatives, enhances collaboration, and provides a scalable blueprint for businesses.

Abstract: Contemporary businesses operate in dynamic environments requiring rapid
adaptation to achieve goals and maintain competitiveness. Existing data
platforms often fall short by emphasizing tools over alignment with business
needs, resulting in inefficiencies and delays. To address this gap, I propose
the Business Semantics Centric, AI Agents Assisted Data System (BSDS), a
holistic system that integrates architecture, workflows, and team organization
to ensure data systems are tailored to business priorities rather than dictated
by technical constraints. BSDS redefines data systems as dynamic enablers of
business success, transforming them from passive tools into active drivers of
organizational growth. BSDS has a modular architecture that comprises curated
data linked to business entities, a knowledge base for context-aware AI agents,
and efficient data pipelines. AI agents play a pivotal role in assisting with
data access and system management, reducing human effort, and improving
scalability. Complementing this architecture, BSDS incorporates workflows
optimized for both exploratory data analysis and production requirements,
balancing speed of delivery with quality assurance. A key innovation of BSDS is
its incorporation of the human factor. By aligning data team expertise with
business semantics, BSDS bridges the gap between technical capabilities and
business needs. Validated through real-world implementation, BSDS accelerates
time-to-market for data-driven initiatives, enhances cross-functional
collaboration, and provides a scalable blueprint for businesses of all sizes.
Future research can build on BSDS to explore optimization strategies using
complex systems and adaptive network theories, as well as developing autonomous
data systems leveraging AI agents.

</details>


### [39] [Avoiding Death through Fear Intrinsic Conditioning](https://arxiv.org/abs/2506.05529)
*Rodney Sanchez,Ferat Sahin,Alexander Ororbia,Jamison Heard*

Main category: cs.AI

TL;DR: 该研究通过记忆增强神经网络引入内在奖励，成功模拟动物规避行为，使智能体能在具非描述性终端条件的环境中解决问题。


<details>
  <summary>Details</summary>
Motivation: 受到生物和心理学概念的启发，该研究旨在创造复杂行为以扩展智能体的能力，但评估这些方法时需要工程化的外部奖励以应对现实环境中的挑战。尤其是环境中存在状态带来高负面奖励但未提供反馈给智能体，比如死亡这一刺激。本研究的动机是寻求一种内在的奖励方法来解决此类问题。

Method: 该研究采用记忆增强神经网络（MANN）架构，并设计了一种受早期杏仁核发育启发的内在奖励功能，用于激励规避行为。研究还通过修改恐惧响应的阈值，产生类似一般性焦虑障碍（GADs）的行为。

Result: 研究通过在Miniworld Sidewalk环境中进行实验证明，智能体能够表现出规避行为，并解决存在非描述性终端条件的部分可观察马尔可夫决策过程（POMDP）环境中的问题。

Conclusion: 本研究提出了一种受生物学启发的内在奖励机制，通过记忆增强神经网络体系结构实现，能够成功地模拟出类似动物的规避行为，使智能体能够在无描述性终端条件的环境中解决问题。

Abstract: Biological and psychological concepts have inspired reinforcement learning
algorithms to create new complex behaviors that expand agents' capacity. These
behaviors can be seen in the rise of techniques like goal decomposition,
curriculum, and intrinsic rewards, which have paved the way for these complex
behaviors. One limitation in evaluating these methods is the requirement for
engineered extrinsic for realistic environments. A central challenge in
engineering the necessary reward function(s) comes from these environments
containing states that carry high negative rewards, but provide no feedback to
the agent. Death is one such stimuli that fails to provide direct feedback to
the agent. In this work, we introduce an intrinsic reward function inspired by
early amygdala development and produce this intrinsic reward through a novel
memory-augmented neural network (MANN) architecture. We show how this intrinsic
motivation serves to deter exploration of terminal states and results in
avoidance behavior similar to fear conditioning observed in animals.
Furthermore, we demonstrate how modifying a threshold where the fear response
is active produces a range of behaviors that are described under the paradigm
of general anxiety disorders (GADs). We demonstrate this behavior in the
Miniworld Sidewalk environment, which provides a partially observable Markov
decision process (POMDP) and a sparse reward with a non-descriptive terminal
condition, i.e., death. In effect, this study results in a
biologically-inspired neural architecture and framework for fear conditioning
paradigms; we empirically demonstrate avoidance behavior in a constructed agent
that is able to solve environments with non-descriptive terminal conditions.

</details>


### [40] [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/abs/2506.05579)
*Quan Shi,Carlos E. Jimenez,Shunyu Yao,Nick Haber,Diyi Yang,Karthik Narasimhan*

Main category: cs.AI

TL;DR: Introduced KITE framework to study human-AI knowledge transfer. Found inconsistent correlation between model performance and knowledge transfer, highlighting the need for optimization.


<details>
  <summary>Details</summary>
Motivation: Investigate whether improvements in AI reasoning also enhance knowledge transfer, i.e., the ability for AI models to communicate reasoning effectively to humans.

Method: Two-phase setup where humans first ideate with AI on problem-solving strategies, then independently implement solutions to measure the influence of model explanations on human understanding.

Result: Model benchmark performance correlates inconsistently with collaborative outcomes, featuring significant outliers. Behavioral and strategic factors are identified as mediators of successful knowledge transfer.

Conclusion: Knowledge transfer requires dedicated optimization as model benchmark performance does not consistently correlate with successful human-AI collaborative outcomes.

Abstract: Recent advancements in AI reasoning have driven substantial improvements
across diverse tasks. A critical open question is whether these improvements
also yields better knowledge transfer: the ability of models to communicate
reasoning in ways humans can understand, apply, and learn from. To investigate
this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a
conceptual and experimental framework for Human-AI knowledge transfer
capabilities and conduct the first large-scale human study (N=118) explicitly
designed to measure it. In our two-phase setup, humans first ideate with an AI
on problem-solving strategies, then independently implement solutions,
isolating model explanations' influence on human understanding. Our findings
reveal that although model benchmark performance correlates with collaborative
outcomes, this relationship is notably inconsistent, featuring significant
outliers, indicating that knowledge transfer requires dedicated optimization.
Our analysis identifies behavioral and strategic factors mediating successful
knowledge transfer. We release our code, dataset, and evaluation framework to
support future work on communicatively aligned models.

</details>


### [41] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: 论文介绍了一项面向表格任务的基准MMTU，评估现代模型在表格数据处理上的能力，并显示出当前模型在这些任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 填补当前LLMs在处理表格任务评估上的空白，以推动对结构化数据处理和分析模型的进一步发展。

Method: 通过设计一项名为MMTU的大规模基准测试，评估模型在专家级别上理解、推理和操控真实表格的能力。任务从几十年的关于表数据的计算机科学研究中汲取灵感。

Result: 包括OpenAI o4-mini和DeepSeek R1在内的先进推理模型在MMTU评估中仅得分约60%，这表明存在显著的改进空间。

Conclusion: MMTU基准通过评估30K个问题和25个真实世界表任务，显示出现代模型在理解、推理和操作真实表格方面的不足，开放了改进的空间。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [42] [Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists](https://arxiv.org/abs/2506.05616)
*Lianhao Zhou,Hongyi Ling,Keqiang Yan,Kaiji Zhao,Xiaoning Qian,Raymundo Arróyave,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.AI

TL;DR: MAPPS通过统一规划、物理和科学家，实现了自主的材料发现，显著提高了结果的稳定性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 设计具有更大自主能力的语言代理，以实现自主的水晶材料发现，超越限定在特定任务的既定工作流程内的现有研究。

Method: MAPPS包括工作流程规划器、工具代码生成器和科学中介，使用大语言模型生成结构化的多步骤工作流程，合成可执行的Python代码以完成任务，并协调沟通和处理错误。

Result: 相较于先前的生成模型，MAPPS在MP-20数据集上的稳定性、独特性和新颖性提高了五倍。

Conclusion: MAPPS提供了统一规划、物理和科学家的方法，实现了更灵活和可靠的材料发现，并在稳定性、独特性和新颖性方面相比之前的生成模型提高了五倍。

Abstract: We aim at designing language agents with greater autonomy for crystal
materials discovery. While most of existing studies restrict the agents to
perform specific tasks within predefined workflows, we aim to automate workflow
planning given high-level goals and scientist intuition. To this end, we
propose Materials Agent unifying Planning, Physics, and Scientists, known as
MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a
Scientific Mediator. The Workflow Planner uses large language models (LLMs) to
generate structured and multi-step workflows. The Tool Code Generator
synthesizes executable Python code for various tasks, including invoking a
force field foundation model that encodes physics. The Scientific Mediator
coordinates communications, facilitates scientist feedback, and ensures
robustness through error reflection and recovery. By unifying planning,
physics, and scientists, MAPPS enables flexible and reliable materials
discovery with greater autonomy, achieving a five-fold improvement in
stability, uniqueness, and novelty rates compared with prior generative models
when evaluated on the MP-20 data. We provide extensive experiments across
diverse tasks to show that MAPPS is a promising framework for autonomous
materials discovery.

</details>


### [43] [Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach](https://arxiv.org/abs/2506.05619)
*Kihyun Kim,Jiawei Zhang,Asuman Ozdaglar,Pablo A. Parrilo*

Main category: cs.AI

TL;DR: 提出新偏好学习框架，人口分布推断，通过软最大化方法平衡政策，验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统偏好学习方法在从多个评估者聚合偏好时，通常优先考虑更广泛持有的观点，这可能导致政策偏向某些类型的意见或群体。本文旨在开发能够按评估者偏好的真实人口分布比例对齐集体意见和政策的新偏好学习框架。

Method: 本文采用软最大化松弛方法，平滑地实现人口比例代表与选择康多西赢家之间的权衡。

Result: 我们的方法在表格推荐任务和大型语言模型对齐等实验中验证了其有效性和可扩展性。

Conclusion: 该论文提出了一种新的偏好学习框架，通过直接从成对比较数据推断评估者人群分布的可行集，构建满足社会选择理论的单调性和帕累托效率等基本公理的新政策。并引入了满足人口比例代表和人口有界鲁棒性的新的公理。

Abstract: Conventional preference learning methods often prioritize opinions held more
widely when aggregating preferences from multiple evaluators. This may result
in policies that are biased in favor of some types of opinions or groups. The
objective of this paper is to develop a novel preference learning framework
capable of aligning aggregate opinions and policies proportionally with the
true population distribution of evaluator preferences. Our approach infers the
feasible set of evaluator population distributions directly from pairwise
comparison data. Using these estimates, the algorithm constructs a policy that
satisfies foundational axioms from social choice theory, namely monotonicity
and Pareto efficiency, as well as our newly-introduced axioms of
population-proportional representation and population-bounded robustness. We
propose a soft-max relaxation method that smoothly trade-offs
population-proportional representation with the selection of the Condorcet
winner (which beats all other options in pairwise comparisons). Finally, we
validate the effectiveness and scalability of our approach through experiments
on both tabular recommendation tasks and large-scale language model alignment.

</details>


### [44] [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/abs/2506.05744)
*Gouki Minegishi,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 研究引入推理图概念，通过分析其结构揭示了蒸馏推理模型在推理能力上的结构优势，并提供了数据集设计的指导，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在数学基准测试中取得了卓越的性能，但其内部机制仍然不为人知。研究旨在通过分析推理图结构来提高对模型成功原因的理解，并提供数据集设计的具体指导。

Method: 通过提取推理步骤中隐藏状态表示的聚类，提出推理图的概念，并系统性地分析其三个关键图论属性：循环性、直径和小世界指数。

Result: 研究发现，蒸馏推理模型（如DeepSeek-R1-Distill-Qwen-32B）展现出了更显著的循环性、更大的图直径和更明显的小世界特征。这些结构优势随着任务难度和模型容量的增加而增长，与准确性呈正相关。

Conclusion: 通过对推理图结构（如循环、直径和小世界指数）的系统分析，研究揭示了较大规模的蒸馏推理模型在推理能力方面的结构优势，这些优势随着任务难度和模型容量的增加而增强，并与准确性呈正相关。

Abstract: Recent large-scale reasoning models have achieved state-of-the-art
performance on challenging mathematical benchmarks, yet the internal mechanisms
underlying their success remain poorly understood. In this work, we introduce
the notion of a reasoning graph, extracted by clustering hidden-state
representations at each reasoning step, and systematically analyze three key
graph-theoretic properties: cyclicity, diameter, and small-world index, across
multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled
reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly
more recurrent cycles (about 5 per sample), substantially larger graph
diameters, and pronounced small-world characteristics (about 6x) compared to
their base counterparts. Notably, these structural advantages grow with task
difficulty and model capacity, with cycle detection peaking at the 14B scale
and exploration diameter maximized in the 32B variant, correlating positively
with accuracy. Furthermore, we show that supervised fine-tuning on an improved
dataset systematically expands reasoning graph diameters in tandem with
performance gains, offering concrete guidelines for dataset design aimed at
boosting reasoning capabilities. By bridging theoretical insights into
reasoning graph structures with practical recommendations for data
construction, our work advances both the interpretability and the efficacy of
large reasoning models.

</details>


### [45] [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/abs/2506.05745)
*Emil Biju,Shayan Talaei,Zhemin Huang,Mohammadreza Pourreza,Azalia Mirhoseini,Amin Saberi*

Main category: cs.AI

TL;DR: SPRINT框架通过并行化推理过程，减少了LRMs在复杂任务中的推理时间，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在复杂推理任务中表现出色，但通常生成冗长的顺序思考链条，导致在到达最终答案之前推理时间较长。

Method: SPRINT通过创新的数据整理管道将自然语言推理轨迹重新组织为长时间规划和并行执行的结构化回合。通过在少量整理数据上微调LRMs，模型学会在扩展推理过程中动态识别独立子任务并有效并行执行。

Result: 微调后的LRMs能够动态识别扩展推理过程中的独立子任务并并行执行它们，减少生成的顺序标记数量，同时在多个复杂域中匹配传统推理模型的性能。

Conclusion: 应用SPRINT框架进行微调的模型能够在复杂领域（如数学）的性能与常规推理模型相匹配，同时在需要生成超过8000个输出标记的问题上减少约39%的顺序标记。对于GPQA和Countdown两项分布外任务，平均顺序标记最多减少45%和65%，并保持微调推理模型的性能。

Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.

</details>


### [46] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
*Emmanuel Anaya Gonzalez,Sairam Vaidya,Kanghee Park,Ruyi Ji,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 我们提出了一种基于MCMC的约束采样框架，能够在满足约束的同时高效生成高质量样本，并在合成和真实任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的约束解码方法经常扭曲模型分布，这在程序模糊测试等需要产生多样化且有效输入的应用中尤为成问题。我们提出的新方法旨在克服这一局限。

Method: 我们基于马尔可夫链蒙特卡罗（MCMC）提出了一种新的约束采样框架，通过构建一个关于有效输出的建议分布并应用基于LM似然的Metropolis-Hastings接受准则，以确保对约束空间的原则性和高效探索。

Result: 我们的采样器在合成基准测试和真实世界的程序模糊测试任务中均优于现有方法。

Conclusion: 我们的框架不仅能够满足各种约束条件，还能通过少量步骤生成高质量样本，同时保证采样过程收敛到真实的条件分布。

Abstract: Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [47] [Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction](https://arxiv.org/abs/2506.05810)
*Yesheng Zhang,Wenjian Sun,Yuheng Chen,Qingwei Liu,Qi Lin,Rui Zhang,Xu Zhao*

Main category: cs.AI

TL;DR: 本文提出Trajectory Entropy度量指标，优化level-k游戏框架，提高自动驾驶中代理交互的效率和准确性，评估显示预测精度提升19.89%，规划提升16.48%。


<details>
  <summary>Details</summary>
Motivation: 复杂的代理交互对自动驾驶提出了重大挑战。现有的level-k游戏框架虽然通过分层游戏层次有效解耦了代理策略，但忽略了代理间驾驶复杂性的差异和游戏层次中的动态变化，导致冗余和易错误计算。

Method: 提出了一种新的Trajectory Entropy指标，通过从多模态轨迹预测结果中提取不确定性统计信号，然后利用信号的信噪比来量化代理的游戏状态。同时，在level-k游戏框架上加入简单的门控机制。

Result: 在Waymo和nuPlan数据集上进行了轨迹预测、开环和闭环规划任务的评估，结果显示预测精度提高了19.89%，规划任务精度提高了16.48%。

Conclusion: 通过Trajectory Entropy和改进的level-k游戏框架，显著提高了整体准确性并减少了计算成本。

Abstract: Complex interactions among agents present a significant challenge for
autonomous driving in real-world scenarios. Recently, a promising approach has
emerged, which formulates the interactions of agents as a level-k game
framework. It effectively decouples agent policies by hierarchical game levels.
However, this framework ignores both the varying driving complexities among
agents and the dynamic changes in agent states across game levels, instead
treating them uniformly. Consequently, redundant and error-prone computations
are introduced into this framework. To tackle the issue, this paper proposes a
metric, termed as Trajectory Entropy, to reveal the game status of agents
within the level-k game framework. The key insight stems from recognizing the
inherit relationship between agent policy uncertainty and the associated
driving complexity. Specifically, Trajectory Entropy extracts statistical
signals representing uncertainty from the multimodality trajectory prediction
results of agents in the game. Then, the signal-to-noise ratio of this signal
is utilized to quantify the game status of agents. Based on the proposed
Trajectory Entropy, we refine the current level-k game framework through a
simple gating mechanism, significantly improving overall accuracy while
reducing computational costs. Our method is evaluated on the Waymo and nuPlan
datasets, in terms of trajectory prediction, open-loop and closed-loop planning
tasks. The results demonstrate the state-of-the-art performance of our method,
with precision improved by up to 19.89% for prediction and up to 16.48% for
planning.

</details>


### [48] [Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs](https://arxiv.org/abs/2506.05887)
*Marilyn Bello,Rafael Bello,Maria-Matilde García,Ann Nowé,Iván Sevillano-García,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文提出一个多层次框架，将解释与不同角色的期望对齐，并探讨如何通过大语言模型加强社会层面的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在敏感领域的应用增加，人们对不仅准确而且可解释和可信的系统的需求越来越强烈。尽管可解释人工智能（XAI）方法已经大量出现，但许多方法没有考虑与AI系统互动的多样化群体：从开发者和领域专家到最终用户及社会。

Method: 本文提出了一个多层次框架，该框架通过算法和领域基础、人本中心、以及社会可解释性三个层次来使解释与不同利益相关者的知识、背景和伦理期望相对齐。

Result: 通过案例研究，展示了这种方法如何促进技术的准确性、用户参与度和社会责任感，并将XAI重新构建为一个动态的、信任构建过程。

Conclusion: 信任AI受到解释设计和交付的影响，通过呈现案例研究，说明该方法如何促进技术可信度、用户参与和社会责任，将XAI重新定位为动态的信任构建过程。

Abstract: The growing application of artificial intelligence in sensitive domains has
intensified the demand for systems that are not only accurate but also
explainable and trustworthy. Although explainable AI (XAI) methods have
proliferated, many do not consider the diverse audiences that interact with AI
systems: from developers and domain experts to end-users and society. This
paper addresses how trust in AI is influenced by the design and delivery of
explanations and proposes a multilevel framework that aligns explanations with
the epistemic, contextual, and ethical expectations of different stakeholders.
The framework consists of three layers: algorithmic and domain-based,
human-centered, and social explainability. We highlight the emerging role of
Large Language Models (LLMs) in enhancing the social layer by generating
accessible, natural language explanations. Through illustrative case studies,
we demonstrate how this approach facilitates technical fidelity, user
engagement, and societal accountability, reframing XAI as a dynamic,
trust-building process.

</details>


### [49] [Proactive Assistant Dialogue Generation from Streaming Egocentric Videos](https://arxiv.org/abs/2506.05904)
*Yichi Zhang,Xin Luna Dong,Zhaojiang Lin,Andrea Madotto,Anuj Kumar,Babak Damavandi,Joyce Chai,Seungwhan Moon*

Main category: cs.AI

TL;DR: 该研究提出了一个全面框架，通过新型数据管道、自动评估指标和端到端模型，为实时任务指导AI系统的开发提供了支持。


<details>
  <summary>Details</summary>
Motivation: 在对话式AI取得重大进展的背景下，实时感知任务的指导系统的开发仍具挑战性。这些系统需要基于视频流输入提供互动和主动的帮助，而数据收集和系统评估过程昂贵且耗费大量人力。

Method: 1. 引入了一种新的数据策展管道，从注释的第一人称视频中合成对话，创建了一个大规模的合成对话数据集。2. 开发了一套自动化评估指标，通过广泛的人体研究进行了验证。3. 提出了一种端到端模型，能够处理视频流输入并生成上下文适当的响应，采用了处理数据不平衡和长时间视频的新技术。

Result: 创建了跨多个领域的大规模合成对话数据集\dataset，开发了经过验证的自动评估指标，并提出了能处理数据不平衡和长视频的新型端到端模型。

Conclusion: 该研究为开发实时、主动的AI助手奠定了基础，这些助手能够引导用户完成各种任务。

Abstract: Recent advances in conversational AI have been substantial, but developing
real-time systems for perceptual task guidance remains challenging. These
systems must provide interactive, proactive assistance based on streaming
visual inputs, yet their development is constrained by the costly and
labor-intensive process of data collection and system evaluation. To address
these limitations, we present a comprehensive framework with three key
contributions. First, we introduce a novel data curation pipeline that
synthesizes dialogues from annotated egocentric videos, resulting in \dataset,
a large-scale synthetic dialogue dataset spanning multiple domains. Second, we
develop a suite of automatic evaluation metrics, validated through extensive
human studies. Third, we propose an end-to-end model that processes streaming
video inputs to generate contextually appropriate responses, incorporating
novel techniques for handling data imbalance and long-duration videos. This
work lays the foundation for developing real-time, proactive AI assistants
capable of guiding users through diverse tasks. Project page:
https://pro-assist.github.io/

</details>


### [50] [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967)
*Katarzyna Kobalczyk,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文将奖励建模问题置于因果范式中，建议使用因果推断的方法来识别和解决挑战，提升模型稳健性。


<details>
  <summary>Details</summary>
Motivation: 奖励建模是将大型语言模型与人类价值观对齐的关键步骤，需要对新颖的提示-响应对进行稳健的泛化。面临的挑战包括因果误识别、偏好异质性和用户特定因素导致的混杂。

Method: 采用因果推断的方法，识别可靠泛化所需的关键假设，并与常规数据收集实践进行对比。通过展示天真奖励模型的失败模式，演示如何通过因果启发的方法改进模型的稳健性。

Result: 因果启发的方法可以提高奖励模型的稳健性，提供了一套丰富的因果工具来识别持续性挑战。此外，未来的研究和实践需要强调对观察数据内在局限性的干预。

Conclusion: 因果启发的方法可以提高奖励模型的稳健性，但需要针对观察数据的内在局限性进行有针对性的干预。

Abstract: Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.

</details>


### [51] [CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents](https://arxiv.org/abs/2506.05981)
*Qingbin Zeng,Ruotong Zhao,Jinzhu Mao,Haoyang Li,Fengli Xu,Yong Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为CrimeMind的新的LLM驱动的ABM框架，比传统模型在预测犯罪热点时更准确，并能够模拟政策干预影响。


<details>
  <summary>Details</summary>
Motivation: 现有的城市犯罪建模方法存在解释性和预测准确性之间的权衡，同时缺乏适应变化环境的灵活性。本文的动机是利用大型语言模型（LLMs）创建一种新的模型框架，以便更好地模拟多模态城市情境下的城市犯罪行为。

Method: 本文提出了一个名为CrimeMind的创新框架，该框架结合了大型语言模型和例行活动理论（RAT）的优势，能够处理丰富的多模态城市特征，并推理犯罪行为。为克服RAT中LLM推断环境安全性难以评估保护效能的问题，收集了小规模人工注释数据集，并通过无训练文本梯度方法对CrimeMind的感知进行了调整。

Result: 实验结果表明，CrimeMind在四个美国主要城市的犯罪热点预测和空间分布准确性上优于传统的ABM和深度学习基准模型，实现了高达24%的性能提升。该框架还能够成功模拟外部事件和政策干预的反事实场景变化，反映其在现实干预评估中的效用。

Conclusion: CrimeMind框架能够细化个体行为建模，并支持对现实世界干预措施的评估，展现了在真实城市犯罪建模中的应用潜力。

Abstract: Modeling urban crime is an important yet challenging task that requires
understanding the subtle visual, social, and cultural cues embedded in urban
environments. Previous work has predominantly focused on rule-based agent-based
modeling (ABM) and deep learning methods. ABMs offer interpretability of
internal mechanisms but exhibit limited predictive accuracy.In contrast, deep
learning methods are often effective in prediction but are less interpretable
and require extensive training data. Moreover, both lines of work lack the
cognitive flexibility to adapt to changing environments. Leveraging the
capabilities of large language models (LLMs), we propose CrimeMind, a novel
LLM-driven ABM framework for simulating urban crime within a multi-modal urban
context.A key innovation of our design is the integration of the Routine
Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to
process rich multi-modal urban features and reason about criminal
behavior.However, RAT requires LLM agents to infer subtle cues in evaluating
environmental safety as part of assessing guardianship, which can be
challenging for LLMs. To address this, we collect a small-scale human-annotated
dataset and align CrimeMind's perception with human judgment via a
training-free textual gradient method.Experiments across four major U.S. cities
demonstrate that CrimeMind outperforms both traditional ABMs and deep learning
baselines in crime hotspot prediction and spatial distribution accuracy,
achieving up to a 24% improvement over the strongest baseline.Furthermore, we
conduct counterfactual simulations of external incidents and policy
interventions and it successfully captures the expected changes in crime
patterns, demonstrating its ability to reflect counterfactual
scenarios.Overall, CrimeMind enables fine-grained modeling of individual
behaviors and facilitates evaluation of real-world interventions.

</details>


### [52] [CP-Bench: Evaluating Large Language Models for Constraint Modelling](https://arxiv.org/abs/2506.06052)
*Kostis Michailidis,Dimos Tsouros,Tias Guns*

Main category: cs.AI

TL;DR: 研究引入了CP-Bench数据集评估LLM在三种约束模型系统的建模能力，通过Python框架和系统提示增强，模型准确率达到70%。


<details>
  <summary>Details</summary>
Motivation: 现有约束模型评估数据集在规模、同质性或领域特定实例上过于有限，无法反映真实世界的多样性。因此需要新的评估数据集来填补这一空白。

Method: 研究采用基于LLM的代码生成技术，进行提示和推理时计算的方法评估，以提高生成有效约束模型的能力。

Result: 引入了一个新的评估数据集CP-Bench，测试了三种不同的约束模型系统中LLM的能力，并发现Python框架的便利性和文档丰富的系统提示有效性，准确率达到70%。

Conclusion: 研究表明 Python 库在约束模型生成方面具有便利性，并且配合文档丰富的系统提示，可以显著提高准确性，达到70% 的正确率。

Abstract: Combinatorial problems are present in a wide range of industries. Constraint
Programming (CP) is a well-suited problem-solving paradigm, but its core
process, namely constraint modelling, is a bottleneck for wider adoption.
Aiming to alleviate this bottleneck, recent studies have explored using Large
Language Models (LLMs) as modelling assistants, transforming combinatorial
problem descriptions to executable constraint models, similar to coding
assistants. However, the existing evaluation datasets for constraint modelling
are often limited to small, homogeneous, or domain-specific instances, which do
not capture the diversity of real-world scenarios. This work addresses this gap
by introducing CP-Bench, a novel benchmark dataset that includes a diverse set
of well-known combinatorial problem classes sourced from the CP community,
structured explicitly for evaluating LLM-driven CP modelling. With this
dataset, and given the variety of constraint modelling frameworks, we compare
and evaluate the modelling capabilities of LLMs for three distinct constraint
modelling systems, which vary in abstraction level and underlying syntax: the
high-level MiniZinc language and Python-based CPMpy library, and the
lower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance
the ability of LLMs to produce valid constraint models, we systematically
evaluate the use of prompt-based and inference-time compute methods adapted
from existing LLM-based code generation research. Our results underscore the
modelling convenience provided by Python-based frameworks, as well as the
effectiveness of documentation-rich system prompts, which, augmented with
repeated sampling and self-verification, achieve further improvements, reaching
up to 70\% accuracy on this new, highly challenging benchmark.

</details>


### [53] [Decomposability-Guaranteed Cooperative Coevolution for Large-Scale Itinerary Planning](https://arxiv.org/abs/2506.06121)
*Ziyu Zhang,Peilan Xu,Yuetong Sun,Yuhui Shi,Wenjian Luo*

Main category: cs.AI

TL;DR: 论文提出了一种新颖的多目标协同进化算法用于大规模行程规划，证明了弱可分解性的有效性，显著优于当前最先进的方法，且随着问题规模的增加，其优势更为明显。


<details>
  <summary>Details</summary>
Motivation: 针对旅行商问题的变体——大规模行程规划，这项研究旨在确定一条最优路径，使得在旅行时限约束下，最大化收集的兴趣点分数，同时最小化旅行时间和成本。本文分析了大规模行程规划的可分解性，并指出严格的可分解性难以满足，因而引入了基于必要条件的弱可分解性定义。

Method: 设计了一种基于标准化适应度的动态分解策略，定义了考虑组分规模和贡献的优化潜力，并开发了一种计算资源分配策略。我们提出了一种新颖的多目标协同进化算法，并在一组真实世界的数据集上对该算法进行了评估。

Result: 通过对真实世界数据集的评估，与最先进的多目标行程规划算法进行对比实验，结果表明提出的算法在性能上优于现有算法，其优越性随着问题规模的增加而提升。

Conclusion: 我们提出的算法在解决大规模行程规划问题上具有优越性，尤其是在问题规模增加的情况下，与最先进的多目标行程规划算法相比，性能优势更为显著。

Abstract: Large-scale itinerary planning is a variant of the traveling salesman
problem, aiming to determine an optimal path that maximizes the collected
points of interest (POIs) scores while minimizing travel time and cost, subject
to travel duration constraints. This paper analyzes the decomposability of
large-scale itinerary planning, proving that strict decomposability is
difficult to satisfy, and introduces a weak decomposability definition based on
a necessary condition, deriving the corresponding graph structures that fulfill
this property. With decomposability guaranteed, we propose a novel
multi-objective cooperative coevolutionary algorithm for large-scale itinerary
planning, addressing the challenges of component imbalance and interactions.
Specifically, we design a dynamic decomposition strategy based on the
normalized fitness within each component, define optimization potential
considering component scale and contribution, and develop a computational
resource allocation strategy. Finally, we evaluate the proposed algorithm on a
set of real-world datasets. Comparative experiments with state-of-the-art
multi-objective itinerary planning algorithms demonstrate the superiority of
our approach, with performance advantages increasing as the problem scale
grows.

</details>


### [54] [Integer Linear Programming Preprocessing for Maximum Satisfiability](https://arxiv.org/abs/2506.06216)
*Jialu Zhang,Chu-Min Li,Sami Cherif,Shuolin Li,Zhifei Zheng*

Main category: cs.AI

TL;DR: 论文研究了ILP预处理技术对MaxSAT求解的效果，结果显示该技术可显著提升求解器性能，同时减少ILP求解器的使用。


<details>
  <summary>Details</summary>
Motivation: 探讨ILP预处理技术对于MaxSAT求解器性能的提升作用，以优化求解器的使用策略。

Method: 采用ILP预处理技术对MaxSAT问题进行求解，结合现有的MaxSAT求解器组合策略。

Result: 实验结果表明，ILP预处理技术使得WMaxCDCL-OpenWbo1200在未加权赛道中多解决了15个实例，并降低了ILP求解器在组合中的调用次数。

Conclusion: 本文提出的ILP预处理技术能够显著增加MaxSAT求解器解决问题的实例数量，同时减少对ILP求解器的调用需求。

Abstract: The Maximum Satisfiability problem (MaxSAT) is a major optimization challenge
with numerous practical applications. In recent MaxSAT evaluations, most MaxSAT
solvers have adopted an ILP solver as part of their portfolios. This paper
investigates the impact of Integer Linear Programming (ILP) preprocessing
techniques on MaxSAT solving. Experimental results show that ILP preprocessing
techniques help WMaxCDCL-OpenWbo1200, the winner of the MaxSAT evaluation 2024
in the unweighted track, solve 15 additional instances. Moreover, current
state-of-the-art MaxSAT solvers heavily use an ILP solver in their portfolios,
while our proposed approach reduces the need to call an ILP solver in a
portfolio including WMaxCDCL or MaxCDCL.

</details>


### [55] [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
*Weizhi Zhang,Xinyang Zhang,Chenwei Zhang,Liangwei Yang,Jingbo Shang,Zhepei Wei,Henry Peng Zou,Zijie Huang,Zhengyang Wang,Yifan Gao,Xiaoman Pan,Lian Xiong,Jingguo Liu,Philip S. Yu,Xian Li*

Main category: cs.AI

TL;DR: 论文提出了PersonaAgent，个性化的LLM代理框架，通过个性化记忆和动作模块实现动态用户体验优化，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型代理往往采用一刀切的方法，难以适应用户多样化的需求和偏好，因此推动开发更灵活的个性化代理框架。

Method: 文章提出了PersonaAgent框架，通过个性化记忆模块和个性化动作模块来实现个性化功能，以及提出了一种测试时用户偏好对齐策略。

Result: 实验评估表明，PersonaAgent在个性化动作空间上表现出色，并且在测试时的真实应用场景中具有良好的扩展性能。

Conclusion: PersonaAgent通过整合个性化记忆模块和个性化动作模块，实现了在各种个性化任务中的显著表现，优于现有的基线方法。

Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced
paradigms that exhibit impressive capabilities in a wide range of domains and
tasks. Despite their potential, current LLM agents often adopt a
one-size-fits-all approach, lacking the flexibility to respond to users'
varying needs and preferences. This limitation motivates us to develop
PersonaAgent, the first personalized LLM agent framework designed to address
versatile personalization tasks. Specifically, PersonaAgent integrates two
complementary components - a personalized memory module that includes episodic
and semantic memory mechanisms; a personalized action module that enables the
agent to perform tool actions tailored to the user. At the core, the persona
(defined as unique system prompt for each user) functions as an intermediary:
it leverages insights from personalized memory to control agent actions, while
the outcomes of these actions in turn refine the memory. Based on the
framework, we propose a test-time user-preference alignment strategy that
simulate the latest n interactions to optimize the persona prompt, ensuring
real-time user preference alignment through textual loss feedback between
simulated and ground-truth responses. Experimental evaluations demonstrate that
PersonaAgent significantly outperforms other baseline methods by not only
personalizing the action space effectively but also scaling during test-time
real-world applications. These results underscore the feasibility and potential
of our approach in delivering tailored, dynamic user experiences.

</details>


### [56] [Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens](https://arxiv.org/abs/2506.06261)
*Jihwan Jeong,Xiaoyu Wang,Jingmin Wang,Scott Sanner,Pascal Poupart*

Main category: cs.AI

TL;DR: 提出了一种新型离线强化学习方法 RefPlan，以应对高不确定性和数据有限的问题，显著改善了策略的性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在在线探索昂贵或不安全时至关重要，但由于数据有限，常常会面临高 epistemic 不确定性。现有方法依赖固定保守策略，限制了适应性和泛化能力。因此提出 RefPlan 以解决这些问题。

Method: 提出了一种称为 Reflect-then-Plan (RefPlan) 的新型双贝叶斯离线模型规划方法，通过将规划重新解释为贝叶斯后验估计来统一不确定性建模和模型规划。

Result: 实证结果表明，RefPlan 显著提高了保守离线 RL 策略的性能，保持了在高不确定性和有限数据条件下的鲁棒性，并表现出对环境动态变化的适应力。

Conclusion: RefPlan 展示了显著的性能提升，同时在高 epistemic 不确定性和数据有限的情况下保持稳定的性能，并且在环境动态变化时表现出韧性。

Abstract: Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [57] [A MARL-based Approach for Easing MAS Organization Engineering](https://arxiv.org/abs/2506.05437)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Louis-Marie Traonouez,Paul Théron*

Main category: cs.MA

TL;DR: AOMEA uses MARL and organizational models to improve MAS design, addressing complexity and safety issues in IoT systems.


<details>
  <summary>Details</summary>
Motivation: Designing effective MAS for complex and distributed environments in IoT systems is challenging due to high complexity and low readability, making traditional methods costly and risking safety.

Method: A new approach called Assisted MAS Organization Engineering Approach (AOMEA) is proposed, which uses Multi-Agent Reinforcement Learning (MARL) combined with organizational models to suggest organizational specifications.

Result: AOMEA offers a way to improve MAS organization by providing relevant specifications through the integration of MARL, facilitating the engineering process and addressing complexity and readability issues.

Conclusion: AOMEA presents a promising approach to enhance MAS organization by integrating MARL with organizational models, potentially leading to more efficient and safer MAS designs.

Abstract: Multi-Agent Systems (MAS) have been successfully applied in industry for
their ability to address complex, distributed problems, especially in IoT-based
systems. Their efficiency in achieving given objectives and meeting design
requirements is strongly dependent on the MAS organization during the
engineering process of an application-specific MAS. To design a MAS that can
achieve given goals, available methods rely on the designer's knowledge of the
deployment environment. However, high complexity and low readability in some
deployment environments make the application of these methods to be costly or
raise safety concerns. In order to ease the MAS organization design regarding
those concerns, we introduce an original Assisted MAS Organization Engineering
Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement
Learning (MARL) process with an organizational model to suggest relevant
organizational specifications to help in MAS engineering.

</details>


### [58] [Sequence Modeling for N-Agent Ad Hoc Teamwork](https://arxiv.org/abs/2506.05527)
*Caroline Wang,Di Yang Shi,Elad Liebman,Ishan Durugkar,Arrasy Rahman,Peter Stone*

Main category: cs.MA

TL;DR: 提出了一种基于Transformer的集中式方法MAT-NAHT，用于应对多代理强化学习中的N-agent ad hoc teamwork挑战，优于已有算法POAM。


<details>
  <summary>Details</summary>
Motivation: POAM的独立学习未能充分捕捉有效协作所需的代理间动态。观察到Transformer在处理变长序列方面的有效性。

Method: 采用基于Transformer的集中式方法，结合所有控制代理的历史观察和动作。

Result: 在星际争霸II任务中，MAT-NAHT表现优于POAM，具有更优的样本效率和泛化能力。

Conclusion: MAT-NAHT比POAM在样本效率和泛化能力方面表现优异。

Abstract: N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent
reinforcement learning, where controlled subteams of varying sizes must
dynamically collaborate with varying numbers and types of unknown teammates
without pre-coordination. The existing learning algorithm (POAM) considers only
independent learning for its flexibility in dealing with a changing number of
agents. However, independent learning fails to fully capture the inter-agent
dynamics essential for effective collaboration. Based on our observation that
transformers deal effectively with sequences with varying lengths and have been
shown to be highly effective for a variety of machine learning problems, this
work introduces a centralized, transformer-based method for N-agent ad hoc
teamwork. Our proposed approach incorporates historical observations and
actions of all controlled agents, enabling optimal responses to diverse and
unseen teammates in partially observable environments. Empirical evaluation on
a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving
superior sample efficiency and generalization, without auxiliary agent-modeling
objectives.

</details>


### [59] [Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars](https://arxiv.org/abs/2506.05555)
*Oliver Slumbers,Joel Z. Leibo,Marco A. Janssen*

Main category: cs.MA

TL;DR: 研究使用大语言模型替代人类研究集体风险社会困境，以提供多样化的实验环境，特别聚焦在机构经济与可持续性领域的案例研究。


<details>
  <summary>Details</summary>
Motivation: 研究集体风险社会困境（CRSD），如气候变化，需理解其社会基础，但传统CRSD方法需大规模实验，难以保证人群多样性。生成式AI提供替代方案。

Method: 使用大语言模型（LLM）替代人类参与者，构建可扩展的经验框架以模拟大规模、多样化的人类实验。

Result: 研究着重验证该方法的有效性，探索LLM能否真实再现复杂的CRSD实验，尤其在机构经济学和可持续性文学中称为火星港的案例中。

Conclusion: 大语言模型可以作为大型人类实验的可行替代方案，以研究集体风险社会困境，尤其在无法达到实验多样性时具有独特优势。

Abstract: Collective risk social dilemmas (CRSD) highlight a trade-off between
individual preferences and the need for all to contribute toward achieving a
group objective. Problems such as climate change are in this category, and so
it is critical to understand their social underpinnings. However, rigorous CRSD
methodology often demands large-scale human experiments but it is difficult to
guarantee sufficient power and heterogeneity over socio-demographic factors.
Generative AI offers a potential complementary approach to address thisproblem.
By replacing human participants with large language models (LLM), it allows for
a scalable empirical framework. This paper focuses on the validity of this
approach and whether it is feasible to represent a large-scale human-like
experiment with sufficient diversity using LLM. In particular, where previous
literature has focused on political surveys, virtual towns and classical
game-theoretic examples, we focus on a complex CRSD used in the institutional
economics and sustainability literature known as Port of Mars

</details>


### [60] [Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game](https://arxiv.org/abs/2506.06032)
*Edward Hughes,Tina O. Zhu,Martin J. Chadwick,Raphael Koster,Antonio García Castañeda,Charles Beattie,Thore Graepel,Matthew M. Botvinick,Joel Z. Leibo*

Main category: cs.MA

TL;DR: 研究展示了多智能体强化学习模型在“清理”游戏中模拟人类社交行为的有效性，强调了身份识别和声誉跟踪的重要性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在验证多智能体强化学习算法在模拟复杂社交行为中的有效性，尤其是在实验室条件下与真人参与者进行的实验尚缺乏实证支持的情况下。

Method: 研究采用了一种名为“清理”的公共物品提供游戏，作为时空复杂环境下多智能体强化学习模型的实验场景。通过操控参与者能否识别他人并跟踪声誉，分析人类组与人工智能组在可识别与匿名条件下的行为表现。

Result: 结果显示，在“清理”游戏中，当人类参与者能够识别他人并跟踪声誉时，他们表现出成功的合作，而在匿名情况下则失败。新的多智能体强化学习模型复现了这一现象，且无论是人类组还是人工智能组，都倾向于通过轮换制解决问题。

Conclusion: 研究表明多智能体强化学习模型能够在复杂环境中成功模拟基于声誉的合作行为，与人类的社交行为非常相似，突显出此类模型在模拟人类社交行为方面的潜力。

Abstract: Multi-agent reinforcement learning algorithms are useful for simulating
social behavior in settings that are too complex for other theoretical
approaches like game theory. However, they have not yet been empirically
supported by laboratory experiments with real human participants. In this work
we demonstrate how multi-agent reinforcement learning can model group behavior
in a spatially and temporally complex public good provision game called Clean
Up. We show that human groups succeed in Clean Up when they can see who is who
and track reputations over time but fail under conditions of anonymity. A new
multi-agent reinforcement learning model of reputation-based cooperation
demonstrates the same difference between identifiable and anonymous conditions.
Furthermore, both human groups and artificial agent groups solve the problem
via turn-taking despite other options being available. Our results highlight
the benefits of using multi-agent reinforcement learning to model human social
behavior in complex environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/abs/2506.05426)
*Wenhao Wu,Fuhong Liu,Haoru Li,Zican Hu,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: T2MIR框架通过引入混合专家模型显著提升了上下文强化学习能力，并在多种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在上下文强化学习领域中，存在多模态状态-动作-奖励数据的内在多样性以及决策任务的多样性。为解决这些挑战，提出了T2MIR框架。

Method: T2MIR框架引入了混合专家模型（MoE）到基于transformer的决策模型，替换为两个并行层：捕捉多个模态输入令牌语义的token-wise MoE层和任务多样性专用专家的task-wise MoE层。通过对比学习增强任务路由，输出经合并输入下一层。

Result: 通过全面实验，T2MIR显著提升了上下文学习能力，并且在各种基线之上表现出色。

Conclusion: T2MIR框架显著提高了上下文学习能力，并在各种基准测试中表现优异，推动了混合专家模式在上下文强化学习领域的潜力和应用。

Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise
\textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that
introduces architectural advances of mixture-of-experts (MoE) into
transformer-based decision models. T2MIR substitutes the feedforward layer with
two parallel layers: a token-wise MoE that captures distinct semantics of input
tokens across multiple modalities, and a task-wise MoE that routes diverse
tasks to specialized experts for managing a broad task distribution with
alleviated gradient conflicts. To enhance task-wise routing, we introduce a
contrastive learning method that maximizes the mutual information between the
task and its router representation, enabling more precise capture of
task-relevant information. The outputs of two MoE components are concatenated
and fed into the next layer. Comprehensive experiments show that T2MIR
significantly facilitates in-context learning capacity and outperforms various
types of baselines. We bring the potential and promise of MoE to ICRL, offering
a simple and scalable architectural enhancement to advance ICRL one step closer
toward achievements in language and vision communities. Our code is available
at https://github.com/NJU-RL/T2MIR.

</details>


### [62] [MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction](https://arxiv.org/abs/2506.05427)
*Zishan Shu,Yufan Deng,Hongyu Zhang,Zhiwei Nie,Jie Chen*

Main category: cs.LG

TL;DR: MTPNet通过结合目标蛋白的信息以改进活性断崖预测，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法仅限于处理单一结合目标，限制了预测模型的适用性。因此，提出一种新的框架来更好地捕捉分子与蛋白质目标之间的互动信息。

Method: 提出了一个包含宏观和微观层次两个组成部分的多层级目标感知网络(MTPNet)，用于动态优化分子表示。

Result: MTPNet在30个代表性活性断崖数据集上的性能显著优于之前的方法，平均RMSE提升了18.95%。

Conclusion: MTPNet通过结合分子与目标蛋白之间的相互作用先验知识，在活性断崖预测中显著优于现有方法。

Abstract: Activity cliff prediction is a critical task in drug discovery and material
design. Existing computational methods are limited to handling single binding
targets, which restricts the applicability of these prediction models. In this
paper, we present the Multi-Grained Target Perception network (MTPNet) to
incorporate the prior knowledge of interactions between the molecules and their
target proteins. Specifically, MTPNet is a unified framework for activity cliff
prediction, which consists of two components: Macro-level Target Semantic (MTS)
guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet
dynamically optimizes molecular representations through multi-grained protein
semantic conditions. To our knowledge, it is the first time to employ the
receptor proteins as guiding information to effectively capture critical
interaction details. Extensive experiments on 30 representative activity cliff
datasets demonstrate that MTPNet significantly outperforms previous approaches,
achieving an average RMSE improvement of 18.95% on top of several mainstream
GNN architectures. Overall, MTPNet internalizes interaction patterns through
conditional deep learning to achieve unified predictions of activity cliffs,
helping to accelerate compound optimization and design. Codes are available at:
https://github.com/ZishanShu/MTPNet.

</details>


### [63] [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/abs/2506.05428)
*Zhihao Tang,Chaozhuo Li,Litian Zhang,Xi Zhang*

Main category: cs.LG

TL;DR: MCI-Diff is a framework for predicting MCI conversion that balances immediacy and accuracy by synthesizing future sMRI from baseline, showing a 5-12% performance boost on standard datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge in early prediction of MCI conversion, balancing between quick predictions from a single baseline sMRI and accuracy through longitudinal scans that depict disease progression.

Method: The method introduces MCI-Diff, which synthesizes future sMRI representations from baseline data using a diffusion-based framework. It employs a multi-task sequence reconstruction strategy for handling irregular follow-up sampling and learning latent trajectories through a shared denoising network. Additionally, a language model-driven "linguistic compass" is used for clinical plausibility sampling to guide realistic disease pattern generation.

Result: The result demonstrates that MCI-Diff achieves superior performance compared to state-of-the-art methods, with a 5-12% improvement in early conversion accuracy on ADNI and AIBL cohorts.

Conclusion: MCI-Diff outperforms existing baselines and improves early conversion accuracy by 5-12% in predicting MCI progression.

Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by
a trade-off between immediacy--making fast predictions from a single baseline
sMRI--and accuracy--leveraging longitudinal scans to capture disease
progression. We propose MCI-Diff, a diffusion-based framework that synthesizes
clinically plausible future sMRI representations directly from baseline data,
achieving both real-time risk assessment and high predictive performance.
First, a multi-task sequence reconstruction strategy trains a shared denoising
network on interpolation and extrapolation tasks to handle irregular follow-up
sampling and learn robust latent trajectories. Second, an LLM-driven
"linguistic compass" is introduced for clinical plausibility sampling:
generated feature candidates are quantized, tokenized, and scored by a
fine-tuned language model conditioned on expected structural biomarkers,
guiding autoregressive generation toward realistic disease patterns.
Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms
state-of-the-art baselines, improving early conversion accuracy by 5-12%.

</details>


### [64] [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/abs/2506.05432)
*Yuxuan Yue,Zukang Xu,Zhihang Yuan,Dawei Yang,Jianglong Wu,Liqiang Nie*

Main category: cs.LG

TL;DR: PCDVQ通过对方向和幅值的独立量化，大幅改善了2-bit量化大语言模型的性能，尤其在零样本任务上。


<details>
  <summary>Details</summary>
Motivation: 现有的向量量化方法通常将向量的方向和幅值一并量化，但研究发现，方向对量化更加敏感，这与现有方法使用的欧氏距离作为相似度度量相悖，因为欧氏距离更注重减少幅值误差。因此，需要一种新的方法来解决这些问题。

Method: 提出了一种极坐标解耦向量量化（PCDVQ）框架，其包括两个主要模块：1）极坐标解耦（PCD），将向量转换为极坐标表示，并独立量化方向和幅值参数；2）分布对齐码本构造（DACC），优化方向和幅值码本以符合源分布。

Result: 实验结果表明，PCDVQ在2-bit级别上至少提高了1.5%的零样本任务准确率，这证明了其在准确性和高压缩率上的优越性。

Conclusion: 本研究提出的PCDVQ框架显著提高了大语言模型在低比特量化下的性能，尤其是在2-bit量化的条件下，能够比现有基线方法提高至少1.5%的零样本准确率。PCDVQ为实现高效且准确的LLMs量化提供了新的范式。

Abstract: Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.

</details>


### [65] [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/abs/2506.05433)
*Zikang Liu,Tongtian Yue,Yepeng Tang,Longteng Guo,Junxian Cai,Qingbin Liu,Xi Chen,Jing Liu*

Main category: cs.LG

TL;DR: Prefix Grouper reduces GRPO's computational costs by eliminating redundant prefix encoding, improving scalability without affecting outcomes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency in GRPO when dealing with long shared prefixes, which incurs substantial overhead due to redundant encoding for each group member, hence posing scalability challenges.

Method: The method, called Prefix Grouper, eliminates redundant computations by restructuring self-attention into two parts. This allows the shared prefix to be encoded only once using a Shared-Prefix Forward strategy, maintaining full differentiability and end-to-end training compatibility.

Result: Experimental results demonstrate that Prefix Grouper achieves equivalent training outcomes as standard GRPO while drastically reducing the computational expense, confirming its efficacy in long-prefix contexts.

Conclusion: Prefix Grouper method retains the same forward outputs and backward gradients as standard GRPO while significantly reducing computational costs, especially in scenarios involving long prefixes. It enables larger group sizes under the same computational resources, thus improving the scalability of GRPO.

Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by
computing gradients from relative comparisons among candidate outputs that
share a common input prefix. Despite its effectiveness, GRPO introduces
substantial computational overhead when processing long shared prefixes, which
must be redundantly encoded for each group member. This inefficiency becomes a
major scalability bottleneck in long-context learning scenarios. We propose
Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant
prefix computation via a Shared-Prefix Forward strategy. In particular, by
restructuring self-attention into two parts, our method enables the shared
prefix to be encoded only once, while preserving full differentiability and
compatibility with end-to-end training. We provide both theoretical and
empirical evidence that Prefix Grouper is training-equivalent to standard GRPO:
it yields identical forward outputs and backward gradients, ensuring that the
optimization dynamics and final policy performance remain unchanged.
Empirically, our experiments confirm that Prefix Grouper achieves consistent
results while significantly reducing the computational cost of training,
particularly in long-prefix scenarios. The proposed method is fully
plug-and-play: it is compatible with existing GRPO-based architectures and can
be seamlessly integrated into current training pipelines as a drop-in
replacement, requiring no structural modifications and only minimal changes to
input construction and attention computation. Prefix Grouper enables the use of
larger group sizes under the same computational budget, thereby improving the
scalability of GRPO to more complex tasks and larger models. Code is now
available at https://github.com/johncaged/PrefixGrouper

</details>


### [66] [Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data](https://arxiv.org/abs/2506.05542)
*Vlastimil Martinek,Andrea Gariboldi,Dimosthenis Tzimotoudis,Aitor Alberdi Escudero,Edward Blake,David Cechak,Luke Cassar,Alessandro Balestrucci,Panagiotis Alexiou*

Main category: cs.LG

TL;DR: Agentomics-ML是一种自主系统，在基因组和转录组数据集上表现出色，缩小了与专家设计模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在应用于异构计算生物数据集时，存在泛化能力和成功率方面的挑战，需要自动方法生成可推广的预测模型。

Method: 提出了一种名为Agentomics-ML的自主代理系统，遵循机器学习实验流程，通过Bash与文件系统交互完成各步骤，以生成分类模型。使用训练和验证指标为反思步骤提供反馈，从而调整数据表示、模型架构和超参数选择。

Result: 在多个已建立的基因组和转录组基准数据集上评估Agentomics-ML，显示其在泛化和成功率上优于现有方法。

Conclusion: Agentomics-ML缩小了全自主系统在计算生物学数据集上的性能差距，并在一个基准数据集上达到了最新技术水平。

Abstract: The adoption of machine learning (ML) and deep learning methods has
revolutionized molecular medicine by driving breakthroughs in genomics,
transcriptomics, drug discovery, and biological systems modeling. The
increasing quantity, multimodality, and heterogeneity of biological datasets
demand automated methods that can produce generalizable predictive models.
Recent developments in large language model-based agents have shown promise for
automating end-to-end ML experimentation on structured benchmarks. However,
when applied to heterogeneous computational biology datasets, these methods
struggle with generalization and success rates. Here, we introduce
Agentomics-ML, a fully autonomous agent-based system designed to produce a
classification model and the necessary files for reproducible training and
inference. Our method follows predefined steps of an ML experimentation
process, repeatedly interacting with the file system through Bash to complete
individual steps. Once an ML model is produced, training and validation metrics
provide scalar feedback to a reflection step to identify issues such as
overfitting. This step then creates verbal feedback for future iterations,
suggesting adjustments to steps such as data representation, model
architecture, and hyperparameter choices. We have evaluated Agentomics-ML on
several established genomic and transcriptomic benchmark datasets and show that
it outperforms existing state-of-the-art agent-based methods in both
generalization and success rates. While state-of-the-art models built by domain
experts still lead in absolute performance on the majority of the computational
biology datasets used in this work, Agentomics-ML narrows the gap for fully
autonomous systems and achieves state-of-the-art performance on one of the used
benchmark datasets. The code is available at
https://github.com/BioGeMT/Agentomics-ML.

</details>


### [67] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/abs/2506.05434)
*Thomas Massena,Léo andéol,Thibaut Boissin,Franck Mamalet,Corentin Friedrich,Mathieu Serrurier,Sébastien Gerchinovitz*

Main category: cs.LG

TL;DR: 提出了一种新的方法利用Lipschitz有界网络来估计鲁棒保序预测集合，提升了在ImageNet等场景下的计算效率和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在面对对抗性攻击时，传统保序预测的保证失效。现有方法在大规模问题上，保证的保序预测集合要么过大，要么计算负担过重，难以在现实场景中部署。本文的目的是开发一种既能提供准确的鲁棒集合又能提高计算效率的方法。

Method: 本文使用Lipschitz有界网络来开发一种新的估计鲁棒保序预测集合的方法。结合1-Lipschitz鲁棒网络，成果在估算精确性和计算效率方面优于现有方法。

Result: 新的lip-rcp方法在中大规模问题上超过了现有方法，提供了更小且计算上更高效的鲁棒CP集合。此外，还研究了经典CP在攻击下的情况，得出了同时对所有攻击等级有效的新最坏情况覆盖界限。

Conclusion: 本文提出了一种利用Lipschitz有界网络的新方法，以精确和高效地估计鲁棒的保序预测（CP）集合。结合1-Lipschitz鲁棒网络，我们的lip-rcp方法在中大规模场景（如ImageNet）中，在鲁棒CP集合的大小和计算效率方面均表现出色。

Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [68] [Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts](https://arxiv.org/abs/2506.05577)
*Saptarshi Nath,Christos Peridis,Eseoghene Benjamin,Xinran Liu,Soheil Kolouri,Peter Kinnell,Zexin Li,Cong Liu,Shirin Dora,Andrea Soltoggio*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为MOSAIC的算法，通过模块化、余弦相似性估算、异步通信实现多个代理独立解决任务，并提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 在分散环境中，代理系统面临带宽限制、异步执行以及缺乏集中模型或共同目标的挑战。利用以前学习的技能、任务相似性和通信能力是实现扩展性、开放性和有益协作学习动态的关键。

Method: 提出了一种称为MOSAIC的代理算法，通过三个机制来实现：1. 通过神经网络掩码进行模块化策略组合；2. 使用Wasserstein嵌入进行余弦相似性估算用于知识选择；3. 异步通信和策略整合。

Result: 实验结果表明，MOSAIC在一些强化学习基准测试上具有比孤立学习者更大的样本效率，即学习速度显著更快，并且在某些情况下可以解决孤立学习者无法解决的任务。还观察到协作学习和共享动态会导致任务理想路径的出现，从简单到困难。

Conclusion: 协作学习在自主系统中的应用支持了更好的和不断发展的性能，从个体和集合水平来看都能获得提升。

Abstract: Agentic AI has gained significant interest as a research paradigm focused on
autonomy, self-directed learning, and long-term reliability of decision making.
Real-world agentic systems operate in decentralized settings on a large set of
tasks or data distributions with constraints such as limited bandwidth,
asynchronous execution, and the absence of a centralized model or even common
objectives. We posit that exploiting previously learned skills, task
similarities, and communication capabilities in a collective of agentic AI are
challenging but essential elements to enabling scalability, open-endedness, and
beneficial collaborative learning dynamics. In this paper, we introduce Modular
Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm
that allows multiple agents to independently solve different tasks while also
identifying, sharing, and reusing useful machine-learned knowledge, without
coordination, synchronization, or centralized control. MOSAIC combines three
mechanisms: (1) modular policy composition via neural network masks, (2) cosine
similarity estimation using Wasserstein embeddings for knowledge selection, and
(3) asynchronous communication and policy integration. Results on a set of RL
benchmarks show that MOSAIC has a greater sample efficiency than isolated
learners, i.e., it learns significantly faster, and in some cases, finds
solutions to tasks that cannot be solved by isolated learners. The
collaborative learning and sharing dynamics are also observed to result in the
emergence of ideal curricula of tasks, from easy to hard. These findings
support the case for collaborative learning in agentic systems to achieve
better and continuously evolving performance both at the individual and
collective levels.

</details>


### [69] [Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning](https://arxiv.org/abs/2506.05435)
*Manon Renault,Hamoud Younes,Hugo Tessier,Ronan Le Roy,Bastien Pasdeloup,Mathieu Léonardon*

Main category: cs.LG

TL;DR: 提出了一种使用嵌入式系统和一维卷积神经网络实现包裹状态检测的方法，通过数据增强和压缩技术提高了模型的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 设计一种能在数年内可靠运行并精准检测可重复使用包裹状态的系统，提升操作效率并促进生态可持续性。

Method: 研究采用了一种一维卷积神经网络架构来对来自嵌入式传感器的加速度计数据进行分类。为解决数据集的不平衡，研究在训练前测试了两种数据增强技术：合成少数过采样技术（SMOTE）和自适应合成采样方法（ADASYN）。训练后，压缩技术被应用以减小模型尺寸。

Result: 在二分类问题中，方法对第一类的精度为94.54%，对第二类的精度为95.83%，同时压缩技术将模型大小缩小了四倍。训练后的模型部署在IoT设备上，于推理时功耗为316 mW。

Conclusion: 本文设计的系统成功在IoT设备上部署，使用深度学习模型进行状态分类，具备高精度和低功耗的优势，解决了包裹状态检测问题。

Abstract: Package monitoring is an important topic in industrial applications, with
significant implications for operational efficiency and ecological
sustainability. In this study, we propose an approach that employs an embedded
system, placed on reusable packages, to detect their state (on a Forklift, in a
Truck, or in an undetermined location). We aim to design a system with a
lifespan of several years, corresponding to the lifespan of reusable packages.
Our analysis demonstrates that maximizing device lifespan requires minimizing
wake time. We propose a pipeline that includes data processing, training, and
evaluation of the deep learning model designed for imbalanced, multiclass time
series data collected from an embedded sensor. The method uses a
one-dimensional Convolutional Neural Network architecture to classify
accelerometer data from the IoT device. Before training, two data augmentation
techniques are tested to solve the imbalance problem of the dataset: the
Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling
approach. After training, compression techniques are implemented to have a
small model size. On the considered twoclass problem, the methodology yields a
precision of 94.54% for the first class and 95.83% for the second class, while
compression techniques reduce the model size by a factor of four. The trained
model is deployed on the IoT device, where it operates with a power consumption
of 316 mW during inference.

</details>


### [70] [An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics](https://arxiv.org/abs/2506.05438)
*Tongda Sun,Chen Yin,Huailiang Zheng,Yining Dong*

Main category: cs.LG

TL;DR: 提出了一种无监督框架构建动态健康指标(HI)的方法，有效捕捉退化过程的动态信息，实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的健康指标(HI)构建方法大多依赖专家知识进行特征提取，无法捕捉序列退化过程中的动态信息，从而限制了HI在退化趋势表示和预测中的能力。

Method: 通过一种无监督框架构建新的动态健康指标(HI)，该方法考虑了HI级别的时间依赖关系。具体来说，首先通过一个基于跳跃连接的自编码器组成的退化特征学习模块，将原始信号映射到代表性的退化特征空间(DFS)，无需专家知识自动提取基本退化特征。随后，在这个DFS中提出了一个新的HI生成模块，嵌入了一个内部HI预测模块用于动态HI构建，其中保证并明确建模了过去和当前HI状态之间的时间依赖关系。

Result: 实验结果表明，在两个轴承生命周期数据集上，所提出的健康指标(HI)构建方法优于比较方法，所构建的动态HI在预测任务中表现出更好的性能。

Conclusion: 设计了基于无监督框架的动态健康指标(HI)构建方法，该方法在退化趋势建模和未来退化预测中表现卓越。

Abstract: Health indicator (HI) plays a key role in degradation assessment and
prognostics of rolling bearings. Although various HI construction methods have
been investigated, most of them rely on expert knowledge for feature extraction
and overlook capturing dynamic information hidden in sequential degradation
processes, which limits the ability of the constructed HI for degradation trend
representation and prognostics. To address these concerns, a novel dynamic HI
that considers HI-level temporal dependence is constructed through an
unsupervised framework. Specifically, a degradation feature learning module
composed of a skip-connection-based autoencoder first maps raw signals to a
representative degradation feature space (DFS) to automatically extract
essential degradation features without the need for expert knowledge.
Subsequently, in this DFS, a new HI-generating module embedded with an inner
HI-prediction block is proposed for dynamic HI construction, where the temporal
dependence between past and current HI states is guaranteed and modeled
explicitly. On this basis, the dynamic HI captures the inherent dynamic
contents of the degradation process, ensuring its effectiveness for degradation
tendency modeling and future degradation prognostics. The experiment results on
two bearing lifecycle datasets demonstrate that the proposed HI construction
method outperforms comparison methods, and the constructed dynamic HI is
superior for prognostic tasks.

</details>


### [71] [UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss](https://arxiv.org/abs/2506.05443)
*Yiyu Lin,Yan Wang,You Zhou,Xinye Ni,Jiahui Wu,Sen Yang*

Main category: cs.LG

TL;DR: 这项研究提出了 UniPTMs 框架，用于多类型蛋白质翻译后修饰预测，表现超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在跨模态特征融合、领域泛化和架构优化方面存在限制。

Method: 提出了一种创新的“主从”双路径协作架构，通过动态权重融合机制智能聚合多模态特征，并使用层次对比损失函数优化特征一致性。

Result: UniPTMs 在五种修饰类型中表现出显著的性能提高，相较于最先进模型具备更高的 MCC 和 AP 增幅。

Conclusion: UniPTMs 超越了单一类型预测范式，展现出显著的性能提升。

Abstract: As a core mechanism of epigenetic regulation in eukaryotes, protein
post-translational modifications (PTMs) require precise prediction to decipher
dynamic life activity networks. To address the limitations of existing deep
learning models in cross-modal feature fusion, domain generalization, and
architectural optimization, this study proposes UniPTMs: the first unified
framework for multi-type PTM prediction. The framework innovatively establishes
a "Master-Slave" dual-path collaborative architecture: The master path
dynamically integrates high-dimensional representations of protein sequences,
structures, and evolutionary information through a Bidirectional Gated
Cross-Attention (BGCA) module, while the slave path optimizes feature
discrepancies and recalibration between structural and traditional features
using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale
Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and
a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level
feature integration across paths, the framework employs a Hierarchical Dynamic
Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal
features. Enhanced by a novel Hierarchical Contrastive loss function for
feature consistency optimization, UniPTMs demonstrates significant performance
improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art
models across five modification types and transcends the Single-Type Prediction
Paradigm. To strike a balance between model complexity and performance, we have
also developed a lightweight variant named UniPTMs-mini.

</details>


### [72] [Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic](https://arxiv.org/abs/2506.05445)
*Thanh Vinh Vo,Young Lee,Haozhe Ma,Chien Lu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: DoSAC addresses hidden confounders in RL using causal intervention estimation, enhancing policy robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Hidden confounders in RL can bias policy learning, leading to suboptimal behavior. Most RL algorithms neglect causal effects, learning from statistical associations instead.

Method: DoSAC incorporates a learnable Backdoor Reconstructor to infer pseudo-past variables for backdoor adjustment, integrated into a soft actor-critic framework for computing interventional policy and entropy.

Result: DoSAC outperforms baseline algorithms in continuous control tasks under confounded conditions, demonstrating improved robustness and generalization capabilities.

Conclusion: DoSAC improves robustness, generalization, and policy reliability in RL by addressing hidden confounders using causal intervention estimation without the need for true confounders or causal labels.

Abstract: Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.

</details>


### [73] [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/abs/2506.05447)
*Andrei Mircea,Supriyo Chakraborty,Nima Chitsazan,Irina Rish,Ekaterina Lobacheva*

Main category: cs.LG

TL;DR: The paper explores how scaling affects language model training, uncovering loss deceleration and zero-sum learning as key factors influencing training dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the dynamics of scaling in language models and specifically why loss deceleration occurs during training, with the aim of improving training methods and model performance.

Method: The research involved studying the training dynamics of language models, particularly focusing on how scaling affects loss deceleration. The study attributes the phenomenon to zero-sum learning, where per-example gradients counteract each other.

Result: Scaling up models is shown to mitigate loss deceleration by decreasing the loss at which it occurs and improving the rate of loss improvement post-deceleration.

Conclusion: The study provides insights into language model scaling laws by highlighting the phenomenon of loss deceleration and zero-sum learning (ZSL), suggesting they could be targeted to improve language models independent of scale.

Abstract: This work aims to understand how scaling improves language models,
specifically in terms of training dynamics. We find that language models
undergo loss deceleration early in training; an abrupt slowdown in the rate of
loss improvement, resulting in piecewise linear behaviour of the loss curve in
log-log space. Scaling up the model mitigates this transition by (1) decreasing
the loss at which deceleration occurs, and (2) improving the log-log rate of
loss improvement after deceleration. We attribute loss deceleration to a type
of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,
per-example gradients become systematically opposed, leading to destructive
interference in per-example changes in loss. As a result, improving loss on one
subset of examples degrades it on another, bottlenecking overall progress. Loss
deceleration and ZSL provide new insights into the training dynamics underlying
language model scaling laws, and could potentially be targeted directly to
improve language models independent of scale. We make our code and artefacts
available at: https://github.com/mirandrom/zsl

</details>


### [74] [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/abs/2506.05454)
*Liang Zhang,Bingcong Li,Kiran Koshy Thekumparampil,Sewoong Oh,Michael Muehlebach,Niao He*

Main category: cs.LG

TL;DR: 零阶优化倾向选择Hessian迹小的解，加强对稳定解的理解。实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 探索零阶优化方法中的隐式正则化效应，以更细致地理解该优化过程如何选择特定解。

Method: 在零阶优化中采用标准的两点估计器来分析Hessian迹对求解过程的影响，并提供零阶优化收敛到平坦极小值的速度。

Result: 零阶优化方法趋向于选择Hessian迹较小的解，并在凸函数及充分光滑函数下能有效收敛到近似的平坦极小值。实验结果支持了理论发现。

Conclusion: 零阶优化方法不仅可以收敛到任意驻点，还对最终求得的解有隐式正则化作用，倾向于选择Hessian迹较小的解。

Abstract: Zeroth-order methods are extensively used in machine learning applications
where gradients are infeasible or expensive to compute, such as black-box
attacks, reinforcement learning, and language model fine-tuning. Existing
optimization theory focuses on convergence to an arbitrary stationary point,
but less is known on the implicit regularization that provides a fine-grained
characterization on which particular solutions are finally reached. We show
that zeroth-order optimization with the standard two-point estimator favors
solutions with small trace of Hessian, which is widely used in previous work to
distinguish between sharp and flat minima. We further provide convergence rates
of zeroth-order optimization to approximate flat minima for convex and
sufficiently smooth functions, where flat minima are defined as the minimizers
that achieve the smallest trace of Hessian among all optimal solutions.
Experiments on binary classification tasks with convex losses and language
model fine-tuning support our theoretical findings.

</details>


### [75] [Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors](https://arxiv.org/abs/2506.05479)
*Matei Gabriel Coşa,Marek Eliáš*

Main category: cs.LG

TL;DR: 研究探讨如何选择和查询启发式以在在线输入处理中实现最佳表现。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是协调多个为不同类型输入实例设计的启发式算法，期望在在线实例处理中能够选择最佳表现的启发式，从而优化性能。

Method: 论文介绍了一种方法，通过查询启发式来处理在线输入，并通过实现不同启发式的时间步骤来比较其绩效。

Result: 本文展示了一种能够实现$O(\text{OPT}^{2/3})$遗憾度的方法，并基于Dekel等人（2013）的构建证明了一个紧的下界。

Conclusion: 论文提出了一种方法，通过查询在线处理输入实例时一个最佳启发式来实现与所有给定启发式中表现最好的启发式相当的性能。

Abstract: We consider the following problem: We are given $\ell$ heuristics for
Metrical Task Systems (MTS), where each might be tailored to a different type
of input instances. While processing an input instance received online, we are
allowed to query the action of only one of the heuristics at each time step.
Our goal is to achieve performance comparable to the best of the given
heuristics. The main difficulty of our setting comes from the fact that the
cost paid by a heuristic at time $t$ cannot be estimated unless the same
heuristic was also queried at time $t-1$. This is related to Bandit Learning
against memory bounded adversaries (Arora et al., 2012). We show how to achieve
regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the
construction of Dekel et al. (2013).

</details>


### [76] [Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?](https://arxiv.org/abs/2506.05484)
*Ruihua Chen,Bangyu Wu,Meng Li,Kai Yang*

Main category: cs.LG

TL;DR: 本文研究了将初始模型先验知识嵌入到神经网络中的两种方法，发现去标准化优于预训练，可以简化流程、加速收敛并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高神经网络参数化全波形反演的稳定性和精度，研究探索了如何将初始模型的先验知识有效嵌入神经网络中。

Method: 研究采用两种方法将初始模型的先验知识嵌入到神经网络中：预训练和去标准化，并系统地分析这两种方法对神经网络参数重构的全波形反演的影响。

Result: 实验结果表明，与预训练相比，去标准化能够简化工作流程，加速收敛，并提高反演的准确性。

Conclusion: 研究表明，与预训练相比，去标准化可以简化工作流程，加速收敛，提高反演精度。

Abstract: Subsurface property neural network reparameterized full waveform inversion
(FWI) has emerged as an effective unsupervised learning framework, which can
invert stably with an inaccurate starting model. It updates the trainable
neural network parameters instead of fine-tuning on the subsurface model
directly. There are primarily two ways to embed the prior knowledge of the
initial model into neural networks, that is, pretraining and denormalization.
Pretraining first regulates the neural networks' parameters by fitting the
initial velocity model; Denormalization directly adds the outputs of the
network into the initial models without pretraining. In this letter, we
systematically investigate the influence of the two ways of initial model
incorporation for the neural network reparameterized FWI. We demonstrate that
pretraining requires inverting the model perturbation based on a constant
velocity value (mean) with a two-stage implementation. It leads to a complex
workflow and inconsistency of objective functions in the two-stage process,
causing the network parameters to become inactive and lose plasticity.
Experimental results demonstrate that denormalization can simplify workflows,
accelerate convergence, and enhance inversion accuracy compared with
pretraining.

</details>


### [77] [Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models](https://arxiv.org/abs/2506.05497)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: CPQ offers an innovative way to quantify uncertainty in generative models, particularly LLMs, by using a query-only framework that improves upon existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for effective uncertainty quantification in generative AI models, especially in applications with high stakes, that do not rely on structured outputs.

Method: CPQ, a new framework for conformal prediction using a query-only approach, grounded in the classical missing mass problem. It involves optimal query policies and mappings for prediction sets.

Result: CPQ showcases its applicability to black box LLMs and shows significant improvement in providing informative prediction sets across tested models and tasks.

Conclusion: CPQ provides more informative prediction sets compared to existing conformal methods in the context of language models.

Abstract: Uncertainty quantification (UQ) is essential for safe deployment of
generative AI models such as large language models (LLMs), especially in high
stakes applications. Conformal prediction (CP) offers a principled uncertainty
quantification framework, but classical methods focus on regression and
classification, relying on geometric distances or softmax scores: tools that
presuppose structured outputs. We depart from this paradigm by studying CP in a
query only setting, where prediction sets must be constructed solely from
finite queries to a black box generative model, introducing a new trade off
between coverage, test time query budget, and informativeness. We introduce
Conformal Prediction with Query Oracle (CPQ), a framework characterizing the
optimal interplay between these objectives. Our finite sample algorithm is
built on two core principles: one governs the optimal query policy, and the
other defines the optimal mapping from queried samples to prediction sets.
Remarkably, both are rooted in the classical missing mass problem in
statistics. Specifically, the optimal query policy depends on the rate of
decay, or the derivative, of the missing mass, for which we develop a novel
estimator. Meanwhile, the optimal mapping hinges on the missing mass itself,
which we estimate using Good Turing estimators. We then turn our focus to
implementing our method for language models, where outputs are vast, variable,
and often under specified. Fine grained experiments on three real world open
ended tasks and two LLMs, show CPQ applicability to any black box LLM and
highlight: (1) individual contribution of each principle to CPQ performance,
and (2) CPQ ability to yield significantly more informative prediction sets
than existing conformal methods for language uncertainty quantification.

</details>


### [78] [The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models](https://arxiv.org/abs/2506.05500)
*Alex Damian,Jason D. Lee,Joan Bruna*

Main category: cs.LG

TL;DR: 该论文研究了高斯多指数模型中的样本复杂度与估计过程，提出了生成跃迁指数并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究通用高斯多指数模型，并探讨隐藏子空间的有效估计步骤。

Method: 引入生成跃迁指数，通过谱U-统计量依据Hermite张量进行估计。

Result: 证明了所需样本复杂度，并计算出多个示例的生成跃迁指数。

Conclusion: 样本复杂度在低阶多项式框架下是必须的，并且通过提出的估计程序是足够的。

Abstract: In this work we consider generic Gaussian Multi-index models, in which the
labels only depend on the (Gaussian) $d$-dimensional inputs through their
projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient
agnostic estimation procedures for this hidden subspace. We introduce the
\emph{generative leap} exponent $k^\star$, a natural extension of the
generative exponent from [Damian et al.'24] to the multi-index setting. We
first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary
in the class of algorithms captured by the Low-Degree-Polynomial framework. We
then establish that this sample complexity is also sufficient, by giving an
agnostic sequential estimation procedure (that is, requiring no prior knowledge
of the multi-index model) based on a spectral U-statistic over appropriate
Hermite tensors. We further compute the generative leap exponent for several
examples including piecewise linear functions (deep ReLU networks with bias),
and general deep neural networks (with $r$-dimensional first hidden layer).

</details>


### [79] [Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates](https://arxiv.org/abs/2506.05513)
*Yunfei Huang,David S. Greenberg*

Main category: cs.LG

TL;DR: 交错网格上的对称性和物理约束显著提高神经PDE代理的泛化能力与准确性。


<details>
  <summary>Details</summary>
Motivation: 提高神经PDE代理在新初始条件下的泛化性，并减少随时间累积的误差。

Method: 引入新的输入和输出层，这些层在交错网格上尊重物理定律和对称性，并系统地研究这些约束对PDE代理准确性的影响。

Result: 结合对称性和物理约束的代理在任务表现、模型规模以及对初始条件和持续时间的泛化能力上均超越基线。

Conclusion: 结合物理约束和对称约束的PDE代理对泛化能力和预测准确性有显著提高。

Abstract: Neural PDE surrogates can improve the cost-accuracy tradeoff of classical
solvers, but often generalize poorly to new initial conditions and accumulate
errors over time. Physical and symmetry constraints have shown promise in
closing this performance gap, but existing techniques for imposing these
inductive biases are incompatible with the staggered grids commonly used in
computational fluid dynamics. Here we introduce novel input and output layers
that respect physical laws and symmetries on the staggered grids, and for the
first time systematically investigate how these constraints, individually and
in combination, affect the accuracy of PDE surrogates. We focus on two
challenging problems: shallow water equations with closed boundaries and
decaying incompressible turbulence. Compared to strong baselines, symmetries
and physical constraints consistently improve performance across tasks,
architectures, autoregressive prediction steps, accuracy measures, and network
sizes. Symmetries are more effective than physical constraints, but surrogates
with both performed best, even compared to baselines with data augmentation or
pushforward training, while themselves benefiting from the pushforward trick.
Doubly-constrained surrogates also generalize better to initial conditions and
durations beyond the range of the training data, and more accurately predict
real-world ocean currents.

</details>


### [80] [Winner-takes-all for Multivariate Probabilistic Time Series Forecasting](https://arxiv.org/abs/2506.05515)
*Adrien Cortés,Rémi Rehm,Victor Letzelter*

Main category: cs.LG

TL;DR: TimeMCL是一种应用于时间序列预测的多选学习方法，通过神经网络和胜者为王损失实现多样化预测，在多样性和计算成本之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 由于MCL在处理不确定和模糊任务上的简单和有效性，引起了极大的关注。为此我们提出一种改编框架以改进时间序列预测。

Method: TimeMCL方法采用了多头神经网络，并利用WTA损失来促进预测结果的多样性。

Result: 实验结果表明，TimeMCL在真实时间序列数据中表现出令人鼓舞的性能，同时计算成本较低。

Conclusion: TimeMCL方法被提出作为一种有效的多样化未来预测工具，能够在计算成本较低的情况下产生多样化预测。

Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)
paradigm to forecast multiple plausible time series futures. Our approach
employs a neural network with multiple heads and utilizes the Winner-Takes-All
(WTA) loss to promote diversity among predictions. MCL has recently gained
attention due to its simplicity and ability to address ill-posed and ambiguous
tasks. We propose an adaptation of this framework for time-series forecasting,
presenting it as an efficient method to predict diverse futures, which we
relate to its implicit quantization objective. We provide insights into our
approach using synthetic data and evaluate it on real-world time series,
demonstrating its promising performance at a light computational cost.

</details>


### [81] [On Fitting Flow Models with Large Sinkhorn Couplings](https://arxiv.org/abs/2506.05526)
*Michal Klein,Alireza Mousavi-Hosseini,Stephen Zhang,Marco Cuturi*

Main category: cs.LG

TL;DR: 研究通过增加Sinkhorn耦合的规模及降低熵正则化来提高流模型的训练效率和效果。


<details>
  <summary>Details</summary>
Motivation: 优化流模型训练过程，通过从源点和目标点中采样对，以适应动态OT问题，降低在生成数据时的训练难度和推理时的计算成本。

Method: 实验分析包括合成任务和图像生成任务，采用Sinkhorn算法，通过新的尺度不变量报告耦合的锐度，并通过分片计算在多个GPU节点上扩大$n$。

Result: 实验结果表明，在合成和图像生成任务中，当流模型使用大型Sinkhorn耦合和低熵正则化进行拟合时，性能增强显著。

Conclusion: 大型Sinkhorn耦合器和低熵正则化能够在流模型的拟合过程中提供显著的优越性。

Abstract: Flow models transform data gradually from one modality (e.g. noise) onto
another (e.g. images). Such models are parameterized by a time-dependent
velocity field, trained to fit segments connecting pairs of source and target
points. When the pairing between source and target points is given, training
flow models boils down to a supervised regression problem. When no such pairing
exists, as is the case when generating data from noise, training flows is much
harder. A popular approach lies in picking source and target points
independently. This can, however, lead to velocity fields that are slow to
train, but also costly to integrate at inference time. In theory, one would
greatly benefit from training flow models by sampling pairs from an optimal
transport (OT) measure coupling source and target, since this would lead to a
highly efficient flow solving the Benamou and Brenier dynamical OT problem. In
practice, recent works have proposed to sample mini-batches of $n$ source and
$n$ target points and reorder them using an OT solver to form better pairs.
These works have advocated using batches of size $n\approx 256$, and considered
OT solvers that return couplings that are either sharp (using e.g. the
Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.
Sinkhorn). We follow in the footsteps of these works by exploring the benefits
of increasing $n$ by three to four orders of magnitude, and look more carefully
on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn
algorithm. Our analysis is facilitated by new scale invariant quantities to
report the sharpness of a coupling, while our sharded computations across
multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic
and image generation tasks, flow models greatly benefit when fitted with large
Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

</details>


### [82] [Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum](https://arxiv.org/abs/2506.05530)
*Snir Hordan,Maya Bechler-Speicher,Gur Lifshitz,Nadav Dym*

Main category: cs.LG

TL;DR: 本文通过引入基于最大特征值多重性的表达层次结构和适应旋转等变神经网络来提高SGNNs的图表达能力，并通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前用于评价SGNNs表现能力的框架与图谱对齐不佳，因此引入一种新的表达层次结构来改善这种不足。

Method: 将旋转等变神经网络适应于图谱设置，并进行图像分类和特征向量规范化实验。

Result: 通过MNIST Superpixel数据集的图像分类实验和ZINC数据集上的特征向量规范化，验证了理论主张。

Conclusion: 通过适应旋转等变神经网络到图谱设置，提出了一种方法来提高SGNNs在简单频谱图上的表达能力。

Abstract: Spectral features are widely incorporated within Graph Neural Networks (GNNs)
to improve their expressive power, or their ability to distinguish among
non-isomorphic graphs. One popular example is the usage of graph Laplacian
eigenvectors for positional encoding in MPNNs and Graph Transformers. The
expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated
via the k-WL graph isomorphism test hierarchy and homomorphism counting. Yet,
these frameworks align poorly with the graph spectra, yielding limited insight
into SGNNs' expressive power. We leverage a well-studied paradigm of
classifying graphs by their largest eigenvalue multiplicity to introduce an
expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete
even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt
rotation equivariant neural networks to the graph spectra setting to propose a
method to provably improve SGNNs' expressivity on simple spectrum graphs. We
empirically verify our theoretical claims via an image classification
experiment on the MNIST Superpixel dataset and eigenvector canonicalization on
graphs from ZINC.

</details>


### [83] [SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms](https://arxiv.org/abs/2506.05538)
*Arnesh Batra,Anushk Kumar,Jashn Khemani,Arush Gumber,Arhan Jain,Somil Gupta*

Main category: cs.LG

TL;DR: 提出了SocialDF数据集和一种新颖的多因素检测方法来应对社交媒体上的深度伪造挑战。这种方法结合了多模态验证技术，包括面部识别、语音转录和多代理LLM管道等。


<details>
  <summary>Details</summary>
Motivation: 现有的检测框架难以区分善意和对抗性生成的深度伪造，而后者是操纵公众认知的工具。为了应对这一挑战，研究团队引入了SocialDF数据集。

Method: 提出了一种新颖的基于大型语言模型（LLM）的多因素检测方法，该方法结合了面部识别、自动语音转录和多代理LLM管道以交叉验证视听线索。

Result: 使用了一种强调强大、多模式验证技术的方法。这种方法结合了语言、行为和情境分析，以有效地辨别合成媒体与真实内容。

Conclusion: 引入了一个名为SocialDF的经过精心策划的数据集，反映了在社交媒体平台上实际存在的深度伪造挑战。这一数据集涵盖了从各种在线生态系统中获取的高保真深度伪造，确保了对操纵技术的广泛覆盖。

Abstract: The rapid advancement of deep generative models has significantly improved
the realism of synthetic media, presenting both opportunities and security
challenges. While deepfake technology has valuable applications in
entertainment and accessibility, it has emerged as a potent vector for
misinformation campaigns, particularly on social media. Existing detection
frameworks struggle to distinguish between benign and adversarially generated
deepfakes engineered to manipulate public perception. To address this
challenge, we introduce SocialDF, a curated dataset reflecting real-world
deepfake challenges on social media platforms. This dataset encompasses
high-fidelity deepfakes sourced from various online ecosystems, ensuring broad
coverage of manipulative techniques. We propose a novel LLM-based multi-factor
detection approach that combines facial recognition, automated speech
transcription, and a multi-agent LLM pipeline to cross-verify audio-visual
cues. Our methodology emphasizes robust, multi-modal verification techniques
that incorporate linguistic, behavioral, and contextual analysis to effectively
discern synthetic media from authentic content.

</details>


### [84] [Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning](https://arxiv.org/abs/2506.05568)
*Arian Raje,Baris Askin,Divyansh Jhunjhunwala,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了一种名为\textsc{Ravan}的自适应多头LoRA方法，可在联邦学习中提高大语言模型的测试准确性，并解决LoRA方法在联邦设置中的准确性下降问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型尚未有效利用大量的边缘设备数据，联邦学习提供了一种有前景的范式，可以在不转移边缘私有数据到云的情况下协作微调大语言模型。

Method: 提出了一种自适应多头LoRA方法，通过重新参数化权重更新来实现，在多个LoRA头的总和中，只有核心矩阵和轻量级缩放因子被训练。

Result: \textsc{Ravan} 在视觉和语言基准测试中，测试准确性比之前的参数高效基线提高了2-8%。

Conclusion: \textsc{Ravan} 是一个鲁棒且可扩展的解决方案，可用于大语言模型的联邦微调，在视觉和语言基准测试中提高了测试准确性。

Abstract: Large language models (LLMs) have not yet effectively leveraged the vast
amounts of edge-device data, and federated learning (FL) offers a promising
paradigm to collaboratively fine-tune LLMs without transferring private edge
data to the cloud. To operate within the computation and communication
constraints of edge devices, recent literature on federated fine-tuning of LLMs
proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient
methods. However, LoRA-based methods suffer from accuracy degradation in FL
settings, primarily because of data and computational heterogeneity across
clients. We propose \textsc{Ravan}, an adaptive multi-head LoRA method that
balances parameter efficiency and model expressivity by reparameterizing the
weight updates as the sum of multiple LoRA heads
$s_i\textbf{B}_i\textbf{H}_i\textbf{A}_i$ in which only the core matrices
$\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These
trainable scaling factors let the optimization focus on the most useful heads,
recovering a higher-rank approximation of the full update without increasing
the number of communicated parameters since clients upload $s_i\textbf{H}_i$
directly. Experiments on vision and language benchmarks show that
\textsc{Ravan} improves test accuracy by 2-8\% over prior parameter-efficient
baselines, making it a robust and scalable solution for federated fine-tuning
of LLMs.

</details>


### [85] [When can in-context learning generalize out of task distribution?](https://arxiv.org/abs/2506.05574)
*Chase Goddard,Lindsay M. Smith,Vudtiwat Ngampruetikorn,David J. Schwab*

Main category: cs.LG

TL;DR: 研究了任务多样性对transformers中ICL出现和泛化的影响，发现增加任务多样性有助于ICL泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 探讨ICL出现在transformers中的条件，并了解如何让ICL的能力泛化到未见过的任务空间。

Method: 使用线性函数研究transformers上的任务多样性与ICL出现的关系，并通过构建相图探讨任务多样性与预训练任务数量之间的交互。

Result: 发现任务多样性增加可导致transformers从专门化解决方案过渡到泛化解决方案，这一现象在非线性回归问题中也存在。

Conclusion: 研究表明，随着任务多样性的增加，transformers会从仅在预训练任务分布内展示ICL的专门化解决方案转变为对整个任务空间的泛化解决方案。

Abstract: In-context learning (ICL) is a remarkable capability of pretrained
transformers that allows models to generalize to unseen tasks after seeing only
a few examples. We investigate empirically the conditions necessary on the
pretraining distribution for ICL to emerge and generalize
\emph{out-of-distribution}. Previous work has focused on the number of distinct
tasks necessary in the pretraining dataset. Here, we use a different notion of
task diversity to study the emergence of ICL in transformers trained on linear
functions. We find that as task diversity increases, transformers undergo a
transition from a specialized solution, which exhibits ICL only within the
pretraining task distribution, to a solution which generalizes out of
distribution to the entire task space. We also investigate the nature of the
solutions learned by the transformer on both sides of the transition, and
observe similar transitions in nonlinear regression problems. We construct a
phase diagram to characterize how our concept of task diversity interacts with
the number of pretraining tasks. In addition, we explore how factors such as
the depth of the model and the dimensionality of the regression problem
influence the transition.

</details>


### [86] [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/abs/2506.05583)
*Nien-Shao Wang,Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 提出了在子群变化的情况下，自适应地调整保形预测的新方法，确保在不明确了解子群结构的情况下仍能保证有效覆盖。


<details>
  <summary>Details</summary>
Motivation: 标准的保形预测在分布变化特别是子群变化的情况下，无法提供保证。为了在这些情况下保持有效的覆盖率，需要新的方法。

Method: 提出了一种新算法，能够在测试环境中针对未知和不同的子群混合分布的情况，自适应地调整保形预测。这些算法可以扩展到高维设置，并在现实的机器学习任务中有效工作。

Result: 在视觉（使用视觉变换器）和语言（使用大型语言模型）基准上的大量实验证明，新方法在标准保形预测失效的场景中，能可靠地维护覆盖率并控制风险。

Conclusion: 在分布变化的情况下，标准的保形预测方法会失效。本文提出的新方法能够在不需要明确了解子群结构的情况下，自适应地处理这种变化，从而保证有效的覆盖率。

Abstract: Conformal prediction is widely used to equip black-box machine learning
models with uncertainty quantification enjoying formal coverage guarantees.
However, these guarantees typically break down in the presence of distribution
shifts, where the data distribution at test time differs from the training (or
calibration-time) distribution. In this work, we address subpopulation shifts,
where the test environment exhibits an unknown and differing mixture of
subpopulations compared to the calibration data. We propose new methods that
provably adapt conformal prediction to such shifts, ensuring valid coverage
without requiring explicit knowledge of subpopulation structure. Our algorithms
scale to high-dimensional settings and perform effectively in realistic machine
learning tasks. Extensive experiments on vision (with vision transformers) and
language (with large language models) benchmarks demonstrate that our methods
reliably maintain coverage and controls risk in scenarios where standard
conformal prediction fails.

</details>


### [87] [TabFlex: Scaling Tabular Learning to Millions with Linear Attention](https://arxiv.org/abs/2506.05584)
*Yuchen Zeng,Tuan Dinh,Wonjun Kang,Andreas C Mueller*

Main category: cs.LG

TL;DR: The paper introduces TabFlex, which improves upon TabPFN by using linear attention mechanisms to handle large tabular datasets efficiently, achieving significant speedups and computational savings while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the scalability and efficiency limitations of recent tabular classification models such as TabPFN when applied to large and complex datasets.

Method: Incorporating linear attention mechanisms as a scalable alternative to complexity-quadratic self-attention, and employing data-efficient techniques like dimensionality reduction and data sampling.

Result: TabFlex achieves over a 2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, demonstrating its superior efficiency across diverse datasets.

Conclusion: TabFlex significantly enhances the efficiency and scalability of tabular data classification, outperforming existing methods in terms of speed and computational cost, while maintaining strong performance across large-scale datasets.

Abstract: Leveraging the in-context learning (ICL) capability of Large Language Models
(LLMs) for tabular classification has gained significant attention for its
training-free adaptability across diverse datasets. Recent advancements, like
TabPFN, excel in small-scale tabular datasets but struggle to scale for large
and complex datasets. Our work enhances the efficiency and scalability of
TabPFN for larger datasets by incorporating linear attention mechanisms as a
scalable alternative to complexity-quadratic self-attention. Our model,
TabFlex, efficiently handles tabular datasets with thousands of features and
hundreds of classes, scaling seamlessly to millions of samples. For instance,
TabFlex processes the poker-hand dataset with over a million samples in just 5
seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a
2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, outperforming 25
tested baselines in terms of efficiency across a diverse range of datasets.
Furthermore, TabFlex remains highly effective on large-scale datasets,
delivering strong performance with significantly reduced computational costs,
especially when combined with data-efficient techniques such as dimensionality
reduction and data sampling.

</details>


### [88] [CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions](https://arxiv.org/abs/2506.05586)
*Isha Puri,Amit Dhurandhar,Tejaswini Pedapati,Kartikeyan Shanmugam,Dennis Wei,Kush R. Varshney*

Main category: cs.LG

TL;DR: 提出了一种新型神经网络架构CoFrNet，具有强大的表现力和解释性，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在神经网络中研究解释性架构较为稀少，此研究旨在提出一种新的神经网络架构以提高模型的解释性，同时具有优良的逼近实数的性质。

Method: 提出了一种新型神经网络架构CoFrNet，该架构受到连分数的启发，具有快速逼近实数的可取性质。通过独特的方法证明这些架构是通用逼近器，并在多个数据集上进行了实验。

Result: 实验证明CoFrNets不仅可以有效地建模非线性合成函数，还能估计特征贡献和较高阶项。在七个不同的数据集测试中，CoFrNets的性能要么与其它可解释模型和多层感知机相当，要么显著优于它们，接近或达到最先进模型的准确性。

Conclusion: CoFrNets是一种新颖的神经网络架构，具有良好的表现力和可解释性，并在多个数据集上表现出色，接近甚至超越了一些最先进的模型。

Abstract: In recent years there has been a considerable amount of research on local
post hoc explanations for neural networks. However, work on building
interpretable neural architectures has been relatively sparse. In this paper,
we present a novel neural architecture, CoFrNet, inspired by the form of
continued fractions which are known to have many attractive properties in
number theory, such as fast convergence of approximations to real numbers. We
show that CoFrNets can be efficiently trained as well as interpreted leveraging
their particular functional form. Moreover, we prove that such architectures
are universal approximators based on a proof strategy that is different than
the typical strategy used to prove universal approximation results for neural
networks based on infinite width (or depth), which is likely to be of
independent interest. We experiment on nonlinear synthetic functions and are
able to accurately model as well as estimate feature attributions and even
higher order terms in some cases, which is a testament to the representational
power as well as interpretability of such architectures. To further showcase
the power of CoFrNets, we experiment on seven real datasets spanning tabular,
text and image modalities, and show that they are either comparable or
significantly better than other interpretable models and multilayer
perceptrons, sometimes approaching the accuracies of state-of-the-art models.

</details>


### [89] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/abs/2506.05596)
*Jes Frellsen,Maher M. Kassem,Tone Bengtsen,Lars Olsen,Kresten Lindorff-Larsen,Jesper Ferkinghoff-Borg,Wouter Boomsma*

Main category: cs.LG

TL;DR: 本研究通过推导和实证评估，提出了几种改进逆折叠模型稳定性预测的方法，实现了零样本性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管逆折叠模型在蛋白质稳定性预测中表现优异，但其氨基酸选择偏好与热力学稳定性之间的关联尚未完全理解。因此，更好地理解这种关联不仅在理论上有意义，还可能为更强的零样本稳定性预测奠定基础。

Method: 本论文通过推导揭示了逆折叠模型中标准似然比方法的局限性，并提出了几种改进路径用于更好地估计相对稳定性，同时进行了实证评估。

Result: 通过实证评估，提出的改进方法在零样本性能上实现了显著提升。

Conclusion: 本论文揭示了基于逆折叠模型的标准似然比方法是一种简化的近似，并提出了几种改进相对稳定性估计的方法。通过实证评估，这些方法能够显著提高零样本性能。

Abstract: Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [90] [FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient Time Series Forecasting](https://arxiv.org/abs/2506.05597)
*Yash Vijay,Harini Subramanyan*

Main category: cs.LG

TL;DR: 提出了FaCTR，一种轻量级时空Transformer，通过结构化设计解决Transformer在时间序列预测中的局限性，并在多个基准测试中实现了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在时间序列预测中由于架构复杂性导致的效果递减问题，并针对结构化变量交互进行条件化以应对时间序列数据的低信息密度和复杂跨通道依赖结构。

Method: 通过可学习的门控机制将动态对称的跨通道交互（通过低秩因子分解机建模）注入到时序背景化的补丁嵌入中。还编码了静态和动态协变量以进行多变量条件化。支持自我监督预训练，以作为后续时间序列任务的基础。

Result: FaCTR以近400K个参数的最大变体在短期和长期预测基准上实现了最先进的性能，平均比竞争对手小50倍。

Conclusion: FaCTR在保持模型紧凑设计的同时，实现了业界领先的时间序列预测性能，并提供了解释性强的跨通道影响评分。

Abstract: While Transformers excel in language and vision-where inputs are semantically
rich and exhibit univariate dependency structures-their architectural
complexity leads to diminishing returns in time series forecasting. Time series
data is characterized by low per-timestep information density and complex
dependencies across channels and covariates, requiring conditioning on
structured variable interactions. To address this mismatch and
overparameterization, we propose FaCTR, a lightweight spatiotemporal
Transformer with an explicitly structural design. FaCTR injects dynamic,
symmetric cross-channel interactions-modeled via a low-rank Factorization
Machine into temporally contextualized patch embeddings through a learnable
gating mechanism. It further encodes static and dynamic covariates for
multivariate conditioning. Despite its compact design, FaCTR achieves
state-of-the-art performance on eleven public forecasting benchmarks spanning
both short-term and long-term horizons, with its largest variant using close to
only 400K parameters-on average 50x smaller than competitive spatiotemporal
transformer baselines. In addition, its structured design enables
interpretability through cross-channel influence scores-an essential
requirement for real-world decision-making. Finally, FaCTR supports
self-supervised pretraining, positioning it as a compact yet versatile
foundation for downstream time series tasks.

</details>


### [91] [When Maximum Entropy Misleads Policy Optimization](https://arxiv.org/abs/2506.05615)
*Ruipeng Zhang,Ya-Chien Chang,Sicun Gao*

Main category: cs.LG

TL;DR: MaxEnt RL虽然提高了任务的探索和鲁棒性，但在需要精确控制的任务中可能导致策略优化失误。


<details>
  <summary>Details</summary>
Motivation: 分析MaxEnt算法在性能关键的控制问题上表现不佳的原因，以及如何在复杂的控制任务中平衡奖励设计与熵最大化。

Method: 通过实验分析MaxEnt算法在各种控制任务中的表现，以研究鲁棒性与最优性之间的权衡如何影响其性能。

Result: 实验显示，熵最大化虽然提高了探索和鲁棒性，但在需要低熵策略的任务中可能会导致策略优化失误。

Conclusion: 在复杂控制任务中，MaxEnt算法的性能受到鲁棒性和最优性之间的权衡影响。虽然熵最大化增强了探索和鲁棒性，但在需要精确、低熵策略的任务中，它可能会误导策略优化。通过对多种控制问题的实验，作者具体展示了这种误导效应。

Abstract: The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.

</details>


### [92] [LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis](https://arxiv.org/abs/2506.05617)
*Antonia van Betteray,Matthias Rottmann,Karsten Kahl*

Main category: cs.LG

TL;DR: 提出了一种基于局部傅里叶分析的新方法，复杂度为 O(N)，用于高效计算卷积映射的奇异值，并已通过实验证明其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 计算卷积映射的奇异值有助于提高卷积神经网络的泛化能力、鲁棒性以及模型压缩。而现有方法计算奇异值资源消耗很大，因此需要一种高效的方法。

Method: 基于局部傅里叶分析和卷积操作的平移不变特性，提出了一种复杂度为 O(N) 的方法，用于计算卷积映射的奇异值。

Result: 通过理论分析和数值实验验证了算法的效率，结果表明该方法在高维卷积映射的奇异值计算上具有可扩展性。

Conclusion: 提出的方法在处理高维卷积映射的奇异值计算方面非常高效，并且适用于实际应用。

Abstract: The singular values of convolutional mappings encode interesting spectral
properties, which can be used, e.g., to improve generalization and robustness
of convolutional neural networks as well as to facilitate model compression.
However, the computation of singular values is typically very
resource-intensive. The naive approach involves unrolling the convolutional
mapping along the input and channel dimensions into a large and sparse
two-dimensional matrix, making the exact calculation of all singular values
infeasible due to hardware limitations. In particular, this is true for
matrices that represent convolutional mappings with large inputs and a high
number of channels. Existing efficient methods leverage the Fast Fourier
transformation (FFT) to transform convolutional mappings into the frequency
domain, enabling the computation of singular values for matrices representing
convolutions with larger input and channel dimensions. For a constant number of
channels in a given convolution, an FFT can compute N singular values in O(N
log N) complexity. In this work, we propose an approach of complexity O(N)
based on local Fourier analysis, which additionally exploits the shift
invariance of convolutional operators. We provide a theoretical analysis of our
algorithm's runtime and validate its efficiency through numerical experiments.
Our results demonstrate that our proposed method is scalable and offers a
practical solution to calculate the entire set of singular values - along with
the corresponding singular vectors if needed - for high-dimensional
convolutional mappings.

</details>


### [93] [Two-dimensional Taxonomy for N-ary Knowledge Representation Learning Methods](https://arxiv.org/abs/2506.05626)
*Xiaohua Lu,Liubov Tupikina,Mehwish Alam*

Main category: cs.LG

TL;DR: 本文综述了知识超图和超关系知识图谱的研究，提出了一个二维分类法以帮助理解不同模型，指明了一些激励未来研究的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 知识图谱常将复杂的n元关系简化为简单的三元组，导致更高阶关系细节的丢失，因此需要更好地捕获复杂结构和角色特定语义的知识超图和超关系知识图谱。

Method: 提出了一个二维分类法来分类不同的模型：第一维度根据方法论进行分类，包括基于翻译、基于张量分解、基于深度神经网络、基于逻辑规则和基于超边扩展的模型；第二维度根据模型对实体角色和位置的感知进行分类，分为无感知、位置感知和角色感知。

Result: 分类了现有模型并讨论了现有数据集和负采样策略，指出了一些开放挑战以激励未来研究。

Conclusion: 本文综述了处理n元关系数据的方法，包括知识超图和超关系知识图谱的研究文献，并提出了一个二维分类法，以便更好地理解这些模型的不同方面。

Abstract: Real-world knowledge can take various forms, including structured,
semi-structured, and unstructured data. Among these, knowledge graphs are a
form of structured human knowledge that integrate heterogeneous data sources
into structured representations but typically reduce complex n-ary relations to
simple triples, thereby losing higher-order relational details. In contrast,
hypergraphs naturally represent n-ary relations with hyperedges, which directly
connect multiple entities together. Yet hypergraph representation learning
often overlooks entity roles in hyperedges, limiting the fine-grained semantic
modelling. To address these issues, knowledge hypergraphs and hyper-relational
knowledge graphs combine the advantages of knowledge graphs and hypergraphs to
better capture the complex structures and role-specific semantics of real-world
knowledge. This survey provides a comprehensive review of methods handling
n-ary relational data, covering both knowledge hypergraphs and hyper-relational
knowledge graphs literatures. We propose a two-dimensional taxonomy: the first
dimension categorises models based on their methodology, i.e.,
translation-based models, tensor factorisation-based models, deep neural
network-based models, logic rules-based models, and hyperedge expansion-based
models. The second dimension classifies models according to their awareness of
entity roles and positions in n-ary relations, dividing them into aware-less,
position-aware, and role-aware approaches. Finally, we discuss existing
datasets, negative sampling strategies, and outline open challenges to inspire
future research.

</details>


### [94] [GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance](https://arxiv.org/abs/2506.05628)
*Jiri Navratil,Jarret Ross,Payel Das,Youssef Mroueh,Samuel C Hoffman,Vijil Chenthamarakshan,Brian Belgodere*

Main category: cs.LG

TL;DR: 提出了GP-MoLFormer-Sim方法，可在无需训练的情况下导航分子空间，结合遗传算法在分子优化任务中效果优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 研究专注于设计在分子之间保持相似性的方法，这对于药物发现、化学设计和生物学中的各种应用至关重要。

Method: 本文提出了一种无需训练的方法，通过生成化学语言模型（CLM），利用分子相似性为导向进行导航和采样。该方法在解码过程中通过追踪当前生成与目标的距离，并更新logits以鼓励生成保留相似性。

Result: GP-MoLFormer-Sim结合遗传算法在标准分子优化基准测试中表现优异，超越了现有黑盒情况下的基准方法。

Conclusion: GP-MoLFormer-Sim结合遗传算法的效果优于现有的无训练基准方法，在优化任务中表现出色。

Abstract: The ability to design molecules while preserving similarity to a target
molecule and/or property is crucial for various applications in drug discovery,
chemical design, and biology. We introduce in this paper an efficient
training-free method for navigating and sampling from the molecular space with
a generative Chemical Language Model (CLM), while using the molecular
similarity to the target as a guide. Our method leverages the contextual
representations learned from the CLM itself to estimate the molecular
similarity, which is then used to adjust the autoregressive sampling strategy
of the CLM. At each step of the decoding process, the method tracks the
distance of the current generations from the target and updates the logits to
encourage the preservation of similarity in generations. We implement the
method using a recently proposed $\sim$47M parameter SMILES-based CLM,
GP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which
enables a test-time update of the deep generative policy to reflect the
contextual similarity to a set of guide molecules. The method is further
integrated into a genetic algorithm (GA) and tested on a set of standard
molecular optimization benchmarks involving property optimization, molecular
rediscovery, and structure-based drug design. Results show that,
GP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing
training-free baseline methods, when the oracle remains black-box. The findings
in this work are a step forward in understanding and guiding the generative
mechanisms of CLMs.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [95] [Robustness of complexity estimation in event-driven signals against accuracy of event detection method](https://arxiv.org/abs/2506.06168)
*Marco Cafiso,Paolo Paradisi*

Main category: physics.comp-ph

TL;DR: 该论文研究了事件检测工具对复杂度估计准确性的影响，使用RTEF-EDDiS管道评估结果显示，即使在较高假阳性率下，复杂度估计依然稳健，误差约为4-7%。


<details>
  <summary>Details</summary>
Motivation: 在复杂动力系统中，快速转换事件模型化为时间轴上的随机点过程，通过事件间时间的幂律分布揭示复杂动态。噪声信号中的快速转换事件检测是一个挑战，误报率较高。因此，研究事件检测工具对复杂度估计准确性的影响是必要的。

Method: 引入RTE-Finder (RTEF)事件检测方法，通过事件驱动的合成信号评估RTEF-EDDiS流水线的性能。采用Event-Driven Diffusion Scaling (EDDiS)，一种利用事件驱动扩散估计时间复杂度的算法，进行系统性评估。

Result: 结果显示RTEF的可靠性很大程度上取决于参数，如百分位数，且假阳性事件远多于真实复杂事件。但复杂度估计仍然相对稳健，估算误差在4-7%的范围内，尤其是在幂律分布的事件间时间的情况下。

Conclusion: 虽然假阳性率较高，但复杂度估计对假阳性仍表现出较好的鲁棒性。对于幂律分布的事件间时间，当指数μ≤2.5时，第二矩尺度H表现出随着假阳性率增加而改进的趋势，误差约为4-7%。

Abstract: Complexity has gained recent attention in machine learning for its ability to
extract synthetic information from large datasets. Complex dynamical systems
are characterized by temporal complexity associated with intermittent
birth-death events of self-organizing behavior. These rapid transition events
(RTEs) can be modelled as a stochastic point process on the time axis, with
inter-event times (IETs) revealing rich dynamics. In particular, IETs with
power-law distribution mark a departure from the Poisson statistics and
indicate the presence of nontrivial complexity that is quantified by the
power-law exponent $\mu$ of the IET distribution. However, detection of RTEs in
noisy signals remains a challenge, since false positives can obscure the
statistical structure of the underlying process. In this paper, we address the
problem of quantifying the effect of the event detection tool on the accuracy
of complexity estimation. This is reached through a systematic evaluation of
the Event-Driven Diffusion Scaling (EDDiS) algorithm, a tool exploiting
event-driven diffusion to estimate temporal complexity.After introducing the
event detection method RTE-Finder (RTEF), we assess the performance of the
RTEF-EDDiS pipeline using event-driven synthetic signals. The reliability of
the RTEF is found to strongly depend on parameters such as the percentile and
the number of false positives can be much higher than the number of genuine
complex events. Despite this, we found that the complexity estimation is quite
robust with respect to the rate of false positives. For the power-law
distributed IETs with $\mu\le2.5$, the second moment scaling $H$ appears to
even improve as the rate of false positives increases, reaching estimation
errors of about 4-7%.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [UAV-UGV Cooperative Trajectory Optimization and Task Allocation for Medical Rescue Tasks in Post-Disaster Environments](https://arxiv.org/abs/2506.06136)
*Kaiyuan Chen,Wanpeng Zhao,Yongxi Liu,Yuanqing Xia,Wannian Liang,Shuo Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于无人机和地面车辆协作的任务分配和轨迹优化框架，以提高灾后医疗资源的运送效率，模拟结果显示该方法在任务完成时间和距离上显著优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 在灾后场景中，由于基础设施严重受损，迅速高效地运送医疗资源至关重要且具有挑战性。因此，提出优化解决方案以提高救援效率。

Method: 研究综合应用遗传算法(GA)进行任务分配，并采用改进的RRT*算法进行无碰撞轨迹生成，同时利用协方差矩阵适应进化策略(CMA-ES)对任务顺序和路径效率进行进一步优化。

Result: 模拟实验表明，与传统策略相比，提出的方法显著减少了任务完成时间和行驶距离，提高了救援效率。系统展示了其在现实环境中的可扩展性和实用性。

Conclusion: 通过合作利用无人机和无人地面车辆，提出的框架在模拟的真实灾后环境中，显著提高了医疗救援操作的整体效率，与传统策略相比，任务完成时间和行驶距离大幅减少，表明了系统的可扩展性和实际应用的可行性。

Abstract: In post-disaster scenarios, rapid and efficient delivery of medical resources
is critical and challenging due to severe damage to infrastructure. To provide
an optimized solution, we propose a cooperative trajectory optimization and
task allocation framework leveraging unmanned aerial vehicles (UAVs) and
unmanned ground vehicles (UGVs). This study integrates a Genetic Algorithm (GA)
for efficient task allocation among multiple UAVs and UGVs, and employs an
informed-RRT* (Rapidly-exploring Random Tree Star) algorithm for collision-free
trajectory generation. Further optimization of task sequencing and path
efficiency is conducted using Covariance Matrix Adaptation Evolution Strategy
(CMA-ES). Simulation experiments conducted in a realistic post-disaster
environment demonstrate that our proposed approach significantly improves the
overall efficiency of medical rescue operations compared to traditional
strategies, showing substantial reductions in total mission completion time and
traveled distance. Additionally, the cooperative utilization of UAVs and UGVs
effectively balances their complementary advantages, highlighting the system' s
scalability and practicality for real-world deployment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [97] [A Red Teaming Roadmap Towards System-Level Safety](https://arxiv.org/abs/2506.05376)
*Zifan Wang,Christina Q. Knight,Jeremy Kritz,Willow E. Primack,Julian Michael*

Main category: cs.CR

TL;DR: 本文认为当前的红队研究没有优先解决正确的问题，建议将重点放在产品安全规格、现实威胁模型以及系统级安全，以应对日益增长的AI威胁。


<details>
  <summary>Details</summary>
Motivation: 当前的红队研究未能优先解决正确的问题，应将重点放在产品安全规格的测试以及现实威胁模型，而非抽象的社会偏见或伦理原则上。

Method: 采取基于现实威胁模型的红队测试，并对产品安全规格进行明确测试，同时关注系统级安全以推动红队研究的进步。

Result: 提出了一系列关于如何提高红队研究效率的优先事项，包括关注产品安全规格、现实威胁模型以及系统级安全。

Conclusion: 为实现红队研究的进步，系统级安全是必要的一步，因为一旦在部署环境中使用，人工智能模型不仅会带来新的威胁，还能够通过检测和封禁恶意用户等方法提供威胁缓解能力。

Abstract: Large Language Model (LLM) safeguards, which implement request refusals, have
become a widely adopted mitigation strategy against misuse. At the intersection
of adversarial machine learning and AI safety, safeguard red teaming has
effectively identified critical vulnerabilities in state-of-the-art
refusal-trained LLMs. However, in our view the many conference submissions on
LLM red teaming do not, in aggregate, prioritize the right research problems.
First, testing against clear product safety specifications should take a higher
priority than abstract social biases or ethical principles. Second, red teaming
should prioritize realistic threat models that represent the expanding risk
landscape and what real attackers might do. Finally, we contend that
system-level safety is a necessary step to move red teaming research forward,
as AI models present new threats as well as affordances for threat mitigation
(e.g., detection and banning of malicious users) once placed in a deployment
context. Adopting these priorities will be necessary in order for red teaming
research to adequately address the slate of new threats that rapid AI advances
present today and will present in the very near future.

</details>


### [98] [How stealthy is stealthy? Studying the Efficacy of Black-Box Adversarial Attacks in the Real World](https://arxiv.org/abs/2506.05382)
*Francesco Panebianco,Mario D'Onghia,Stefano Zanero aand Michele Carminati*

Main category: cs.CR

TL;DR: The paper presents ECLIPSE, a novel method for black-box adversarial attacks in computer vision, improving the balance between robustness, stealthiness to detection, and inspection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address adversarial vulnerabilities in deep learning systems, especially in black-box scenarios like computer vision, by balancing multiple attack feasibility criteria.

Method: The method proposed is called ECLIPSE, which uses Gaussian blurring on sampled gradients and a local surrogate model to conduct adversarial attacks.

Result: ECLIPSE shows advantages in experiments by providing a better trade-off between robustness to compression, stealthiness to automatic detection, and human inspection compared to State-of-the-Art methods.

Conclusion: ECLIPSE method enhances the trade-off balance among robustness to compression, stealthiness to automatic detection, and stealthiness to human inspection in black-box adversarial attacks.

Abstract: Deep learning systems, critical in domains like autonomous vehicles, are
vulnerable to adversarial examples (crafted inputs designed to mislead
classifiers). This study investigates black-box adversarial attacks in computer
vision. This is a realistic scenario, where attackers have query-only access to
the target model. Three properties are introduced to evaluate attack
feasibility: robustness to compression, stealthiness to automatic detection,
and stealthiness to human inspection. State-of-the-Art methods tend to
prioritize one criterion at the expense of others. We propose ECLIPSE, a novel
attack method employing Gaussian blurring on sampled gradients and a local
surrogate model. Comprehensive experiments on a public dataset highlight
ECLIPSE's advantages, demonstrating its contribution to the trade-off between
the three properties.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [99] [Designing DSIC Mechanisms for Data Sharing in the Era of Large Language Models](https://arxiv.org/abs/2506.05379)
*Seyed Moein Ayyoubzadeh,Kourosh Shahnazari,Mohammmadali Keshtparvar,MohammadAmin Fazli*

Main category: cs.GT

TL;DR: 论文提出了一个机制设计框架，通过 Q-MIA 和 Mixed-MIA 机制，在公平和隐私保护的前提下，实现了高效的数据共享和激励。


<details>
  <summary>Details</summary>
Motivation: 当前数据获取方法依赖无法验证的信任，或忽略不同提供者的成本差异。需要一种确保数据质量和学习效用，且兼顾各种约束的新方法。

Method: 提出了一种机制设计框架，名为 Q-MIA 和 Mixed-MIA，结合质量加权边际激励拍卖和边际效益代币，实现了 DSIC 和预算可行性。

Result: 提出的机制在理论和实证上优于基于数据量和信任的基准，能够在预算约束下引出更高质量的数据，并且对错误报告和合谋具有鲁棒性。

Conclusion: 通过引入 Q-MIA 和 MUT 机制，该论文建立了一个原则性基础，用于未来 LLMs 的可持续和公平的数据市场。

Abstract: Training large language models (LLMs) requires vast amounts of high-quality
data from institutions that face legal, privacy, and strategic constraints.
Existing data procurement methods often rely on unverifiable trust or ignore
heterogeneous provider costs. We introduce a mechanism-design framework for
truthful, trust-minimized data sharing that ensures dominant-strategy incentive
compatibility (DSIC), individual rationality, and weak budget balance, while
rewarding data based on both quality and learning utility. We formalize a model
where providers privately know their data cost and quality, and value arises
solely from the data's contribution to model performance. Based on this, we
propose the Quality-Weighted Marginal-Incentive Auction (Q-MIA), which ranks
providers using a virtual cost metric and uses Myerson-style payments to ensure
DSIC and budget feasibility. To support settings with limited liquidity or
long-term incentives, we introduce the Marginal Utility Token (MUT), which
allocates future rights based on marginal contributions. We unify these in
Mixed-MIA, a hybrid mechanism balancing upfront payments and deferred rewards.
All mechanisms support verifiable, privacy-preserving implementation.
Theoretically and empirically, they outperform volume-based and trust-based
baselines, eliciting higher-quality data under budget constraints while
remaining robust to misreporting and collusion. This establishes a principled
foundation for sustainable and fair data markets for future LLMs.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [100] [Infinite Time Turing Machines and their Applications](https://arxiv.org/abs/2506.05351)
*Rukmal Weerawarana,Maxwell Braun*

Main category: cs.CC

TL;DR: 本文通过利用无限时间图灵机（ITTMs）分析深度学习系统的局限性，并提出了通用状态机（USM）作为新的计算范式，以实现可扩展的人工智能系统。


<details>
  <summary>Details</summary>
Motivation: 通过利用无限时间图灵机（ITTMs）拓展经典计算理论至超限序数步，建立深度学习系统分析的严格理论基础。

Method: 利用无限时间图灵机（ITTMs）重新解释现代架构（如Transformers），揭示其在可扩展性、效率和可解释性方面的根本限制，并引入一种动态、可查询的计算图来进行实时演变。

Result: 引入了一种新的计算范式，即通用状态机（USM），支持模块化、可解释和资源高效的计算。

Conclusion: 本文提出了一种新的计算范式，即通用状态机（USM），旨在解决当前模型的效率低下和僵化问题，并为可扩展、可泛化的人工智能系统奠定基础。

Abstract: This work establishes a rigorous theoretical foundation for analyzing deep
learning systems by leveraging Infinite Time Turing Machines (ITTMs), which
extend classical computation into transfinite ordinal steps. Using ITTMs, we
reinterpret modern architectures like Transformers, revealing fundamental
limitations in scalability, efficiency, and interpretability. Building on these
insights, we propose the Universal State Machine (USM), a novel computational
paradigm designed from first principles. The USM employs a dynamic, queryable
computation graph that evolves in real time, enabling modular, interpretable,
and resource-efficient computation. This framework not only overcomes the
inefficiencies and rigidity of current models but also lays the groundwork for
scalable, generalizable artificial intelligence systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [101] [Category Query Learning for Human-Object Interaction Classification](https://arxiv.org/abs/2303.14005)
*Chi Xie,Fangao Zeng,Yue Hu,Shuang Liang,Yichen Wei*

Main category: cs.CV

TL;DR: 提出了一种新的类别查询学习方法用于HOI分类，并取得了优秀成绩。


<details>
  <summary>Details</summary>
Motivation: 该方法的灵感来源于早期的多标签图像分类方法，旨在解决先前大多数HOI方法仅关注于学习更好的人与物体特征的问题。

Method: 方法使用了一种称为类别查询学习的创新技术，通过transformer解码器将查询转换为特定图像的类别表示，并通过辅助图像级分类任务进行学习。

Result: 方法在三个具有代表性的HOI基准上进行了验证，并在两个基准上取得了新的领先结果。

Conclusion: 我们的简单、通用而有效的方法首次在具有挑战性的HOI分类任务中应用并在两个标准基准上实现了新的最先进结果。

Abstract: Unlike most previous HOI methods that focus on learning better human-object
features, we propose a novel and complementary approach called category query
learning. Such queries are explicitly associated to interaction categories,
converted to image specific category representation via a transformer decoder,
and learnt via an auxiliary image-level classification task. This idea is
motivated by an earlier multi-label image classification method, but is for the
first time applied for the challenging human-object interaction classification
task. Our method is simple, general and effective. It is validated on three
representative HOI baselines and achieves new state-of-the-art results on two
benchmarks.

</details>


### [102] [Can ChatGPT Perform Image Splicing Detection? A Preliminary Study](https://arxiv.org/abs/2506.05358)
*Souradip Nath*

Main category: cs.CV

TL;DR: GPT-4V, tested on CASIA v2.0 for image splicing detection, shows over 85% accuracy in zero-shot scenarios. Chain-of-Thought prompting helps it achieve a balanced detection, highlighting its potential in forensic applications despite some limitations compared to specialized models.


<details>
  <summary>Details</summary>
Motivation: To explore and understand the capabilities of GPT-4V in image forensics, particularly in detecting image splicing manipulations, by leveraging its multimodal reasoning abilities.

Method: The study evaluates the out-of-the-box capabilities of GPT-4V using three prompting strategies - Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT) - on the CASIA v2.0 splicing dataset.

Result: GPT-4V achieves over 85% accuracy in zero-shot settings, with the CoT prompting offering a balanced detection across different types of images. It uses both low-level visual artifacts and high-level contextual reasoning.

Conclusion: GPT-4V, without task-specific fine-tuning, shows competitive performance in detecting image splicing, especially with CoT prompting. Despite lagging behind specialized models, it demonstrates good generalization and reasoning capabilities.

Abstract: Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning
across text and image modalities, showing promise in a variety of complex
vision-language tasks. In this preliminary study, we investigate the
out-of-the-box capabilities of GPT-4V in the domain of image forensics,
specifically, in detecting image splicing manipulations. Without any
task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:
Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a
curated subset of the CASIA v2.0 splicing dataset.
  Our results show that GPT-4V achieves competitive detection performance in
zero-shot settings (more than 85% accuracy), with CoT prompting yielding the
most balanced trade-off across authentic and spliced images. Qualitative
analysis further reveals that the model not only detects low-level visual
artifacts but also draws upon real-world contextual knowledge such as object
scale, semantic consistency, and architectural facts, to identify implausible
composites. While GPT-4V lags behind specialized state-of-the-art splicing
detection models, its generalizability, interpretability, and encyclopedic
reasoning highlight its potential as a flexible tool in image forensics.

</details>


### [103] [Speaking images. A novel framework for the automated self-description of artworks](https://arxiv.org/abs/2506.05368)
*Valentine Bernasconi,Gustavo Marfia*

Main category: cs.CV

TL;DR: 提出了一种使用生成性人工智能创建自我解释的数字化文化艺术品的新框架。


<details>
  <summary>Details</summary>
Motivation: 生成性人工智能的突破为艺术和文化遗产领域的研究带来新的视角，需要创新以便更容易访问和突出数字收藏的内容。

Method: 本文方法涉及使用开源的大型语言模型、面部检测、文本到语音和音频到动画模型，自动组装短视频，使数字化艺术品中的主要角色活跃起来并解释其内容。

Result: 从数字化艺术作品开始，自动生成短视频，其中主要角色动画化并解释其内容。

Conclusion: 本文提出了一种新框架，通过使用开源的大型语言模型、面部检测、文本到语音和音频到动画模型，创造自我解释的文化艺术品。

Abstract: Recent breakthroughs in generative AI have opened the door to new research
perspectives in the domain of art and cultural heritage, where a large number
of artifacts have been digitized. There is a need for innovation to ease the
access and highlight the content of digital collections. Such innovations
develop into creative explorations of the digital image in relation to its
malleability and contemporary interpretation, in confrontation to the original
historical object. Based on the concept of the autonomous image, we propose a
new framework towards the production of self-explaining cultural artifacts
using open-source large-language, face detection, text-to-speech and
audio-to-animation models. The goal is to start from a digitized artwork and to
automatically assemble a short video of the latter where the main character
animates to explain its content. The whole process questions cultural biases
encapsulated in large-language models, the potential of digital images and
deepfakes of artworks for educational purposes, along with concerns of the
field of art history regarding such creative diversions.

</details>


### [104] [Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment](https://arxiv.org/abs/2506.05384)
*Zhuoxuan Cai,Jian Zhang,Xinbin Yuan,Pengtao Jiang,Wenxiang Chen,Bowen Tang,Lujian Yao,Qiyuan Wang,Jinwen Chen,Bo Li*

Main category: cs.CV

TL;DR: 提出了一种新的基于强化学习的训练框架，提升多模态语言模型在视觉质量评估中的精度和解释性，模型Q-Ponder表现优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉质量评估中，将质量评分与推理描述视作独立任务进行处理，导致模型在质量推理描述和精确评估之间难以兼顾。这一限制使得模型无法充分发挥其在视觉质量评估中的潜力。因此，提出了一种能够在高精度评分和解释性之间实现优势互补的训练方法。

Method: 提出了一种由两个阶段组成的统一训练框架：冷启动阶段和基于强化学习的微调阶段。在第一个阶段，通过教师模型和专家设计的提示蒸馏高质量数据，通过交叉熵损失监督初始化推理能力。在第二个阶段，引入了一种新的奖励机制——组相对政策优化（Group Relative Policy Optimization, GRPO），以共同优化评分准确性和推理一致性。

Result: Q-Ponder在质量得分回归基准测试中达到当前最先进水平，与跨领域数据集上的SOTA相比，SRCC提升达6.5%。此外，在描述型任务中，Q-Ponder在描述的准确性和合理性上显著超过现有的SOTA模型。

Conclusion: 模型Q-Ponder在视觉质量评估中展现了卓越的性能，尤其在跨领域的数据集上展示了高精度的得分回归能力，同时该模型在描述精度和合理性方面也优于现有的SOTA模型，体现了其在多任务中的泛化潜力。

Abstract: Recent studies demonstrate that multimodal large language models (MLLMs) can
proficiently evaluate visual quality through interpretable assessments.
However, existing approaches typically treat quality scoring and reasoning
descriptions as separate tasks with disjoint optimization objectives, leading
to a trade-off: models adept at quality reasoning descriptions struggle with
precise score regression, while score-focused models lack interpretability.
This limitation hinders the full potential of MLLMs in visual quality
assessment, where accuracy and interpretability should be mutually reinforcing.
To address this, we propose a unified two-stage training framework comprising a
cold-start stage and a reinforcement learning-based fine-tuning stage.
Specifically, in the first stage, we distill high-quality data from a teacher
model through expert-designed prompts, initializing reasoning capabilities via
cross-entropy loss supervision. In the second stage, we introduce a novel
reward with Group Relative Policy Optimization (GRPO) to jointly optimize
scoring accuracy and reasoning consistency. We designate the models derived
from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show
that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score
regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain
datasets. Furthermore, Q-Ponder significantly outperforms description-based
SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in
description accuracy and reasonableness, demonstrating the generalization
potential over diverse tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [105] [Towards provable probabilistic safety for scalable embodied AI systems](https://arxiv.org/abs/2506.05171)
*Linxuan He,Qing-Shan Jia,Ang Li,Hongyan Sang,Ling Wang,Jiwen Lu,Tao Zhang,Jie Zhou,Yi Zhang,Yisen Wang,Peng Wei,Zhongyuan Wang,Henry X. Liu,Shuo Feng*

Main category: eess.SY

TL;DR: 本文提出了可证明的概率安全性，以应对嵌入式AI系统在复杂环境中保证安全性的挑战，从而支持其在安全关键领域的大规模部署。


<details>
  <summary>Details</summary>
Motivation: 嵌入式AI系统在安全关键领域（如自动驾驶、医疗设备和机器人）的大规模部署受到安全性难以保证的制约，特别是由于系统故障的罕见性，验证所有可能场景下的系统安全性是不切实际的。

Method: 本文引入了可证明的概率安全性，通过统计方法建立系统性能的概率安全边界，而非穷尽验证所有可能情况下的安全性。这种方法增强了系统在大规模部署中的可行性和可扩展性。

Result: 通过引入可证明的概率安全性，本文为嵌入式AI系统在大规模部署中的安全性提供了一种可行的替代方案。这种方法允许在既定风险阈值下实现系统的安全性，支持大规模应用并允许持续的安全性保障优化。

Conclusion: 为了解决在复杂操作环境中保持嵌入式AI系统安全性的挑战，本文提出了可证明的概率安全性。通过建立概率安全边界，本文为大规模部署嵌入式AI提供了一条从理论上的安全保证到实际应用的路径。

Abstract: Embodied AI systems, comprising AI models and physical plants, are
increasingly prevalent across various applications. Due to the rarity of system
failures, ensuring their safety in complex operating environments remains a
major challenge, which severely hinders their large-scale deployment in
safety-critical domains, such as autonomous vehicles, medical devices, and
robotics. While achieving provable deterministic safety--verifying system
safety across all possible scenarios--remains theoretically ideal, the rarity
and complexity of corner cases make this approach impractical for scalable
embodied AI systems. To address this challenge, we introduce provable
probabilistic safety, which aims to ensure that the residual risk of
large-scale deployment remains below a predefined threshold. Instead of
attempting exhaustive safety proof across all corner cases, this paradigm
establishes a probabilistic safety boundary on overall system performance,
leveraging statistical methods to enhance feasibility and scalability. A
well-defined probabilistic safety boundary enables embodied AI systems to be
deployed at scale while allowing for continuous refinement of safety
guarantees. Our work focuses on three core questions: what is provable
probabilistic safety, how to prove the probabilistic safety, and how to achieve
the provable probabilistic safety. By bridging the gap between theoretical
safety assurance and practical deployment, our work offers a pathway toward
safer, large-scale adoption of embodied AI systems in safety-critical
applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [106] [Gen4D: Synthesizing Humans and Scenes in the Wild](https://arxiv.org/abs/2506.05397)
*Jerrin Bright,Zhibo Wang,Yuhao Chen,Sirisha Rambhatla,John Zelek,David Clausi*

Main category: cs.GR

TL;DR: 提出了Gen4D自动生成4D人类动画的管道，以及基于此的SportPAL合成数据集，用于解决野外人类视觉任务中数据集多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界数据收集复杂且不切实际，尤其是在体育这种不常见的人类中心领域，因此缺乏野外活动输入数据导致各种计算机视觉任务的低性能表现。

Method: 引入Gen4D，一个用于生成多样化和逼真4D人类动画的全自动管道，并整合了专家驱动的运动编码、基于扩散的高斯点引导的头像生成，以及具有人的背景合成。

Result: 该研究提出了一种新的方法生成高多样性和逼真度的人类动画序列，并基于此提出了一个大规模的合成数据集SportPAL，涵盖了三种体育项目。

Conclusion: Gen4D和SportPAL能够生成多样化的高逼真度4D人类动画，提供了一个无需手动3D建模或场景设计的可扩展基础，用于构建适合野外人类中心视觉任务的合成数据集。

Abstract: Lack of input data for in-the-wild activities often results in low
performance across various computer vision tasks. This challenge is
particularly pronounced in uncommon human-centric domains like sports, where
real-world data collection is complex and impractical. While synthetic datasets
offer a promising alternative, existing approaches typically suffer from
limited diversity in human appearance, motion, and scene composition due to
their reliance on rigid asset libraries and hand-crafted rendering pipelines.
To address this, we introduce Gen4D, a fully automated pipeline for generating
diverse and photorealistic 4D human animations. Gen4D integrates expert-driven
motion encoding, prompt-guided avatar generation using diffusion-based Gaussian
splatting, and human-aware background synthesis to produce highly varied and
lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale
synthetic dataset spanning three sports: baseball, icehockey, and soccer.
Together, Gen4D and SportPAL provide a scalable foundation for constructing
synthetic datasets tailored to in-the-wild human-centric vision tasks, with no
need for manual 3D modeling or scene design.

</details>
