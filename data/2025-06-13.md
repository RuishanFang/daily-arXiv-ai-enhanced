<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 89]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.MM](#cs.MM) [Total: 9]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 23]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 11]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.IV](#eess.IV) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

TL;DR: 本文提出了一个统一分类法，系统化地组织文本、图像和音频生成的自动评估方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 目前在生成内容自动评估领域缺乏一个系统化框架来全面组织文本、视觉和音频模态中的评估方法，因此有必要进行分类整理。

Method: 通过系统回顾现有的自动评估方法，并提出一个统一的分类法，将这些方法在文本、图像和音频生成领域进行组织和分类。

Result: 提出了一个涵盖三种模态的生成内容自动评估方法的统一分类法，并识别出五种基本范式。同时，研究表明该分类法具有广泛适用性，涵盖了文本、图像和音频三种生成任务。

Conclusion: 本文总结了现有生成内容自动评估方法的特点，并提供了未来交叉模态评估方法研究的潜在方向。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [2] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
*Yixin Ou,Yujie Luo,Jingsheng Zheng,Lanning Wei,Shuofei Qiao,Jintian Zhang,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: AutoMind是一个改进的LLM代理，利用专家知识库、知情搜索和自适应编码策略，在自动化数据科学中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的数据科学代理在真实世界中的有效性有限，尤其是在应对复杂、创新任务时表现不足。AutoMind旨在通过引入更灵活和专业的框架克服这些问题。

Method: AutoMind引入了三项关键进步：（1）一个策划的专家知识库，（2）一个知情的代理树搜索算法，以及（3）一种自适应的编码策略。

Result: 在两个自动化数据科学基准上，AutoMind的表现优于最先进的基线。

Conclusion: AutoMind是一个自适应的、多知识的LLM代理框架，通过克服现有框架的局限性，实现了在自动化数据科学基准上的优越表现。

Abstract: Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.

</details>


### [3] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: TaskCraft提供了一种生成复杂自主任务的新方法，优化提示和模型微调，支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有的指令数据缺乏工具交互，现有的自主基准测试依赖于昂贵的人类标注，限制了其可扩展性。

Method: 我们引入了TaskCraft，这是一种自动化流程，使用基于深度和广度的扩展技术来创建结构和层次上复杂的挑战，同时生成执行轨迹。

Result: 经验结果表明，这些任务提高了生成工作流程中的提示优化，并增强了自主基础模型的监督微调。我们提供了一个大规模的合成数据集，大约包含36000个具有不同难度的任务，支持未来关于代理调优和评估的研究。

Conclusion: 我们提出了一种可以生成具有可变难度、多工具，以及可验证的自主任务的新方法，称为TaskCraft。这种方法能够提高生成工作流程中的提示优化，并增强自主基础模型的监督微调。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [4] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

TL;DR: 研究表明，语言的复杂性使得理解唯一意图的概率消失，提出用贝叶斯方法更好地表征语言意义。


<details>
  <summary>Details</summary>
Motivation: 由于自然语言内在的语义退化性，大型语言模型在理解复杂表达式时面临固有限制，该研究旨在探讨语言形式的意义如何通过观察者相关的解释行为得以实现。

Method: 进行语义Bell不等式测试，使用各种大型语言模型作为“计算认知系统”在不同上下文中解析模糊词对。

Result: 实验结果显示，多个独立实验中，平均CHSH期望值在1.2到2.8之间，且数次实验结果显著违反经典界限，表明模糊条件下的语言解释可呈现非经典上下文性。

Conclusion: 经典频率分析方法在自然语言处理中不可避免地存在信息损失，而贝叶斯风格的重复采样方法可以更有效地表征语言的上下文意义。

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [5] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

TL;DR: Chat-of-Thought is a novel system using AI-driven, multi-agent collaboration to improve the generation and validation of industrial FMEA documents.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating Failure Modes and Effects Analysis (FMEA) documents for industrial assets using AI-driven methods, improving efficiency and accuracy through collaborative agent interactions.

Method: The paper introduces a multi-agent system, Chat-of-Thought, using collaborative Large Language Model (LLM)-based agents assigned specific roles and dynamic task routing for optimized FMEA table generation.

Result: The study demonstrates the successful application of Chat-of-Thought in industrial equipment monitoring, showing its efficiency in overcoming challenges via template-driven workflows and context-aware collaborations.

Conclusion: Chat-of-Thought can effectively facilitate the generation and validation of Failure Modes and Effects Analysis (FMEA) documents for industrial assets through dynamic, multi-agent collaboration.

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [6] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

TL;DR: 研究了大型语言模型如何对语义不变但措辞不同的提示做出反应，提出了PBSS框架来测量此现象，表明模型行为对提示措辞具有敏感性并影响模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型在面对语义不变但措辞不同的提示时的行为变化，称为提示变化问题。

Method: 提出了提示语义转移（PBSS）这一诊断框架，用于测量在语义等效的提示改写下LLM的行为漂移。

Result: 应用于十项受限任务中，PBSS揭示了持续的、特定于模型的响应变化，表明与分词和解码相关的统计规律。

Conclusion: 提示再措辞下的模型评估稳定性被忽视，分词策略和解码动态可能导致训练后服务质量的不稳定。

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [7] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
*Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: ChartReasoner是一个两阶段框架，可以高保真地将图表图像转换为结构化代码，并通过数据合成和筛选生成图表推理轨迹，最终实现高效的多模态推理。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理方法将视觉推理任务转化为文本推理任务，通常会丢失视觉化中嵌入的重要结构和语义信息，尤其是在需要大量视觉细节的任务中。因此，我们提出ChartReasoner，使得对图表进行精确且可解释的推理成为可能。

Method: 我们首先训练了一个高保真模型，将各种图表图像转换为结构化的ECharts代码，尽可能无损地保留布局和数据语义。然后，我们设计了一个通用的图表推理数据合成管道，利用这个预训练的传输模型自动和可扩展地生成图表推理轨迹，并利用代码验证器过滤低质量样本。最后，我们结合监督微调和强化学习，在合成的图表推理数据集上训练最终的多模态模型。

Result: 实验结果表明，ChartReasoner在四个公共基准测试中表现出色。

Conclusion: ChartReasoner可以在尽可能多地保留图表原始细节的同时，与最先进的开源模型相媲美，并在使用更少参数的条件下在域外设置中接近专有系统的性能。

Abstract: Recently, large language models have shown remarkable reasoning capabilities
through long-chain reasoning before responding. However, how to extend this
capability to visual reasoning tasks remains an open challenge. Existing
multimodal reasoning approaches transfer such visual reasoning task into
textual reasoning task via several image-to-text conversions, which often lose
critical structural and semantic information embedded in visualizations,
especially for tasks like chart question answering that require a large amount
of visual details. To bridge this gap, we propose ChartReasoner, a code-driven
novel two-stage framework designed to enable precise, interpretable reasoning
over charts. We first train a high-fidelity model to convert diverse chart
images into structured ECharts codes, preserving both layout and data semantics
as lossless as possible. Then, we design a general chart reasoning data
synthesis pipeline, which leverages this pretrained transport model to
automatically and scalably generate chart reasoning trajectories and utilizes a
code validator to filter out low-quality samples. Finally, we train the final
multimodal model using a combination of supervised fine-tuning and
reinforcement learning on our synthesized chart reasoning dataset and
experimental results on four public benchmarks clearly demonstrate the
effectiveness of our proposed ChartReasoner. It can preserve the original
details of the charts as much as possible and perform comparably with
state-of-the-art open-source models while using fewer parameters, approaching
the performance of proprietary systems like GPT-4o in out-of-domain settings.

</details>


### [8] [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
*Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike*

Main category: cs.CL

TL;DR: 提出了内部一致性最大化（ICM）算法，无需外部监督，通过生成标签对语言模型进行微调，表现优于人类监督，尤其是在超人能力任务中。


<details>
  <summary>Details</summary>
Motivation: 面对具有超人能力的模型时，难以提供高质量的人类监督。为此需要一种无监督的算法来引导预训练语言模型。

Method: 引入了一种称为内部一致性最大化（ICM）的无监督算法，来对预训练的语言模型进行微调。这种方法无需外部监督，使用的是模型自身生成的标签，与ICM方法一起还使用了强化学习来训练模型。

Result: 在GSM8k-verification、TruthfulQA和Alpaca奖励建模任务中，ICM方法不仅匹配传统黄金标准监督训练的性能，还优于众包人类监督。尤其是在超人能力任务中，ICM方法显著优于人类标签训练。通过使用ICM，奖励模型和助理模型在性能上超越了人类监督的同行。

Conclusion: ICM方法可以通过生成标签在无需人工监督的情况下对语言模型进行微调，并能够显著提高其性能，特别是在模型具有超人能力的任务上。

Abstract: To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.

</details>


### [9] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型在情感沟通标注中的表现接近专家并优于众包工人。


<details>
  <summary>Details</summary>
Motivation: 调查大型语言模型在判断情感沟通的细微差别时的可靠性。

Method: 通过对四个评估框架进行比较，评估LLM、专家和众包工人在情感沟通中的标注一致性。

Result: LLM在所有框架中一致接近专家水平，并超过了众包工人的一致性。

Conclusion: 大型语言模型在情感敏感应用中可以作为对话助手，以支持透明度和监督。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [10] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
*Bidyarthi Paul,SM Musfiqur Rahman,Dipta Biswas,Md. Ziaul Hasan,Md. Zahid Hossain*

Main category: cs.CL

TL;DR: 该研究通过多种机器学习模型和技术，提升了孟加拉语情感识别的效率及可解释性。


<details>
  <summary>Details</summary>
Motivation: 推动低资源语言（如孟加拉语）中的情感分析技术的发展。

Method: 采用线性SVM、KNN和随机森林等机器学习模型，以及TF-IDF向量化中的n-gram数据进行语言分析。同时，研究了PCA在降低维度上的影响，使用BiLSTM模型和AdaBoost改进决策树，并通过LIME解释AdaBoost分类器的预测结果。

Result: 研究表明，结合不同技术能够有效提高孟加拉语言的情感识别效率。

Conclusion: 该研究通过探讨多种情感分析方法，致力于提高对低资源语言（如孟加拉语）情感识别的效率。

Abstract: Research on understanding emotions in written language continues to expand,
especially for understudied languages with distinctive regional expressions and
cultural features, such as Bangla. This study examines emotion analysis using
22,698 social media comments from the EmoNoBa dataset. For language analysis,
we employ machine learning models: Linear SVM, KNN, and Random Forest with
n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA
affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model
and AdaBoost to improve decision trees. To make our machine learning models
easier to understand, we used LIME to explain the predictions of the AdaBoost
classifier, which uses decision trees. With the goal of advancing sentiment
analysis in languages with limited resources, our work examines various
techniques to find efficient techniques for emotion identification in Bangla.

</details>


### [11] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
*Elizabeth Demers,Victor Xiaoqi Wang,Kean Wu*

Main category: cs.CL

TL;DR: 研究使用机器学习算法开发了人力资本相关关键词的词汇表，并提供了相关工具供研究者使用。


<details>
  <summary>Details</summary>
Motivation: 随着人力资本在人力资源领域的重要性日益增加，目前缺乏明确的衡量和披露规则。

Method: 使用机器学习算法（word2vec）对已确认的人力资本披露进行了训练，从而开发出人力资本相关关键词的完整列表，并将其划分为五个子类别。

Result: 提供了人力资本词汇、企业人力资本披露以及用于开发词汇的Python代码，并给出了如何使用这些数据和代码的详细示例。

Conclusion: 研究者可以使用或修改此人力资本词汇和代码，以解决相关的人力资本问题，也讨论了人力资本管理和披露的未来研究机会。

Abstract: Human capital (HC) is increasingly important to corporate value creation.
Unlike other assets, however, HC is not currently subject to well-defined
measurement or disclosure rules. We use a machine learning algorithm (word2vec)
trained on a confirmed set of HC disclosures to develop a comprehensive list of
HC-related keywords classified into five subcategories (DEI; health and safety;
labor relations and culture; compensation and benefits; and demographics and
other) that capture the multidimensional nature of HC management. We share our
lexicon, corporate HC disclosures, and the Python code used to develop the
lexicon, and we provide detailed examples of using our data and code, including
for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the
code to capture another construct of interest) with their samples of corporate
communications to address pertinent HC questions. We close with a discussion of
future research opportunities related to HC management and disclosure.

</details>


### [12] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
*Yi Wang,Max Kreminski*

Main category: cs.CL

TL;DR: 该研究通过解决叙事规划问题探讨LLMs的故事生成能力，发现GPT-4级别的模型能生成因果合理的故事，但在复杂的角色意图和戏剧冲突方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在生成高质量故事方面的能力有限，旨在通过解决叙事规划问题加深对LLMs故事生成能力的理解。

Method: 通过利用LLMs解决叙事规划问题，提出了一种基于文学范例的评估基准，侧重于因果合理性、角色意图和戏剧冲突。

Result: 实验表明，GPT-4级别的LLMs能够在小规模上生成因果合理的故事，但规划角色意图和戏剧冲突仍有挑战。该结果提供了LLMs生成故事的质量规模方面的见解。

Conclusion: GPT-4级别的LLMs能够在小规模上生成因果合理的故事，但在角色意图和戏剧冲突方面的规划仍有挑战。

Abstract: Story generation has been a prominent application of Large Language Models
(LLMs). However, understanding LLMs' ability to produce high-quality stories
remains limited due to challenges in automatic evaluation methods and the high
cost and subjectivity of manual evaluation. Computational narratology offers
valuable insights into what constitutes a good story, which has been applied in
the symbolic narrative planning approach to story generation. This work aims to
deepen the understanding of LLMs' story generation capabilities by using them
to solve narrative planning problems. We present a benchmark for evaluating
LLMs on narrative planning based on literature examples, focusing on causal
soundness, character intentionality, and dramatic conflict. Our experiments
show that GPT-4 tier LLMs can generate causally sound stories at small scales,
but planning with character intentionality and dramatic conflict remains
challenging, requiring LLMs trained with reinforcement learning for complex
reasoning. The results offer insights on the scale of stories that LLMs can
generate while maintaining quality from different aspects. Our findings also
highlight interesting problem solving behaviors and shed lights on challenges
and considerations for applying LLM narrative planning in game environments.

</details>


### [13] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 该论文提出Q2E方法，通过信息熵融合各类多模态信息，在零样本多语言文本到视频检索中取得了优于其他方法的效果。


<details>
  <summary>Details</summary>
Motivation: 提高对与复杂真实世界事件相关的视频的识别和检索，通过自动提取这些事件的潜在参数知识。

Method: 提出了一种Q2E方法，即Query-to-Event分解方法，适用于零样本多语言文本到视频检索；该方法通过分解查询借助LLM和VLM中嵌入的知识，增强对人类查询的理解；采用了熵基融合评分来进行零样本融合。

Result: Q2E方法在多个检索指标上表现出优于多种最新的基准方法，且通过整合音频信息显著提升了文本到视频检索的效果。

Conclusion: Q2E方法通过信息熵融合评分使得多模态知识结合在一起，并在两个多样化的数据集上优于一些最新的基线方法。

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


### [14] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
*Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 引入TTT-Bench基准测试LRMs的基础推理能力，结果发现在复杂数学问题上表现良好的模型在简单推理游戏上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨LRMs在更广泛任务领域中的推理能力，特别是在简单战略、空间和逻辑推理方面。

Method: 提出了一种简单且可扩展的方法来生成TTT-Bench的可验证双人游戏问题，并对各种前沿的LRM进行了评估。

Result: 被评估的推理模型在TTT-Bench的表现远低于在MATH 500和AIME 2024上的表现，尤其是在长期战略推理方面面临困难。

Conclusion: 尽管在复杂的数学问题上表现出色的模型在简单的推理游戏中常常失败。

Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning
capabilities across a broad range of tasks including Olympiad-level
mathematical problems, indicating evidence of their complex reasoning
abilities. While many reasoning benchmarks focus on the STEM domain, the
ability of LRMs to reason correctly in broader task domains remains
underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark
that is designed to evaluate basic strategic, spatial, and logical reasoning
abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games
that humans can effortlessly solve from a young age. We propose a simple yet
scalable programmatic approach for generating verifiable two-player game
problems for TTT-Bench. Although these games are trivial for humans, they
require reasoning about the intentions of the opponent, as well as the game
board's spatial configurations, to ensure a win. We evaluate a diverse set of
state-of-the-art LRMs, and \textbf{discover that the models that excel at hard
math problems frequently fail at these simple reasoning games}. Further testing
reveals that our evaluated reasoning models score on average $\downarrow$ 41\%
\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024
respectively, with larger models achieving higher performance using shorter
reasoning traces, where most of the models struggle on long-term strategic
reasoning situations on simple and new TTT-Bench tasks.

</details>


### [15] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
*Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 论文探讨了如何利用计算方法识别不可靠叙述者，并建立了一个人类注释的数据集进行实验和分析，研究表明这一任务充满挑战并且具有潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发计算方法识别那些无意中误报信息的不可靠叙述者，并应用于多个领域的文本数据。

Method: 该论文利用叙述学的文学理论定义不可靠叙述者类型，并提出使用LLM进行不可靠叙述者的分类，通过少样本学习、微调和课程学习环境进行实验。

Result: 实验结果表明识别不可靠叙述者的任务非常具有挑战性，并表明使用LLMs进行识别的潜力。

Conclusion: 研究表明，使用语言模型识别不可靠叙述者是一项具有挑战性的任务，但具有潜力。作者发布了专家注解的数据集和代码，鼓励未来的研究。

Abstract: Often when we interact with a first-person account of events, we consider
whether or not the narrator, the primary speaker of the text, is reliable. In
this paper, we propose using computational methods to identify unreliable
narrators, i.e. those who unintentionally misrepresent information. Borrowing
literary theory from narratology to define different types of unreliable
narrators based on a variety of textual phenomena, we present TUNa, a
human-annotated dataset of narratives from multiple domains, including blog
posts, subreddit posts, hotel reviews, and works of literature. We define
classification tasks for intra-narrational, inter-narrational, and
inter-textual unreliabilities and analyze the performance of popular
open-weight and proprietary LLMs for each. We propose learning from literature
to perform unreliable narrator classification on real-world text data. To this
end, we experiment with few-shot, fine-tuning, and curriculum learning
settings. Our results show that this task is very challenging, and there is
potential for using LLMs to identify unreliable narrators. We release our
expert-annotated dataset and code and invite future research in this area.

</details>


### [16] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
*Iago Alves Brito,Julia Soares Dollis,Fernanda Bufon Färber,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: ToxSyn-PT 是第一个能够进行九个法定保护少数群体细粒度仇恨言论分类的大规模葡萄牙语语料库，通过新颖的四阶段流程创建，并在多个公共数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语数据集中主要以社交媒体为主，且缺乏对九个法定保护少数群体的细粒度仇恨言论分类数据集。ToxSyn-PT 旨在填补这一空白，并推进关于合成数据和低资源环境中仇恨言论检测的研究。

Method: 论文采用了一个新的四阶段流程来创建 ToxSyn-PT 语料库：首先是人工整理的种子数据集，其次是通过指令调优的大型语言模型进行少样本扩展，然后是基于释义的增强，最后是补充和添加中性文本以防止对特定群体线索的过拟合。

Result: 实验表明，尽管与传统基准存在领域差异，但对于二元和多标签分类任务，该语料库在五个公共葡萄牙语仇恨言论数据集中取得了强劲的结果。

Conclusion: ToxSyn-PT 语料库在五个公开的葡萄牙语仇恨言论数据集上表现出色，展示了其在跨领域中的强大泛化能力。

Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables
fine-grained hate-speech classification across nine legally protected minority
groups. The dataset contains 53,274 synthetic sentences equally distributed
between minorities groups and toxicity labels. ToxSyn-PT is created through a
novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot
expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and
(4) enrichment, plus additional neutral texts to curb overfitting to
group-specific cues. The resulting corpus is class-balanced, stylistically
diverse, and free from the social-media domain that dominate existing
Portuguese datasets. Despite domain differences with traditional benchmarks,
experiments on both binary and multi-label classification on the corpus yields
strong results across five public Portuguese hate-speech datasets,
demonstrating robust generalization even across domain boundaries. The dataset
is publicly released to advance research on synthetic data and hate-speech
detection in low-resource settings.

</details>


### [17] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
*Andrea Yaoyun Cui,Pengfei Yu*

Main category: cs.CL

TL;DR: 研究表明，语言模型在某些条件下可以表现出接近确定性的行为，挑战了传统采样假设。提出了一种方法区分吉布斯采样中的随机与确定性决策，防止错误先验推断。


<details>
  <summary>Details</summary>
Motivation: 重新审视语言模型是否具有贝叶斯头脑的假设，并挑战传统采样假设以及以往用以引发人类先验的实验方法的有效性。

Method: 我们提出了一种简单的方法来区分吉布斯采样中的随机和确定性决策模式。并通过在多种大型语言模型上的实验来识别不同情况下的决策模式。

Result: 实验证明，语言模型在某些条件下可以表现出接近确定性决策行为，并且模拟吉布斯采样可能导致错误的先验推断。我们的简单方法能有效地区分模型的随机和确定性决策模式。

Conclusion: 在某些条件下，语言模型可以表现出接近确定性的决策行为，例如产生最大似然估计，即使在非零采样温度下。这对采样假设提出了挑战，并削弱了以往用以引发类似人类先验的实验方法。此外，未仔细分辨下，一种具有确定性行为的系统经历模拟吉布斯采样时，可能会收敛于一个“错误的先验”。为此，我们提出了一种简单的方法，以区分吉布斯采样中的随机和确定性决策模式，帮助防止推断出误导性的语言模型先验。

Abstract: Language models are essentially probability distributions over token
sequences. Auto-regressive models generate sentences by iteratively computing
and sampling from the distribution of the next token. This iterative sampling
introduces stochasticity, leading to the assumption that language models make
probabilistic decisions, similar to sampling from unknown distributions.
Building on this assumption, prior research has used simulated Gibbs sampling,
inspired by experiments designed to elicit human priors, to infer the priors of
language models. In this paper, we revisit a critical question: Do language
models possess Bayesian brains? Our findings show that under certain
conditions, language models can exhibit near-deterministic decision-making,
such as producing maximum likelihood estimations, even with a non-zero sampling
temperature. This challenges the sampling assumption and undermines previous
methods for eliciting human-like priors. Furthermore, we demonstrate that
without proper scrutiny, a system with deterministic behavior undergoing
simulated Gibbs sampling can converge to a "false prior." To address this, we
propose a straightforward approach to distinguish between stochastic and
deterministic decision patterns in Gibbs sampling, helping to prevent the
inference of misleading language model priors. We experiment on a variety of
large language models to identify their decision patterns under various
circumstances. Our results provide key insights in understanding decision
making of large language models.

</details>


### [18] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
*Zige Wang,Qi Zhu,Fei Mi,Minghui Xu,Ruochun Jin,Wenjing Yang*

Main category: cs.CL

TL;DR: 提出ClusterUCB框架，用于高效数据选择，减少计算消耗并保持效果。


<details>
  <summary>Details</summary>
Motivation: 梯度计算在大型语言模型的有监督微调过程中资源消耗过高，因此需要一种有效的数据选择方法来减少计算需求。

Method: 使用聚类和改进的上置信界算法进行高效的梯度数据选择。通过聚类首先对训练数据池进行分类，然后通过建模为计算预算分配问题，采用多臂赌博问题进行优化，并在迭代采样过程中记录历史数据影响信息。

Result: 实验结果表明，ClusterUCB在不同基准测试中表现出与原始梯度数据选择方法相当的效果，同时大幅减少了计算资源消耗。

Conclusion: 提出了ClusterUCB框架，可以显著减少计算资源消耗，同时保持与原始梯度数据选择方法相当的效果。

Abstract: Gradient-based data influence approximation has been leveraged to select
useful data samples in the supervised fine-tuning of large language models.
However, the computation of gradients throughout the fine-tuning process
requires too many resources to be feasible in practice. In this paper, we
propose an efficient gradient-based data selection framework with clustering
and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition
that data samples with similar gradient features will have similar influences,
we first perform clustering on the training data pool. Then, we frame the
inter-cluster data selection as a constrained computing budget allocation
problem and consider it a multi-armed bandit problem. A modified UCB algorithm
is leveraged to solve this problem. Specifically, during the iterative sampling
process, historical data influence information is recorded to directly estimate
the distributions of each cluster, and a cold start is adopted to balance
exploration and exploitation. Experimental results on various benchmarks show
that our proposed framework, ClusterUCB, can achieve comparable results to the
original gradient-based data selection methods while greatly reducing computing
consumption.

</details>


### [19] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
*Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak*

Main category: cs.CL

TL;DR: Flick提出了一种新的伪标签精炼方法，在低资源语言环境下的文本分类中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的文本分类方法在低资源语言上下文中常遇到伪标签噪声和领域适应问题，使得精确分类变得困难。为解决这一挑战，提出了Flick。

Method: Flick通过广泛的初始集群中提炼高置信度伪标签的方法提高伪标签质量，采用伪标签精炼组件，识别和利用表现优异的伪标签集群。

Result: Flick在包括阿拉伯语、乌尔都语和Setswana等各种低资源语言的14个不同数据集上进行了验证，表现出色。

Conclusion: Flick在多种复杂的低资源语言环境中表现出色，展示了其优越的性能和适应能力。

Abstract: Training deep learning networks with minimal supervision has gained
significant research attention due to its potential to reduce reliance on
extensive labelled data. While self-training methods have proven effective in
semi-supervised learning, they remain vulnerable to errors from noisy pseudo
labels. Moreover, most recent approaches to the few-label classification
problem are either designed for resource-rich languages such as English or
involve complex cascading models that are prone to overfitting. To address the
persistent challenge of few-label text classification in truly low-resource
linguistic contexts, where existing methods often struggle with noisy
pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods
that rely on generic multi-cluster pseudo-labelling or complex cascading
architectures, Flick leverages the fundamental insight that distilling
high-confidence pseudo-labels from a broader set of initial clusters can
dramatically improve pseudo-label quality, particularly for linguistically
diverse, low-resource settings. Flick introduces a novel pseudo-label
refinement component, a departure from traditional pseudo-labelling strategies
by identifying and leveraging top-performing pseudo-label clusters. This
component specifically learns to distil highly reliable pseudo-labels from an
initial broad set by focusing on single-cluster cohesion and leveraging an
adaptive top-k selection mechanism. This targeted refinement process is crucial
for mitigating the propagation of errors inherent in low-resource data,
allowing for robust fine-tuning of pre-trained language models with only a
handful of true labels. We demonstrate Flick's efficacy across 14 diverse
datasets, encompassing challenging low-resource languages such as Arabic, Urdu,
and Setswana, alongside English, showcasing its superior performance and
adaptability.

</details>


### [20] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
*Chuck Arvin*

Main category: cs.CL

TL;DR: 用户提示对LLMs在教育情境中的回答质量影响显著，错误提示会降低正确性，而正确提示会提升。较小模型的偏差更明显。


<details>
  <summary>Details</summary>
Motivation: 探讨在模拟的教育情境中，用户提供的建议如何影响大型语言模型（LLMs）的回答准确性，尤其在奉承性偏见下，可能导致教育公平性的问题。

Method: 研究测试了来自OpenAI的五个不同的GPT-4o和GPT-4.1模型，并在五种实验条件下，分析了在学生给出正确或错误答案提示时，LLM的反应质量。

Result: 研究发现，模型的回答质量随查询框架而显著波动。学生提及错误答案会使LLM的正确性降低最多15个百分点，而提及正确答案则提升准确性。较小的模型如GPT-4.1-nano受此影响最大，达30%。

Conclusion: 该研究显示，大型语言模型（LLMs）在教育情境中，其回答质量会因用户的提示而显著变化。特别是在错误提示的情况下，模型的正确性会降低，而正确提示则能提升模型的准确性。

Abstract: This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.

</details>


### [21] [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
*Hayato Futami,Emiru Tsunoo,Yosuke Kashiwagi,Yuki Ito,Hassan Shahmohammadi,Siddhant Arora,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出一种交替使用语音和文本单元的训练方法，提高了语音到语音翻译的效果，尤其在训练数据有限的情况下效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型主要基于文本数据训练，导致在进行语音到语音翻译时面临模式适应的问题。尤其是在语音-语音数据有限的情况下，文本到语音的模态转换成为挑战。该研究针对这种训练困难提出改进方法。

Method: 本文提出了一种定期交织语音-文本训练的方法。在训练过程中使用交织的语音-文本单元，这些单元在字词水平上交织排列，以促进从文本到语音的逐步模式适应。随着训练的进展，逐步减少文本的比例。

Result: 通过在CVSS数据集上对LLaMA3.2-1B模型进行微调实验，验证了所提方法能够一致地改善翻译性能。

Conclusion: 本文提出的定期交织语音-文本训练方法有效地提高了语音到语音翻译的性能，特别是在训练数据有限的语言中表现显著。

Abstract: Speech-to-speech translation (S2ST) has been advanced with large language
models (LLMs), which are fine-tuned on discrete speech units. In such
approaches, modality adaptation from text to speech has been an issue. LLMs are
trained on text-only data, which presents challenges to adapt them to speech
modality with limited speech-to-speech data. To address the training
difficulty, we propose scheduled interleaved speech--text training in this
study. We use interleaved speech--text units instead of speech units during
training, where aligned text tokens are interleaved at the word level. We
gradually decrease the ratio of text as training progresses, to facilitate
progressive modality adaptation from text to speech. We conduct experimental
evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show
that the proposed method consistently improves the translation performances,
especially for languages with limited training data.

</details>


### [22] [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
*Dongwon Jung,Wenxuan Zhou,Muhao Chen*

Main category: cs.CL

TL;DR: 我们提出了一种利用程序执行生成高质量CoT监督数据集的方法，该方法有效提高LLM的推理能力并减少推理过程中不必要的重复。


<details>
  <summary>Details</summary>
Motivation: 当前的逻辑推理数据集生成方法依赖昂贵的人为标注或容易出错的LLM生成的CoT，而我们的方法试图克服这些挑战。

Method: 该方法通过从代码执行中提取可验证的、逐步的推理轨迹，将其转换为自然语言的CoT推理，来生成高质量的CoT监督数据集。

Result: 实验表明，该方法在减少推理过程中的无意义重复和过度思考的同时，有效地提供了跨领域任务的可转移推理能力。此外，消融研究验证了该方法生成的推理数据具有高度准确性。

Conclusion: 我们的研究表明，利用程序执行的确定性生成高质量的逻辑推理监督数据集是一种可行且有效的方法，可增强LLM的推理能力。

Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision
has proven effective for enhancing their reasoning abilities. However,
obtaining reliable and accurate reasoning supervision remains a significant
challenge. We propose a scalable method for generating a high-quality CoT
supervision dataset by leveraging the determinism of program execution. Unlike
existing reasoning dataset generation methods that rely on costly human
annotations or error-prone LLM-generated CoT, our approach extracts verifiable,
step-by-step reasoning traces from code execution and transforms them into a
natural language CoT reasoning. Experiments on reasoning benchmarks across
various domains show that our method effectively equips LLMs with transferable
reasoning abilities across diverse tasks. Furthermore, the ablation studies
validate that our method produces highly accurate reasoning data and reduces
overall token length during inference by reducing meaningless repetition and
overthinking.

</details>


### [23] [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
*Xiaohan Yu,Pu Jian,Chong Chen*

Main category: cs.CL

TL;DR: TableRAG addresses the limitations of prevailing RAG methods in handling heterogeneous documents by integrating text and tabular data processing, resulting in improved performance in question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG approaches have critical limitations when dealing with heterogeneous documents, including information loss and undermined reasoning capabilities, especially in multi-hop, global queries.

Method: TableRAG operates in four steps: context-sensitive query decomposition, text retrieval, SQL programming and execution, and compositional intermediate answer generation.

Result: TableRAG consistently outperforms existing baselines on both public datasets and the newly developed HeteQA benchmark.

Conclusion: TableRAG achieves a new state-of-the-art performance in heterogeneous document question answering.

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable
effectiveness in open-domain question answering. However, when applied to
heterogeneous documents, comprising both textual and tabular components,
existing RAG approaches exhibit critical limitations. The prevailing practice
of flattening tables and chunking strategies disrupts the intrinsic tabular
structure, leads to information loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose
TableRAG, an hybrid framework that unifies textual understanding and complex
manipulations over tabular data. TableRAG iteratively operates in four steps:
context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop
HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous
reasoning capabilities. Experimental results demonstrate that TableRAG
consistently outperforms existing baselines on both public datasets and our
HeteQA, establishing a new state-of-the-art for heterogeneous document question
answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.

</details>


### [24] [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/abs/2506.10406)
*Yuhua Jiang,Yuwen Xiong,Yufeng Yuan,Chao Xin,Wenyuan Xu,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.CL

TL;DR: 提出了一种新的框架（PAG），通过在RL中交替执行策略和验证角色来提高语言模型的自我纠错能力，并通过选择性修订机制避免无效的修订。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言模型在验证自身产出正确性时的困境，提高其自我纠错能力，同时避免规模扩展的瓶颈。

Method: 提出了一个新的框架，称为生成验证器政策（PAG），在统一的多轮强化学习模式中让模型交替执行策略和验证器角色，采用选择性修订机制，只有在检测到错误时才修订答案。

Result: 通过广泛的实验，PAG作为策略提高了直接生成和自我纠错的准确性，作为验证器，其自我验证性能超越了自我一致性。

Conclusion: PAG框架不仅缓解了模型崩溃问题，还同时提升了推理和验证能力。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
complex reasoning tasks, yet they still struggle to reliably verify the
correctness of their own outputs. Existing solutions to this verification
challenge often depend on separate verifier models or require multi-stage
self-correction training pipelines, which limit scalability. In this paper, we
propose Policy as Generative Verifier (PAG), a simple and effective framework
that empowers LLMs to self-correct by alternating between policy and verifier
roles within a unified multi-turn reinforcement learning (RL) paradigm.
Distinct from prior approaches that always generate a second attempt regardless
of model confidence, PAG introduces a selective revision mechanism: the model
revises its answer only when its own generative verification step detects an
error. This verify-then-revise workflow not only alleviates model collapse but
also jointly enhances both reasoning and verification abilities. Extensive
experiments across diverse reasoning benchmarks highlight PAG's dual
advancements: as a policy, it enhances direct generation and self-correction
accuracy; as a verifier, its self-verification outperforms self-consistency.

</details>


### [25] [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/abs/2506.10415)
*Yingjin Song,Yupei Du,Denis Paperno,Albert Gatt*

Main category: cs.CL

TL;DR: TempVS benchmark tests MLLMs' temporal reasoning; results show significant performance gaps compared to humans, highlighting future research directions.


<details>
  <summary>Details</summary>
Motivation: To test and enhance the temporal grounding and reasoning abilities of Multimodal Large Language Models in image sequences.

Method: TempVS benchmark evaluates MLLMs on three temporal reasoning tasks: event relation inference, sentence ordering, and image ordering, accompanied by basic grounding tests.

Result: 38 state-of-the-art MLLMs were evaluated, revealing their difficulty in performing temporal reasoning tasks on the TempVS benchmark.

Conclusion: MLLMs exhibit significant performance gaps in temporal reasoning compared to humans, indicating areas for improvement.

Abstract: This paper introduces the TempVS benchmark, which focuses on temporal
grounding and reasoning capabilities of Multimodal Large Language Models
(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event
relation inference, sentence ordering and image ordering), each accompanied
with a basic grounding test. TempVS requires MLLMs to rely on both visual and
linguistic modalities to understand the temporal order of events. We evaluate
38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,
with a substantial performance gap compared to human capabilities. We also
provide fine-grained insights that suggest promising directions for future
research. Our TempVS benchmark data and code are available at
https://github.com/yjsong22/TempVS.

</details>


### [26] [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/abs/2506.10421)
*Avneet Kaur,Arnav Arora*

Main category: cs.CL

TL;DR: 研究分析了以色列-巴勒斯坦战争报道中的新闻框架，揭示了战争报道的倾向及跨国媒体中的偏见差异。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体在冲突时期的表述框架可能会极大地影响读者的意见，并可能加剧冲突。然而，目前关于冲突框架的研究由于其质性性质或仅局限于表面层次的通用框架，因此见解有限。

Method: 本文利用计算方法，结合框架语义和大型语言模型，识别新闻报道中的交流框架及其与语言框架的联系。

Result: 我们的分析揭示了新闻报道中更侧重于战争而非和平的倾向。美国、英国和中东新闻媒体在报道中的偏见表现出显著差异，尤其在冲突中谁是攻击者和受害者的框架方面。

Conclusion: 媒体在冲突报道中倾向于以战争为基础的报道，且在不同国家的新闻媒体中存在明显的架构偏见。

Abstract: Framing used by news media, especially in times of conflict, can have
substantial impact on readers' opinion, potentially aggravating the conflict
itself. Current studies on the topic of conflict framing have limited insights
due to their qualitative nature or only look at surface level generic frames
without going deeper. In this work, we identify indicators of war and peace
journalism, as outlined by prior work in conflict studies, in a corpus of news
articles reporting on the Israel-Palestine war. For our analysis, we use
computational approaches, using a combination of frame semantics and large
language models to identify both communicative framing and its connection to
linguistic framing. Our analysis reveals a higher focus on war based reporting
rather than peace based. We also show substantial differences in reporting
across the US, UK, and Middle Eastern news outlets in framing who the assailant
and victims of the conflict are, surfacing biases within the media.

</details>


### [27] [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/abs/2506.10446)
*Zehui Ling,Deshu Chen,Hongwei Zhang,Yifeng Jiao,Xin Guo,Yuan Cheng*

Main category: cs.CL

TL;DR: 该研究通过优化奖励函数，改进了大语言模型推理的效率和准确性，在多个数据集上的评估显示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理能力上显示出显著进步，但生成的长输出增加了计算延迟。尽管有些方法通过强化学习缩短推理，但通常不考虑问题的复杂性，导致效果不佳。

Method: 通过划分奖励函数并引入输出长度的新惩罚机制来管理模型的推理效率，从而在简单问题中提倡简洁，同时在复杂问题中保留充分的推理以保证准确性。

Result: 在三个数据集GSM8K、MATH500和AIME2024的基准评估中，我们的方法表现出色。对于较简单的数据集GSM8K和MATH500，我们的方法有效缩短了输出长度，同时保持或提高了准确性。在要求更高的AIME2024数据集中，我们的方法提高了准确性。

Conclusion: 通过管理输出长度与推理准确性之间的平衡，我们的方法提高了大语言模型的总体性能。

Abstract: Large language models (LLMs) have demonstrated significant advancements in
reasoning capabilities, performing well on various challenging benchmarks.
Techniques like Chain-of-Thought prompting have been introduced to further
improve reasoning. However, these approaches frequently generate longer
outputs, which in turn increase computational latency. Although some methods
use reinforcement learning to shorten reasoning, they often apply uniform
penalties without considering the problem's complexity, leading to suboptimal
outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by
promoting conciseness for simpler problems while preserving sufficient
reasoning for more complex ones for accuracy, thus improving the model's
overall performance. Specifically, we manage the model's reasoning efficiency
by dividing the reward function and including a novel penalty for output
length. Our approach has yielded impressive outcomes in benchmark evaluations
across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively
simpler datasets GSM8K and MATH500, our method has effectively shortened output
lengths while preserving or enhancing accuracy. On the more demanding AIME2024
dataset, our approach has resulted in improved accuracy.

</details>


### [28] [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
*Xanh Ho,Sunisth Kumar,Yun-Ang Wu,Florian Boudin,Atsuhiro Takasu,Akiko Aizawa*

Main category: cs.CL

TL;DR: 对表格文本对齐进行解释任务的重新定义，提高了声明验证性能，但大型语言模型未能提供准确的解释。


<details>
  <summary>Details</summary>
Motivation: 仅预测最终标签无法揭示模型的推理过程，因而解释性有限。

Method: 构建了一个新的数据集，通过扩展SciTab基准并加入人工注释的单元格级别的解释。注释员验证声明标签并突出支持其决定的最小单元格集，还提出了处理模糊情况的分类方法。

Result: 加入表格对齐信息提高了声明验证性能，但大多数大型语言模型虽然能预测正确标签，却未能提供与人为对齐的解释。

Conclusion: 将表格文本对齐重新定义为解释任务，通过要求模型识别验证声明所需的重要表格单元，提高了声明验证的性能。大部分大型语言模型虽然能预测正确标签，但未能恢复与人为对齐的解释，表明其预测无法反映真实的推理过程。

Abstract: Scientific claim verification against tables typically requires predicting
whether a claim is supported or refuted given a table. However, we argue that
predicting the final label alone is insufficient: it reveals little about the
model's reasoning and offers limited interpretability. To address this, we
reframe table-text alignment as an explanation task, requiring models to
identify the table cells essential for claim verification. We build a new
dataset by extending the SciTab benchmark with human-annotated cell-level
rationales. Annotators verify the claim label and highlight the minimal set of
cells needed to support their decision. After the annotation process, we
utilize the collected information and propose a taxonomy for handling ambiguous
cases. Our experiments show that (i) incorporating table alignment information
improves claim verification performance, and (ii) most LLMs, while often
predicting correct labels, fail to recover human-aligned rationales, suggesting
that their predictions do not stem from faithful reasoning.

</details>


### [29] [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
*Aleksandra Sorokovikova,Pavel Chizhov,Iuliia Eremenko,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型中的偏见，通过不同任务发现偏见显著，尤其在薪资谈判建议中。


<details>
  <summary>Details</summary>
Motivation: 由于现代语言模型训练使用了包含争议和刻板印象内容的大量数据，导致生成的内容含有性别、起源、年龄等方面的偏见，需对此进行研究。

Method: 研究使用了多主体基准（MMLU）评估了预标记角色的语言模型，并通过重新表述任务和请求模型对用户答案进行评分来测量偏见。还对模型在薪资谈判建议任务中的表现进行了分析。

Result: 当任务被重新表述为让模型对用户的答案打分时，偏见出现得更加明显；在请求薪资谈判建议时，答案中展示了显著的偏见。

Conclusion: 现代大型语言模型在个性化和辅助记忆方面不需要用户预先提示其个性描述，因为模型已拥有用户的社会人口统计信息，存在偏见的问题需要更多注意。

Abstract: Modern language models are trained on large amounts of data. These data
inevitably include controversial and stereotypical content, which contains all
sorts of biases related to gender, origin, age, etc. As a result, the models
express biased points of view or produce different results based on the
assigned personality or the personality of the user. In this paper, we
investigate various proxy measures of bias in large language models (LLMs). We
find that evaluating models with pre-prompted personae on a multi-subject
benchmark (MMLU) leads to negligible and mostly random differences in scores.
However, if we reformulate the task and ask a model to grade the user's answer,
this shows more significant signs of bias. Finally, if we ask the model for
salary negotiation advice, we see pronounced bias in the answers. With the
recent trend for LLM assistant memory and personalization, these problems open
up from a different angle: modern LLM users do not need to pre-prompt the
description of their persona since the model already knows their
socio-demographics.

</details>


### [30] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Tina showcases cost-effective language model reasoning capabilities using a minimalist approach, outperforming some SOTA models at a fraction of the cost using low-rank adaptation in RL.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how strong reasoning abilities can be achieved in language models in a cost-effective manner, given the increasing demand for such capabilities in AI applications.

Method: The paper utilizes reinforcement learning with parameter-efficient updates through low-rank adaptation (LoRA) applied to a small 1.5 billion parameter base model, enabling substantial reasoning performance with minimal computational resources post-training.

Result: The resulting Tina model achieved over 20% improvement in reasoning performance and 43.33% Pass@1 accuracy on AIME24 dataset, at a cost of only $9 USD for post-training and evaluation, representing a 260x cost reduction compared to existing models.

Conclusion: Tina demonstrates the potential of achieving strong reasoning capabilities in language models with high cost-efficiency, surpassing existing SOTA models in some cases. This is possible by utilizing parameter-efficient updates and low-rank adaptation techniques in RL, while maintaining a low computational cost.

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>


### [31] [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/abs/2506.10504)
*Sangmin Song,Juhwan Choi,JungMin Yun,YoungBin Kim*

Main category: cs.CL

TL;DR: 研究表明，现有LLM在处理多用户对话状态跟踪时的效果不佳，揭示了其局限性，并强调了对这方面模型改进的需求。


<details>
  <summary>Details</summary>
Motivation: 由于传统DST基准测试关注于结构化的用户代理对话，缺少对真实多用户交互复杂性的覆盖，该研究评估LLM在多用户DST中的鲁棒性，同时尽量降低数据集构建成本。

Method: 通过利用言语行为理论生成第二用户的发言，系统性地扩展并融入现有DST数据集，以在多用户环境中评价LLM的表现。

Result: 实验结果显示，相较于单用户DST，LLM在多用户环境中的性能显著下降。

Conclusion: 现有大型语言模型在多用户对话状态跟踪中性能显著下降，强调了未来研究提升模型应对多用户场景的必要性。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
zero-shot dialogue state tracking (DST), reducing the need for task-specific
training. However, conventional DST benchmarks primarily focus on structured
user-agent conversations, failing to capture the complexities of real-world
multi-user interactions. In this study, we assess the robustness of LLMs in
multi-user DST while minimizing dataset construction costs. Inspired by recent
advances in LLM-based data annotation, we extend an existing DST dataset by
generating utterances of a second user based on speech act theory. Our
methodology systematically incorporates a second user's utterances into
conversations, enabling a controlled evaluation of LLMs in multi-user settings.
Experimental results reveal a significant performance drop compared to
single-user DST, highlighting the limitations of current LLMs in extracting and
tracking dialogue states amidst multiple speakers. Our findings emphasize the
need for future research to enhance LLMs for multi-user DST scenarios, paving
the way for more realistic and robust DST models.

</details>


### [32] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: The paper presents Resa, a system for efficiently improving reasoning in language models using a novel SAE-Tuning approach, greatly reducing training time and cost while retaining strong performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a cost-effective and efficient method to enhance reasoning abilities in language models by leveraging their existing representations, without extensive retraining or resource investment.

Method: This paper introduces SAE-Tuning, a process utilizing a Sparse Autoencoder to capture reasoning abilities from a source model, and then guides supervised fine-tuning on a target model. All training is done using verified question-answer data without reasoning traces.

Result: The method reduces training costs by over 2000x and time by over 450x while retaining over 97% of reasoning performance compared to RL-trained counterparts. It also enables high reasoning performance in lightly RL-trained models with minimal additional costs.

Conclusion: Resa's methodology effectively elicits strong reasoning in language models at a significantly reduced cost and training time, while maintaining high performance. The extracted reasoning capabilities are generalizable and modular, enhancing performance across different datasets and models without retraining.

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [33] [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/abs/2506.10508)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Bo Li,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出了RRP框架，有效结合了知识图谱和大语言模型的长处，生成高质量的推理路径，显著提高了复杂问题的解答能力。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱增强LLMs虽然能够补充事实知识，但在解决复杂问题方面仍然存在困难，需要改进事实关系的推理路径以实现逻辑一致性。

Method: 提出RRP框架，通过结合LLMs的语义力量与关系嵌入和双向分布学习获得的结构信息，挖掘知识图谱。此外，引入重新思考模块，根据重要性评估和优化推理路径。

Result: 在两个公共数据集上的实验结果表明，与现有的基线方法相比，RRP达到了最先进的性能。

Conclusion: RRP可以轻松集成到各种LLMs中，以增强其推理能力，并通过生成针对特定问题的高质量推理路径，为LLM推理提供有效指导。

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks
due to a lack of background knowledge and a tendency to hallucinate. To address
these limitations, integrating knowledge graphs (KGs) with LLMs has been
intensively studied. Existing KG-enhanced LLMs focus on supplementary factual
knowledge, but still struggle with solving complex questions. We argue that
refining the relationships among facts and organizing them into a logically
consistent reasoning path is equally important as factual knowledge itself.
Despite their potential, extracting reliable reasoning paths from KGs poses the
following challenges: the complexity of graph structures and the existence of
multiple generated paths, making it difficult to distinguish between useful and
redundant ones. To tackle these challenges, we propose the RRP framework to
mine the knowledge graph, which combines the semantic strengths of LLMs with
structural information obtained through relation embedding and bidirectional
distribution learning. Additionally, we introduce a rethinking module that
evaluates and refines reasoning paths according to their significance.
Experimental results on two public datasets show that RRP achieves
state-of-the-art performance compared to existing baseline methods. Moreover,
RRP can be easily integrated into various LLMs to enhance their reasoning
abilities in a plug-and-play manner. By generating high-quality reasoning paths
tailored to specific questions, RRP distills effective guidance for LLM
reasoning.

</details>


### [34] [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/abs/2506.10614)
*Promise Dodzi Kpoglu*

Main category: cs.CL

TL;DR: 提出一种结合数据驱动推理和基于规则启发的混合方法，用于无监督重构原型词形，在拉丁原型词形重构任务上显示出优于现有方法的显著效果。


<details>
  <summary>Details</summary>
Motivation: 重构原型词形是语言学的重要任务，通过推断原始词形可以更好地理解现代语言的演变过程。现有方法主要依赖于语音编辑的概率模型，但这些方法受限于它们以数据驱动为主的特性，因此有必要探索新的方法。

Method: 该研究提出了一种将数据驱动推理与基于规则的启发式方法结合在一起的无监督原型词形重构方法。这种混合方法在演化优化框架中，利用统计模式和语言学驱动的约束条件来指导重构过程。

Result: 在五种罗曼语的同源词数据集上进行了实验评估，结果表明，与既有基线方法相比，新方法在字符级别准确性和语音学合理性度量上有显著提高。

Conclusion: 该研究的方法有效结合了数据驱动和基于规则的过程，为原型词形重构提供了一种新颖且高效的方法，在实验中表现优于传统方法。

Abstract: We propose an unsupervised method for the reconstruction of protoforms i.e.,
ancestral word forms from which modern language forms are derived. While prior
work has primarily relied on probabilistic models of phonological edits to
infer protoforms from cognate sets, such approaches are limited by their
predominantly data-driven nature. In contrast, our model integrates data-driven
inference with rule-based heuristics within an evolutionary optimization
framework. This hybrid approach leverages on both statistical patterns and
linguistically motivated constraints to guide the reconstruction process. We
evaluate our method on the task of reconstructing Latin protoforms using a
dataset of cognates from five Romance languages. Experimental results
demonstrate substantial improvements over established baselines across both
character-level accuracy and phonological plausibility metrics.

</details>


### [35] [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/abs/2506.10622)
*Sergio Burdisso,Esaú Villatoro-Tello,Petr Motlicek*

Main category: cs.CL

TL;DR: SDialog is a Python toolkit for synthetic dialogue generation, utilizing LLMs to create realistic conversational data, aiding in the reproducibility of AI research.


<details>
  <summary>Details</summary>
Motivation: To provide high-quality, flexible, and reproducible synthetic dialogues for training, evaluation, and benchmarking of conversational AI systems.

Method: SDialog is a Python toolkit that uses instruction-tuned Large Language Models (LLMs) to provide abstractions for personas, orchestration, and scenario management.

Result: SDialog enables the creation of realistic, diverse, and controllable conversational data, supporting workflows such as multi-agent simulation and scenario-driven generation.

Conclusion: SDialog represents an advancement in standardizing tools and frameworks for synthetic dialogue generation, crucial for ensuring reproducibility in research.

Abstract: The advancement of conversational AI systems relies on the availability of
high-quality, flexible, and reproducible synthetic dialogues for training,
evaluation, and benchmarking. SDialog is a modular, extensible Python toolkit
designed to address the challenges of synthetic dialogue generation and
analysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog
provides abstractions for personas, orchestration, and scenario management,
enabling the creation of realistic, diverse, and controllable conversational
data for research and development. SDialog supports workflows such as
multi-agent simulation and scenario-driven generation, and represents a step
forward in the standardization of tools and frameworks for synthetic data
generation, a crucial advancement for ensuring reproducibility in today's
fast-evolving research landscape.

</details>


### [36] [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/abs/2506.10627)
*Numaan Naeem,Sarfraz Ahmad,Momina Ahsan,Hasan Iqbal*

Main category: cs.CL

TL;DR: 我们探讨了多种模型，最终结合大语言模型和示例驱动的提示生成效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 任务的目的是评估导师的回应是否能够正确识别学生数学推理中的错误。

Method: 我们探索了四种方法：1) 对多个预训练语言模型的合并标记嵌入进行机器学习模型的集成；2) 使用冷冻句子转换器的[CLS]嵌入和MLP分类器；3) 具有记忆感知的多头注意力模型，用于令牌级历史和响应嵌入之间；4) 使用大型语言模型（如GPT 4o）的检索增强小样本提示系统。最后的系统检索语义相似的示例，构建结构化提示，并使用模式引导的输出解析生成可解释的预测。

Result: 最终系统在所有基准上表现优越。

Conclusion: 我们的系统优于所有基线，展示了结合示例驱动提示与LLM推理在教学反馈评估中的有效性。

Abstract: This paper presents our system for Track 1: Mistake Identification in the BEA
2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The
task involves evaluating whether a tutor's response correctly identifies a
mistake in a student's mathematical reasoning. We explore four approaches: (1)
an ensemble of machine learning models over pooled token embeddings from
multiple pretrained language models (LMs); (2) a frozen sentence-transformer
using [CLS] embeddings with an MLP classifier; (3) a history-aware model with
multi-head attention between token-level history and response embeddings; and
(4) a retrieval-augmented few-shot prompting system with a large language model
(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,
constructs structured prompts, and uses schema-guided output parsing to produce
interpretable predictions. It outperforms all baselines, demonstrating the
effectiveness of combining example-driven prompting with LLM reasoning for
pedagogical feedback assessment. Our code is available at
https://github.com/NaumanNaeem/BEA_2025.

</details>


### [37] [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/abs/2506.10641)
*Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型在字符级信息编码上的不足，拼写过程依赖中间及高级层重建字符信息，取得了拼写能力上的突破。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在拼写时如何内部表示和利用字符级信息，以解决其在复杂字符级任务上的不足。

Method: 通过探测分类器、知识神经元的识别以及注意力权重的检查，对大型语言模型在拼写过程中如何利用字符级信息进行了研究分析。

Result: 研究发现，模型的嵌入层没有完全编码字符级信息，尤其是在第一个字符之后。模型需要依赖中间和更高的Transformer层来重建字符级知识，并在拼写行为中出现突破性表现。

Conclusion: 大型语言模型在拼写单词时，尽管整体表现出色，但在处理字符级任务时遇到了困难，特别是在识别复合子成分的时候。

Abstract: Large language models (LLMs) can spell out tokens character by character with
high accuracy, yet they struggle with more complex character-level tasks, such
as identifying compositional subcomponents within tokens. In this work, we
investigate how LLMs internally represent and utilize character-level
information during the spelling-out process. Our analysis reveals that,
although spelling out is a simple task for humans, it is not handled in a
straightforward manner by LLMs. Specifically, we show that the embedding layer
does not fully encode character-level information, particularly beyond the
first character. As a result, LLMs rely on intermediate and higher Transformer
layers to reconstruct character-level knowledge, where we observe a distinct
"breakthrough" in their spelling behavior. We validate this mechanism through
three complementary analyses: probing classifiers, identification of knowledge
neurons, and inspection of attention weights.

</details>


### [38] [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/abs/2506.10687)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.CL

TL;DR: 使用大型语言模型检测生命威胁语言，比传统方法更有效，特别是在多种数据平衡情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 检测生命威胁语言对于保护处于困境中的个人、促进心理健康和预防潜在的伤害与生命损失至关重要。

Method: 使用大型语言模型（Gemma、Mistral 和 Llama-2）进行生命威胁文本检测，并将其与传统方法如词袋、词嵌入、主题建模和双向编码器表示进行比较。在不同的数据集情景中进行微调，采用上采样技术处理数据不平衡问题。

Result: 实验结果显示，Mistral 和 Llama-2 模型在数据平衡和不平衡场景中表现优异，而 Gemma 稍逊。上采样技术对传统方法有益，但对大型语言模型影响较小。

Conclusion: 研究表明大型语言模型在检测生命威胁语言方面具有卓越的性能，尤其在数据平衡和不平衡场景中比传统方法更有效。

Abstract: Detecting life-threatening language is essential for safeguarding individuals
in distress, promoting mental health and well-being, and preventing potential
harm and loss of life. This paper presents an effective approach to identifying
life-threatening texts using large language models (LLMs) and compares them
with traditional methods such as bag of words, word embedding, topic modeling,
and Bidirectional Encoder Representations from Transformers. We fine-tune three
open-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter
variants on different datasets, which are constructed with class balance,
imbalance, and extreme imbalance scenarios. Experimental results demonstrate a
strong performance of LLMs against traditional methods. More specifically,
Mistral and Llama-2 models are top performers in both balanced and imbalanced
data scenarios while Gemma is slightly behind. We employ the upsampling
technique to deal with the imbalanced data scenarios and demonstrate that while
this method benefits traditional approaches, it does not have as much impact on
LLMs. This study demonstrates a great potential of LLMs for real-world
life-threatening language detection problems.

</details>


### [39] [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/abs/2506.10715)
*Lorenzo Augello,John P. McCrae*

Main category: cs.CL

TL;DR: 研究探讨并预测形容词的上义关系，创建了新资源并调整语言模型以适应这一任务的需要。


<details>
  <summary>Details</summary>
Motivation: 在OntoLex-lemon中发布的开放英语Wordnet正在缺少许多链接，本研究旨在解决这一问题，特别是针对形容词的上义关系。

Method: 对形容词的上义关系进行理论讨论，并开发适用于形容词上义关系的新资源，同时微调大型语言模型以预测形容词的上义关系。

Result: 成功开发了一个用于形容词上义关系的新资源，并证明大型语言模型的微调方法在预测形容词的上义关系上是有效的。

Conclusion: 通过调整大型语言模型预测形容词的上义关系，并证明TaxoLLaMa的方法可以适应这一任务。

Abstract: Open English Wordnet is a key resource published in OntoLex-lemon as part of
the linguistic linked open data cloud. There are, however, many links missing
in the resource, and in this paper, we look at how we can establish hypernymy
between adjectives. We present a theoretical discussion of the hypernymy
relation and how it differs for adjectives in contrast to nouns and verbs. We
develop a new resource for adjective hypernymy and fine-tune large language
models to predict adjective hypernymy, showing that the methodology of
TaxoLLaMa can be adapted to this task.

</details>


### [40] [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/abs/2506.10716)
*Ye Yu,Yaoning Yu,Haohan Wang*

Main category: cs.CL

TL;DR: PREMISE框架通过提示优化，在不改动模型权重的情况下，显著降低了推理过程中的冗余计算和成本，同时保持答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大推理模型在使用较长的链式推理时，常出现不必要的冗长，增加了token消耗和成本，这在延迟敏感或API受限的环境中限制了部署的可能性。

Method: 提出PREMISE框架，通过提示优化策略减少冗余计算，同时确保答案的准确性。

Result: 在GSM8K、SVAMP和Math500基准上，与Claude和Gemini模型相比，PREMISE在略微提高或保持准确率的同时，将推理token数量减少了高达87.5%，并且削减了69%-82%的成本。

Conclusion: PREMISE框架在不影响推理质量的情况下通过prompt优化显著减少了大推理模型的推理语料和成本。

Abstract: Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve
strong performance on mathematical benchmarks using lengthy chain-of-thought
(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This
inflates token usage and cost, limiting deployment in latency-sensitive or
API-constrained settings. We introduce PREMISE (PRompt-based Efficient
Mathematical Inference with Strategic Evaluation), a prompt-only framework that
reduces reasoning overhead without modifying model weights. PREMISE combines
trace-level diagnostics with gradient-inspired prompt optimization to minimize
redundant computation while preserving answer accuracy. The approach jointly
optimizes brevity and correctness through a multi-objective textual search that
balances token length and answer validity. Unlike prior work, PREMISE runs in a
single-pass black-box interface, so it can be applied directly to commercial
LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy
($96\%\rightarrow96\%$ with Claude, $91\%\rightarrow92\%$ with Gemini) while
reducing reasoning tokens by up to $87.5\%$ and cutting dollar cost by
$69$--$82\%$. These results show that prompt-level optimization is a practical
and scalable path to efficient LRM inference without compromising reasoning
quality.

</details>


### [41] [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/abs/2506.10728)
*Priyanka Kargupta,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 提出了ClaimSpect框架，通过层级结构分析声明的不同方面，并在语料库中发现新子方面和观点，有效性通过案例研究和评估得到验证。


<details>
  <summary>Details</summary>
Motivation: 传统的方法难以对具有复杂性和细微差别的声明做出全面评价，提出一种新的框架来更全面和结构化地评估声明的不同方面。

Method: 使用检索增强的生成框架，构建层次结构，对输入语料进行分区以检索相关片段。

Result: 在多个真实世界的案例研究和人工评价中，ClaimSpect展示其鲁棒性和准确性，优于多种基线。

Conclusion: ClaimSpect可以有效地解构复杂的科学和政治声明，并在语料库中展示不同的观点。

Abstract: Claims made by individuals or entities are oftentimes nuanced and cannot be
clearly labeled as entirely "true" or "false" -- as is frequently the case with
scientific and political claims. However, a claim (e.g., "vaccine A is better
than vaccine B") can be dissected into its integral aspects and sub-aspects
(e.g., efficacy, safety, distribution), which are individually easier to
validate. This enables a more comprehensive, structured response that provides
a well-rounded perspective on a given problem while also allowing the reader to
prioritize specific angles of interest within the claim (e.g., safety towards
children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based
framework for automatically constructing a hierarchy of aspects typically
considered when addressing a claim and enriching them with corpus-specific
perspectives. This structure hierarchically partitions an input corpus to
retrieve relevant segments, which assist in discovering new sub-aspects.
Moreover, these segments enable the discovery of varying perspectives towards
an aspect of the claim (e.g., support, neutral, or oppose) and their respective
prevalence (e.g., "how many biomedical papers believe vaccine A is more
transportable than B?"). We apply ClaimSpect to a wide variety of real-world
scientific and political claims featured in our constructed dataset, showcasing
its robustness and accuracy in deconstructing a nuanced claim and representing
perspectives within a corpus. Through real-world case studies and human
evaluation, we validate its effectiveness over multiple baselines.

</details>


### [42] [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/abs/2506.10737)
*Priyanka Kargupta,Nan Zhang,Yunyi Zhang,Rui Zhang,Prasenjit Mitra,Jiawei Han*

Main category: cs.CL

TL;DR: TaxoAdapt是一种动态调整的分类法框架，提升了科学文献分类的颗粒度和连贯性，解决了现有方法的普遍性和动态性问题。


<details>
  <summary>Details</summary>
Motivation: 科技领域的快速发展使得组织和检索科学文献面临挑战，传统的专家策划分类法虽然在一定程度上解决了这个问题，但过程耗时且昂贵。现有的自动分类方法要么过于依赖特定语料库，影响通用性，要么过于依赖大型语言模型在预训练数据集中包含的一般知识，常常忽略科学领域发展的动态特性。此外，这些方法未能考虑科学文献的多面性，一篇研究论文可能涉及多个维度。

Method: 提出了一个动态调整生成的语言模型分类法的框架TaxoAdapt，该框架实现多个方面的迭代层次分类，基于语料库的主题分布扩展分类法的广度和深度。

Result: 通过多个计算机科学会议的数据集展示了TaxoAdapt在捕捉科学领域的结构和演变方面的性能，生成的分类法在保持颗粒度的方面提升了26.51%，在连贯性上提升了50.41%。

Conclusion: TaxoAdapt能够有效解决当前自动分类方法的不足之处，其生成的分类法在颗粒度和连贯性上显著优于当前最具竞争力的基准。

Abstract: The rapid evolution of scientific fields introduces challenges in organizing
and retrieving scientific literature. While expert-curated taxonomies have
traditionally addressed this need, the process is time-consuming and expensive.
Furthermore, recent automatic taxonomy construction methods either (1)
over-rely on a specific corpus, sacrificing generalizability, or (2) depend
heavily on the general knowledge of large language models (LLMs) contained
within their pre-training datasets, often overlooking the dynamic nature of
evolving scientific domains. Additionally, these approaches fail to account for
the multi-faceted nature of scientific literature, where a single research
paper may contribute to multiple dimensions (e.g., methodology, new tasks,
evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a
framework that dynamically adapts an LLM-generated taxonomy to a given corpus
across multiple dimensions. TaxoAdapt performs iterative hierarchical
classification, expanding both the taxonomy width and depth based on corpus'
topical distribution. We demonstrate its state-of-the-art performance across a
diverse set of computer science conferences over the years to showcase its
ability to structure and capture the evolution of scientific fields. As a
multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more
granularity-preserving and 50.41% more coherent than the most competitive
baselines judged by LLMs.

</details>


### [43] [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/abs/2506.10766)
*Diana Abagyan,Alejandro R. Salamanca,Andres Felipe Cruz-Salinas,Kris Cao,Hangyu Lin,Acyr Locatelli,Marzieh Fadaee,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 通过使用覆盖更多语言的通用分词器提高模型对新语言的适应能力，显著提升语言塑性，对主要预训语言性能无明显影响。


<details>
  <summary>Details</summary>
Motivation: 解决因语言模型的容量有限、数据和计算资源稀缺而导致的大规模多语言预训练挑战，并提升模型在新语言上的适应能力。

Method: 通过设计通用分词器，其针对的语言比主要预训练语言更多，从而在预训练后能更高效地扩展语言覆盖范围。

Result: 使用通用分词器在跨越不同语言组和训练策略的系统实验中显示出高达20.2%的胜率提升。此外，对于在分词器和预训练中完全未见的语言，其塑性提高可达5%。

Conclusion: 使用通用分词器可以显著提高大语言模型对于新语言的适应能力，同时对训练中包含的主要语言的性能几乎没有影响。

Abstract: Pretraining massively multilingual Large Language Models (LLMs) for many
languages at once is challenging due to limited model capacity, scarce
high-quality data, and compute constraints. Moreover, the lack of language
coverage of the tokenizer makes it harder to address the gap for new languages
purely at the post-training stage. In this work, we study what relatively cheap
interventions early on in training improve "language plasticity", or adaptation
capabilities of the model post-training to new languages. We focus on tokenizer
design and propose using a universal tokenizer that is trained for more
languages than the primary pretraining languages to enable efficient adaptation
in expanding language coverage after pretraining. Our systematic experiments
across diverse groups of languages and different training strategies show that
a universal tokenizer enables significantly higher language adaptation, with up
to 20.2% increase in win rates compared to tokenizers specific to pretraining
languages. Furthermore, a universal tokenizer also leads to better plasticity
towards languages that are completely unseen in the tokenizer and pretraining,
by up to 5% win rate gain. We achieve this adaptation to an expanded set of
languages with minimal compromise in performance on the majority of languages
included in pretraining.

</details>


### [44] [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/abs/2506.10769)
*Alberto Testoni,Iacer Calixto*

Main category: cs.CL

TL;DR: 在临床多选问题中，评估了不同不确定性估计方法，发现基于行为信号的简单估计方法接近复杂方法的性能，结果因专科和问题类型而异。


<details>
  <summary>Details</summary>
Motivation: 为临床决策支持等高风险领域部署大型语言模型时，准确且良好校准的不确定性估计至关重要。

Method: 比较标准的单生成和基于采样的方法，并进行案例研究，探讨基于推理过程行为信号的简单单遍估计器。

Result: 这些轻量级的方法在性能上接近语义熵估计，但仅需一次生成。结果显示各专科和问题类型之间的表现存在较大差异。

Conclusion: 在临床多选问题回答任务中，不同不确定性估计方法的表现存在显著差异，这表明选择模型时需要考虑问题的性质和模型的特定优势。

Abstract: Accurate and well-calibrated uncertainty estimates are essential for
deploying large language models (LLMs) in high-stakes domains such as clinical
decision support. We present a fine-grained evaluation of uncertainty
estimation methods for clinical multiple-choice question answering, covering
ten open-source LLMs (general-purpose, biomedical, and reasoning models) across
two datasets, eleven medical specialties, and six question types. We compare
standard single-generation and sampling-based methods, and present a case study
exploring simple, single-pass estimators based on behavioral signals in
reasoning traces. These lightweight methods approach the performance of
Semantic Entropy while requiring only one generation. Our results reveal
substantial variation across specialties and question types, underscoring the
importance of selecting models based on both the nature of the question and
model-specific strengths.

</details>


### [45] [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/abs/2506.10779)
*Viet Anh Trinh,Xinlu He,Jacob Whitehill*

Main category: cs.CL

TL;DR: 该论文提出了利用大语言模型修正ASR命名实体错误的方法，在MIT数据集上实现了30%的WER相对减少。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统在命名实体上的识别错误率较高，影响下游应用，需要提高命名实体的识别准确性。

Method: 利用大语言模型(LLM)的推理能力和局部上下文（如讲义）的正确命名实体集进行修正。

Result: 在NER-MIT-OpenCourseWare数据集上，所提技术对命名实体的WER减少了高达30%。

Conclusion: 引入大语言模型(LLM)修正机制，显著降低了ASR在命名实体上的WER，提高了识别准确性。

Abstract: With recent advances in modeling and the increasing amount of supervised
training data, automatic speech recognition (ASR) systems have achieved
remarkable performance on general speech. However, the word error rate (WER) of
state-of-the-art ASR remains high for named entities. Since named entities are
often the most critical keywords, misrecognizing them can affect all downstream
applications, especially when the ASR system functions as the front end of a
complex system. In this paper, we introduce a large language model (LLM)
revision mechanism to revise incorrect named entities in ASR predictions by
leveraging the LLM's reasoning ability as well as local context (e.g., lecture
notes) containing a set of correct named entities. Finally, we introduce the
NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses
for development and testing. On this dataset, our proposed technique achieves
up to 30\% relative WER reduction for named entities.

</details>


### [46] [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/abs/2506.10800)
*Wei Sun,Tingyu Qu,Mingxiao Li,Jesse Davis,Marie-Francine Moens*

Main category: cs.CL

TL;DR: 提出LangEdit框架，通过投影语言更新到正交补空间，避免参数干扰，实现有效多语言知识更新。


<details>
  <summary>Details</summary>
Motivation: 解决在大语言模型中更新多语言知识时的参数干扰问题，提高多语言知识更新的效率和准确性。

Method: 提出LangEdit框架，使用零空间约束技术，将每种语言的参数更新投影到之前更新子空间的正交补上。

Result: LangEdit在三个模型架构、六种语言和四个下游任务上的评估中超越了现有最先进的编辑方法，有效缓解了参数干扰。

Conclusion: LangEdit框架可以有效促成大语言模型中的多语言知识更新，并减少更新过程中参数干扰的问题。

Abstract: Efficiently updating multilingual knowledge in large language models (LLMs),
while preserving consistent factual representations across languages, remains a
long-standing and unresolved challenge. While deploying separate editing
systems for each language might seem viable, this approach incurs substantial
costs due to the need to manage multiple models. A more efficient solution
involves integrating knowledge updates across all languages into a unified
model. However, performing sequential edits across languages often leads to
destructive parameter interference, significantly degrading multilingual
generalization and the accuracy of injected knowledge. To address this
challenge, we propose LangEdit, a novel null-space constrained framework
designed to precisely isolate language-specific knowledge updates. The core
innovation of LangEdit lies in its ability to project parameter updates for
each language onto the orthogonal complement of previous updated subspaces.
This approach mathematically guarantees update independence while preserving
multilingual generalization capabilities. We conduct a comprehensive evaluation
across three model architectures, six languages, and four downstream tasks,
demonstrating that LangEdit effectively mitigates parameter interference and
outperforms existing state-of-the-art editing methods. Our results highlight
its potential for enabling efficient and accurate multilingual knowledge
updates in LLMs. The code is available at
https://github.com/VRCMF/LangEdit.git.

</details>


### [47] [ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization](https://arxiv.org/abs/2506.10822)
*Zhensheng Jin,Xinze Li,Yifan Ji,Chunyi Peng,Zhenghao Liu,Qi Shi,Yukun Yan,Shuo Wang,Furong Peng,Ge Yu*

Main category: cs.CL

TL;DR: 提出了一种ReCUT方法，通过模型参数插值平衡推理路径的长度和准确性，减少推理长度30-50%。


<details>
  <summary>Details</summary>
Motivation: 现有的Chain-of-Thought提示方法在改善大型语言模型的推理能力时，常常因过度思考导致冗长或重复的推理路径，因此需要一种方法来平衡推理路径的准确性和长度。

Method: 提出了一种称为Reasoning Compression ThroUgh Stepwise Trials (ReCUT)的方法，通过逐步探索机制和长短切换采样策略，训练两个专门的模型，一个优化推理准确性，另一个优化推理长度。最终通过插值这两个模型的参数得到一个集成模型。

Result: 实验结果表明，ReCUT在多个数学推理数据集和主干模型上，将推理长度减少约30-50%，同时保持或提高推理准确性。

Conclusion: ReCUT方法能够显著减少推理长度，同时保持或提高推理准确性。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
improved the reasoning capabilities of Large Language Models (LLMs). However,
these methods often suffer from overthinking, leading to unnecessarily lengthy
or redundant reasoning traces. Existing approaches attempt to mitigate this
issue through curating multiple reasoning chains for training LLMs, but their
effectiveness is often constrained by the quality of the generated data and
prone to overfitting. To address the challenge, we propose Reasoning
Compression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing
the accuracy and length of reasoning trajectory. Specifically, ReCUT employs a
stepwise exploration mechanism and a long-short switched sampling strategy,
enabling LLMs to incrementally generate diverse reasoning paths. These paths
are evaluated and used to construct preference pairs to train two specialized
models (Gemini LLMs)-one optimized for reasoning accuracy, the other for
shorter reasoning. A final integrated model is obtained by interpolating the
parameters of these two models. Experimental results across multiple math
reasoning datasets and backbone models demonstrate that ReCUT significantly
reduces reasoning lengths by approximately 30-50%, while maintaining or
improving reasoning accuracy compared to various baselines. All codes and data
will be released via https://github.com/NEUIR/ReCUT.

</details>


### [48] [CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training](https://arxiv.org/abs/2506.10844)
*Alireza Salemi,Mukta Maddipatla,Hamed Zamani*

Main category: cs.CL

TL;DR: The mRAG framework with specialized agents shows improved performance in RAG tasks and offers effective solutions for complex real-world challenges.


<details>
  <summary>Details</summary>
Motivation: To optimize inter-agent collaboration and enhance response generation in retrieval-augmented generation tasks.

Method: mRAG is designed with specialized agents for subtasks like planning, searching, reasoning, and coordination, using a self-training paradigm with reward-guided trajectory sampling.

Result: mRAG demonstrates superior performance over conventional RAG systems in evaluated datasets and showcases strengths through case studies.

Conclusion: mRAG outperforms conventional RAG baselines and is effective for complex real-world RAG tasks.

Abstract: This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)
framework composed of specialized agents for subtasks such as planning,
searching, reasoning, and coordination. Our system uses a self-training
paradigm with reward-guided trajectory sampling to optimize inter-agent
collaboration and enhance response generation. Evaluated on DataMorgana-derived
datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms
conventional RAG baselines. We further analyze competition outcomes and
showcase the framework's strengths with case studies, demonstrating its
efficacy for complex, real-world RAG tasks.

</details>


### [49] [Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles](https://arxiv.org/abs/2506.10848)
*Qingyan Wei,Yaojie Zhang,Zhiyuan Liu,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了SlowFast动态采样策略，为dLLMs提高推理速度，并结合缓存实现显著加速，性能优于自回归模型。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归LLMs在推理速度方面存在延迟，而扩散式语言模型（dLLMs）提供了并行生成的可能性，但现有的采样策略效率和灵活性不足。

Method: 提出了SlowFast采样策略，这是一种动态采样策略，通过在探索性和加速解码阶段之间自适应切换，提高解码效率。该策略遵循三个黄金原则：确定性原则、收敛原则和位置原则，并结合dLLM缓存以减少重复计算。

Result: SlowFast采样策略在基准测试和模型上实现了高达15.63倍的加速，结合缓存时更达到34.22倍的加速，且准确性损失最小。在吞吐量方面超过了强自回归基线模型，如LLaMA3 8B。

Conclusion: 慢快采样策略通过设计良好的采样机制，充分发挥了dLLMs的潜力，实现了快速且高质量的生成。

Abstract: Diffusion-based language models (dLLMs) have emerged as a promising
alternative to traditional autoregressive LLMs by enabling parallel token
generation and significantly reducing inference latency. However, existing
sampling strategies for dLLMs, such as confidence-based or semi-autoregressive
decoding, often suffer from static behavior, leading to suboptimal efficiency
and limited flexibility. In this paper, we propose SlowFast Sampling, a novel
dynamic sampling strategy that adaptively alternates between exploratory and
accelerated decoding stages. Our method is guided by three golden principles:
certainty principle, convergence principle, and positional principle, which
govern when and where tokens can be confidently and efficiently decoded. We
further integrate our strategy with dLLM-Cache to reduce redundant computation.
Extensive experiments across benchmarks and models show that SlowFast Sampling
achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and
up to 34.22$\times$ when combined with caching. Notably, our approach
outperforms strong autoregressive baselines like LLaMA3 8B in throughput,
demonstrating that well-designed sampling can unlock the full potential of
dLLMs for fast and high-quality generation.

</details>


### [50] [Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models](https://arxiv.org/abs/2506.10855)
*Michele Gubian,Ioana Krehan,Oli Liu,James Kirby,Sharon Goldwater*

Main category: cs.CL

TL;DR: 研究wav2vec2在不同语言环境中信息编码方式，发现其表示结构独立于预训练语音材料。


<details>
  <summary>Details</summary>
Motivation: 探讨自监督语音模型在不同语言环境中的信息编码方式。

Method: 使用探测分类器和几何分析研究wav2vec2模型在不同语言环境下如何编码音位、词汇声调以及说话人信息。

Result: 发现所有预训练和测试语言中，编码音位、声调和说话人的子空间大部分是正交的，并且各层的探测准确率模式相似。

Conclusion: wav2vec2模型学习到的表示结构大体上与预训练时使用的语音材料无关。

Abstract: Analyses of self-supervised speech models have begun to reveal where and how
they represent different types of information. However, almost all analyses
have focused on English. Here, we examine how wav2vec2 models trained on four
different languages encode both language-matched and non-matched speech. We use
probing classifiers and geometric analyses to examine how phones, lexical
tones, and speaker information are represented. We show that for all
pretraining and test languages, the subspaces encoding phones, tones, and
speakers are largely orthogonal, and that layerwise patterns of probing
accuracy are similar, with a relatively small advantage for matched-language
phone and tone (but not speaker) probes in the later layers. Our findings
suggest that the structure of representations learned by wav2vec2 is largely
independent of the speech material used during pretraining.

</details>


### [51] [Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment](https://arxiv.org/abs/2506.10877)
*Hongda Sun,Jiaren Peng,Wenzhong Yang,Liang He,Bo Du,Rui Yan*

Main category: cs.CL

TL;DR: MedRef, a new medical dialogue system, improves knowledge selection and response accuracy through advanced filtering and adaptive prompts, excelling in benchmark tests.


<details>
  <summary>Details</summary>
Motivation: Existing medical dialogue systems struggle with identifying relevant medical knowledge and generating personalized, medically accurate responses, which MedRef aims to improve.

Method: MedRef employs a knowledge refining mechanism to filter irrelevant data and uses a comprehensive prompt structure. It incorporates two modules, Triplet Filter and Demo Selector, to allow real-time adaptability in conversations.

Result: The MedRef system has outperformed state-of-the-art baselines on the MedDG and KaMed benchmarks, indicating its enhanced generation quality and medical entity accuracy.

Conclusion: MedRef system shows significant improvements over existing MDS in terms of response quality and accuracy of medical entities, demonstrating its potential for effective application in real-world healthcare.

Abstract: Medical dialogue systems (MDS) have emerged as crucial online platforms for
enabling multi-turn, context-aware conversations with patients. However,
existing MDS often struggle to (1) identify relevant medical knowledge and (2)
generate personalized, medically accurate responses. To address these
challenges, we propose MedRef, a novel MDS that incorporates knowledge refining
and dynamic prompt adjustment. First, we employ a knowledge refining mechanism
to filter out irrelevant medical data, improving predictions of critical
medical entities in responses. Additionally, we design a comprehensive prompt
structure that incorporates historical details and evident details. To enable
real-time adaptability to diverse patient conditions, we implement two key
modules, Triplet Filter and Demo Selector, providing appropriate knowledge and
demonstrations equipped in the system prompt. Extensive experiments on MedDG
and KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in
both generation quality and medical entity accuracy, underscoring its
effectiveness and reliability for real-world healthcare applications.

</details>


### [52] [Slimming Down LLMs Without Losing Their Minds](https://arxiv.org/abs/2506.10885)
*Qingda,Mai*

Main category: cs.CL

TL;DR: 本文研究了微调对大型语言模型性能的影响，表明LoRA方法有效提高性能，同时保持计算效率，并强调数据集与任务之间的一致性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究和验证微调对大型语言模型性能的影响，并探索参数高效的方法，以提供理论洞察和实际指导。

Method: 我们评估了模型在三个主要领域中的能力：常识推理（HellaSwag）、数学推理（GSM8K）和多领域知识（MMLU-CS）。

Result: LoRA方法有效地提高了特定任务的性能，同时保持了计算效率。性能在很大程度上依赖于微调数据集与基准任务之间的一致性。

Conclusion: 通过研究，验证了微调对大型语言模型性能的影响，特别是参数高效的方法（LoRA和QLoRA）。

Abstract: This paper investigates and validates the impact of fine-tuning on large
language model performance, focusing on parameter-efficient methods (LoRA and
QLoRA). We evaluate model capabilities across three key domains: (1)
commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)
multi-domain knowledge (MMLU-CS).
  Our findings demonstrate that: (1) LoRA-based methods effectively improve
task-specific performance while maintaining computational efficiency, and (2)
performance strongly depends on alignment between fine-tuning dataset and
benchmark tasks. The study provides both theoretical insights into
parameter-efficient mechanisms and practical guidance for developers
implementing efficient LLM adaptation with limited resources.

</details>


### [53] [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
*Yixiao Huang,Hanlin Zhu,Tianyu Guo,Jiantao Jiao,Somayeh Sojoudi,Michael I. Jordan,Stuart Russell,Song Mei*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型在泛化和产生幻觉方面的机制，即通过概念关联进行推理的能力，提出了表外推理的理论模型及实验验证，并探讨了模型学习效率的内在结构。


<details>
  <summary>Details</summary>
Motivation: 本文旨在理解大型语言模型在泛化真实信息与幻觉错误信息生成的双重行为机制。这些行为的成因仍然知之甚少，因此需要探索其背后的机制。

Method: 作者通过实验研究了五个大型语言模型，确立表外推理机制的作用，并形式化为合成事实回忆任务。理论分析借助梯度下降的隐式偏差，展示了解这一机制的数学结构。

Result: 实验结果显示，表外推理行为在很大程度上驱动了模型的泛化和幻觉生成行为。研究指出，矩阵分解在解决合成事实回忆任务中的重要角色，以及如何通过核范数最小化的偏差影响模型能力。

Conclusion: 本文为理解模型一般化和幻觉生成行为提供了理论基础解释，即通过识别和利用概念间的关联进行推理的能力。
理论分析解释了模型为何能通过高度样本效率学习事实和推论之间的关联，无论相关性是因果的还是仅仅是虚假的。

Abstract: Large language models (LLMs) can acquire new knowledge through fine-tuning,
but this process exhibits a puzzling duality: models can generalize remarkably
from new facts, yet are also prone to hallucinating incorrect information.
However, the reasons for this phenomenon remain poorly understood. In this
work, we argue that both behaviors stem from a single mechanism known as
out-of-context reasoning (OCR): the ability to deduce implications by
associating concepts, even those without a causal link. Our experiments across
five prominent LLMs confirm that OCR indeed drives both generalization and
hallucination, depending on whether the associated concepts are causally
related. To build a rigorous theoretical understanding of this phenomenon, we
then formalize OCR as a synthetic factual recall task. We empirically show that
a one-layer single-head attention-only transformer with factorized output and
value matrices can learn to solve this task, while a model with combined
weights cannot, highlighting the crucial role of matrix factorization. Our
theoretical analysis shows that the OCR capability can be attributed to the
implicit bias of gradient descent, which favors solutions that minimize the
nuclear norm of the combined output-value matrix. This mathematical structure
explains why the model learns to associate facts and implications with high
sample efficiency, regardless of whether the correlation is causal or merely
spurious. Ultimately, our work provides a theoretical foundation for
understanding the OCR phenomenon, offering a new lens for analyzing and
mitigating undesirable behaviors from knowledge injection.

</details>


### [54] [BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP](https://arxiv.org/abs/2506.10896)
*Thomas Sounack,Joshua Davis,Brigitte Durieux,Antoine Chaffin,Tom J. Pollard,Eric Lehman,Alistair E. W. Johnson,Matthew McDermott,Tristan Naumann,Charlotta Lindvall*

Main category: cs.CL

TL;DR: BioClinical ModernBERT是一种领域适配的编码器，通过长文本处理和多数据集训练，在生物医学和临床NLP中性能卓越。


<details>
  <summary>Details</summary>
Motivation: 生物医学和临床领域的编码器发展缓慢，导致这些领域适应性不足，急需更高效的编码器来提取结构化信息。

Method: 在最新的ModernBERT版本基础上进行领域适配编码器，通过继续预训练和长文本处理来提升速度和性能，并利用来自不同机构、领域和地区的20个数据集进行训练。

Result: 训练得到的BioClinical ModernBERT在生物医学和临床NLP领域显著提升性能，基于53.5亿标记的最大语料库进行预训练，并超越现有编码器。

Conclusion: BioClinical ModernBERT显著优于现有生物医学和临床编码器，在四个下游任务中表现出色。

Abstract: Encoder-based transformer models are central to biomedical and clinical
Natural Language Processing (NLP), as their bidirectional self-attention makes
them well-suited for efficiently extracting structured information from
unstructured text through discriminative tasks. However, encoders have seen
slower development compared to decoder models, leading to limited domain
adaptation in biomedical and clinical settings. We introduce BioClinical
ModernBERT, a domain-adapted encoder that builds on the recent ModernBERT
release, incorporating long-context processing and substantial improvements in
speed and performance for biomedical and clinical NLP. BioClinical ModernBERT
is developed through continued pretraining on the largest biomedical and
clinical corpus to date, with over 53.5 billion tokens, and addresses a key
limitation of prior clinical encoders by leveraging 20 datasets from diverse
institutions, domains, and geographic regions, rather than relying on data from
a single source. It outperforms existing biomedical and clinical encoders on
four downstream tasks spanning a broad range of use cases. We release both base
(150M parameters) and large (396M parameters) versions of BioClinical
ModernBERT, along with training checkpoints to support further research.

</details>


### [55] [Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning](https://arxiv.org/abs/2506.10903)
*Lan Zhang,Marco Valentino,Andre Freitas*

Main category: cs.CL

TL;DR: 本文引入了一种基于LLMs的系统性方法，通过细粒度的评估标准提升对自动形式化任务的评估效果，实验验证了其在高级数学中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着领域复杂性的增加，自动评估数学推理自动化变得困难，尤其是在高级数学中，人类评估需要大量时间和领域专业知识。现有的评估方法通常采用粗粒度的通用评价标准，限制了它们在高级形式数学推理中的有效性。

Method: 引入了一种系统的自动化方法来评估自动形式化任务，该方法基于一个由大型语言模型（LLMs）组成的知识和形式基础的评估系统（EFG ensemble），其评价标准包括逻辑保留、数学一致性、形式有效性和形式质量。

Result: 实验表明，与粗粒度的模型相比，EFG评估系统与人类评估的相关性更强，特别是在评估形式质量时。这表明，在精心定义的原子属性指导下，LLM-作为评估者可以提供一个可扩展、可解释且可靠的支持。

Conclusion: 具有精细衡量标准的LLM评估系统可以作为自动形式化评估的有效代理，为评估形式数学推理提供可靠的支持。

Abstract: Autoformalization plays a crucial role in formal mathematical reasoning by
enabling the automatic translation of natural language statements into formal
languages. While recent advances using large language models (LLMs) have shown
promising results, methods for automatically evaluating autoformalization
remain underexplored. As one moves to more complex domains (e.g., advanced
mathematics), human evaluation requires significant time and domain expertise,
especially as the complexity of the underlying statements and background
knowledge increases. LLM-as-a-judge presents a promising approach for
automating such evaluation. However, existing methods typically employ
coarse-grained and generic evaluation criteria, which limit their effectiveness
for advanced formal mathematical reasoning, where quality hinges on nuanced,
multi-granular dimensions. In this work, we take a step toward addressing this
gap by introducing a systematic, automatic method to evaluate autoformalization
tasks. The proposed method is based on an epistemically and formally grounded
ensemble (EFG) of LLM judges, defined on criteria encompassing logical
preservation (LP), mathematical consistency (MC), formal validity (FV), and
formal quality (FQ), resulting in a transparent assessment that accounts for
different contributing factors. We validate the proposed framework to serve as
a proxy for autoformalization assessment within the domain of formal
mathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM
judges is a suitable emerging proxy for evaluation, more strongly correlating
with human assessments than a coarse-grained model, especially when assessing
formal qualities. These findings suggest that LLM-as-judges, especially when
guided by a well-defined set of atomic properties, could offer a scalable,
interpretable, and reliable support for evaluating formal mathematical
reasoning.

</details>


### [56] [Magistral](https://arxiv.org/abs/2506.10910)
*Mistral-AI,:,Abhinav Rastogi,Albert Q. Jiang,Andy Lo,Gabrielle Berrada,Guillaume Lample,Jason Rute,Joep Barmentlo,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Léonard Blier,Lucile Saulnier,Matthieu Dinot,Maxime Darrin,Neha Gupta,Roman Soletskyi,Sagar Vaze,Teven Le Scao,Yihan Wang,Adam Yang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Andy Ehrenberg,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jean-Hadrien Chabran,Jean-Malo Delignon,Joachim Studnia,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Kush Jain,Lingxiao Zhao,Louis Martin,Luyu Gao,Lélio Renard Lavaud,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Maximilian Augustin,Mickaël Seznec,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Romain Sauvestre,Rémi Delacourt,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yunhao Tang*

Main category: cs.CL

TL;DR: 研究了Magistral，Mistral的第一个推理模型，通过自有的RL管道探索语言模型的纯RL训练极限，并展示了该训练有助于多模态理解等能力。


<details>
  <summary>Details</summary>
Motivation: 希望技术基础设施和模型完全自有化，并探索纯RL在大型语言模型上的应用极限。

Method: 采用全新的从零开始的策略，基于我们自己的模型和基础设施，而非依赖现有的RL实现或先前模型的RL跟踪。

Result: 我们发现基于文本的RL可以保持或提高多模态理解、指令执行和函数调用能力。我们发布了Magistral Medium，仅通过RL训练在Mistral Medium 3基础上进行推理，并开源了Magistral Small。

Conclusion: 我们展示了一种能够探索LLM纯RL训练极限的模型堆栈，并且提出了一种简单的方法来强化模型的推理语言。

Abstract: We introduce Magistral, Mistral's first reasoning model and our own scalable
reinforcement learning (RL) pipeline. Instead of relying on existing
implementations and RL traces distilled from prior models, we follow a ground
up approach, relying solely on our own models and infrastructure. Notably, we
demonstrate a stack that enabled us to explore the limits of pure RL training
of LLMs, present a simple method to force the reasoning language of the model,
and show that RL on text data alone maintains most of the initial checkpoint's
capabilities. We find that RL on text maintains or improves multimodal
understanding, instruction following and function calling. We present Magistral
Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we
open-source Magistral Small (Apache 2.0) which further includes cold-start data
from Magistral Medium.

</details>


### [57] [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
*Or Shafran,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 该论文通过SNMF改进了LLMs的机制解释，优于早期的SAEs和一些监督方法。


<details>
  <summary>Details</summary>
Motivation: 由于早期研究集中在个体神经元上，但发现神经元通常编码多个概念，激励着研究转向分析激活空间中的方向，寻找能够以无监督方式捕获可解释特征的方向。

Method: 通过半非负矩阵分解(SNMF)直接分解MLP激活，使所学习的特征成为稀疏线性组合的共同激活神经元，并映射到其激活输入，从而使其直接可解释。

Result: SNMF衍生特征在因果控制上表现优于SAEs和一个强监督基线，并与人类可解释概念一致。同时发现特定神经元组合在语义相关特征中被重复使用，展示了MLP激活空间中的层级结构。

Conclusion: SNMF提供了一种简单有效的工具来识别可解释特征和剖析大语言模型中的概念表示。

Abstract: A central goal for mechanistic interpretability has been to identify the
right units of analysis in large language models (LLMs) that causally explain
their outputs. While early work focused on individual neurons, evidence that
neurons often encode multiple concepts has motivated a shift toward analyzing
directions in activation space. A key question is how to find directions that
capture interpretable features in an unsupervised manner. Current methods rely
on dictionary learning with sparse autoencoders (SAEs), commonly trained over
residual stream activations to learn directions from scratch. However, SAEs
often struggle in causal evaluations and lack intrinsic interpretability, as
their learning is not explicitly tied to the computations of the model. Here,
we tackle these limitations by directly decomposing MLP activations with
semi-nonnegative matrix factorization (SNMF), such that the learned features
are (a) sparse linear combinations of co-activated neurons, and (b) mapped to
their activating inputs, making them directly interpretable. Experiments on
Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs
and a strong supervised baseline (difference-in-means) on causal steering,
while aligning with human-interpretable concepts. Further analysis reveals that
specific neuron combinations are reused across semantically-related features,
exposing a hierarchical structure in the MLP's activation space. Together,
these results position SNMF as a simple and effective tool for identifying
interpretable features and dissecting concept representations in LLMs.

</details>


### [58] [Dynamic Epistemic Friction in Dialogue](https://arxiv.org/abs/2506.10934)
*Timothy Obiso,Kenneth Lai,Abhijnan Nath,Nikhil Krishnaswamy,James Pustejovsky*

Main category: cs.CL

TL;DR: 该论文探讨了在动态认知逻辑框架下的动态认知摩擦如何影响信念更新，并展示了模型对复杂对话的潜在适应能力。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型（LLMs）与人类偏好以提高人机协作的有效性。

Method: 将动态认知摩擦定位于动态认知逻辑框架内，通过情景协作任务的分析展示认知摩擦模型如何有效预测对话中的信念更新。

Result: 认知摩擦模型能够有效预测对话中的信念更新，并且信念对齐模型作为认知抵抗或摩擦的一种衡量方式，可以自然地变得更加复杂，以适应现实对话场景的复杂性。

Conclusion: 信念对齐模型衡量认知抵抗或摩擦的方式可以提升对复杂对话场景的适应能力。

Abstract: Recent developments in aligning Large Language Models (LLMs) with human
preferences have significantly enhanced their utility in human-AI collaborative
scenarios. However, such approaches often neglect the critical role of
"epistemic friction," or the inherent resistance encountered when updating
beliefs in response to new, conflicting, or ambiguous information. In this
paper, we define dynamic epistemic friction as the resistance to epistemic
integration, characterized by the misalignment between an agent's current
belief state and new propositions supported by external evidence. We position
this within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,
2011), where friction emerges as nontrivial belief-revision during the
interaction. We then present analyses from a situated collaborative task that
demonstrate how this model of epistemic friction can effectively predict belief
updates in dialogues, and we subsequently discuss how the model of belief
alignment as a measure of epistemic resistance or friction can naturally be
made more sophisticated to accommodate the complexities of real-world dialogue
scenarios.

</details>


### [59] [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training](https://arxiv.org/abs/2506.10952)
*Mozhi Zhang,Howe Tissue,Lu Wang,Xipeng Qiu*

Main category: cs.CL

TL;DR: Domain2Vec decomposes datasets into meta-domains for efficient LM pretraining, reducing computation by nearly half and improving downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To find optimal data mixtures for language model pretraining more efficiently by introducing meta-domains to capture key dataset features, thereby reducing computational costs and enhancing the scalability of previous methods.

Method: Domain2Vec maintains a vocabulary of meta-domains and uses classifiers to decompose datasets into domain vectors, representing distributions over the vocabulary. It models the relationship between domain vectors and LM performance, aligning training and validation data distributions.

Result: Experiments show that Domain2Vec reduces computation by 48.5% while achieving the same validation loss. Under the same compute budget, it improves downstream performance by 2.83% on average.

Conclusion: Domain2Vec can significantly reduce computational overhead while maintaining or even improving downstream performance by identifying optimal data mixtures for language model pretraining.

Abstract: We introduce~\textsc{Domain2Vec}, a novel approach that decomposes any
dataset into a linear combination of several \emph{meta-domains}, a new concept
designed to capture the key underlying features of datasets.
\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a
classifier to decompose any given dataset into a domain vector that corresponds
to a distribution over this vocabulary. These domain vectors enable the
identification of the optimal data mixture for language model (LM) pretraining
in a training-free manner under the \emph{\textbf{D}istribution
\textbf{A}lignment \textbf{A}ssumption} (DA$^{2}$), which suggests that when
the data distributions of the training set and the validation set are better
aligned, a lower validation loss is achieved. Moreover, \textsc{Domain2vec} can
be seamlessly integrated into previous works to model the relationship between
domain vectors and LM performance, greatly enhancing the efficiency and
scalability of previous methods. Extensive experiments demonstrate that
\textsc{Domain2Vec} helps find the data mixture that enhances downstream task
performance with minimal computational overhead. Specifically,
\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only
$51.5\%$ of the computation required when training on the original mixture of
The Pile dataset. Under equivalent compute budget, \textsc{Domain2Vec} improves
downstream performance by an average of $2.83\%$.

</details>


### [60] [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960)
*Kangwei Liu,Siyuan Cheng,Bozhong Tian,Xiaozhuan Liang,Yuyang Yin,Meng Han,Ningyu Zhang,Bryan Hooi,Xi Chen,Shumin Deng*

Main category: cs.CL

TL;DR: 该研究构建了一个专业注释的中文有害内容检测基准，并提出了知识增强的基线，使小模型性能接近顶尖的大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在有害内容检测中的应用日益增多，但针对中文的资源仍然稀缺且范围有限。

Method: 我们提出了一种知识增强的基线，结合了人工注释的知识规则和大型语言模型中的隐性知识。

Result: 我们提供了一个涵盖六个代表性类别的专业注释中文内容有害检测基准，并从中获得了支持大型语言模型的知识规则库。

Conclusion: 我们开发的中文内容有害检测基准，显著提升了小模型的性能，使其接近当前最先进的大型语言模型水平。

Abstract: Large language models (LLMs) have been increasingly applied to automated
harmful content detection tasks, assisting moderators in identifying policy
violations and improving the overall efficiency and accuracy of content review.
However, existing resources for harmful content detection are predominantly
focused on English, with Chinese datasets remaining scarce and often limited in
scope. We present a comprehensive, professionally annotated benchmark for
Chinese content harm detection, which covers six representative categories and
is constructed entirely from real-world data. Our annotation process further
yields a knowledge rule base that provides explicit expert knowledge to assist
LLMs in Chinese harmful content detection. In addition, we propose a
knowledge-augmented baseline that integrates both human-annotated knowledge
rules and implicit knowledge from large language models, enabling smaller
models to achieve performance comparable to state-of-the-art LLMs. Code and
data are available at https://github.com/zjunlp/ChineseHarm-bench.

</details>


### [61] [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/abs/2506.10979)
*Sohee Yang,Sang-Woo Lee,Nora Kassner,Daniela Gottesman,Sebastian Riedel,Mor Geva*

Main category: cs.CL

TL;DR: 研究表明推理模型可以识别不利思维，但在不利思维注入后难以恢复，需要改善自我重估能力。较小的模型受干扰较少。


<details>
  <summary>Details</summary>
Motivation: 探究推理模型能否有效进行自我重估，识别并纠正不利思维，以提高推理准确性。

Method: 通过实验研究模型如何识别和从四种无效思维中恢复：无信息性漫谈、与问题不相关的思维、错误定向问题的思维以及导致错误答案的思维。

Result: 模型可以识别多数不利思维，但在不利思维注入后难以恢复，尤其是较大的模型比较小的模型更难处理短小无关的思维。

Conclusion: 研究发现，尽管推理模型能识别不利的思维，但在这些思维注入其推理过程后，模型难以从中恢复，表现大幅下降。需要改善模型的自我重估能力，以开发更好的推理和更安全的系统。

Abstract: Recent reasoning models show the ability to reflect, backtrack, and
self-validate their reasoning, which is crucial in spotting mistakes and
arriving at accurate solutions. A natural question that arises is how
effectively models can perform such self-reevaluation. We tackle this question
by investigating how well reasoning models identify and recover from four types
of unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to
the question, thoughts misdirecting the question as a slightly different
question, and thoughts that lead to incorrect answers. We show that models are
effective at identifying most unhelpful thoughts but struggle to recover from
the same thoughts when these are injected into their thinking process, causing
significant performance drops. Models tend to naively continue the line of
reasoning of the injected irrelevant thoughts, which showcases that their
self-reevaluation abilities are far from a general "meta-cognitive" awareness.
Moreover, we observe non/inverse-scaling trends, where larger models struggle
more than smaller ones to recover from short irrelevant thoughts, even when
instructed to reevaluate their reasoning. We demonstrate the implications of
these findings with a jailbreak experiment using irrelevant thought injection,
showing that the smallest models are the least distracted by
harmful-response-triggering thoughts. Overall, our findings call for
improvement in self-reevaluation of reasoning models to develop better
reasoning and safer systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] [A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon](https://arxiv.org/abs/2506.10326)
*Cameron Angliss,Jiaxun Cui,Jiaheng Hu,Arrasy Rahman,Peter Stone*

Main category: cs.AI

TL;DR: 我们提出了VGC-Bench，一个促进Pokémon VGC领域策略泛化研究的基准工具，解决了在超大团队配置空间中策略泛化的挑战，并发现在多种团队策略条件下策略泛化仍是个开放问题。


<details>
  <summary>Details</summary>
Motivation: Pokémon VGC是一个拥有巨大团队配置空间的领域，团队建设的高度离散组合性质导致最佳策略会根据所操控的团队和对手的团队显著变化，使得泛化具有挑战性。

Method: 我们引入了VGC-Bench：一个提供关键基础设施、标准化评估协议，并提供了人类游戏数据集和一系列基线的方法，包括大型语言模型代理和行为克隆、强化学习以及自我博弈、虚构博弈和双重Oracle等经验博弈论方法。

Result: 我们广泛评估了所有基线方法在逐渐增大的团队集合上的表现，即使是在单一团队设置中表现最佳的算法在团队规模扩大时也很难扩展。

Conclusion: 在单一团队配置下训练和评估的情况下，我们的方法可以战胜专业的VGC选手，但即便是在单一团队设置中表现最佳的算法在团队规模扩大时也很难扩展。因此，跨多样团队策略的策略泛化仍然是社区面临的一个开放挑战。

Abstract: Developing AI agents that can robustly adapt to dramatically different
strategic landscapes without retraining is a central challenge for multi-agent
learning. Pok\'emon Video Game Championships (VGC) is a domain with an
extraordinarily large space of possible team configurations of approximately
$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,
combinatorial nature of team building in Pok\'emon VGC causes optimal
strategies to shift dramatically depending on both the team being piloted and
the opponent's team, making generalization uniquely challenging. To advance
research on this problem, we introduce VGC-Bench: a benchmark that provides
critical infrastructure, standardizes evaluation protocols, and supplies
human-play datasets and a range of baselines - from large-language-model agents
and behavior cloning to reinforcement learning and empirical game-theoretic
methods such as self-play, fictitious play, and double oracle. In the
restricted setting where an agent is trained and evaluated on a single-team
configuration, our methods are able to win against a professional VGC
competitor. We extensively evaluated all baseline methods over progressively
larger team sets and find that even the best-performing algorithm in the
single-team setting struggles at scaling up as team size grows. Thus, policy
generalization across diverse team strategies remains an open challenge for the
community. Our code is open sourced at
https://github.com/cameronangliss/VGC-Bench.

</details>


### [63] [A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/abs/2506.10130)
*Luciano Floridi*

Main category: cs.AI

TL;DR: 该论文提出一个关于AI系统可证明正确性与数据映射能力之间的权衡的猜想，并分析其对知识论和技术哲学的影响。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是探讨在AI系统中可证明的正确性与广泛的数据映射能力之间的根本性权衡。通过明确化这一之前隐含的权衡，并将其开放给严格的验证，重构AI的工程目标和哲学期望。

Method: 该论文使用信息论的形式陈述了这一猜想，并将其置于更广泛的知识论、形式验证和技术哲学的辩论中。随后，该论文通过对不充分确定、谨慎的知识风险和道德责任的概念进行分析，以探讨其影响和结果。

Result: 这个猜想将重塑评估标准、治理框架和混合系统设计。讨论澄清了这一猜想对技术哲学的潜在影响，尤其在道德责任和审慎知识风险方面的重要性。

Conclusion: 如果这个猜想得到验证或推翻，将对可信AI的未来产生重要影响。

Abstract: This article introduces a conjecture that formalises a fundamental trade-off
between provable correctness and broad data-mapping capacity in Artificial
Intelligence (AI) systems. When an AI system is engineered for deductively
watertight guarantees (demonstrable certainty about the error-free nature of
its outputs) -- as in classical symbolic AI -- its operational domain must be
narrowly circumscribed and pre-structured. Conversely, a system that can input
high-dimensional data to produce rich information outputs -- as in contemporary
generative models -- necessarily relinquishes the possibility of zero-error
performance, incurring an irreducible risk of errors or misclassification. By
making this previously implicit trade-off explicit and open to rigorous
verification, the conjecture significantly reframes both engineering ambitions
and philosophical expectations for AI. After reviewing the historical
motivations for this tension, the article states the conjecture in
information-theoretic form and contextualises it within broader debates in
epistemology, formal verification, and the philosophy of technology. It then
offers an analysis of its implications and consequences, drawing on notions of
underdetermination, prudent epistemic risk, and moral responsibility. The
discussion clarifies how, if correct, the conjecture would help reshape
evaluation standards, governance frameworks, and hybrid system design. The
conclusion underscores the importance of eventually proving or refuting the
inequality for the future of trustworthy AI.

</details>


### [64] [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/abs/2506.10157)
*Michelle M. Li,Ben Y. Reis,Adam Rodman,Tianxi Cai,Noa Dagan,Ran D. Balicer,Joseph Loscalzo,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出医疗AI需具备情景转换能力，能无需再训练地适应新环境与情境。


<details>
  <summary>Details</summary>
Motivation: 目前的模型在适应新环境、专科或设置时需要耗时且复杂的调整，限制了它们处理未曾遇到的临床情境的能力。

Method: 文章中提出“情景转换”理念，强调无需重新训练即可动态适应不同专科、群体及工作流程的AI模型。

Result: 文章提出了未来医疗AI的发展方向，即实现无需训练即可适应新环境的“情景转换”模型，旨在扩展医疗保健的可及性。

Conclusion: 当前的医疗基础模型在适应不同环境及临床情况时存在局限，需要通过重新训练、提示或知识库检索进行调整。

Abstract: Medical foundation models, including language models trained on clinical
notes, vision-language models on medical images, and multimodal models on
electronic health records, can summarize clinical notes, answer medical
questions, and assist in decision-making. Adapting these models to new
populations, specialties, or settings typically requires fine-tuning, careful
prompting, or retrieval from knowledge bases. This can be impractical, and
limits their ability to interpret unfamiliar inputs and adjust to clinical
situations not represented during training. As a result, models are prone to
contextual errors, where predictions appear reasonable but fail to account for
critical patient-specific or contextual information. These errors stem from a
fundamental limitation that current models struggle with: dynamically adjusting
their behavior across evolving contexts of medical care. In this Perspective,
we outline a vision for context-switching in medical AI: models that
dynamically adapt their reasoning without retraining to new specialties,
populations, workflows, and clinical roles. We envision context-switching AI to
diagnose, manage, and treat a wide range of diseases across specialties and
regions, and expand access to medical care.

</details>


### [65] [Correlation vs causation in Alzheimer's disease: an interpretability-driven study](https://arxiv.org/abs/2506.10179)
*Hamzah Dabool,Raghad Mustafa*

Main category: cs.AI

TL;DR: 研究通过结合XGBoost和SHAP值，分析阿尔茨海默病中的关键特征与其贡献，揭示了因果关系与关联关系的区别，强调需要慎重解读相关数据。


<details>
  <summary>Details</summary>
Motivation: 在阿尔茨海默病研究中，因果关系与关联关系的区别影响疾病的诊断、治疗以及病因识别，因此需要清晰区分并理解这些关系。

Method: 研究采用了相关分析、机器学习分类和模型解释技术。使用了XGBoost算法来识别影响阿尔茨海默病分类的关键特征，并结合SHAP值来解析特征在疾病各个阶段的贡献。

Result: 结果显示，虽然存在强关联，但这并不一定意味着因果关系。提供了对特征贡献的深入见解，强调了谨慎解读与疾病阶段相关的数据的必要性。

Conclusion: 研究揭示了在阿尔茨海默病研究中，区分因果关系与关联关系的重要性。通过特征重要性与解释性的方法，研究为未来揭示疾病机制的因果推断研究打下了基础。

Abstract: Understanding the distinction between causation and correlation is critical
in Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and
the identification of true disease drivers. This experiment investigates the
relationships among clinical, cognitive, genetic, and biomarker features using
a combination of correlation analysis, machine learning classification, and
model interpretability techniques. Employing the XGBoost algorithm, we
identified key features influencing AD classification, including cognitive
scores and genetic risk factors. Correlation matrices revealed clusters of
interrelated variables, while SHAP (SHapley Additive exPlanations) values
provided detailed insights into feature contributions across disease stages.
Our results highlight that strong correlations do not necessarily imply
causation, emphasizing the need for careful interpretation of associative data.
By integrating feature importance and interpretability with classical
statistical analysis, this work lays groundwork for future causal inference
studies aimed at uncovering true pathological mechanisms. Ultimately,
distinguishing causal factors from correlated markers can lead to improved
early diagnosis and targeted interventions for Alzheimer's disease.

</details>


### [66] [Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems](https://arxiv.org/abs/2506.10192)
*Filip Cano*

Main category: cs.AI

TL;DR: 本文探讨了AI系统在安全、公平、透明和问责方面的进展，提出多种技术和框架提升AI系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 在人工智能日益影响重要社会领域的背景下，确保其负责任的使用变得刻不容缓，推动AI系统在安全性、公平性、透明度和问责性上的知识发展。

Method: 引入延迟观测下的确定性屏蔽技术，并实施在模拟自动驾驶车辆的确定性和概率安全屏障，提出公平屏障的后处理方法，以及评估智能体意图行为的正式框架，最后通过“反应性决策”框架统一这些贡献。

Result: 延伸了经典的确定性屏蔽技术，使其在延迟观测下依然具有韧性；将安全屏障应用于模拟自动驾驶车辆防止碰撞；引入在有限和周期时间范围内的公平屏障；提出评估智能体行为的定量指标；提供统一的反应性决策框架。

Conclusion: 本文对AI系统的安全性、公平性、透明度和问责性进行了深入研究，为可信AI的发展奠定了坚实的基础。

Abstract: Ensuring responsible use of artificial intelligence (AI) has become
imperative as autonomous systems increasingly influence critical societal
domains. However, the concept of trustworthy AI remains broad and
multi-faceted. This thesis advances knowledge in the safety, fairness,
transparency, and accountability of AI systems. In safety, we extend classical
deterministic shielding techniques to become resilient against delayed
observations, enabling practical deployment in real-world conditions. We also
implement both deterministic and probabilistic safety shields into simulated
autonomous vehicles to prevent collisions with road users, validating the use
of these techniques in realistic driving simulators. We introduce fairness
shields, a novel post-processing approach to enforce group fairness in
sequential decision-making settings over finite and periodic time horizons. By
optimizing intervention costs while strictly ensuring fairness constraints,
this method efficiently balances fairness with minimal interference. For
transparency and accountability, we propose a formal framework for assessing
intentional behaviour in probabilistic decision-making agents, introducing
quantitative metrics of agency and intention quotient. We use these metrics to
propose a retrospective analysis of intention, useful for determining
responsibility when autonomous systems cause unintended harm. Finally, we unify
these contributions through the ``reactive decision-making'' framework,
providing a general formalization that consolidates previous approaches.
Collectively, the advancements presented contribute practically to the
realization of safer, fairer, and more accountable AI systems, laying the
foundations for future research in trustworthy AI.

</details>


### [67] [WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2506.10264)
*Qiyue Yin,Pei Xu,Qiaozhe Li,Shengda Liu,Shengqi Shen,Tong Wang,Yihong Han,Xiaonan Zhao,Likun Yang,Shiyue Cao,Shiyu Qiu,Yuxuan Liu,Shizhao Yu,Lei Cui,Chengxin Yan,Jie Sun,Xiangquan Tang,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出了首个以战争游戏为环境的大语言模型战略推理基准WGSR-Bench，用于评估模型在动态多智能体环境中的决策与策略适应能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理任务上性能显著提升，但其在多智能体动态环境下的战略推理能力尚未系统评估或建模。本文通过引入依托战争游戏的WGSR-Bench填补这一空白。

Method: 利用战争游戏作为评估环境，设计了WGSR-Bench战略推理基准，围绕环境态势感知、对手风险建模和政策生成三个核心任务进行测试样例设计。

Result: 设计了一种基于大语言模型的战争游戏代理，用于全面评估其战略推理能力，并通过WGSR-Bench测试样例，分析其在决策制定、意图推断及反事实推理中的表现。

Conclusion: 引入WGSR-Bench为评估大语言模型在博弈论战略推理中的能力与局限性提供了新工具，并推动大型模型驱动的战略智能研究的进步。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have led to a
qualitative leap in artificial intelligence' s performance on reasoning tasks,
particularly demonstrating remarkable capabilities in mathematical, symbolic,
and commonsense reasoning. However, as a critical component of advanced human
cognition, strategic reasoning, i.e., the ability to assess multi-agent
behaviors in dynamic environments, formulate action plans, and adapt
strategies, has yet to be systematically evaluated or modeled. To address this
gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark
for LLMs using wargame as its evaluation environment. Wargame, a quintessential
high-complexity strategic scenario, integrates environmental uncertainty,
adversarial dynamics, and non-unique strategic choices, making it an effective
testbed for assessing LLMs' capabilities in multi-agent decision-making, intent
inference, and counterfactual reasoning. WGSR-Bench designs test samples around
three core tasks, i.e., Environmental situation awareness, Opponent risk
modeling and Policy generation, which serve as the core S-POE architecture, to
systematically assess main abilities of strategic reasoning. Finally, an
LLM-based wargame agent is designed to integrate these parts for a
comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess
the strengths and limitations of state-of-the-art LLMs in game-theoretic
strategic reasoning and to advance research in large model-driven strategic
intelligence.

</details>


### [68] [Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution](https://arxiv.org/abs/2506.10281)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: The paper frames AI as a cognitive revolution, enhancing human intellect and productivity, requiring new approaches in skills and policies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to position AI as a cognitive revolution, similar to the transformative impact of written language, emphasizing its role as a driver of productivity in cognitive tasks and reshaping work and society.

Method: The paper uses a multidisciplinary perspective, combining advances in computer science with economic insights and sociological perspectives, and uses conceptual frameworks to visualize the shift from manual to cognitive productivity.

Result: AI amplifies knowledge work and necessitates a rethinking of skills, organizations, and policies.

Conclusion: AI complements human cognitive abilities, marking a new chapter in productivity evolution.

Abstract: Artificial Intelligence (AI) is reframed as a cognitive engine driving a
novel productivity revolution distinct from the Industrial Revolution's
physical thrust. This paper develops a theoretical framing of AI as a cognitive
revolution akin to written language - a transformative augmentation of human
intellect rather than another mechanized tool. We compare AI's emergence to
historical leaps in information technology to show how it amplifies knowledge
work. Examples from various domains demonstrate AI's impact as a driver of
productivity in cognitive tasks. We adopt a multidisciplinary perspective
combining computer science advances with economic insights and sociological
perspectives on how AI reshapes work and society. Through conceptual
frameworks, we visualize the shift from manual to cognitive productivity. Our
central argument is that AI functions as an engine of cognition - comparable to
how human language revolutionized knowledge - heralding a new productivity
paradigm. We discuss how this revolution demands rethinking of skills,
organizations, and policies. This paper, balancing academic rigor with clarity,
concludes that AI's promise lies in complementing human cognitive abilities,
marking a new chapter in productivity evolution.

</details>


### [69] [The Alignment Trap: Complexity Barriers](https://arxiv.org/abs/2506.10304)
*Jasper Yao*

Main category: cs.AI

TL;DR: AI safety verification becomes exponentially complex with increased system capabilities, leading to a strategic trilemma for AI development.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring AI safety as the capabilities of AI systems increase, making current safety verification methods computationally infeasible.

Method: The paper leverages computational complexity theory and formalizes the Capability-Risk Scaling (CRS) dynamic, using core theorems to demonstrate the relationship between system expressiveness and verification complexity.

Result: The study finds that AI safety verification becomes exponentially complex and coNP-complete as system capabilities increase, and safe policies occupy an insignificant portion of policy space.

Conclusion: AI development must choose between constraining system complexity for safety verification, accepting unverifiable risks, or creating new safety paradigms beyond traditional verification.

Abstract: We establish fundamental computational complexity barriers to verifying AI
safety as system capabilities scale. Our main results show that for AI systems
with expressiveness EXP$(m)$ above a critical threshold $\tau$, safety
verification requires exponential time and is coNP-complete. We formalize the
Capability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI
capability drives societal safety requirements toward perfection, creating an
inescapable tension with verification complexity. Through four core theorems,
we prove that (1) verification complexity grows exponentially with system
expressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the
policy space, (3) no finite set of alignment techniques can provide universal
coverage, and (4) robust safety properties form measure-zero sets for neural
networks. These results characterize an "intractability gap" where practical
safety requirements fall within the region of computational intractability. We
conclude by presenting a strategic trilemma: AI development must either
constrain system complexity to maintain verifiable safety, accept unverifiable
risks while scaling capabilities, or develop fundamentally new safety paradigms
beyond verification. Our work provides the first systematic
complexity-theoretic analysis of AI alignment and establishes rigorous bounds
that any safety approach must confront. A formal verification of the core
theorems in Lean4 is currently in progress.

</details>


### [70] [Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts](https://arxiv.org/abs/2506.10357)
*Zaijing Li,Yuquan Xie,Rui Shao,Gongwei Chen,Weili Guan,Dongmei Jiang,Liqiang Nie*

Main category: cs.AI

TL;DR: 通过创新的数据生成流程、专家混合架构和多模态推理增强强化学习方法，Optimus-3在Minecraft环境中取得了显著优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在开世界环境（如Minecraft）中构建具备感知、规划、行动、落地和反思能力的通才代理面临挑战。

Method: 提出了三大核心创新：知识增强的数据生成流程、用于缓解异构任务干扰的专家混合架构（MoE）以及用于增强视觉多样性推理能力的多模态推理增强强化学习方法。

Result: 实验结果表明，Optimus-3显著优于现有模型和代理。

Conclusion: Optimus-3在Minecraft环境中的多项任务上超越了现有的通用多模态大语言模型和现有最先进的代理。

Abstract: Recently, agents based on multimodal large language models (MLLMs) have
achieved remarkable progress across various domains. However, building a
generalist agent with capabilities such as perception, planning, action,
grounding, and reflection in open-world environments like Minecraft remains
challenges: insufficient domain-specific data, interference among heterogeneous
tasks, and visual diversity in open-world settings. In this paper, we address
these challenges through three key contributions. 1) We propose a
knowledge-enhanced data generation pipeline to provide scalable and
high-quality training data for agent development. 2) To mitigate interference
among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture
with task-level routing. 3) We develop a Multimodal Reasoning-Augmented
Reinforcement Learning approach to enhance the agent's reasoning ability for
visual diversity in Minecraft. Built upon these innovations, we present
Optimus-3, a general-purpose agent for Minecraft. Extensive experimental
results demonstrate that Optimus-3 surpasses both generalist multimodal large
language models and existing state-of-the-art agents across a wide range of
tasks in the Minecraft environment. Project page:
https://cybertronagent.github.io/Optimus-3.github.io/

</details>


### [71] [NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War](https://arxiv.org/abs/2506.10384)
*Jim O'Connor,Yeonghun Lee,Gary B Parker*

Main category: cs.AI

TL;DR: NeuroPAL框架结合NEAT和PAL，显著提升了星际争霸AI的训练效率和策略发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统的星际争霸AI方法依赖于基于规则的系统或监督深度学习，但在适应性和计算效率上存在限制，因此需要新的方法来提高效率和能力。

Method: 提出了NeuroPAL框架，将拓扑结构增强神经进化（NEAT）与随时学习的交错训练（PAL）相结合，通过在频繁的低保真训练和周期性的高保真评估间切换，提升进化训练的效率。

Result: PAL显著加速了学习过程，使得代理能够在大约一半的NEAT训练时间内达到竞争水平。另外，进化的代理表现出如专家人类玩家常用的策略，如代理兵营放置和防御建筑优化等。

Conclusion: 结构化评估机制如PAL可以提升神经进化在复杂实时战略环境中的可扩展性和有效性。

Abstract: StarCraft: Brood War remains a challenging benchmark for artificial
intelligence research, particularly in the domain of macromanagement, where
long-term strategic planning is required. Traditional approaches to StarCraft
AI rely on rule-based systems or supervised deep learning, both of which face
limitations in adaptability and computational efficiency. In this work, we
introduce NeuroPAL, a neuroevolutionary framework that integrates
Neuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning
(PAL) to improve the efficiency of evolutionary training. By alternating
between frequent, low-fidelity training and periodic, high-fidelity
evaluations, PAL enhances the sample efficiency of NEAT, enabling agents to
discover effective strategies in fewer training iterations. We evaluate
NeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and
compare its performance to standard NEAT-based training. Our results show that
PAL significantly accelerates the learning process, allowing the agent to reach
competitive levels of play in approximately half the training time required by
NEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as
proxy barracks placement and defensive building optimization, strategies
commonly used by expert human players. These findings suggest that structured
evaluation mechanisms like PAL can enhance the scalability and effectiveness of
neuroevolution in complex real-time strategy environments.

</details>


### [72] [Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills](https://arxiv.org/abs/2506.10387)
*Yuquan Xie,Zaijing Li,Rui Shao,Gongwei Chen,Kaiwen Zhou,Yinchuan Li,Dongmei Jiang,Liqiang Nie*

Main category: cs.AI

TL;DR: 通过引入分层多模态技能模块和技能增强蒙特卡罗树搜索算法，新提出的GUI代理Mirage-1在长时间任务中表现优异，实验结果显著优于现有代理。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理在长时间任务中表现不佳，主要是由于知识不足和离线与在线领域之间的固有差距。

Method: 提出了一种分层多模态技能（HMS）模块，通过逐步抽象轨迹到执行技能、核心技能和最终元技能，提供层次知识结构进行长时间任务规划。同时，提出了技能增强蒙特卡罗树搜索（SA-MCTS）算法，在在线树探索中有效利用离线环境中获得的技能。

Result: 实验结果表明，Mirage-1在AndroidWorld, MobileMiniWob++, Mind2Web-Live, 和AndroidLH上的表现分别比之前的代理提高了32%, 19%, 15%, 79%。

Conclusion: 通过分层多模态技能和技能增强蒙特卡罗树搜索算法，在长时间任务规划中取得了显著进步，特别是在线环境中。

Abstract: Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI
agents have yielded promising outcomes. However, these agents still struggle
with long-horizon tasks in online environments, primarily due to insufficient
knowledge and the inherent gap between offline and online domains. In this
paper, inspired by how humans generalize knowledge in open-ended environments,
we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of
insufficient knowledge. It progressively abstracts trajectories into execution
skills, core skills, and ultimately meta-skills, providing a hierarchical
knowledge structure for long-horizon task planning. To bridge the domain gap,
we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,
which efficiently leverages skills acquired in offline environments to reduce
the action search space during online tree exploration. Building on HMS, we
propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To
validate the performance of Mirage-1 in real-world long-horizon scenarios, we
constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1
outperforms previous agents by 32\%, 19\%, 15\%, and 79\% on AndroidWorld,
MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:
https://cybertronagent.github.io/Mirage-1.github.io/

</details>


### [73] [Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges](https://arxiv.org/abs/2506.10408)
*Jintao Liang,Gang Su,Huifeng Lin,You Wu,Rui Zhao,Ziyue Li*

Main category: cs.AI

TL;DR: 本文综述了推理代理RAG方法，提出了分类体系，并探讨了未来发展方向，以增强系统的灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 克服大语言模型（LLM）的知识局限性，尤其是在需要复杂推理、动态检索和多模态集成的真实场景中，传统的静态RAG系统表现不佳。

Method: 对推理代理RAG方法进行综合审查，将其分为两种主要系统：预定义推理和代理推理，并分析其各自的架构设计、推理策略和工具协调。

Result: 提出了推理代理RAG的一系列方法分类，并讨论了架构设计和推理策略，以及关键研究挑战和未来发展方向。

Conclusion: 推理代理RAG可以通过嵌入决策和适应性工具使用直接于检索过程来增强RAG系统的灵活性、鲁棒性和适用性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
overcome the knowledge limitations of Large Language Models (LLMs) by
integrating external retrieval with language generation. While early RAG
systems based on static pipelines have shown effectiveness in well-structured
tasks, they struggle in real-world scenarios requiring complex reasoning,
dynamic retrieval, and multi-modal integration. To address these challenges,
the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds
decision-making and adaptive tool use directly into the retrieval process. In
this paper, we present a comprehensive review of Reasoning Agentic RAG methods,
categorizing them into two primary systems: predefined reasoning, which follows
fixed modular pipelines to boost reasoning, and agentic reasoning, where the
model autonomously orchestrates tool interaction during inference. We analyze
representative techniques under both paradigms, covering architectural design,
reasoning strategies, and tool coordination. Finally, we discuss key research
challenges and propose future directions to advance the flexibility,
robustness, and applicability of reasoning agentic RAG systems. Our collection
of the relevant research has been organized into a
https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.

</details>


### [74] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
*Boris Sedlak,Alireza Furutanpey,Zihang Wang,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.AI

TL;DR: 本研究提出一种基于代理的自动扩展框架，通过动态调整资源和服务配置，在资源受限环境中实现最大化需求满足。


<details>
  <summary>Details</summary>
Motivation: 边缘计算由于资源限制而突破传统的自动扩展，激励更灵活的扩展行为，使用多种弹性维度。

Method: 通过比较四种不同类型的扩展代理：主动推理、深度Q网络、结构知识分析和深度主动推理，来实现动态调整硬件资源和内部服务配置。

Result: 所有代理实现了可接受的SLO性能，具有不同的收敛模式；深度Q网络从预训练中获益，结构分析迅速收敛，而深度主动推理代理结合理论基础和实际可扩展性优势。

Conclusion: 多维度代理自动扩展在边缘环境中的可行性得到验证，鼓励未来在这一研究方向上的进一步工作。

Abstract: Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [75] [OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics](https://arxiv.org/abs/2506.10481)
*Yaoming Zhu,Junxin Wang,Yiyang Li,Lin Qiu,ZongYu Wang,Jun Xu,Xuezhi Cao,Yuhuai Wei,Mingshi Wang,Xunliang Cai,Rong Ma*

Main category: cs.AI

TL;DR: 引入了高质量、私有及具挑战性的奥林匹克级信息学数据集OIBench，通过开放源码发布以推动未来LLM的代码推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着模型越来越复杂，传统算法基准测试日渐饱和，需要更具挑战性的基准来指导算法推理的未来改进。

Method: 介绍了OIBench，一个包含250个精心策划的问题的奥林匹克级信息学数据集，并通过实验展示其耐污染属性。提出时间/空间完成曲线以进行更细粒度的效率分析，并通过高级参与者评估实现人类与模型的直接比较。

Result: 实验表明，虽然开源模型落后于闭源模型，但当前的SOTA模型在正确性和效率方面已经超越了大多数人类参与者，但仍比不上规范解决方案。

Conclusion: OIBench作为一个高质量的开放源码资源发布，希望能够促进未来大型语言模型代码推理能力的提升。

Abstract: As models become increasingly sophisticated, conventional algorithm
benchmarks are increasingly saturated, underscoring the need for more
challenging benchmarks to guide future improvements in algorithmic reasoning.
This paper introduces OIBench, a high-quality, private, and challenging
olympiad-level informatics dataset comprising 250 carefully curated original
problems. We detail the construction methodology of the benchmark, ensuring a
comprehensive assessment across various programming paradigms and complexities,
and we demonstrate its contamination-resistant properties via experiments. We
propose Time/Space Completion Curves for finer-grained efficiency analysis and
enable direct human-model comparisons through high-level participant
evaluations. Our experiments reveal that while open-source models lag behind
closed-source counterparts, current SOTA models already outperform most human
participants in both correctness and efficiency, while still being suboptimal
compared to the canonical solutions. By releasing OIBench as a fully
open-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we
hope this benchmark will contribute to advancing code reasoning capabilities
for future LLMs.

</details>


### [76] [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
*Yuhao Zhou,Yiheng Wang,Xuming He,Ruoyao Xiao,Zhiwei Li,Qiantai Feng,Zijie Guo,Yuejin Yang,Hao Wu,Wenxuan Huang,Jiaqi Wei,Dan Si,Xiuqi Yao,Jia Bu,Haiwen Huang,Tianfan Fu,Shixiang Tang,Ben Fei,Dongzhan Zhou,Fenghua Ling,Yan Lu,Siqi Sun,Chenhui Li,Guanjie Zheng,Jiancheng Lv,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 提出了SFE基准来评估MLLMs的科学认知能力，实验显示当前模型在此基准上的表现较低，表明有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 目前的科学基准主要侧重评估MLLMs的知识理解能力，无法充分评估其感知和推理能力。

Method: 提出了科学家初试（SFE）基准，通过三个互联层次评估MLLMs的科学认知能力。这些层次包括科学信号感知、科学属性理解和科学比较推理。特别地，SFE包含830个专家验证的VQA对，涉及五个高价值学科的66个多模态任务。

Result: 目前最先进的GPT-o3和InternVL-3在SFE基准上的成绩仅为34.08%和26.52%。

Conclusion: 现有的多模态大语言模型在科学领域尚有很大的提升空间。

Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.

</details>


### [77] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
*Yanan Cai,Ahmed Salem,Besmira Nushi,Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan基准用于评估大语言模型逻辑规划能力，结果显示多种模型在复杂任务中有显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在逻辑规划和复杂关系结构推理能力上的表现，以支持其在生成和查询网络基础设施、知识库或业务流程等结构化关系图中的应用。

Method: LogiPlan 基准测试通过控制对象数量、关系数量和关系链最小深度，评估模型在不同难度任务中的表现。

Result: 评价了多种模型在三项任务上的表现，发现性能差异与模型规模和架构相关。模型在简单实例中表现优异，但在更复杂配置中表现欠佳。

Conclusion: 现有的增强推理模型在简单任务中表现良好，但在处理需要更深层逻辑规划的复杂任务时仍面临挑战。

Abstract: We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [78] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: 介绍了Primender序列，作为评估大规模语言模型符号推理能力的新基准。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是通过可解释的规则测试来评估大型语言模型推断隐藏规则、验证数学假设和扩展符号逻辑的能力，填补数论、人工智能和软件工程之间的空白。

Method: 设计了一个结构化的提示和评估框架，以此测试多个先进的大型语言模型，如ChatGPT、Copilot、DeepSeek、Gemini、Grok和LLaMA。在此框架下，模型需要识别底层规则、验证假设，并生成序列的下一个100,000个项。使用规则推断准确性、假设评估、序列有效性和符号解释质量等比较指标来评估模型性能。

Result: 研究结果表明，Primender序列提供了一种独特且可衡量的基准，用于评估大型语言模型在符号推理、假设测试和可扩展模式推广方面的性能。

Conclusion: 该研究提出了一种新方法来评估大规模语言模型的符号推理能力，通过测试这些模型推导规则、验证假设以及生成序列的能力，提供了一种新颖的数学构造和可重复的方法。

Abstract: This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


### [79] [Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information](https://arxiv.org/abs/2506.10613)
*Henrik Sebastian Steude,Alexander Diedrich,Ingo Pill,Lukas Moddemann,Daniel Vranješ,Oliver Niggemann*

Main category: cs.AI

TL;DR: We developed a new diagnostic method for cyber-physical systems with limited prior knowledge, using neural networks and graph algorithms, showing promising results in simulated and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a diagnostic method that operates effectively with minimal prior knowledge, addressing the challenge of obtaining detailed system models or comprehensive training data for complex cyber-physical systems.

Method: We introduce a neural network-based symptom generator for subsystem-level anomaly detection, combined with a novel graph diagnosis algorithm. This approach utilizes minimal causal relationship information between subsystems.

Result: Experiments indicate that our method identifies the true causal component in 82% of cases and reduces the search space in 73% of scenarios. Tests on the Secure Water Treatment dataset demonstrate the approach's practical potential.

Conclusion: Our approach exhibits significant potential for practical applications in large and complex cyber-physical systems, even when prior knowledge is limited.

Abstract: Diagnostic processes for complex cyber-physical systems often require
extensive prior knowledge in the form of detailed system models or
comprehensive training data. However, obtaining such information poses a
significant challenge. To address this issue, we present a new diagnostic
approach that operates with minimal prior knowledge, requiring only a basic
understanding of subsystem relationships and data from nominal operations. Our
method combines a neural network-based symptom generator, which employs
subsystem-level anomaly detection, with a new graph diagnosis algorithm that
leverages minimal causal relationship information between
subsystems-information that is typically available in practice. Our experiments
with fully controllable simulated datasets show that our method includes the
true causal component in its diagnosis set for 82 p.c. of all cases while
effectively reducing the search space in 73 p.c. of the scenarios. Additional
tests on the real-world Secure Water Treatment dataset showcase the approach's
potential for practical scenarios. Our results thus highlight our approach's
potential for practical applications with large and complex cyber-physical
systems where limited prior knowledge is available.

</details>


### [80] [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/abs/2506.10674)
*Vincenzo Colle,Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Fadhel Ayed,Merouane Debbah*

Main category: cs.AI

TL;DR: 引入TeleMath数据集评估LLM在电信数学问题中的表现，发现专用逻辑推理模型优于通用模型，并发布数据集支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在电信领域解决数学问题的能力，弥补在信号处理、网络优化和性能分析等专业领域应用研究的不足。

Method: 提出TeleMath基准数据集，并描述了QnA生成流程，包括由领域专家精心设计的问题种子。

Result: 通过评估多种开源LLM，发现专用的数学和逻辑推理模型在电信问题解决中表现优异，并已发布数据集及评估代码以促进结果重现性和后续研究。

Conclusion: 专为数学和逻辑推理设计的近期LLM模型在TeleMath数据集上表现最佳，而通用模型即使参数众多，在此任务中也面临挑战。

Abstract: The increasing adoption of artificial intelligence in telecommunications has
raised interest in the capability of Large Language Models (LLMs) to address
domain-specific, mathematically intensive tasks. Although recent advancements
have improved the performance of LLMs in general mathematical reasoning, their
effectiveness within specialized domains, such as signal processing, network
optimization, and performance analysis, remains largely unexplored. To address
this gap, we introduce TeleMath, the first benchmark dataset specifically
designed to evaluate LLM performance in solving mathematical problems with
numerical solutions in the telecommunications domain. Comprising 500
question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the
telecommunications field. This paper outlines the proposed QnAs generation
pipeline, starting from a selected seed of problems crafted by Subject Matter
Experts. The evaluation of a wide range of open-source LLMs reveals that best
performance on TeleMath is achieved by recent models explicitly designed for
mathematical or logical reasoning. In contrast, general-purpose models, even
those with a large number of parameters, often struggle with these challenges.
We have released the dataset and the evaluation code to ease result
reproducibility and support future research.

</details>


### [81] [Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL](https://arxiv.org/abs/2506.10678)
*Tom Westermann,Aljosha Köcher,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 该文介绍了一个流程，将AML文本约束形式化为SHACL约束，并通过本体验证，实现了自动自然语言解释，不需要用户掌握形式化方法。


<details>
  <summary>Details</summary>
Motivation: 现有关于AML建模的建议大多为非正式的文本约束，不能在AML内进行自动验证。

Method: 首先，将AML模型通过RML和SPARQL映射为OWL本体，然后使用大型语言模型将文本规则转换为SHACL约束，并对先前生成的AML本体进行验证。最后，SHACL验证结果被自动解释为自然语言。

Result: 结果表明，即使是复杂的建模规则也可以进行半自动检查，而无需用户理解形式化方法或本体技术。

Conclusion: 该论文展示了一种方法，可以将文本形式的制约条件形式化为可验证的约束，并自动生成自然语言解释。

Abstract: AutomationML (AML) enables standardized data exchange in engineering, yet
existing recommendations for proper AML modeling are typically formulated as
informal and textual constraints. These constraints cannot be validated
automatically within AML itself. This work-in-progress paper introduces a
pipeline to formalize and verify such constraints. First, AML models are mapped
to OWL ontologies via RML and SPARQL. In addition, a Large Language Model
translates textual rules into SHACL constraints, which are then validated
against the previously generated AML ontology. Finally, SHACL validation
results are automatically interpreted in natural language. The approach is
demonstrated on a sample AML recommendation. Results show that even complex
modeling rules can be semi-automatically checked -- without requiring users to
understand formal methods or ontology technologies.

</details>


### [82] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
*Michael Bartholomew,Joohyung Lee*

Main category: cs.AI

TL;DR: 提出了aspsmt2smt编译器，通过结合ASP和SMT技术，有效处理涉及连续变化的实数计算问题。


<details>
  <summary>Details</summary>
Motivation: 结合答案集编程和理论满意度的问题，以解决涉及连续变化的实数计算问题。

Method: 提出编译器aspsmt2smt，通过将ASPMT程序的紧致片段转化为SMT实例，以利用SMT求解器计算这些程序的稳定模型。

Result: 系统实现能够有效处理实数计算，并进行连续变化的推理。

Conclusion: 通过使用ASP grounder gringo和SMT solver z3，该系统能有效结合ASP和SMT技术，实现对连续变化的实时数值计算。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [83] [Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering](https://arxiv.org/abs/2506.10753)
*Adam Ishay,Zhun Yang,Joohyung Lee,Ilgu Kang,Dongjae Lim*

Main category: cs.AI

TL;DR: 本文提出了一种增强神经符号模型进行反事实推理的方法，并在CLEVRER和CRAFT基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 解决关于视频动态的因果和时间推理问题，尤其是神经符号模型在回答反事实问题上的局限性。

Method: 本文采用符号推理和神经感知结合的方法，通过定义因果图并使用答案集编程（ASP）来协调感知和仿真模块。

Result: 在CLEVRER挑战中，该方法显著优于现有模型；通过提供由符号因果推理指导的替代提示，在CRAFT基准测试中也提升了反事实问题的性能。

Conclusion: 该方法在两个基准测试中验证了有效性，在CLEVRER挑战中实现了最先进的性能，并在CRAFT基准测试中使用大型预训练语言模型来提升性能。

Abstract: Causal and temporal reasoning about video dynamics is a challenging problem.
While neuro-symbolic models that combine symbolic reasoning with neural-based
perception and prediction have shown promise, they exhibit limitations,
especially in answering counterfactual questions. This paper introduces a
method to enhance a neuro-symbolic model for counterfactual reasoning,
leveraging symbolic reasoning about causal relations among events. We define
the notion of a causal graph to represent such relations and use Answer Set
Programming (ASP), a declarative logic programming method, to find how to
coordinate perception and simulation modules. We validate the effectiveness of
our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves
state-of-the-art performance on the CLEVRER challenge, significantly
outperforming existing models. In the case of the CRAFT benchmark, we leverage
a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a
dynamics simulator. Our findings show that this method can further improve its
performance on counterfactual questions by providing alternative prompts
instructed by symbolic causal reasoning.

</details>


### [84] [OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems](https://arxiv.org/abs/2506.10764)
*Xiaozhe Li,Jixuan Chen,Xinyu Fang,Shengyuan Ding,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 介绍了OPT-BENCH基准及OPT-Agent框架，通过实验验证历史上下文信息可以提升大语言模型在优化问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在通过学习先前反馈来迭代优化复杂解决方案方面的能力，并评估其在大规模搜索空间优化问题上的表现。

Method: 使用OPT-Agent框架，通过生成、验证以及利用历史反馈迭代改进解决方案，模拟人类在处理复杂问题时的推理过程。

Result: 通过对6个模型家族的9种最新大语言模型进行广泛实验，发现历史上下文信息明显改善解决方案质量和收敛性。

Conclusion: 通过整合历史上下文信息可以显著提升在机器学习和NP问题上的优化表现。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in solving
diverse tasks. However, their proficiency in iteratively optimizing complex
solutions through learning from previous feedback remains insufficiently
explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark
designed to evaluate LLM agents on large-scale search space optimization
problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from
Kaggle and 10 classical NP problems, offering a diverse and challenging
environment for assessing LLM agents on iterative reasoning and solution
refinement. To enable rigorous evaluation, we introduce OPT-Agent, an
end-to-end optimization framework that emulates human reasoning when tackling
complex problems by generating, validating, and iteratively improving solutions
through leveraging historical feedback. Through extensive experiments on 9
state-of-the-art LLMs from 6 model families, we analyze the effects of
optimization iterations, temperature settings, and model architectures on
solution quality and convergence. Our results demonstrate that incorporating
historical context significantly enhances optimization performance across both
ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to
promote further research in advancing LLM-driven optimization and iterative
reasoning. Project page:
\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.

</details>


### [85] [A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models](https://arxiv.org/abs/2506.10853)
*Yu Zhang,Yang Hu,De Wang*

Main category: cs.AI

TL;DR: 引入结合思维链推理和模型上下文协议的框架提升大型语言模型模拟人类时空行为的能力，实验显示其在生成质量和效率方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则和统计的方法在计算成本、泛化能力和可扩展性方面存在问题，而大型语言模型虽然有潜力，但在时空推理能力上有不足。

Method: 本文提出一种新的框架，结合思维链推理（CoT）与模型上下文协议（MCP），通过五阶段认知框架和六类专门MCP工具的结合，来增强大型语言模型模拟时空行为的能力。

Result: 在上海陆家嘴地区的实验验证了该框架的有效性，生成的样本与真实数据的相似性高，质量得分为7.86到8.36。并行处理实验显示生成效率提升，实现从1.30到0.17分钟的缩短。

Conclusion: 该研究通过引入结合CoT推理和MCP的方法，显著提升了大型语言模型在城市人类行为模拟中的效率和准确性，为城市计算及合成移动数据生成提供了实用方法。

Abstract: Human spatiotemporal behavior simulation is critical for urban planning
research, yet traditional rule-based and statistical approaches suffer from
high computational costs, limited generalizability, and poor scalability. While
large language models (LLMs) show promise as "world simulators," they face
challenges in spatiotemporal reasoning including limited spatial cognition,
lack of physical constraint understanding, and group homogenization tendencies.
This paper introduces a framework integrating chain-of-thought (CoT) reasoning
with Model Context Protocol (MCP) to enhance LLMs' capability in simulating
spatiotemporal behaviors that correspond with validation data patterns. The
methodology combines human-like progressive reasoning through a five-stage
cognitive framework with comprehensive data processing via six specialized MCP
tool categories: temporal management, spatial navigation, environmental
perception, personal memory, social collaboration, and experience evaluation.
Experiments in Shanghai's Lujiazui district validate the framework's
effectiveness across 1,000 generated samples. Results demonstrate high
similarity with real mobile signaling data, achieving generation quality scores
of 7.86 to 8.36 across different base models. Parallel processing experiments
show efficiency improvements, with generation times decreasing from 1.30 to
0.17 minutes per sample when scaling from 2 to 12 processes. This work
contributes to integrating CoT reasoning with MCP for urban behavior modeling,
advancing LLMs applications in urban computing and providing a practical
approach for synthetic mobility data generation. The framework offers a
foundation for smart city planning, transportation forecasting, and
participatory urban design applications.

</details>


### [86] [GenPlanX. Generation of Plans and Execution](https://arxiv.org/abs/2506.10897)
*Daniel Borrajo,Giuseppe Canonaco,Tomás de la Rosa,Alfredo Garrachón,Sriram Gopalakrishnan,Simerjot Kaur,Marianela Morales,Sunandita Patra,Alberto Pozanco,Keshav Ramani,Charese Smiley,Pietro Totis,Manuela Veloso*

Main category: cs.AI

TL;DR: GenPlanX integrates LLMs with AI planning for natural language task handling, proving effective in office task assistance.


<details>
  <summary>Details</summary>
Motivation: Classical AI Planning lacks natural language understanding capability, whereas LLMs can interpret human intents effectively.

Method: Integrating LLMs for natural language-based task description with a classical AI planning engine and an execution/monitoring framework.

Result: GenPlanX effectively assists users with office tasks, streamlining workflows and enhancing productivity.

Conclusion: GenPlanX demonstrates efficacy in assisting with office-related tasks and enhancing productivity.

Abstract: Classical AI Planning techniques generate sequences of actions for complex
tasks. However, they lack the ability to understand planning tasks when
provided using natural language. The advent of Large Language Models (LLMs) has
introduced novel capabilities in human-computer interaction. In the context of
planning tasks, LLMs have shown to be particularly good in interpreting human
intents among other uses. This paper introduces GenPlanX that integrates LLMs
for natural language-based description of planning tasks, with a classical AI
planning engine, alongside an execution and monitoring framework. We
demonstrate the efficacy of GenPlanX in assisting users with office-related
tasks, highlighting its potential to streamline workflows and enhance
productivity through seamless human-AI collaboration.

</details>


### [87] [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/abs/2506.10912)
*Fei Lin,Ziyang Gong,Cong Wang,Yonglin Tian,Tengchao Zhang,Xue Yang,Gen Luo,Fei-Yue Wang*

Main category: cs.AI

TL;DR: ToxiMol定义了分子毒性修复的基准测试任务，通过评价框架ToxiEval对现有模型进行系统评估，显示出模型在毒性理解和结构编辑上的潜力。


<details>
  <summary>Details</summary>
Motivation: 毒性问题是药物开发早期阶段失败的主要原因之一。修复分子毒性是一个重要任务，但该任务尚未系统定义或进行基准测试。

Method: 引入ToxiMol作为通用多模态大语言模型的基准任务，构建标准化数据集，并设计了以提示注释管道和自动化评估框架。评估框架包括毒性端点预测、合成可行性、药物相似性和结构相似性。

Result: 当前的多模态大语言模型在分子毒性修复任务中仍面临很大挑战，但在毒性理解、语义约束遵守和基于结构的分子编辑方面开始展现出有希望的能力。

Conclusion: 这项研究为分子毒性修复定义了一个系统化的基准测试任务，尽管现有模型面临挑战，但它们显示出潜力。

Abstract: Toxicity remains a leading cause of early-stage drug development failure.
Despite advances in molecular design and property prediction, the task of
molecular toxicity repair - generating structurally valid molecular
alternatives with reduced toxicity - has not yet been systematically defined or
benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task
for general-purpose Multimodal Large Language Models (MLLMs) focused on
molecular toxicity repair. We construct a standardized dataset covering 11
primary tasks and 560 representative toxic molecules spanning diverse
mechanisms and granularities. We design a prompt annotation pipeline with
mechanism-aware and task-adaptive capabilities, informed by expert
toxicological knowledge. In parallel, we propose an automated evaluation
framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic
accessibility, drug-likeness, and structural similarity into a high-throughput
evaluation chain for repair success. We systematically assess nearly 30
mainstream general-purpose MLLMs and design multiple ablation studies to
analyze key factors such as evaluation criteria, candidate diversity, and
failure attribution. Experimental results show that although current MLLMs
still face significant challenges on this task, they begin to demonstrate
promising capabilities in toxicity understanding, semantic constraint
adherence, and structure-aware molecule editing.

</details>


### [88] [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/abs/2506.10947)
*Rulin Shao,Shuyue Stella Li,Rui Xin,Scott Geng,Yiping Wang,Sewoong Oh,Simon Shaolei Du,Nathan Lambert,Sewon Min,Ranjay Krishna,Yulia Tsvetkov,Hannaneh Hajishirzi,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TL;DR: RLVR虽然使用虚假奖励信号，但在Qwen2.5-Math-7B模型中仍能显著提升数学推理能力，未来研究应在多种模型上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨在没有有用奖励信号的情况下，RLVR是否能够激发模型较强的推理能力，并分析它在不同模型上的表现差异。

Method: 使用可验证奖励的强化学习（RLVR）方法。

Result: 对于Qwen2.5-Math-7B模型，即使采用虚假的奖励信号，其MATH-500性能也显著提高。例如，随机奖励提高了21.4个百分点，错误标签奖励提高了24.1个百分点，单次训练提高26.0个百分点等。尽管对于其他模型如Llama3或OLMo2表现不佳，但Qwen显著表现出代码推理能力。

Conclusion: RLVR能够在某些模型中引发强大的数学推理能力，即使奖励信号虚假且与正确答案无关。

Abstract: We show that reinforcement learning with verifiable rewards (RLVR) can elicit
strong mathematical reasoning in certain models even with spurious rewards that
have little, no, or even negative correlation with the correct answer. For
example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute
points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect
label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the
29.1% gained with ground truth rewards. However, the spurious rewards that work
for Qwen often fail to yield gains with other model families like Llama3 or
OLMo2. In particular, we find code reasoning -- thinking in code without actual
code execution -- to be a distinctive Qwen2.5-Math behavior that becomes
significantly more frequent after RLVR, from 65% to over 90%, even with
spurious rewards. Overall, we hypothesize that, given the lack of useful reward
signal, RLVR must somehow be surfacing useful reasoning representations learned
during pretraining, although the exact mechanism remains a topic for future
work. We suggest that future RLVR research should possibly be validated on
diverse models rather than a single de facto choice, as we show that it is easy
to get significant performance gains on Qwen models even with completely
spurious reward signals.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [89] [AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation](https://arxiv.org/abs/2506.10540)
*Haoyuan Shi,Yunxin Li,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang*

Main category: cs.MA

TL;DR: AniMaker是一个多代理框架，通过智能化候选空间导航和特定的评估方法，提高了从文本到动画生成的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型难以在多个场景和角色之间生成连贯的故事视频。现有方法的问题在于生硬的关键帧转换和不稳定的视频生成质量。

Method: AniMaker使用多代理框架，包括导演代理、摄影代理、评价代理和后期制作代理。重要技术包括摄影代理中的MCTS-Gen策略和评论代理中的AniEval评估框架。

Result: 实验表明，AniMaker在VBench及其自主开发的AniEval框架所测评的多个指标上质量更佳，同时显著提升了多候选生成的效率。

Conclusion: AniMaker通过多代理框架和专门设计的评估方法实现了增强的故事一致性和更高质量的动画生成。

Abstract: Despite rapid advancements in video generation models, generating coherent
storytelling videos that span multiple scenes and characters remains
challenging. Current methods often rigidly convert pre-generated keyframes into
fixed-length clips, resulting in disjointed narratives and pacing issues.
Furthermore, the inherent instability of video generation models means that
even a single low-quality clip can significantly degrade the entire output
animation's logical coherence and visual continuity. To overcome these
obstacles, we introduce AniMaker, a multi-agent framework enabling efficient
multi-candidate clip generation and storytelling-aware clip selection, thus
creating globally consistent and story-coherent animation solely from text
input. The framework is structured around specialized agents, including the
Director Agent for storyboard generation, the Photography Agent for video clip
generation, the Reviewer Agent for evaluation, and the Post-Production Agent
for editing and voiceover. Central to AniMaker's approach are two key technical
components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search
(MCTS)-inspired strategy that intelligently navigates the candidate space to
generate high-potential clips while optimizing resource usage; and AniEval in
Reviewer Agent, the first framework specifically designed for multi-shot
animation evaluation, which assesses critical aspects such as story-level
consistency, action completion, and animation-specific features by considering
each clip in the context of its preceding and succeeding clips. Experiments
demonstrate that AniMaker achieves superior quality as measured by popular
metrics including VBench and our proposed AniEval framework, while
significantly improving the efficiency of multi-candidate generation, pushing
AI-generated storytelling animation closer to production standards.

</details>


### [90] [Higher-Order Uncoupled Learning Dynamics and Nash Equilibrium](https://arxiv.org/abs/2506.10874)
*Sarah A. Toonsi,Jeff S. Shamma*

Main category: cs.MA

TL;DR: 研究高阶复制者动态与非耦合动态中的混合策略NE的可学习性，发现高阶非耦合学习动态可以局部收敛到孤立的完全混合策略NE，并提出ABR特性以探讨NE的兼容性。


<details>
  <summary>Details</summary>
Motivation: 通过引入ABR特性，研究在对允许学习动态施加自然限制的情况下，NE的兼容性。

Method: 使用高阶复制者动态和高阶非耦合异质动态类来研究混合策略纳什均衡（NE）的可学习性。

Result: 提出了高阶非耦合学习动态与反馈稳定性和分散控制的关联，展示了学习动态与控制理论中同时稳定化概念之间的联系。

Conclusion: 我们证明了在存在孤立的完全混合策略NE的有限博弈中，存在高阶非耦合学习动态可以局部收敛到该NE。我们还构建了两个博弈，表明任何一种能学习一个博弈的完全混合策略NE的高阶动态无法学习另一个的。

Abstract: We study learnability of mixed-strategy Nash Equilibrium (NE) in general
finite games using higher-order replicator dynamics as well as classes of
higher-order uncoupled heterogeneous dynamics. In higher-order uncoupled
learning dynamics, players have no access to utilities of opponents (uncoupled)
but are allowed to use auxiliary states to further process information
(higher-order). We establish a link between uncoupled learning and feedback
stabilization with decentralized control. Using this association, we show that
for any finite game with an isolated completely mixed-strategy NE, there exist
higher-order uncoupled learning dynamics that lead (locally) to that NE. We
further establish the lack of universality of learning dynamics by linking
learning to the control theoretic concept of simultaneous stabilization. We
construct two games such that any higher-order dynamics that learn the
completely mixed-strategy NE of one of these games can never learn the
completely mixed-strategy NE of the other. Next, motivated by imposing natural
restrictions on allowable learning dynamics, we introduce the Asymptotic Best
Response (ABR) property. Dynamics with the ABR property asymptotically learn a
best response in environments that are asymptotically stationary. We show that
the ABR property relates to an internal stability condition on higher-order
learning dynamics. We provide conditions under which NE are compatible with the
ABR property. Finally, we address learnability of mixed-strategy NE in the
bandit setting using a bandit version of higher-order replicator dynamics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [91] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/abs/2506.09999)
*Yukun Chen,Zihuan Qiu,Fanman Meng,Hongliang Li,Linfeng Xu,Qingbo Wu*

Main category: cs.LG

TL;DR: 本文提出了一种新的MCIL方法，利用多模态预训练模型及多种新模块，在整合信息和减缓遗忘方面取得了良好效果并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的MCIL方法只关注视觉和文本，而本文则探索了视、音、文本模态的MCIL，旨在解决整合互补信息和缓解灾难性遗忘的挑战。

Method: 该方法提出了基于多模态预训练模型的MCIL方法，并引入了多模态增量特征提取器（MIFE）、自适应音视频融合模块（AAVFM）、一种新的多模态类增量对比训练损失，以及两个MCIL特定的评估指标。

Result: 通过三个多模态数据集的广泛实验，验证了提出方法的有效性。

Conclusion: 我们提出的方法在解决跨模态信息整合和减缓灾难性遗忘方面表现出色，并通过多项实验验证了其有效性。

Abstract: Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


### [92] [NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing](https://arxiv.org/abs/2506.10014)
*Wei Li,Mengcheng Lan,Jiaxing Xu,Yiping Ke*

Main category: cs.LG

TL;DR: NOCL framework improves graph modeling by transforming node attributes into natural language, enhancing performance in zero-shot scenarios compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Graph Neural Networks have limited generalization in label-scarce scenarios, and self-supervised approaches still need labeled fine-tuning. Large Language Models struggle with graph tasks due to issues like reasoning abilities preservation and handling rich node attributes. The paper focuses on overcoming these limitations to enhance performance in zero-shot scenarios.

Method: The Node-Oriented Conceptualization LLM (NOCL) framework uses two core techniques: node description and node concept. Node description transforms heterogeneous node attributes into structured natural language, and node concept encodes these descriptions into compact semantic embeddings. Additionally, NOCL employs graph representation descriptors to unify graph tasks into a shared language-based query format.

Result: The experimental results show NOCL's competitive performance in supervised tasks and superior generalization capabilities in zero-shot settings.

Conclusion: NOCL achieves competitive performance compared to traditional MPNNs and hybrid LLM-MPNN methods while demonstrating superior generalization in zero-shot scenarios.

Abstract: Graphs are essential for modeling complex interactions across domains such as
social networks, biology, and recommendation systems. Traditional Graph Neural
Networks, particularly Message Passing Neural Networks (MPNNs), rely heavily on
supervised learning, limiting their generalization and applicability in
label-scarce scenarios. Recent self-supervised approaches still require labeled
fine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,
Large Language Models (LLMs) excel in natural language tasks but face
significant challenges when applied to graphs, including preserving reasoning
abilities, managing extensive token lengths from rich node attributes, and
being limited to textual-attributed graphs (TAGs) and a single level task. To
overcome these limitations, we propose the Node-Oriented Conceptualization LLM
(NOCL), a novel framework that leverages two core techniques: 1) node
description, which converts heterogeneous node attributes into structured
natural language, extending LLM from TAGs to non-TAGs; 2) node concept, which
encodes node descriptions into compact semantic embeddings using pretrained
language models, significantly reducing token lengths by up to 93.9% compared
to directly using node descriptions. Additionally, our NOCL employs graph
representation descriptors to unify graph tasks at various levels into a
shared, language-based query format, paving a new direction for Graph
Foundation Models. Experimental results validate NOCL's competitive supervised
performance relative to traditional MPNNs and hybrid LLM-MPNN methods and
demonstrate superior generalization in zero-shot settings.

</details>


### [93] [Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks](https://arxiv.org/abs/2506.10044)
*Uijun Jung,Deokho Jang,Sungchul Kim,Jungho Kim*

Main category: cs.LG

TL;DR: 利用深度学习和TNN结构（特别是CNN-LSTM配置），能更高效地完成薄膜传输光谱的逆向设计。


<details>
  <summary>Details</summary>
Motivation: 薄膜的光学性质受到每一层厚度的显著影响，精确预测这些厚度及其相应的光学性质对于薄膜的光学逆向设计至关重要。传统的方法需要大量的数值模拟和优化过程，耗时耗力。

Method: 通过实现串联神经网络（TNN），对SiO2/TiO2多层薄膜的传输光谱进行逆向设计。TNN由逆向神经网络和一个预训练的前向神经网络背靠背组合而成，这两者基于多层感知机（MLP）算法实现。

Result: LSTM-LSTM结构的TNN尽管训练时间最长，但获得了最高的准确性。而CNN-LSTM结构TNN在准确性和速度方面达到最优平衡。

Conclusion: 在使用深度学习进行的多层薄膜传输光谱的逆向设计中，CNN-LSTM结构的TNN以其精确性和速度的平衡，成为最佳解决方案。

Abstract: Optical properties of thin film are greatly influenced by the thickness of
each layer. Accurately predicting these thicknesses and their corresponding
optical properties is important in the optical inverse design of thin films.
However, traditional inverse design methods usually demand extensive numerical
simulations and optimization procedures, which are time-consuming. In this
paper, we utilize deep learning for the inverse design of the transmission
spectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural
network (TNN), which can solve the one-to-many mapping problem that greatly
degrades the performance of deep-learning-based inverse designs. In general,
the TNN has been implemented by a back-to-back connection of an inverse neural
network and a pre-trained forward neural network, both of which have been
implemented based on multilayer perceptron (MLP) algorithms. In this paper, we
propose to use not only MLP, but also convolutional neural network (CNN) or
long short-term memory (LSTM) algorithms in the configuration of the TNN. We
show that an LSTM-LSTM-based TNN yields the highest accuracy but takes the
longest training time among nine configurations of TNNs. We also find that a
CNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed
because it could integrate the strengths of the CNN and LSTM algorithms.

</details>


### [94] [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
*Shangpin Peng,Weinong Wang,Zhuotao Tian,Senqiao Yang,Xing Wu,Haotian Xu,Chengquan Zhang,Takashi Isobe,Baotian Hu,Min Zhang*

Main category: cs.LG

TL;DR: Omni-DPO improves data utilization in RLHF by adaptively weighting preference pairs, showing superior performance over existing models in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing DPO-based methods fail to consider variations in preference pairs, resulting in suboptimal performance and data usage.

Method: Omni-DPO uses a dual-perspective optimization framework that adaptively weights samples based on data quality and model learning dynamics.

Result: Omni-DPO exceeds leading models like Claude 3 Opus by significant margins on benchmarks like Arena-Hard, demonstrating its effectiveness and robustness.

Conclusion: Omni-DPO framework enhances the performance of RLHF by considering both the inherent quality of preference pairs and the model's performance during training, leading to better data utilization and improved outcomes.

Abstract: Direct Preference Optimization (DPO) has become a cornerstone of
reinforcement learning from human feedback (RLHF) due to its simplicity and
efficiency. However, existing DPO-based approaches typically treat all
preference pairs uniformly, ignoring critical variations in their inherent
quality and learning utility, leading to suboptimal data utilization and
performance. To address this challenge, we propose Omni-DPO, a dual-perspective
optimization framework that jointly accounts for (1) the inherent quality of
each preference pair and (2) the model's evolving performance on those pairs.
By adaptively weighting samples according to both data quality and the model's
learning dynamics during training, Omni-DPO enables more effective training
data utilization and achieves better performance. Experimental results on
various models and benchmarks demonstrate the superiority and generalization
capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it
finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant
margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning
tasks, Omni-DPO consistently outperforms the baseline methods across all
benchmarks, providing strong empirical evidence for the effectiveness and
robustness of our approach. Code and models will be available at
https://github.com/pspdada/Omni-DPO.

</details>


### [95] [Textual Bayes: Quantifying Uncertainty in LLM-Based Systems](https://arxiv.org/abs/2506.10060)
*Brendan Leigh Ross,Noël Vouitsis,Atiyeh Ashari Ghomi,Rasa Hosseinzadeh,Ji Xin,Zhaoyan Liu,Yi Sui,Shiyi Hou,Kin Kwan Leung,Gabriel Loaiza-Ganem,Jesse C. Cresswell*

Main category: cs.LG

TL;DR: 本文通过贝叶斯视角解析LLM系统的提示，并引入新的MCMC算法，改善了预测准确性与不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 许多最先进的大型语言模型（LLMs）由于其封闭源码和黑箱性质，在高风险领域中的应用受到了限制，它们对绑定其的提示非常敏感，需要大量手动调整。准确量化这些模型的不确定性仍是一个亟待解决的问题。

Method: 提出了一种将提示视为统计模型中文本参数的方法，并引入了一种新的马尔科夫链蒙特卡洛（MCMC）算法，即Metropolis-Hastings通过LLM提案（MHLP），结合提示优化技术和标准MCMC方法进行贝叶斯推断。

Result: 通过实验证明，该方法在一系列LLM基准和不确定性量化（UQ）任务中提高了预测准确性和不确定性量化。

Conclusion: 展示了一条将丰富的贝叶斯文献中的方法纳入LLM时代的可行路径，为更可靠和校准的LLM系统奠定了基础。

Abstract: Although large language models (LLMs) are becoming increasingly capable of
solving challenging real-world tasks, accurately quantifying their uncertainty
remains a critical open problem, which limits their applicability in
high-stakes domains. This challenge is further compounded by the closed-source,
black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can
be highly sensitive to the prompts that bind them together, which often require
significant manual tuning (i.e., prompt engineering). In this work, we address
these challenges by viewing LLM-based systems through a Bayesian lens. We
interpret prompts as textual parameters in a statistical model, allowing us to
use a small training dataset to perform Bayesian inference over these prompts.
This novel perspective enables principled uncertainty quantification over both
the model's textual parameters and its downstream predictions, while also
incorporating prior beliefs about these parameters expressed in free-form text.
To perform Bayesian inference, a difficult problem even for well-studied data
modalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a
novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt
optimization techniques with standard MCMC methods. MHLP is a turnkey
modification to existing LLM pipelines, including those that rely exclusively
on closed-source models. Empirically, we demonstrate that our method yields
improvements in both predictive accuracy and uncertainty quantification (UQ) on
a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a
viable path for incorporating methods from the rich Bayesian literature into
the era of LLMs, paving the way for more reliable and calibrated LLM-based
systems.

</details>


### [96] [Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection](https://arxiv.org/abs/2506.10089)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.LG

TL;DR: 本文提出了一种优化层次变分自编码器(HVAEs)中潜在维度分配的方法，提高了深度生成模型在OOD检测任务中的鲁棒性和表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在层间分配潜在容量时常常过于随意，导致无效的表示或后验崩溃，因此需要一种理论指导的方法来优化潜在维度的分配，提升HVAEs在OOD检测任务中的性能。

Method: 引入了一个理论框架，该框架基于信息论原则，形式化了信息损失与表示衰减之间的权衡，并证明了在固定的潜在预算下存在一个最优分配比例。

Result: 通过优化潜在层的分配比例，提高了概率生成模型在离散分布检测任务中的鲁棒性和性能，在不同数据集和架构上表现均优于基线配置。

Conclusion: 提出了一种基于信息论的理论框架，用于优化HVAEs中的潜在维度分配，并通过实验证明优化分配比例能显著提升OOD检测性能。

Abstract: Out-of-distribution (OOD) detection is a critical task in machine learning,
particularly for safety-critical applications where unexpected inputs must be
reliably flagged. While hierarchical variational autoencoders (HVAEs) offer
improved representational capacity over traditional VAEs, their performance is
highly sensitive to how latent dimensions are distributed across layers.
Existing approaches often allocate latent capacity arbitrarily, leading to
ineffective representations or posterior collapse. In this work, we introduce a
theoretically grounded framework for optimizing latent dimension allocation in
HVAEs, drawing on principles from information theory to formalize the trade-off
between information loss and representational attenuation. We prove the
existence of an optimal allocation ratio $r^{\ast}$ under a fixed latent
budget, and empirically show that tuning this ratio consistently improves OOD
detection performance across datasets and architectures. Our approach
outperforms baseline HVAE configurations and provides practical guidance for
principled latent structure design, leading to more robust OOD detection with
deep generative models.

</details>


### [97] [Efficient kernelized bandit algorithms via exploration distributions](https://arxiv.org/abs/2506.10091)
*Bingshan Hu,Zheng He,Danica J. Sutherland*

Main category: cs.LG

TL;DR: 本文提出了一类高效的核化bandit算法GP-Generic，通过探索分布设计实现 \tilde{O}(\gamma_T\sqrt{T}) 后悔界限，实践中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究核化bandit问题，目标是开发计算效率高的算法，利用探索分布概念提高实际表现。

Method: 本文提出了一种新的概念——探索分布，设计了一系列核化bandit算法，称为GP-Generic。这些算法通过对探索分布的巧妙选择，能够实现一系列具体算法，这些算法在某个再生核希尔波特空间（RKHS）中拥有有限范数的目标函数。

Result: 所提出的通用算法在实现上达到了与现有基于UCB和Thompson Sampling算法相匹配的后悔界限。同时，实验证明，适当的随机化在实践中可以取得更优异的效果。

Conclusion: 本文提出的GP-Generic算法在探索分布的选择上表现出多样性，不仅涵盖了现有的上置信界方法，还推出了一系列实现 \tilde{O}(\gamma_T\sqrt{T}) 后悔上界的随机算法，实践中随机化表现更佳。

Abstract: We consider a kernelized bandit problem with a compact arm set ${X} \subset
\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm
in some Reproducing Kernel Hilbert Space (RKHS). We propose a class of
computationally efficient kernelized bandit algorithms, which we call
GP-Generic, based on a novel concept: exploration distributions. This class of
algorithms includes Upper Confidence Bound-based approaches as a special case,
but also allows for a variety of randomized algorithms. With careful choice of
exploration distribution, our proposed generic algorithm realizes a wide range
of concrete algorithms that achieve $\tilde{O}(\gamma_T\sqrt{T})$ regret
bounds, where $\gamma_T$ characterizes the RKHS complexity. This matches known
results for UCB- and Thompson Sampling-based algorithms; we also show that in
practice, randomization can yield better practical results.

</details>


### [98] [Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders](https://arxiv.org/abs/2506.10094)
*Md. Faizul Islam Ansari*

Main category: cs.LG

TL;DR: 研究通过两阶段深度自编码器，结合重构误差与KMeans损失，实现MNIST数字的高效无监督聚类，达到数据重构与聚类纯度的最佳组合。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一种能实现高效聚类性能的系统，尤其是在MNIST手写数字上，希望结合数据重构精度与聚类纯度，提供可解释且可扩展的实现方案。

Method: 研究采用两阶段深度自编码器架构进行无监督学习：第一阶段通过最小化重构误差训练自编码器以获取图像的简要有解释的表示；第二阶段结合重构误差和KMeans聚类损失，通过联合的基于距离的目标函数进行聚类。

Result: 通过内在度量（如Silhouette Score和Davies-Bouldin Index）和外在度量（如NMI和ARI）显示，框架在广泛测试中表现出优越的聚类性能。t-SNE可视化展示了清晰的数字簇，系统实现了数据重构精度和簇间分离纯度的最佳结合。

Conclusion: 该研究提出了一种基于两阶段深度自编码器架构的无监督聚类系统，在MNIST手写数字上表现出优越的聚类性能，并为大规模图像聚类应用提供了可靠的基础。

Abstract: This research implements an advanced unsupervised clustering system for MNIST
handwritten digits through two-phase deep autoencoder architecture. A deep
neural autoencoder requires a training process during phase one to develop
minimal yet interpretive representations of images by minimizing reconstruction
errors. During the second phase we unify the reconstruction error with a KMeans
clustering loss for learned latent embeddings through a joint distance-based
objective. Our model contains three elements which include batch normalization
combined with dropout and weight decay for achieving generalized and stable
results. The framework achieves superior clustering performance during
extensive tests which used intrinsic measurements including Silhouette Score
and Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when
processing image features. The research uses t-SNE visualization to present
learned embeddings that show distinct clusters for digits. Our approach reaches
an optimal combination between data reconstruction accuracy and cluster
separation purity when adding the benefit of understandable results and
scalable implementations. The approach creates a dependable base that helps
deploy unsupervised representation learning in different large-scale image
clustering applications.

</details>


### [99] [Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach](https://arxiv.org/abs/2506.10102)
*Ahmed Elbakary,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出了一种新颖的联邦多任务学习方法，通过利用跨客户端相似性和社区检测，实现个性化学习和通信效率，显著优于现有基线并促进公平性。


<details>
  <summary>Details</summary>
Motivation: 动机在于实现个性化学习，同时避免传输整个模型造成的通信负担，并防止在客户协作中出现负面效应。

Method: 我们提出了一种新颖的联邦多任务学习方法，该方法利用跨客户端相似性实现个性化学习。在不传输整个模型到参数服务器的情况下，引入了一种通信高效的方案，通过特征锚定（一个紧凑的向量表示）来总结从客户端本地类中学习的特征，并与服务器共享。此外，客户端共享分类头——一个轻量级的线性层，并执行基于图的正则化以促进客户端间的协作。通过将客户端之间的协作建模为动态图，并不断更新和细化该图，可以考虑来自客户端的任何漂移。为确保有益的知识转移并防止负面协作，采用基于社区检测的方法，将此动态图划分为同质社区，最大化每个社区内任务相似性的总和。

Result: 方法实现了每个社区内高度相似客户端的正向交互和个性化保持，通过对两个异构数据集的广泛实验验证了其优越性。

Conclusion: 实验表明，该方法在异构数据集上显著优于现有的最先进基线，并且在计算和通信效率方面表现优异，还促进了客户间的公平性。

Abstract: We present a novel federated multi-task learning method that leverages
cross-client similarity to enable personalized learning for each client. To
avoid transmitting the entire model to the parameter server, we propose a
communication-efficient scheme that introduces a feature anchor, a compact
vector representation that summarizes the features learned from the client's
local classes. This feature anchor is shared with the server to account for
local clients' distribution. In addition, the clients share the classification
heads, a lightweight linear layer, and perform a graph-based regularization to
enable collaboration among clients. By modeling collaboration between clients
as a dynamic graph and continuously updating and refining this graph, we can
account for any drift from the clients. To ensure beneficial knowledge transfer
and prevent negative collaboration, we leverage a community detection-based
approach that partitions this dynamic graph into homogeneous communities,
maximizing the sum of task similarities, represented as the graph edges'
weights, within each community. This mechanism restricts collaboration to
highly similar clients within their formed communities, ensuring positive
interaction and preserving personalization. Extensive experiments on two
heterogeneous datasets demonstrate that our method significantly outperforms
state-of-the-art baselines. Furthermore, we show that our method exhibits
superior computation and communication efficiency and promotes fairness across
clients.

</details>


### [100] [NnD: Diffusion-based Generation of Physically-Nonnegative Objects](https://arxiv.org/abs/2506.10112)
*Nadav Torem,Tamar Sde-Chen,Yoav Y. Schechner*

Main category: cs.LG

TL;DR: 为了减少复杂自然物体模拟的计算成本，提出了一种基于非负扩散的生成模型，能够生成符合物理规律的3D云。


<details>
  <summary>Details</summary>
Motivation: 由于许多自然物体具有复杂性和变化性，直接模拟这些对象需要高昂的计算成本，因此需要开发一种新的方法来降低复杂对象模拟的资源消耗。

Method: 提出了一种称为非负扩散的生成模型，该模型通过得分扩散机制来学习。利用退火的Langevin动力学来在迭代的场景生成和分析过程中强制保持非负性。模型在高质量的物理模拟对象上进行训练，训练后可以用于生成和推断。

Result: 成功展示了3D体积云的生成，这些云符合云物理趋势，且专家难以辨别其非物理特性。

Conclusion: NnD模型能够在显著降低计算成本的情况下生成和推断物理一致的3D体积云。

Abstract: Most natural objects have inherent complexity and variability. While some
simple objects can be modeled from first principles, many real-world phenomena,
such as cloud formation, require computationally expensive simulations that
limit scalability. This work focuses on a class of physically meaningful,
nonnegative objects that are computationally tractable but costly to simulate.
To dramatically reduce computational costs, we propose nonnegative diffusion
(NnD). This is a learned generative model using score based diffusion. It
adapts annealed Langevin dynamics to enforce, by design, non-negativity
throughout iterative scene generation and analysis (inference). NnD trains on
high-quality physically simulated objects. Once trained, it can be used for
generation and inference. We demonstrate generation of 3D volumetric clouds,
comprising inherently nonnegative microphysical fields. Our generated clouds
are consistent with cloud physics trends. They are effectively not
distinguished as non-physical by expert perception.

</details>


### [101] [GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments](https://arxiv.org/abs/2506.10120)
*Maryam Khalid,Akane Sano*

Main category: cs.LG

TL;DR: 引入了GRAIL框架，评估动态环境中的graph AL策略，强调预测性能和用户负担的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的graph AL方法通常在静态图数据集上进行评估，主要关注预测准确性，忽略了用户中心考虑，如采样多样性、查询公平性和对动态设置的适应性。为了解决这个问题，引入GRAIL。

Method: 引入GRAIL，一个新的基准测试框架，设计用于评估动态真实环境中的graph AL策略。

Result: 在动态现实人类传感器数据集上的广泛实验揭示了预测性能和用户负担之间的权衡，并突出了现有AL策略中的局限性。

Conclusion: GRAIL展示了在动态环境中平衡节点重要性、查询多样性和网络拓扑的重要性，并提供了一种评估graph AL解决方案的机制。

Abstract: Graph-based Active Learning (AL) leverages the structure of graphs to
efficiently prioritize label queries, reducing labeling costs and user burden
in applications like health monitoring, human behavior analysis, and sensor
networks. By identifying strategically positioned nodes, graph AL minimizes
data collection demands while maintaining model performance, making it a
valuable tool for dynamic environments. Despite its potential, existing graph
AL methods are often evaluated on static graph datasets and primarily focus on
prediction accuracy, neglecting user-centric considerations such as sampling
diversity, query fairness, and adaptability to dynamic settings. To bridge this
gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate
graph AL strategies in dynamic, real-world environments. GRAIL introduces novel
metrics to assess sustained effectiveness, diversity, and user burden, enabling
a comprehensive evaluation of AL methods under varying conditions. Extensive
experiments on datasets featuring dynamic, real-life human sensor data reveal
trade-offs between prediction performance and user burden, highlighting
limitations in existing AL strategies. GRAIL demonstrates the importance of
balancing node importance, query diversity, and network topology, providing an
evaluation mechanism for graph AL solutions in dynamic environments.

</details>


### [102] [Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms](https://arxiv.org/abs/2506.10127)
*Xinyi Hu,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 本研究提出了一种用于去中心化无感知MMAB问题的新算法A-CAPELLA，该算法在多臂未知承载能力情况下通过协作假设检验协议实现了有效学习，达到了对数遗憾。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决去中心化的多玩家多臂赌博机（MMAB）问题，尤其是在没有感知能力的环境中进行研究。这种环境中，玩家只能获得自己的奖励，无法获取碰撞信息，且每个臂具有未知承载能力。超过承载能力的玩家数将导致所有玩家获得零奖励。这一设置扩展了经典的单承载能力模型并引入了新的协调及承载能力发现的挑战。

Method: 我们提出了一种名为A-CAPELLA（适用于承载能力感知的并行消除学习和分配算法）的去中心化算法，该算法通过精心设计的碰撞模式进行协作假设检验，从而实现同步的连续消除和承载能力估计。

Result: A-CAPELLA算法在这一广泛的情况下实现了对数遗憾。所提出的方法在去中心化无感知的MMAB环境中具有未知臂承载能力时证明是有效的学习结果。

Conclusion: 通过引入协作假设检验协议，我们成功解决了在反馈受限的情况下协调和发现承载能力的问题。这代表了在去中心化无感知的MMAB环境中一个有效的学习结果。

Abstract: We study the decentralized multi-player multi-armed bandits (MMAB) problem
under a no-sensing setting, where each player receives only their own reward
and obtains no information about collisions. Each arm has an unknown capacity,
and if the number of players pulling an arm exceeds its capacity, all players
involved receive zero reward. This setting generalizes the classical
unit-capacity model and introduces new challenges in coordination and capacity
discovery under severe feedback limitations. We propose A-CAPELLA (Algorithm
for Capacity-Aware Parallel Elimination for Learning and Allocation), a
decentralized algorithm that achieves logarithmic regret in this generalized
regime. Our main contribution is a collaborative hypothesis testing protocol
that enables synchronized successive elimination and capacity estimation
through carefully structured collision patterns. This represents a provably
efficient learning result in decentralized no-sensing MMAB with unknown arm
capacities.

</details>


### [103] [Provable Sim-to-Real Transfer via Offline Domain Randomization](https://arxiv.org/abs/2506.10133)
*Arnaud Fickinger,Abderrahim Bendahi,Stuart Russell*

Main category: cs.LG

TL;DR: 研究了离线领域随机化（ODR），其通过离线数据集调整模拟器参数分布，获得更准确的sim-to-real误差估计，并引入E-DROPO通过熵奖励促进更鲁棒的迁移效果。


<details>
  <summary>Details</summary>
Motivation: 标准的领域随机化（DR）忽略了已经从真实系统中获得的离线数据，而现有实证研究报告了使用算法如DROPO取得了显著增益，但ODR的理论基础仍然缺乏探索。

Method: 将ODR形式化为参数化模拟器族的最大似然估计，证明该估计器在温和的规则性和可辨识性条件下的一致性，展示其随着数据集的增长收敛于真实动态。推导出间隙界，展示ODR的sim-to-real误差比统一DR的误差更小。引入E-DROPO版本，通过增加熵奖励来防止方差崩溃。

Result: 推导出的间隙界展示了ODR在有限模拟器情况下的sim-to-real误差高达O(M)因子收紧，同时在连续设置下取得类似增益效果。引入E-DROPO版本，通过增加熵奖励来防止方差崩溃，更广泛的随机化处理使得实践中获得更鲁棒的迁移效果。

Conclusion: 在有限模拟器情况下，ODR的sim-to-real误差比统一DR的误差更小，且在连续设置下取得类似增益效果。引入的E-DROPO通过增加熵奖励来防止方差崩溃，促进更广泛的随机化效果和更鲁棒的零样本迁移效果。

Abstract: Reinforcement-learning agents often struggle when deployed from simulation to
the real-world. A dominant strategy for reducing the sim-to-real gap is domain
randomization (DR) which trains the policy across many simulators produced by
sampling dynamics parameters, but standard DR ignores offline data already
available from the real system. We study offline domain randomization (ODR),
which first fits a distribution over simulator parameters to an offline
dataset. While a growing body of empirical work reports substantial gains with
algorithms such as DROPO, the theoretical foundations of ODR remain largely
unexplored. In this work, we (i) formalize ODR as a maximum-likelihood
estimation over a parametric simulator family, (ii) prove consistency of this
estimator under mild regularity and identifiability conditions, showing it
converges to the true dynamics as the dataset grows, (iii) derive gap bounds
demonstrating ODRs sim-to-real error is up to an O(M) factor tighter than
uniform DR in the finite-simulator case (and analogous gains in the continuous
setting), and (iv) introduce E-DROPO, a new version of DROPO which adds an
entropy bonus to prevent variance collapse, yielding broader randomization and
more robust zero-shot transfer in practice.

</details>


### [104] [Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning](https://arxiv.org/abs/2506.10137)
*Daniel Lawson,Adriana Hugessen,Charlotte Cloutier,Glen Berseth,Khimya Khetarpal*

Main category: cs.LG

TL;DR: 提出了 BYOL-γ 增强的GCBC，可在没有对比样本或TD学习的情况下有效逼近成功者表示，显示出强大的组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的GCBC方法在处理新的状态-目标对的组合泛化任务时存在局限性，这主要是由于BC学习的状态表示缺乏时间一致性。因此，通过促进表示空间的时间一致性以提升组合泛化能力。

Method: 本文提出了一种新的表示学习目标：BYOL-γ 增强的GCBC，不依赖对比样本或时序差分学习的方法来逼近成功者表示。

Result: BYOL-γ 增强的GCBC在一系列复杂任务中展现了其出色的组合泛化性能，达到了高效的经验性能表现。

Conclusion: BYOL-γ 结合了 GCBC 提供了一种有效的方法，在没有对比样本或TD学习的情况下，理论上可以在有限MDP情况下逼近成功者表示，并在需要组合泛化的任务中表现出竞争力。

Abstract: Behavioral cloning (BC) methods trained with supervised learning (SL) are an
effective way to learn policies from human demonstrations in domains like
robotics. Goal-conditioning these policies enables a single generalist policy
to capture diverse behaviors contained within an offline dataset. While
goal-conditioned behavior cloning (GCBC) methods can perform well on
in-distribution training tasks, they do not necessarily generalize zero-shot to
tasks that require conditioning on novel state-goal pairs, i.e. combinatorial
generalization. In part, this limitation can be attributed to a lack of
temporal consistency in the state representation learned by BC; if temporally
related states are encoded to similar latent representations, then the
out-of-distribution gap for novel state-goal pairs would be reduced. Hence,
encouraging this temporal consistency in the representation space should
facilitate combinatorial generalization. Successor representations, which
encode the distribution of future states visited from the current state, nicely
encapsulate this property. However, previous methods for learning successor
representations have relied on contrastive samples, temporal-difference (TD)
learning, or both. In this work, we propose a simple yet effective
representation learning objective, $\text{BYOL-}\gamma$ augmented GCBC, which
is not only able to theoretically approximate the successor representation in
the finite MDP case without contrastive samples or TD learning, but also,
results in competitive empirical performance across a suite of challenging
tasks requiring combinatorial generalization.

</details>


### [105] [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138)
*Mohammad Taufeeque,Aaron David Tucker,Adam Gleave,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 研究逆向工程了一个用于Sokoban游戏的神经网络，揭示其使用类似经典搜索算法的机制来增强解题能力。


<details>
  <summary>Details</summary>
Motivation: 此前的工作表明，该网络在测试中计算能力越强，解决的关卡越多。本文试图分析这一现象背后的机制。

Method: 对训练解谜游戏Sokoban的卷积递归神经网络进行部分逆向工程分析，并通过模型无关的强化学习探索其解决问题的机制。

Result: 研究显示神经网络中的状态行为激活类似于函数值，其大小决定了何时回溯及哪个计划分支会被修剪。

Conclusion: 通过对训练解谜游戏Sokoban的卷积递归神经网络进行逆向工程分析，发现模型使用了一些类似经典双向搜索的机制，表明模型并非难以理解，而是可以通过经典算法术语进行解释。

Abstract: We partially reverse-engineer a convolutional recurrent neural network (RNN)
trained to play the puzzle game Sokoban with model-free reinforcement learning.
Prior work found that this network solves more levels with more test-time
compute. Our analysis reveals several mechanisms analogous to components of
classic bidirectional search. For each square, the RNN represents its plan in
the activations of channels associated with specific directions. These
state-action activations are analogous to a value function - their magnitudes
determine when to backtrack and which plan branch survives pruning. Specialized
kernels extend these activations (containing plan and value) forward and
backward to create paths, forming a transition model. The algorithm is also
unlike classical search in some ways. State representation is not unified;
instead, the network considers each box separately. Each layer has its own plan
representation and value function, increasing search depth. Far from being
inscrutable, the mechanisms leveraging test-time compute learned in this
network by model-free training can be understood in familiar terms.

</details>


### [106] [Survival Analysis as Imprecise Classification with Trainable Kernels](https://arxiv.org/abs/2506.10140)
*Andrei V. Konstantinov,Vlada A. Efremenko,Lev V. Utkin*

Main category: cs.LG

TL;DR: 本文提出三种结合不精确概率和注意力机制的新生存模型，可有效处理删失数据，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的非参数方法，如Beran估计器，在应对复杂数据结构和严重删失情况下表现有限。因此，本文提出了新模型以改进此类问题的处理。

Method: 论文引入了三种新的生存模型：iSurvM、iSurvQ和iSurvJ，结合了不精确概率理论与注意力机制，没有参数假设地处理删失数据。这些模型通过在事件时刻之间的时间间隔上为每个实例表示删失观测的区间值概率分布，使用带可训练注意力权重的基于核的Nadaraya-Watson回归计算整个数据集的时间间隔上的不精确概率分布，并使用对应的三种决策策略进行训练。

Result: 实验结果表明，与Beran估计器相比，新提出的模型在合成数据和真实数据集上表现出更高的准确性和更低的计算复杂度。

Conclusion: 提出的三种模型（iSurvM、iSurvQ、iSurvJ）在处理复杂数据结构和大量删失数据方面表现优异，尤其是iSurvJ模型，在准确性和计算复杂度上优于传统的Beran估计器。

Abstract: Survival analysis is a fundamental tool for modeling time-to-event data in
healthcare, engineering, and finance, where censored observations pose
significant challenges. While traditional methods like the Beran estimator
offer nonparametric solutions, they often struggle with the complex data
structures and heavy censoring. This paper introduces three novel survival
models, iSurvM (the imprecise Survival model based on Mean likelihood
functions), iSurvQ (the imprecise Survival model based on the Quantiles of
likelihood functions), and iSurvJ (the imprecise Survival model based on the
Joint learning), that combine imprecise probability theory with attention
mechanisms to handle censored data without parametric assumptions. The first
idea behind the models is to represent censored observations by interval-valued
probability distributions for each instance over time intervals between events
moments. The second idea is to employ the kernel-based Nadaraya-Watson
regression with trainable attention weights for computing the imprecise
probability distribution over time intervals for the entire dataset. The third
idea is to consider three decision strategies for training, which correspond to
the proposed three models. Experiments on synthetic and real datasets
demonstrate that the proposed models, especially iSurvJ, consistently
outperform the Beran estimator from the accuracy and computational complexity
points of view. Codes implementing the proposed models are publicly available.

</details>


### [107] [Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities](https://arxiv.org/abs/2506.10144)
*Yaowen Zhang,Libera Fresiello,Peter H. Veltink,Dirk W. Donker,Ying Wang*

Main category: cs.LG

TL;DR: 研究开发了一种通过氧气摄取数据估计心率的神经网络模型，在心率估计上表现出高准确性，并能实现个性化心脏监测。


<details>
  <summary>Details</summary>
Motivation: 心力衰竭是全球健康的一大挑战，早期检测可以改善病情。

Method: 引入了一种基于生理模型的神经网络（PMB-NN）框架，通过日常活动中的氧气摄取（VO2）数据估算心率。

Result: PMB-NN模型在12名参与者的个体数据上进行训练和测试，表现出高估计准确性，中位R^2值为0.8，RMSE为8.3 bpm。与传统生理模型相比具有显著优势。

Conclusion: PMB-NN框架可以实现个性化与实时的心脏监测，将来能用于日常生活中的身体活动。

Abstract: Heart failure (HF) poses a significant global health challenge, with early
detection offering opportunities for improved outcomes. Abnormalities in heart
rate (HR), particularly during daily activities, may serve as early indicators
of HF risk. However, existing HR monitoring tools for HF detection are limited
by their reliability on population-based averages. The estimation of
individualized HR serves as a dynamic digital twin, enabling precise tracking
of cardiac health biomarkers. Current HR estimation methods, categorized into
physiologically-driven and purely data-driven models, struggle with efficiency
and interpretability. This study introduces a novel physiological-model-based
neural network (PMB-NN) framework for HR estimation based on oxygen uptake
(VO2) data during daily physical activities. The framework was trained and
tested on individual datasets from 12 participants engaged in activities
including resting, cycling, and running. By embedding physiological
constraints, which were derived from our proposed simplified human movement
physiological model (PM), into the neural network training process, the PMB-NN
model adheres to human physiological principles while achieving high estimation
accuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative
statistical analysis demonstrates that the PMB-NN achieves performance on par
with the benchmark neural network model while significantly outperforming
traditional physiological model (p=0.002). In addition, our PMB-NN is adept at
identifying personalized parameters of the PM, enabling the PM to generate
reasonable HR estimation. The proposed framework with a precise VO2 estimation
system derived from body movements enables the future possibilities of
personalized and real-time cardiac monitoring during daily life physical
activities.

</details>


### [108] [Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors](https://arxiv.org/abs/2506.10146)
*Tejaswi Kasarla,Max van Spengler,Pascal Mettes*

Main category: cs.LG

TL;DR: 研究表明，使用平衡双曲嵌入算法改进的分层嵌入可以有效提升分布外识别的效果。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中分布外识别问题，目的是过滤掉不属于训练网络的分布的样本。

Method: 论文介绍了一种平衡双曲学习方法，提出了一种双曲类别嵌入算法，同时优化分层失真和平衡浅层和宽子层次结构，并将类别嵌入用作分类的双曲原型。

Result: 在13个数据集和13个评分函数上的实验证明，双曲嵌入在相同数据和骨干上训练时优于现有的分布外方法，同时胜过其他双曲方法和先进的对比方法，并且本身能够实现层次化的分布外推广。

Conclusion: 采用良好的分层双曲嵌入可以更好地区分分布内和分布外的样本。

Abstract: Out-of-distribution recognition forms an important and well-studied problem
in deep learning, with the goal to filter out samples that do not belong to the
distribution on which a network has been trained. The conclusion of this paper
is simple: a good hierarchical hyperbolic embedding is preferred for
discriminating in- and out-of-distribution samples. We introduce Balanced
Hyperbolic Learning. We outline a hyperbolic class embedding algorithm that
jointly optimizes for hierarchical distortion and balancing between shallow and
wide subhierarchies. We then use the class embeddings as hyperbolic prototypes
for classification on in-distribution data. We outline how to generalize
existing out-of-distribution scoring functions to operate with hyperbolic
prototypes. Empirical evaluations across 13 datasets and 13 scoring functions
show that our hyperbolic embeddings outperform existing out-of-distribution
approaches when trained on the same data with the same backbones. We also show
that our hyperbolic embeddings outperform other hyperbolic approaches, beat
state-of-the-art contrastive methods, and natively enable hierarchical
out-of-distribution generalization.

</details>


### [109] [Probabilistic Variational Contrastive Learning](https://arxiv.org/abs/2506.10159)
*Minoh Jeong,Seonho Kim,Alfred Hero*

Main category: cs.LG

TL;DR: 变分对比学习（VCL）引入概率嵌入，增强分类性能并提供不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 确定性嵌入缺乏不确定性量化的机制。

Method: 提出了一种无解码器的框架，通过将InfoNCE损失解释为替代重建项，并添加KL散度正则化项的方式，最大化证据下界（ELBO）。同时，将近似后验$q_\theta(z|x)$建模为一个投影正态分布，从而实现概率嵌入的采样。

Result: 实验结果表明，VCL减轻了维度崩溃，加强了与类别标签的互信息，并在分类准确性上匹配或优于确定性基线，同时通过后验模型提供了有意义的不确定性估计。

Conclusion: 变分对比学习（VCL）为对比学习赋予了概率基础，提供了不确定性估计，并在分类准确性上匹配或超越了确定性基线。

Abstract: Deterministic embeddings learned by contrastive learning (CL) methods such as
SimCLR and SupCon achieve state-of-the-art performance but lack a principled
mechanism for uncertainty quantification. We propose Variational Contrastive
Learning (VCL), a decoder-free framework that maximizes the evidence lower
bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction
term and adding a KL divergence regularizer to a uniform prior on the unit
hypersphere. We model the approximate posterior $q_\theta(z|x)$ as a projected
normal distribution, enabling the sampling of probabilistic embeddings. Our two
instantiations--VSimCLR and VSupCon--replace deterministic embeddings with
samples from $q_\theta(z|x)$ and incorporate a normalized KL term into the
loss. Experiments on multiple benchmarks demonstrate that VCL mitigates
dimensional collapse, enhances mutual information with class labels, and
matches or outperforms deterministic baselines in classification accuracy, all
the while providing meaningful uncertainty estimates through the posterior
model. VCL thus equips contrastive learning with a probabilistic foundation,
serving as a new basis for contrastive approaches.

</details>


### [110] [The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset](https://arxiv.org/abs/2506.10165)
*Gilad Landau,Miran Özdogan,Gereon Elvers,Francesco Mantegna,Pratik Somaiya,Dulhan Jayalath,Luisa Kurth,Teyun Kwon,Brendan Shillingford,Greg Farquhar,Minqi Jiang,Karim Jerbi,Hamza Abdelhedi,Yorguin Mantilla Ramos,Caglar Gulcehre,Mark Woolrich,Natalie Voets,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: 介绍了一个用于语音解码的非侵入性脑数据竞赛，提供了大型MEG数据集与相应的工具，旨在通过社区合作实现关键技术突破。


<details>
  <summary>Details</summary>
Motivation: 通过利用机器学习社区的集体力量，实现非侵入性神经解码的突破，从而恢复因语言障碍如构音困难（dysarthria）而失去的交流能力。

Method: 提供了迄今为止最大的单主体MEG数据集（LibriBrain）以及一个易于使用的Python库（pnpl），以便于与深度学习框架的集成，并定义了两个基础任务和标准化的数据划分与评估指标等。

Result: 构建了LibriBrain数据集和pnpl库，用于推动非侵入式脑-计算机接口在语音解码领域的进步。

Conclusion: 这项研究的推进可能在非侵入性脑数据的语音解码方面产生深远的社会影响，特别是在无创手术的基础上恢复瘫痪个体的沟通能力。

Abstract: The advance of speech decoding from non-invasive brain data holds the
potential for profound societal impact. Among its most promising applications
is the restoration of communication to paralysed individuals affected by speech
deficits such as dysarthria, without the need for high-risk surgical
interventions. The ultimate aim of the 2025 PNPL competition is to produce the
conditions for an "ImageNet moment" or breakthrough in non-invasive neural
decoding, by harnessing the collective power of the machine learning community.
  To facilitate this vision we present the largest within-subject MEG dataset
recorded to date (LibriBrain) together with a user-friendly Python library
(pnpl) for easy data access and integration with deep learning frameworks. For
the competition we define two foundational tasks (i.e. Speech Detection and
Phoneme Classification from brain data), complete with standardised data splits
and evaluation metrics, illustrative benchmark models, online tutorial code, a
community discussion board, and public leaderboard for submissions. To promote
accessibility and participation the competition features a Standard track that
emphasises algorithmic innovation, as well as an Extended track that is
expected to reward larger-scale computing, accelerating progress toward a
non-invasive brain-computer interface for speech.

</details>


### [111] [Wasserstein Barycenter Soft Actor-Critic](https://arxiv.org/abs/2506.10167)
*Zahra Shahrooei,Ali Baheri*

Main category: cs.LG

TL;DR: 该论文提出了WBSAC算法，通过引入新的探索策略提高样本效率，特别是在稀疏奖励环境下，并在MuJoCo任务中取得了优于现有算法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的深度off-policy actor-critic算法在稀疏奖励环境下样本效率低下，本文旨在通过提供一种合理的探索策略来改善这一问题。

Method: 通过引入基于Wasserstein重心的探索策略，结合悲观和乐观策略，调节学习过程中的探索程度，提出了Wasserstein Barycenter Soft Actor-Critic (WBSAC)算法。

Result: 实验结果表明，WBSAC算法在MuJoCo连续控制任务中的样本效率优于当前的最先进算法。

Conclusion: WBSAC算法在连续控制任务中比现有的off-policy actor-critic算法在样本效率上有更好的表现。

Abstract: Deep off-policy actor-critic algorithms have emerged as the leading framework
for reinforcement learning in continuous control domains. However, most of
these algorithms suffer from poor sample efficiency, especially in environments
with sparse rewards. In this paper, we take a step towards addressing this
issue by providing a principled directed exploration strategy. We propose
Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from
a pessimistic actor for temporal difference learning and an optimistic actor to
promote exploration. This is achieved by using the Wasserstein barycenter of
the pessimistic and optimistic policies as the exploration policy and adjusting
the degree of exploration throughout the learning process. We compare WBSAC
with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is
more sample-efficient on MuJoCo continuous control tasks.

</details>


### [112] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/abs/2506.10177)
*Defang Chen,Zhenyu Zhou,Can Wang,Siwei Lyu*

Main category: cs.LG

TL;DR: 发现扩散生成模型的采样轨迹具有共同的低维“回旋镖”形状，并提出了一种改进采样时间表的策略，提升了图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 研究使用扩散生成模型的采样动力学，并揭示其几何规律，以改善生成性能。

Method: 通过分析确定性采样动态，利用核估计的数据建模，研究采样轨迹的几何规律，并提出了一种基于动态规划的方案来更好地对齐采样时间表。

Result: 提出的策略在不明显增加计算复杂度的情况下，提高了图像生成的性能，特别是在仅进行5到10次函数评估的区域。

Conclusion: 采样轨迹遵循低维结构，并且提议的方案改进了采样时间表的对齐，提升了生成质量。

Abstract: Diffusion-based generative models employ stochastic differential equations
(SDEs) and their equivalent probability flow ordinary differential equations
(ODEs) to establish a smooth transformation between complex high-dimensional
data distributions and tractable prior distributions. In this paper, we reveal
a striking geometric regularity in the deterministic sampling dynamics: each
simulated sampling trajectory lies within an extremely low-dimensional
subspace, and all trajectories exhibit an almost identical ''boomerang'' shape,
regardless of the model architecture, applied conditions, or generated content.
We characterize several intriguing properties of these trajectories,
particularly under closed-form solutions based on kernel-estimated data
modeling. We also demonstrate a practical application of the discovered
trajectory regularity by proposing a dynamic programming-based scheme to better
align the sampling time schedule with the underlying trajectory structure. This
simple strategy requires minimal modification to existing ODE-based numerical
solvers, incurs negligible computational overhead, and achieves superior image
generation performance, especially in regions with only $5 \sim 10$ function
evaluations.

</details>


### [113] [A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes](https://arxiv.org/abs/2506.10180)
*Mowafaq Salem Alzboon,Mohammad Al-Batah,Muhyeeddin Alqaraleh,Ahmad Abuashour,Ahmad Fuad Bader*

Main category: cs.LG

TL;DR: 研究评估了多种机器学习算法在糖尿病预测上的效果，神经网络算法表现最好。


<details>
  <summary>Details</summary>
Motivation: 糖尿病在许多国家是一个重要的健康问题，早期识别和控制至关重要。本研究旨在评估机器学习方法在糖尿病预测中的有效性。

Method: 使用机器学习方法对Pima族印第安人糖尿病数据集进行分析，评估不同算法的预测效果。涉及的算法有逻辑回归、决策树、随机森林、k近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络。

Result: 神经网络算法表现最佳，准确率为78.57%，其次是随机森林方法，准确率为76.30%。

Conclusion: 机器学习算法可以有效地进行糖尿病预测，并且是早期检测的有效工具。

Abstract: In many nations, diabetes is becoming a significant health problem, and early
identification and control are crucial. Using machine learning algorithms to
predict diabetes has yielded encouraging results. Using the Pima Indians
Diabetes dataset, this study attempts to evaluate the efficacy of several
machine-learning methods for diabetes prediction. The collection includes
information on 768 patients, such as their ages, BMIs, and glucose levels. The
techniques assessed are Logistic Regression, Decision Tree, Random Forest,
k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,
and Neural Network. The findings indicate that the Neural Network algorithm
performed the best, with an accuracy of 78.57 percent, followed by the Random
Forest method, with an accuracy of 76.30 percent. The study implies that
machine learning algorithms can aid diabetes prediction and be an efficient
early detection tool.

</details>


### [114] [Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition](https://arxiv.org/abs/2506.10184)
*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh*

Main category: cs.LG

TL;DR: 通过探索不同特征选择和降维方法，本研究揭示了这些技术如何改善MLP性能，并为特征工程提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索不同技术对MLP网络性能的影响，并为广泛的机器学习任务提供实践指导。

Method: 该研究包括了三种关键方法：1）使用默认设置的MLP基线训练，2）基于遗传算法（GA）的特征选择，3）基于主成分分析（PCA）的降维。

Result: PCA在低维和无噪声数据集中表现突出，而GA在复杂数据集中通过识别关键特征持续提高准确性。

Conclusion: 特征选择和降维在提高MLP性能方面发挥了相互依赖的作用。该研究为特征工程和神经网络参数优化提供了实践指导。

Abstract: This study conducts an empirical examination of MLP networks investigated
through a rigorous methodical experimentation process involving three diverse
datasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes
three key methods: a) a baseline training using the default settings for the
Multi-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)
based refinement c) Principal Component Analysis (PCA) based dimension
reduction. The results show important information on how such techniques affect
performance. While PCA had showed benefits in low-dimensional and noise-free
datasets GA consistently increased accuracy in complex datasets by accurately
identifying critical features. Comparison reveals that feature selection and
dimensionality reduction play interdependent roles in enhancing MLP
performance. The study contributes to the literature on feature engineering and
neural network parameter optimization, offering practical guidelines for a wide
range of machine learning tasks

</details>


### [115] [Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment](https://arxiv.org/abs/2506.10186)
*Yuhui Ding,Thomas Hofmann*

Main category: cs.LG

TL;DR: 提出了一种放松等变约束的分子生成方法，通过学习样本相关的SO(3)变换对每个分子构建对齐的潜在空间，并在其上训练非等变扩散模型，实现了与等变模型相当的性能和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的等变扩散模型在3D分子生成中表现出色，但由于特化等变架构的限制，其可扩展性和效率受到限制。

Method: 我们的方法为每个分子学习一个样本相关的SO(3)变换来构建对齐的潜在空间，然后在对齐的表示上训练非等变扩散模型。

Result: 实验结果表明，我们的方法在不等变的条件下能显著优于之前报告的非等变模型，同时样本质量与最先进的等变扩散模型相当，并且提高了训练和采样效率。

Conclusion: 我们的方法在不等变的条件下显著优于现有模型，同时具备与最先进等变扩散模型相当的样本质量，并提高了训练和采样效率。

Abstract: Equivariant diffusion models have achieved impressive performance in 3D
molecule generation. These models incorporate Euclidean symmetries of 3D
molecules by utilizing an SE(3)-equivariant denoising network. However,
specialized equivariant architectures limit the scalability and efficiency of
diffusion models. In this paper, we propose an approach that relaxes such
equivariance constraints. Specifically, our approach learns a sample-dependent
SO(3) transformation for each molecule to construct an aligned latent space. A
non-equivariant diffusion model is then trained over the aligned
representations. Experimental results demonstrate that our approach performs
significantly better than previously reported non-equivariant models. It yields
sample quality comparable to state-of-the-art equivariant diffusion models and
offers improved training and sampling efficiency. Our code is available at
https://github.com/skeletondyh/RADM

</details>


### [116] [Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction](https://arxiv.org/abs/2506.10189)
*Mohammad Subhi Al-Batah,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon*

Main category: cs.LG

TL;DR: 机器学习和数据挖掘技术正在革命性地改变口腔癌的诊断方法，此研究发现神经网络模型在预测口腔癌方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 口腔癌在肿瘤学中是一个巨大的挑战，早期诊断和准确预后对于提高患者生存率至关重要。

Method: 本研究采用数据挖掘方法，包括神经网络、K-最近邻(KNN)、支持向量机(SVM)和集成学习技术，并进行了严格的比较分析。

Result: 研究结果表明，神经网络的分类准确率为93.6%，优于其他模型。

Conclusion: 高级数据挖掘技术在口腔癌症的早期检测、治疗优化和最终改善患者预后方面具有显著的潜力。

Abstract: Oral cancer presents a formidable challenge in oncology, necessitating early
diagnosis and accurate prognosis to enhance patient survival rates. Recent
advancements in machine learning and data mining have revolutionized
traditional diagnostic methodologies, providing sophisticated and automated
tools for differentiating between benign and malignant oral lesions. This study
presents a comprehensive review of cutting-edge data mining methodologies,
including Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), and ensemble learning techniques, specifically applied to the diagnosis
and prognosis of oral cancer. Through a rigorous comparative analysis, our
findings reveal that Neural Networks surpass other models, achieving an
impressive classification accuracy of 93,6 % in predicting oral cancer.
Furthermore, we underscore the potential benefits of integrating feature
selection and dimensionality reduction techniques to enhance model performance.
These insights underscore the significant promise of advanced data mining
techniques in bolstering early detection, optimizing treatment strategies, and
ultimately improving patient outcomes in the realm of oral oncology.

</details>


### [117] [DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection](https://arxiv.org/abs/2506.10200)
*Tina Behrouzi,Sana Tonekaboni,Rahul G. Krishnan,Anna Goldenberg*

Main category: cs.LG

TL;DR: DynaSubVAE通过动态分组和自适应OOD检测提升预测准确性，尤其在缺失整个类别时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型往往忽视环境中存在或新兴的异质细分群体，导致预测不准确或有害。为了解决这一问题，DynaSubVAE通过动态更新其潜在结构来适应新兴模式。

Method: DynaSubVAE是一种动态子组变分自编码器框架，通过同时进行表示学习和自适应OOD检测来工作，采用了一种受高斯混合模型启发的非参数聚类机制，以发现和建模基于嵌入相似度的潜在子群。

Result: 实验表明，DynaSubVAE在近OOD和远OOD检测中表现出了竞争力，尤其在整个类别缺失的情况下表现尤佳，并且其动态分组机制在OOD准确性和精度方面优于独立的聚类方法，如GMM和KMeans++。

Conclusion: DynaSubVAE在处理不同种类的OOD检测任务中表现出了出色的性能，尤其在训练过程中缺失整个类别的情况下达到了优秀的效果。

Abstract: Real-world observational data often contain existing or emerging
heterogeneous subpopulations that deviate from global patterns. The majority of
models tend to overlook these underrepresented groups, leading to inaccurate or
even harmful predictions. Existing solutions often rely on detecting these
samples as Out-of-domain (OOD) rather than adapting the model to new emerging
patterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational
Autoencoder framework that jointly performs representation learning and
adaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with
the data by dynamically updating its latent structure to capture new trends. It
leverages a novel non-parametric clustering mechanism, inspired by Gaussian
Mixture Models, to discover and model latent subgroups based on embedding
similarity. Extensive experiments show that DynaSubVAE achieves competitive
performance in both near-OOD and far-OOD detection, and excels in class-OOD
scenarios where an entire class is missing during training. We further
illustrate that our dynamic subgrouping mechanism outperforms standalone
clustering methods such as GMM and KMeans++ in terms of both OOD accuracy and
regret precision.

</details>


### [118] [AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent](https://arxiv.org/abs/2506.10205)
*Jing Liu,Toshiaki Koike-Akino,Ye Wang,Hassan Mansour,Matthew Brand*

Main category: cs.LG

TL;DR: 该研究提出了一种新的基于激活感知的权重剪枝和量化方法AWP，证明其优于现有方法，并提供了理论上的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs体积庞大，特别是在边缘设备上，通常需要采用模型压缩方法，如量化和剪枝。

Method: 提出了一种基于投影梯度下降的激活感知权重剪枝和量化的统一方法，称为AWP。

Result: 实验表明，AWP比当前最先进的LLM剪枝和量化方法效果更好。

Conclusion: AWP outperforms current LLM pruning and quantization methods, with theoretical convergence guarantees.

Abstract: To address the enormous size of Large Language Models (LLMs), model
compression methods, such as quantization and pruning, are often deployed,
especially on edge devices. In this work, we focus on layer-wise post-training
quantization and pruning. Drawing connections between activation-aware weight
pruning and sparse approximation problems, and motivated by the success of
Iterative Hard Thresholding (IHT), we propose a unified method for
Activation-aware Weight pruning and quantization via Projected gradient descent
(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM
pruning and quantization methods. Theoretical convergence guarantees of the
proposed method for pruning are also provided.

</details>


### [119] [Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms](https://arxiv.org/abs/2506.10212)
*Sajjad Karimi,Amit J. Shah,Gari D. Clifford,Reza Sameni*

Main category: cs.LG

TL;DR: 研究探索了心电图（ECG）和心音图（PCG）的交叉重构潜力，发现非线性模型尤其是非因果LSTM表现优异，尤其是在估计ECG中临床相关生物标志物方面。结果展示了心电和心机械模态之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管心电图和心音图在记录心脏电和机械活动时提供了全面的多模态视角，但这些信号的信息内容尚未完全理解。因此，探讨它们在不同生理条件和个体之间的重构和生物标志物提取的潜力显得尤为重要。

Method: 研究使用了一系列线性和非线性机器学习模型，包括非因果长短时记忆网络（LSTM），来从一种信号重构另一种信号，并分析因果关系、生理状态和跨主体差异的影响。

Result: 研究结果表明，非线性模型尤其是非因果LSTM提供了更好的重构性能，其中从PCG重构ECG比反向重构更具可行性。此外，基于包络的建模利用瞬时幅度特征显著改善了跨主体学习的广义化能力。临床相关的ECG生物标志物可在跨主体设置中从PCG估计。

Conclusion: 这项研究揭示了心电图（ECG）和心音图（PCG）之间的关系，并强调了非线性模型在进行心电图和心音图之间的双向重构方面的优势，尤其是在跨主体情况下估计心电图中临床相关生物标志物的可行性。

Abstract: Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a
comprehensive, multimodal perspective on cardiac function by capturing the
heart's electrical and mechanical activities, respectively. However, the
distinct and overlapping information content of these signals, as well as their
potential for mutual reconstruction and biomarker extraction, remains
incompletely understood, especially under varying physiological conditions and
across individuals.
  In this study, we systematically investigate the common and exclusive
characteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous
ECG-PCG recordings during rest and exercise. We employ a suite of linear and
nonlinear machine learning models, including non-causal LSTM networks, to
reconstruct each modality from the other and analyze the influence of
causality, physiological state, and cross-subject variability. Our results
demonstrate that nonlinear models, particularly non-causal LSTM, provide
superior reconstruction performance, with reconstructing ECG from PCG proving
more tractable than the reverse. Exercise and cross-subject scenarios present
significant challenges, but envelope-based modeling that utilizes instantaneous
amplitude features substantially improves cross-subject generalizability for
cross-modal learning. Furthermore, we demonstrate that clinically relevant ECG
biomarkers, such as fiducial points and QT intervals, can be estimated from PCG
in cross-subject settings.
  These findings advance our understanding of the relationship between
electromechanical cardiac modalities, in terms of both waveform characteristics
and the timing of cardiac events, with potential applications in novel
multimodal cardiac monitoring technologies.

</details>


### [120] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
*Chen-Chia Chang,Wan-Hsuan Lin,Yikang Shen,Yiran Chen,Xin Zhang*

Main category: cs.LG

TL;DR: LaMAGIC2提升了模拟拓扑生成的精度和效率，并取得较高的成功率和较低的误差。


<details>
  <summary>Details</summary>
Motivation: 由于现代应用需要大量的手动工程努力，模拟拓扑设计的自动化变得至关重要。

Method: 本文通过引入LaMAGIC2，一种采用标识符的简洁浮点输入标准化方法来进行模拟拓扑生成。

Result: LaMAGIC2在严格公差0.01下取得了34%的更高成功率和10倍更低的MSE，对具有更多顶点的电路也表现出更好的转移性，最高提升达58.5%。

Conclusion: LaMAGIC2被确立为一种用于模拟拓扑生成的健壮框架，展示出更高的成功率和转移能力。

Abstract: Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [121] [A new type of federated clustering: A non-model-sharing approach](https://arxiv.org/abs/2506.10244)
*Yuji Kawamata,Kaoru Kamijo,Maki Kihira,Akihiro Toyoda,Tomoru Nakayama,Akira Imakura,Tetsuya Sakurai,Yukihiko Okada*

Main category: cs.LG

TL;DR: 提出了一种新的联邦聚类方法DC-Clustering，可以在复杂数据划分下进行聚类，具有隐私保护、通信效率和灵活性等优点，适用于医疗和金融等领域。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习聚类方法通常假设简单的数据划分场景，如水平或垂直划分，难以处理更复杂的分布式结构。为了解决这一问题，研究提出了一种新的联邦聚类方法以实现复杂数据划分场景下的聚类。

Method: 该研究提出了一种新的联邦聚类方法，即数据协作聚类（DC-Clustering）。此方法允许在水平和垂直划分共存的复杂数据分区情况下进行聚类。每个机构仅共享中间表示而非原始数据，以确保隐私，同时实现协作聚类。方法支持k-means和谱聚类的灵活选择，并仅需与中央服务器进行一次通信即可达到最终结果。

Result: 实验结果表明，DC-Clustering方法在合成和公开基准数据集上的聚类性能与集中式聚类相媲美。

Conclusion: DC-Clustering在实验中表现出了与集中式聚类相当的性能，并有效填补了现有联邦学习研究中的空白。其隐私保护、通信效率和灵活性特性使其在健康和金融等隐私敏感领域具有应用潜力。

Abstract: In recent years, the growing need to leverage sensitive data across
institutions has led to increased attention on federated learning (FL), a
decentralized machine learning paradigm that enables model training without
sharing raw data. However, existing FL-based clustering methods, known as
federated clustering, typically assume simple data partitioning scenarios such
as horizontal or vertical splits, and cannot handle more complex distributed
structures. This study proposes data collaboration clustering (DC-Clustering),
a novel federated clustering method that supports clustering over complex data
partitioning scenarios where horizontal and vertical splits coexist. In
DC-Clustering, each institution shares only intermediate representations
instead of raw data, ensuring privacy preservation while enabling collaborative
clustering. The method allows flexible selection between k-means and spectral
clustering, and achieves final results with a single round of communication
with the central server. We conducted extensive experiments using synthetic and
open benchmark datasets. The results show that our method achieves clustering
performance comparable to centralized clustering where all data are pooled.
DC-Clustering addresses an important gap in current FL research by enabling
effective knowledge discovery from distributed heterogeneous data. Its
practical properties -- privacy preservation, communication efficiency, and
flexibility -- make it a promising tool for privacy-sensitive domains such as
healthcare and finance.

</details>


### [122] [Meta-learning Representations for Learning from Multiple Annotators](https://arxiv.org/abs/2506.10259)
*Atsutoshi Kumagai,Tomoharu Iwata,Taishi Nishiyama,Yasutoshi Ida,Yasuhiro Fujiwara*

Main category: cs.LG

TL;DR: 提出了一种元学习方法，从多个带噪声的标注者中学习。方法通过使用不同但相关任务的标记数据提高分类器性能，并在真实数据集上显示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，如众包服务，监督学习的标签由多个标注者提供。由于标注者技能或偏见不同，给定标签可能存在噪音。现有方法需要大量噪声标注数据来训练准确分类器，但实际中可能无法获得足够数据。

Method: 该方法使用神经网络将任务中的每个实例嵌入到一个潜在空间，并构建一个概率模型以在该潜在空间上学习任务特定的分类器，同时估计标注者的能力。分类器的适应通过最大化期望最大化算法的后验概率来执行。EM算法的每一步都易于作为封闭形式计算且可微，因此该方法可以有效地通过EM算法的损失进行反向传播，以元学习神经网络。

Result: 方法在带有合成噪声的真实数据集和真实的众包数据集上显示了其有效性。

Conclusion: 该方法有效地利用不同但相关任务中获得的标记数据，能够在有限的标注数据条件下，通过元学习方法提高分类器的测试分类性能。

Abstract: We propose a meta-learning method for learning from multiple noisy
annotators. In many applications such as crowdsourcing services, labels for
supervised learning are given by multiple annotators. Since the annotators have
different skills or biases, given labels can be noisy. To learn accurate
classifiers, existing methods require many noisy annotated data. However,
sufficient data might be unavailable in practice. To overcome the lack of data,
the proposed method uses labeled data obtained in different but related tasks.
The proposed method embeds each example in tasks to a latent space by using a
neural network and constructs a probabilistic model for learning a
task-specific classifier while estimating annotators' abilities on the latent
space. This neural network is meta-learned to improve the expected test
classification performance when the classifier is adapted to a given small
amount of annotated data. This classifier adaptation is performed by maximizing
the posterior probability via the expectation-maximization (EM) algorithm.
Since each step in the EM algorithm is easily computed as a closed-form and is
differentiable, the proposed method can efficiently backpropagate the loss
through the EM algorithm to meta-learn the neural network. We show the
effectiveness of our method with real-world datasets with synthetic noise and
real-world crowdsourcing datasets.

</details>


### [123] [Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification](https://arxiv.org/abs/2506.10269)
*Ryota Ueda,Takami Sato,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: SDP relaxation is promising for DNN verification but struggles with deep networks due to feasibility issues. The paper suggests solutions addressing these challenges, solving 88% of previously difficult problems.


<details>
  <summary>Details</summary>
Motivation: SDP relaxation offers tighter bounds for DNN verification but faces issues with interior-point vanishing, which impacts numerical stability and scalability for deeper networks.

Method: Investigated five solutions to enhance feasibility conditions in SDP relaxation, using both theoretical and empirical analysis to address interior-point vanishing.

Result: Successfully solved 88% of previously unsolvable problems, accounting for 41% of the total, by improving feasibility conditions and revealing that traditional constraints are often detrimental.

Conclusion: This study identifies limitations in SDP relaxation for deep networks and provides solutions to improve feasibility and applicability for deeper neural networks, enhancing reliability and security.

Abstract: Semidefinite programming (SDP) relaxation has emerged as a promising approach
for neural network verification, offering tighter bounds than other convex
relaxation methods for deep neural networks (DNNs) with ReLU activations.
However, we identify a critical limitation in the SDP relaxation when applied
to deep networks: interior-point vanishing, which leads to the loss of strict
feasibility -- a crucial condition for the numerical stability and optimality
of SDP. Through rigorous theoretical and empirical analysis, we demonstrate
that as the depth of DNNs increases, the strict feasibility is likely to be
lost, creating a fundamental barrier to scaling SDP-based verification. To
address the interior-point vanishing, we design and investigate five solutions
to enhance the feasibility conditions of the verification problem. Our methods
can successfully solve 88% of the problems that could not be solved by existing
methods, accounting for 41% of the total. Our analysis also reveals that the
valid constraints for the lower and upper bounds for each ReLU unit are
traditionally inherited from prior work without solid reasons, but are actually
not only unbeneficial but also even harmful to the problem's feasibility. This
work provides valuable insights into the fundamental challenges of SDP-based
DNN verification and offers practical solutions to improve its applicability to
deeper neural networks, contributing to the development of more reliable and
secure systems with DNNs.

</details>


### [124] [Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning](https://arxiv.org/abs/2506.10282)
*Jiajin Liu,Dongzhe Fan,Jiacheng Shen,Chuanhao Ji,Daochen Zha,Qiaoyu Tan*

Main category: cs.LG

TL;DR: 本文提出了一个名为Graph-MLLM的统一基准，用于评估多模态图学习中的不同方法，并通过实验发现融合视觉和文本属性提高了性能，微调可以在多数场景中取得先进的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态图学习方法缺乏一个统一的基准来公正地评估这些方法的进展。因此需要一个综合性的基准来系统地评估多模态图学习的方法。

Method: 本文介绍了Graph-MLLM，这是一种综合基准，通过系统地评估三种范式：Encoder、Aligner和Predictor，跨越六个不同领域的数据集进行多模态图学习。

Result: 通过广泛的实验发现，将视觉属性转换为文本描述比直接使用视觉输入进一步提高了性能。此外，微调特定MMGs上的MLLMs在大多数情况下可以达到最先进的结果。

Conclusion: 综合考虑节点的视觉和文本属性有助于图学习，特别是在使用预训练的文本到图像对齐模型作为编码器时。在大多数情况下，即使不使用明确的图结构信息，针对特定MMGs微调MLLMs依然能够实现最先进的结果。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in representing and understanding diverse modalities. However,
they typically focus on modality alignment in a pairwise manner while
overlooking structural relationships across data points. Integrating
multimodality with structured graph information (i.e., multimodal graphs, MMGs)
is essential for real-world applications such as social networks, healthcare,
and recommendation systems. Existing MMG learning methods fall into three
paradigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.
MLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via
multimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in
language or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor
treats MLLMs as standalone reasoners with in-context learning or fine-tuning.
Despite their advances, the MMG field lacks a unified benchmark to fairly
evaluate across these approaches, making it unclear what progress has been
made. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for
multimodal graph learning by systematically evaluating these three paradigms
across six datasets with different domains. Through extensive experiments, we
observe that jointly considering the visual and textual attributes of the nodes
benefits graph learning, even when using pre-trained text-to-image alignment
models (e.g., CLIP) as encoders. We also find that converting visual attributes
into textual descriptions further improves performance compared to directly
using visual inputs. Moreover, we observe that fine-tuning MLLMs on specific
MMGs can achieve state-of-the-art results in most scenarios, even without
explicit graph structure information. We hope that our open-sourced library
will facilitate rapid, equitable evaluation and inspire further innovative
research in this field.

</details>


### [125] [Collaborative Min-Max Regret in Grouped Multi-Armed Bandits](https://arxiv.org/abs/2506.10313)
*Moïse Blanchard,Vineet Goyal*

Main category: cs.LG

TL;DR: 研究了多臂老虎机的群体协作探索问题，提出了一种优化探索成本的算法Col-UCB，降低了群体之间的协作遗憾。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机的情境中，探索成本在群体间不平衡的问题显著存在，需要一种能够在群体间平衡探索负担的算法。

Method: 引入了一种名为Col-UCB的算法，动态协调群体之间的探索。该算法能在共享的操作集结构下自适应地降低协作遗憾。

Result: Col-UCB算法成功减少了协作遗憾，达到了最佳的最小最大化和与实例依赖相关的协作遗憾上限。

Conclusion: 通过在多臂老虎机问题中引入Col-UCB算法，能够在群体间实现动态的探索协调，从而在协作遗憾上达到最优的最小最大化和实例依赖性。

Abstract: We study the impact of sharing exploration in multi-armed bandits in a
grouped setting where a set of groups have overlapping feasible action sets
[Baek and Farias '24]. In this grouped bandit setting, groups share reward
observations, and the objective is to minimize the collaborative regret,
defined as the maximum regret across groups. This naturally captures
applications in which one aims to balance the exploration burden between groups
or populations -- it is known that standard algorithms can lead to
significantly imbalanced exploration cost between groups. We address this
problem by introducing an algorithm Col-UCB that dynamically coordinates
exploration across groups. We show that Col-UCB achieves both optimal minimax
and instance-dependent collaborative regret up to logarithmic factors. These
bounds are adaptive to the structure of shared action sets between groups,
providing insights into when collaboration yields significant benefits over
each group learning their best action independently.

</details>


### [126] [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
*Luc Raszewski,Christine De Kock*

Main category: cs.LG

TL;DR: Applying meta-learning to sockpuppet detection on Wikipedia increases adaptability and precision in author-specific contexts, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current machine learning approaches to sockpuppet detection on Wikipedia lack adaptability to specific author behaviours, resulting in decreased effectiveness, especially with limited text data.

Method: Meta-learning, which optimizes models for rapid adaptation by training across multiple tasks, was applied to improve detection results in data-scarce environments.

Result: The application of meta-learning showed a significant improvement in the precision of detecting sockpuppets compared to existing pre-trained models.

Conclusion: Meta-learning significantly enhances the precision of sockpuppet detection on Wikipedia, providing better adaptability to author-specific styles compared to pre-trained models.

Abstract: Malicious sockpuppet detection on Wikipedia is critical to preserving access
to reliable information on the internet and preventing the spread of
disinformation. Prior machine learning approaches rely on stylistic and
meta-data features, but do not prioritise adaptability to author-specific
behaviours. As a result, they struggle to effectively model the behaviour of
specific sockpuppet-groups, especially when text data is limited. To address
this, we propose the application of meta-learning, a machine learning technique
designed to improve performance in data-scarce settings by training models
across multiple tasks. Meta-learning optimises a model for rapid adaptation to
the writing style of a new sockpuppet-group. Our results show that
meta-learning significantly enhances the precision of predictions compared to
pre-trained models, marking an advancement in combating sockpuppetry on open
editing platforms. We release a new dataset of sockpuppet investigations to
foster future research in both sockpuppetry and meta-learning fields.

</details>


### [127] [PyLO: Towards Accessible Learned Optimizers in PyTorch](https://arxiv.org/abs/2506.10315)
*Paul Janson,Benjamin Therien,Quentin Anthony,Xiaolong Huang,Abhinav Moudgil,Eugene Belilovsky*

Main category: cs.LG

TL;DR: PyLO library makes learned optimizers more accessible to PyTorch users, offering speed improvements and enhanced functionality through integration with existing tools.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to make learned optimizers more accessible to the broader machine learning community by providing a user-friendly, PyTorch-based library.

Method: PyLO integrates learned optimizers into PyTorch, enabling CUDA-accelerated optimizations and compatibility with existing PyTorch tools like learning rate schedules and weight decay.

Result: With PyLO, significant speed improvements in training throughput are achieved, and learned optimizers benefit from integration with PyTorch's optimization tools.

Conclusion: PyLO effectively bridges the gap in accessibility by providing a PyTorch-based library for learned optimizers, enhancing their practical use in the machine learning community.

Abstract: Learned optimizers have been an active research topic over the past decade,
with increasing progress toward practical, general-purpose optimizers that can
serve as drop-in replacements for widely used methods like Adam. However,
recent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --
remain largely inaccessible to the broader community, in part due to their
reliance on JAX and the absence of user-friendly packages for applying the
optimizers after meta-training. To address this gap, we introduce PyLO, a
PyTorch-based library that brings learned optimizers to the broader machine
learning community through familiar, widely adopted workflows. Unlike prior
work focused on synthetic or convex tasks, our emphasis is on applying learned
optimization to real-world large-scale pre-training tasks. Our release includes
a CUDA-accelerated version of the small_fc_lopt learned optimizer architecture
from (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to
205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO
also allows us to easily combine learned optimizers with existing optimization
tools such as learning rate schedules and weight decay. When doing so, we find
that learned optimizers can substantially benefit. Our code is available at
https://github.com/Belilovsky-Lab/pylo

</details>


### [128] [Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data](https://arxiv.org/abs/2506.10332)
*Aaryam Sharma*

Main category: cs.LG

TL;DR: 利用时空图神经网络显着提高了空气质量指数的预测精度，发现了新的空气质量模式和空间关系。


<details>
  <summary>Details</summary>
Motivation: 当前政府发布的空气质量数据无法准确反映局部空气污染情况，因此需要更精确的预测方法。

Method: 我们使用了时空图神经网络（Spatio-temporal GNNs）来进行 AQI 预测。

Result: 我们在未见过的坐标上实现了 79% 的误差减少，取得了 71.654 的 MSE 改进，同时还发现了 AQI 的一些新见解，例如短期模式和空间关系的变化。

Conclusion: 通过使用新的方法，我们成功地改进了 AQI 的预测，从而可以更好地捕捉局部空气质量现实。

Abstract: Air pollution has become a significant health risk in developing countries.
While governments routinely publish air-quality index (AQI) data to track
pollution, these values fail to capture the local reality, as sensors are often
very sparse. In this paper, we address this gap by predicting AQI in 1 km^2
neighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal
GNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen
coordinates. New insights about AQI such as the existence of strong repetitive
short-term patterns and changing spatial relations are also discovered. The
code is available on GitHub.

</details>


### [129] [Provably Learning from Language Feedback](https://arxiv.org/abs/2506.10341)
*Wanqiao Xu,Allen Nie,Ruijie Zheng,Aditya Modi,Adith Swaminathan,Ching-An Cheng*

Main category: cs.LG

TL;DR: 该研究提出了语言反馈学习问题的复杂性测量，开发了HELiX算法，实现了从语言反馈中快速学习的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管观察到语言模型代理的显著演示，现有方法在决策问题的原则性框架上仍然缺乏，我们旨在通过规范化语言反馈学习（LLF）问题来填补此空白。

Method: 引入了一种新的复杂性度量“传输消除维度”，并开发了一种无悔算法HELiX，通过顺序交互解决语言反馈学习问题，性能随问题的传输消除维度而变化。

Result: HELiX算法在多个经验领域表现良好，即使在反复提示大型语言模型时效果不可靠，依然能够提供有性能保证的解决方案。

Conclusion: 我们首次在复杂性测量和算法上为从语言反馈学习问题提供了规范化的框架，为交互学习算法的设计奠定了基础。

Abstract: Interactively learning from observation and language feedback is an
increasingly studied area driven by the emergence of large language model (LLM)
agents. While impressive empirical demonstrations have been shown, so far a
principled framing of these decision problems remains lacking. In this paper,
we formalize the Learning from Language Feedback (LLF) problem, assert
sufficient assumptions to enable learning despite latent rewards, and introduce
$\textit{transfer eluder dimension}$ as a complexity measure to characterize
the hardness of LLF problems. We show that transfer eluder dimension captures
the intuition that information in the feedback changes the learning complexity
of the LLF problem. We demonstrate cases where learning from rich language
feedback can be exponentially faster than learning from reward. We develop a
no-regret algorithm, called $\texttt{HELiX}$, that provably solves LLF problems
through sequential interactions, with performance guarantees that scale with
the transfer eluder dimension of the problem. Across several empirical domains,
we show that $\texttt{HELiX}$ performs well even when repeatedly prompting LLMs
does not work reliably. Our contributions mark a first step towards designing
principled interactive learning algorithms from generic language feedback.

</details>


### [130] [PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation](https://arxiv.org/abs/2506.10351)
*Yanlong Chen,Mattia Orlandi,Pierangelo Maria Rapa,Simone Benatti,Luca Benini,Yawei Li*

Main category: cs.LG

TL;DR: 提出了一种基于小波的方法来分析生理信号，并通过两个大规模预训练模型（EMG和ECG）及多模态框架，解决了低信噪比和高变异性等挑战，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决生理信号中由运动伪影、基线漂移和其他低信噪比干扰所导致的分析困难，以及信号的强非平稳性，传统的时域或过滤方法难以有效表示这些信号的动态演变。

Method: 采用一种新颖的小波方法来进行生理信号分析，捕获多尺度时间-频率特征；并引入了两个大规模的预训练EMG和ECG模型，以及一个统一的多模态框架，通过学习的加权融合来整合各自预训练模型。

Result: 所提出的方法在下游任务中达到了优于现有方法的性能，并在多模态任务中表现突出。

Conclusion: 提出的基于小波的方法为多种生理信号分析奠定了坚实基础，并暗示下一代生理信号处理的可能性，可能对可穿戴健康监测、临床诊断及更广泛的生物医学应用产生影响。

Abstract: Physiological signals are often corrupted by motion artifacts, baseline
drift, and other low-SNR disturbances, which pose significant challenges for
analysis. Additionally, these signals exhibit strong non-stationarity, with
sharp peaks and abrupt changes that evolve continuously, making them difficult
to represent using traditional time-domain or filtering methods. To address
these issues, a novel wavelet-based approach for physiological signal analysis
is presented, aiming to capture multi-scale time-frequency features in various
physiological signals. Leveraging this technique, two large-scale pretrained
models specific to EMG and ECG are introduced for the first time, achieving
superior performance and setting new baselines in downstream tasks.
Additionally, a unified multi-modal framework is constructed by integrating
pretrained EEG model, where each modality is guided through its dedicated
branch and fused via learnable weighted fusion. This design effectively
addresses challenges such as low signal-to-noise ratio, high inter-subject
variability, and device mismatch, outperforming existing methods on multi-modal
tasks. The proposed wavelet-based architecture lays a solid foundation for
analysis of diverse physiological signals, while the multi-modal design points
to next-generation physiological signal processing with potential impact on
wearable health monitoring, clinical diagnostics, and broader biomedical
applications.

</details>


### [131] [History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials](https://arxiv.org/abs/2506.10352)
*Binyao Guo,Zihan Lin,QiZhi He*

Main category: cs.LG

TL;DR: 该研究开发了称为HANO的神经算子模型，用于路径依赖非弹性材料的建模，表现出色，适合与传统数值求解器结合使用。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是开发一种基于数据驱动的建模框架，用于模拟路径依赖的非弹性材料的不可逆演变，并通过可观察数据推断隐藏的动态。

Method: 研究开发了历史感知神经算子(HANO),这是一种自回归模型,通过傅里叶神经算子主干实现离散不变学习,并嵌入分级自注意力机制以便进行多尺度特征提取。

Result: HANO在预测精度、泛化能力和鲁棒性方面对基准问题表现优异，能够有效模拟非弹性材料并适合与经典数值求解器集成。

Conclusion: HANO克服了RNN模型中的自一致性问题和对初始状态的敏感性，能够适应复杂条件下的变应变路径。

Abstract: This study presents an end-to-end learning framework for data-driven modeling
of path-dependent inelastic materials using neural operators. The framework is
built on the premise that irreversible evolution of material responses,
governed by hidden dynamics, can be inferred from observable data.
  We develop the History-Aware Neural Operator (HANO), an autoregressive model
that predicts path-dependent material responses from short segments of recent
strain-stress history without relying on hidden state variables, thereby
overcoming self-consistency issues commonly encountered in recurrent neural
network (RNN)-based models. Built on a Fourier-based neural operator backbone,
HANO enables discretization-invariant learning. To enhance its ability to
capture both global loading patterns and critical local path dependencies, we
embed a hierarchical self-attention mechanism that facilitates multiscale
feature extraction.
  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial
hidden states, a commonly overlooked issue that can lead to instability in
recurrent models when applied to generalized loading paths. By modeling
stress-strain evolution as a continuous operator rather than relying on fixed
input-output mappings, HANO naturally accommodates varying path discretizations
and exhibits robust performance under complex conditions, including irregular
sampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate
HANO on two benchmark problems: elastoplasticity with hardening and progressive
anisotropic damage in brittle solids. Results show that HANO consistently
outperforms baseline models in predictive accuracy, generalization, and
robustness. With its demonstrated capabilities, HANO provides an effective
data-driven surrogate for simulating inelastic materials and is well-suited for
integration with classical numerical solvers.

</details>


### [132] [TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree](https://arxiv.org/abs/2506.10355)
*Yu-Yang Qian,Yuan-Ze Xu,Zhen-Yu Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 提出了一种称为TreeLoRA的新方法，提高大规模预训练模型在持续学习中的效率，通过层次梯度相似性和稀疏梯度更新等技术，并证明其在视觉和自然语言处理任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于大规模预训练模型的计算需求和参数规模日益增长，提高持续学习的效率变得越来越重要。

Method: 通过使用层次梯度相似性构建层状适配器，并使用带状技术进行任务结构探索，还利用稀疏梯度更新促进参数优化。

Result: 实验和理论分析表明，该方法能够有效减少任务相似性估计的计算负担，并提高大规模预训练模型的适应能力。

Conclusion: 本研究引入了TreeLoRA，一种通过利用层次梯度相似性构建层状适配器的方法，提高了大规模预训练模型在持续学习中的效率。实验结果证明了TreeLoRA在视觉处理和自然语言处理任务中的有效性和效率。

Abstract: Many real-world applications collect data in a streaming environment, where
learning tasks are encountered sequentially. This necessitates continual
learning (CL) to update models online, enabling adaptation to new tasks while
preserving past knowledge to prevent catastrophic forgetting. Nowadays, with
the flourish of large pre-trained models (LPMs), efficiency has become
increasingly critical for CL, due to their substantial computational demands
and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of
Low-Rank Adapters), a novel approach that constructs layer-wise adapters by
leveraging hierarchical gradient similarity to enable efficient CL,
particularly for LPMs. To reduce the computational burden of task similarity
estimation, we employ bandit techniques to develop an algorithm based on lower
confidence bounds to efficiently explore the task structure. Furthermore, we
use sparse gradient updates to facilitate parameter optimization, making the
approach better suited for LPMs. Theoretical analysis is provided to justify
the rationale behind our approach, and experiments on both vision transformers
(ViTs) and large language models (LLMs) demonstrate the effectiveness and
efficiency of our approach across various domains, including vision and natural
language processing tasks.

</details>


### [133] [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
*Penguin Huang,Chhavi Yadav,Ruihan Wu,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究介绍了PropInfer任务和两种攻击方法，揭示LLMs在属性推断攻击中的潜在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨大语言模型(LLMs)在经过领域特定数据集微调时，其属性推断攻击是否有效，并引入新的评估任务和攻击手段。

Method: 提出了PropInfer基准任务，用于评估LLM在两种微调范式下的属性推断能力，并设计了两种专门的攻击方式：基于提示的生成攻击和利用词频信号的影子模型攻击。

Result: 实验结果显示，多种预训练LLM在我们的攻击下成功地暴露出之前未被认识到的漏洞。

Conclusion: 本研究揭示了大语言模型在特定领域的数据调整中存在未被识别的脆弱性，尤其是在属性推断攻击方面。

Abstract: Large language models (LLMs) are increasingly fine-tuned on domain-specific
datasets to support applications in fields such as healthcare, finance, and
law. These fine-tuning datasets often have sensitive and confidential
dataset-level properties -- such as patient demographics or disease prevalence
-- that are not intended to be revealed. While prior work has studied property
inference attacks on discriminative models (e.g., image classification models)
and generative models (e.g., GANs for image data), it remains unclear if such
attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark
task for evaluating property inference in LLMs under two fine-tuning paradigms:
question-answering and chat-completion. Built on the ChatDoctor dataset, our
benchmark includes a range of property types and task configurations. We
further propose two tailored attacks: a prompt-based generation attack and a
shadow-model attack leveraging word frequency signals. Empirical evaluations
across multiple pretrained LLMs show the success of our attacks, revealing a
previously unrecognized vulnerability in LLMs.

</details>


### [134] [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/abs/2506.10378)
*Jikai Jin,Vasilis Syrgkanis,Sham Kakade,Hanlin Zhang*

Main category: cs.LG

TL;DR: 提出一种因果表示学习方法，通过线性因果结构解释语言模型基准测试的性能变化，揭示能力发展的因果路径。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的能力评估中面临复杂的混杂效应和庞大的计算消耗，为此，作者提出了一种新方法来应对这些挑战。

Method: 提出因果表示学习框架，通过线性变换模型化观察到的基准表现，将基模型处理为共同混杂因素后识别潜在因果关系。

Result: 在超过1500个模型和六个基准测试数据集中，成功识别出能够解释性能变化的三节点线性因果结构，并提供了基于因果结构的科学见解。

Conclusion: 研究揭示了在语言模型的基准测试表现中存在三节点线性因果结构，展示了一条从一般问题解决能力，经由指令跟随能力，到数学推理能力的因果路径。

Abstract: Faithful evaluation of language model capabilities is crucial for deriving
actionable insights that can inform model development. However, rigorous causal
evaluations in this domain face significant methodological challenges,
including complex confounding effects and prohibitive computational costs
associated with extensive retraining. To tackle these challenges, we propose a
causal representation learning framework wherein observed benchmark performance
is modeled as a linear transformation of a few latent capability factors.
Crucially, these latent factors are identified as causally interrelated after
appropriately controlling for the base model as a common confounder. Applying
this approach to a comprehensive dataset encompassing over 1500 models
evaluated across six benchmarks from the Open LLM Leaderboard, we identify a
concise three-node linear causal structure that reliably explains the observed
performance variations. Further interpretation of this causal structure
provides substantial scientific insights beyond simple numerical rankings:
specifically, we reveal a clear causal direction starting from general
problem-solving capabilities, advancing through instruction-following
proficiency, and culminating in mathematical reasoning ability. Our results
underscore the essential role of carefully controlling base model variations
during evaluation, a step critical to accurately uncovering the underlying
causal relationships among latent model capabilities.

</details>


### [135] [EQA-RM: A Generative Embodied Reward Model with Test-time Scaling](https://arxiv.org/abs/2506.10389)
*Yuhang Chen,Zhen Tan,Tianlong Chen*

Main category: cs.LG

TL;DR: 引入EQA-RM，一个专为化身问答任务设计的奖励模型，使用新的C-GRPO策略进行训练，在Benchmark中表现出色。代码和数据集已开放。


<details>
  <summary>Details</summary>
Motivation: 目前的通用方法未能充分考虑化身问答任务中代理的空间、时间和逻辑理解，因此需要新的奖励模型来评估这些复杂任务。

Method: 使用一种名为Contrastive Group Relative Policy Optimization (C-GRPO)的策略来训练新的生成多模态奖励模型EQA-RM。

Result: EQA-RM在使用700样本情况下的EQA-RM-Bench上实现了61.9%的准确率，优于强大的专有基线和开放源码的最新模型。

Conclusion: EQA-RM提供了一种新的方式来评估化身问答任务的奖励模型，其创新的C-GRPO策略可生成细粒度的行为区分。

Abstract: Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.

</details>


### [136] [Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation](https://arxiv.org/abs/2506.10403)
*Tzu-Heng Huang,Harit Vishwakarma,Frederic Sala*

Main category: cs.LG

TL;DR: PAJAMA通过程序化的评估替代直接用LLMs评分，显著降低成本，提高评估一致性并减少偏见。


<details>
  <summary>Details</summary>
Motivation: 使用大型语言模型（LLMs）进行生成和响应质量评估存在高成本、不确定性和偏见等问题。

Method: 提出一种新方案PAJAMA，通过使用LLM合成可执行的评估程序以替代直接评分，这些程序可本地存储和运行。

Result: 降低了偏见，提高了一致性，整体判断一致性提升了15.83%，偏见减少了23.7%；在奖励基准的CHAT-HARD子集中表现优于LLM评判者，同时成本降低了三个数量级。

Conclusion: PAJAMA方案不仅降低了成本，而且在评估一致性和减少偏见方面显著优于直接使用LLMs进行评估。

Abstract: Large language models (LLMs) are widely used to evaluate the quality of LLM
generations and responses, but this leads to significant challenges: high API
costs, uncertain reliability, inflexible pipelines, and inherent biases. To
address these, we introduce PAJAMA (Program-As-a-Judge for Automated Model
Assessment), a new alternative that uses LLMs to synthesize executable judging
programs instead of directly scoring responses. These synthesized programs can
be stored and run locally, costing orders of magnitude less while providing
interpretable, and auditable judging logic that can be easily adapted.
Program-based judges mitigate biases, improving judgment consistency by 15.83%
and reducing biased responses by 23.7% on average compared to a
Qwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a
model, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of
RewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the
JudgeLM dataset, all at three orders of magnitude lower cost.

</details>


### [137] [Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height](https://arxiv.org/abs/2506.10404)
*Bryan Shaddy,Brianna Binder,Agnimitra Dasgupta,Haitong Qin,James Haley,Angel Farguell,Kyle Hilburn,Derek V. Mallia,Adam Kochanski,Jan Mandel,Assad Oberai*

Main category: cs.LG

TL;DR: 研究开发了一种基于生成对抗网络的火灾进展估计模型，通过结合卫星观测、点火数据和地形信息，提高了大气-野火模型的预测精度，与实际观测结果的相似度较高。


<details>
  <summary>Details</summary>
Motivation: 随着野火的频繁发生，对野火蔓延预测的需求日益增加。然而，即使是最复杂的野火模型在多日模拟中也会偏离实际观测结果，这促使人们需要进行数据同化。本研究的目的是开发一种方法，将测量数据同化到大气-野火模型中以提高野火进展预测的精度。

Method: 研究中使用条件生成对抗网络（GAN）结合WRF-SFIRE大气-火灾模型的模拟数据进行训练。火灾进展以火灾到达时间表示，模型在火灾到达时间、观测数据和地形数据组成的元组上进行训练。训练完成后，模型使用真实火灾的观测数据和相应的地形数据生成火灾到达时间的样本。

Result: 模型在五个太平洋地区的美国野火上进行验证，与通过飞行器测量的高分辨率火灾边界相比，平均Sorensen-Dice系数达到0.81。研究还评估了地形高度对火灾到达时间推断的影响，发现当推断基于卫星观测时，地形对结果的影响很小。

Conclusion: 本研究开发了一种结合VIIRS活跃火灾观测、GOES点火时间和地形高度数据来估计火灾进展的模型。通过训练条件生成对抗网络并结合WRF-SFIRE模拟的历史火灾数据，成功将大气-火灾模型中的物理特性整合到火灾进展估计中，并在真实火灾的验证中取得了良好的效果。

Abstract: Increasing wildfire occurrence has spurred growing interest in wildfire
spread prediction. However, even the most complex wildfire models diverge from
observed progression during multi-day simulations, motivating need for data
assimilation. A useful approach to assimilating measurement data into complex
coupled atmosphere-wildfire models is to estimate wildfire progression from
measurements and use this progression to develop a matching atmospheric state.
In this study, an approach is developed for estimating fire progression from
VIIRS active fire measurements, GOES-derived ignition times, and terrain height
data. A conditional Generative Adversarial Network is trained with simulations
of historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus
allowing incorporation of WRF-SFIRE physics into estimates. Fire progression is
succinctly represented by fire arrival time, and measurements for training are
obtained by applying an approximate observation operator to WRF-SFIRE
solutions, eliminating need for satellite data during training. The model is
trained on tuples of fire arrival times, measurements, and terrain, and once
trained leverages measurements of real fires and corresponding terrain data to
generate samples of fire arrival times. The approach is validated on five
Pacific US wildfires, with results compared against high-resolution perimeters
measured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.
The influence of terrain height on the arrival time inference is also evaluated
and it is observed that terrain has minimal influence when the inference is
conditioned on satellite measurements.

</details>


### [138] [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
*Ching Chang,Jeehyun Hwang,Yidan Shi,Haixin Wang,Wen-Chih Peng,Tien-Fu Chen,Wei Wang*

Main category: cs.LG

TL;DR: 本文引入了Time-IMM数据集和IMM-TSF基准库，用于改善不规则多模态时间序列预测的表现，并且已经公开发布供研究使用。


<details>
  <summary>Details</summary>
Motivation: 现有基准常假定时间序列数据是干净、定期采样和单一模式的，与真实世界应用中的不规则、多模态和混乱的情况不符，因此需要展示更接近现实应用的数据集和评估方法。

Method: 引入了Time-IMM数据集和IMM-TSF基准库，分别用于捕捉因果驱动的多模态多变量时间序列不规则性，以及在不规则多模态时间序列上进行预测的基准测试。

Result: 实证结果表明，在不规则时间序列数据中明确建模多模态性能够显著提高预测性能。

Conclusion: 研究表明，通过明确建模不规则时间序列中的多模态性能够显著提升预测性能。Time-IMM和IMM-TSF为在真实世界条件下推进时间序列分析提供了坚实基础。

Abstract: Time series data in real-world applications such as healthcare, climate
modeling, and finance are often irregular, multimodal, and messy, with varying
sampling rates, asynchronous modalities, and pervasive missingness. However,
existing benchmarks typically assume clean, regularly sampled, unimodal data,
creating a significant gap between research and real-world deployment. We
introduce Time-IMM, a dataset specifically designed to capture cause-driven
irregularity in multimodal multivariate time series. Time-IMM represents nine
distinct types of time series irregularity, categorized into trigger-based,
constraint-based, and artifact-based mechanisms. Complementing the dataset, we
introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal
time series, enabling asynchronous integration and realistic evaluation.
IMM-TSF includes specialized fusion modules, including a timestamp-to-text
fusion module and a multimodality fusion module, which support both
recency-aware averaging and attention-based integration strategies. Empirical
results demonstrate that explicitly modeling multimodality on irregular time
series data leads to substantial gains in forecasting performance. Time-IMM and
IMM-TSF provide a foundation for advancing time series analysis under
real-world conditions. The dataset is publicly available at
https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the
benchmark library can be accessed at
https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.

</details>


### [139] [Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization](https://arxiv.org/abs/2506.10419)
*Weiying Zhao,Aleksei Unagaev,Natalia Efremova*

Main category: cs.LG

TL;DR: 研究提出光谱聚类结合条件拉丁超立方采样的方法，可以提高土壤有机碳采样的协变量特征空间和空间异质性覆盖，有望提高土壤有机碳预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的土壤有机碳监测方法中，选择具有代表性的采样地点往往基于环境协变量。现有的条件拉丁超立方采样（cLHS）存在一些不足，可能忽略较小但重要的环境簇群，因此有必要开发一种新的方法提高采样的代表性。

Method: 本研究提出了一种新的混合方法，将光谱聚类和条件拉丁超立方采样（cLHS）结合，应用于土壤有机碳监测的采样地点选择。光谱聚类首先依据多变量协变量数据将研究区域划分为K个同质区，然后在每个区域内部应用条件拉丁超立方采样（cLHS）以选择采样地点。

Result: 使用真实的土壤有机碳地图数据集进行实验，结果表明光谱-cLHS相较于标准cLHS能在协变量特征空间和空间异质性上提供更均匀的覆盖。

Conclusion: 利用光谱聚类与条件拉丁超立方采样相结合的方法，提升了土壤有机碳取样地点的代表性，使得即使是较小但重要的环境簇群也能够被取样，并改善了协变量特征空间和空间异质性的覆盖。

Abstract: Soil organic carbon (SOC) monitoring often relies on selecting representative
field sampling locations based on environmental covariates. We propose a novel
hybrid methodology that integrates spectral clustering - an unsupervised
machine learning technique with conditioned Latin hypercube sampling (cLHS) to
enhance the representativeness of SOC sampling. In our approach, spectral
clustering partitions the study area into $K$ homogeneous zones using
multivariate covariate data, and cLHS is then applied within each zone to
select sampling locations that collectively capture the full diversity of
environmental conditions. This hybrid spectral-cLHS method ensures that even
minor but important environmental clusters are sampled, addressing a key
limitation of vanilla cLHS which can overlook such areas. We demonstrate on a
real SOC mapping dataset that spectral-cLHS provides more uniform coverage of
covariate feature space and spatial heterogeneity than standard cLHS. This
improved sampling design has the potential to yield more accurate SOC
predictions by providing better-balanced training data for machine learning
models.

</details>


### [140] [System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters](https://arxiv.org/abs/2506.10434)
*Nart Gashi,Panagiotis Kakosimos,George Papafotiou*

Main category: cs.LG

TL;DR: This paper demonstrates the effectiveness of Kolmogorov-Arnold Networks (KANs) in identifying and analyzing dynamic systems, providing improved scalability and interpretability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the Kolmogorov-Arnold representation theorem to offer improved scalability, accuracy, and interpretability for system identification in dynamic systems compared to traditional neural networks.

Method: The methodology includes approximating state derivatives using Kolmogorov-Arnold Networks (KANs), constructing interpretable state-space representations, and validating these models through numerical experiments.

Result: The results show that KANs can effectively model and analyze the dynamics of a buck converter system, demonstrating their potential for accurate system identification and parameter estimation in industrial applications.

Conclusion: Kolmogorov-Arnold Networks (KANs) can accurately identify system dynamics, verify model consistency, and detect parameter changes in dynamic systems, particularly demonstrated through a buck converter model.

Abstract: Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for
interpretable and efficient system identification in dynamic systems. By
leveraging the Kolmogorov-Arnold representation theorem, KANs enable function
approximation through learnable activation functions, offering improved
scalability, accuracy, and interpretability compared to traditional neural
networks. This paper investigates the application of KANs to model and analyze
the dynamics of a buck converter system, focusing on state-space parameter
estimation along with discovering the system equations. Using simulation data,
the methodology involves approximating state derivatives with KANs,
constructing interpretable state-space representations, and validating these
models through numerical experiments. The results demonstrate the ability of
KANs to accurately identify system dynamics, verify model consistency, and
detect parameter changes, providing valuable insights into their applicability
for system identification in modern industrial systems.

</details>


### [141] [MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices](https://arxiv.org/abs/2506.10443)
*Zhaode Wang,Jingbang Yang,Xinyu Qian,Shiwen Xing,Xiaotang Jiang,Chengfei Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: MNN-LLM optimizes LLMs for mobile devices, achieving substantial speedups and reduced memory usage by employing quantization, hybrid storage, and performance enhancement strategies.


<details>
  <summary>Details</summary>
Motivation: The substantial scale and computational resource requirement of LLMs lead to high inference costs, motivating the need for LLM deployment on resource-limited edge devices.

Method: MNN-LLM utilizes model quantization, DRAM-Flash hybrid storage, rearranges weights and inputs as per mobile CPU and GPU architecture, and uses multicore load balancing, mixed-precision operations, and geometric computations.

Result: MNN-LLM effectively reduces memory usage and enhances performance for edge inference of LLMs, realizing a significant speed increase.

Conclusion: MNN-LLM achieves significant performance improvements with up to 8.6x speed increase compared to existing LLM frameworks on edge devices.

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a variety of tasks. However, their substantial scale leads to significant
computational resource consumption during inference, resulting in high costs.
Consequently, edge device inference presents a promising solution. The primary
challenges of edge inference include memory usage and inference speed. This
paper introduces MNN-LLM, a framework specifically designed to accelerate the
deployment of large language models on mobile devices. MNN-LLM addresses the
runtime characteristics of LLMs through model quantization and DRAM-Flash
hybrid storage, effectively reducing memory usage. It rearranges weights and
inputs based on mobile CPU instruction sets and GPU characteristics while
employing strategies such as multicore load balancing, mixed-precision
floating-point operations, and geometric computations to enhance performance.
Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current
mainstream LLM-specific frameworks.

</details>


### [142] [Equivariant Neural Diffusion for Molecule Generation](https://arxiv.org/abs/2506.10532)
*François Cornet,Grigory Bartosh,Mikkel N. Schmidt,Christian A. Naesseth*

Main category: cs.LG

TL;DR: END is a new equivariant 3D molecule generation model with a learnable forward process, showing competitive results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the state-of-the-art in equivariant diffusion models for 3D molecule generation by introducing a learnable forward process rather than a pre-specified one.

Method: The method of this paper is the Equivariant Neural Diffusion (END), a novel diffusion model that uses a learnable forward process parameterized through a time- and data-dependent transformation that is equivariant to rigid transformations.

Result: The experimental results on standard molecule generation benchmarks show that END performs competitively compared to several strong baseline models.

Conclusion: END exhibits competitive performance in molecule generation benchmarks, both in unconditional and conditional generation settings, suggesting its effectiveness compared to existing models.

Abstract: We introduce Equivariant Neural Diffusion (END), a novel diffusion model for
molecule generation in 3D that is equivariant to Euclidean transformations.
Compared to current state-of-the-art equivariant diffusion models, the key
innovation in END lies in its learnable forward process for enhanced generative
modelling. Rather than pre-specified, the forward process is parameterized
through a time- and data-dependent transformation that is equivariant to rigid
transformations. Through a series of experiments on standard molecule
generation benchmarks, we demonstrate the competitive performance of END
compared to several strong baselines for both unconditional and conditional
generation.

</details>


### [143] [Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows](https://arxiv.org/abs/2506.10536)
*Vasilis Michalakopoulos,Christoforos Menos-Aikateriniadis,Elissaios Sarmas,Antonis Zakynthinos,Pavlos S. Georgilakis,Dimitris Askounis*

Main category: cs.LG

TL;DR: 研究评估了四个机器学习模型在三个欧洲市场中的预测性能，发现LightGBM在短窗口情况下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨在数据稀缺情况下，短窗口训练的机器学习模型在预测电力日前市场价格时的性能表现，特别是检测季节性趋势和价格尖峰。

Method: 使用了四个模型：LSTM与前馈纠错（FFEC）、XGBoost、LightGBM和CatBoost。利用ENTSO-E预测数据衍生的特征集，评估模型在三个欧洲能源市场（希腊、比利时、爱尔兰）的表现。

Result: LightGBM在预测准确性和稳健性方面表现最佳，尤其是在45天和60天的训练窗口条件下，并在检测季节性效应和价格峰值事件上优于LSTM和其他提升模型。

Conclusion: 短窗口训练方法结合提升方法，能够在波动较大且数据稀少的环境中有效支持日间市场预测。

Abstract: This study investigates the performance of machine learning models in
forecasting electricity Day-Ahead Market (DAM) prices using short historical
training windows, with a focus on detecting seasonal trends and price spikes.
We evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),
XGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,
Belgium, Ireland) using feature sets derived from ENTSO-E forecast data.
Training window lengths range from 7 to 90 days, allowing assessment of model
adaptability under constrained data availability. Results indicate that
LightGBM consistently achieves the highest forecasting accuracy and robustness,
particularly with 45 and 60 day training windows, which balance temporal
relevance and learning depth. Furthermore, LightGBM demonstrates superior
detection of seasonal effects and peak price events compared to LSTM and other
boosting models. These findings suggest that short-window training approaches,
combined with boosting methods, can effectively support DAM forecasting in
volatile, data-scarce environments.

</details>


### [144] [Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics](https://arxiv.org/abs/2506.10577)
*Pascal Plettenberg,André Alcalde,Bernhard Sick,Josephine M. Thomas*

Main category: cs.LG

TL;DR: This paper presents a method to automate PCB design optimization using Graph Neural Networks, showing potential for improved efficiency and reduced costs.


<details>
  <summary>Details</summary>
Motivation: The shortage of skilled engineers and the time-consuming nature of manual optimizations lead to these practices being neglected, increasing troubleshooting costs and generating electronic waste. Automating this process could alleviate these issues.

Method: The researchers used Graph Neural Networks (GNNs) to automate the addition of new components into PCB schematics by representing schematics as bipartite graphs.

Result: The approach successfully applied GNNs to three PCB design optimization tasks, showing high accuracy and potential for automating the process.

Conclusion: GNNs can accurately solve PCB design optimization tasks, allowing for automated, time- and cost-efficient design enhancements.

Abstract: The design and optimization of Printed Circuit Board (PCB) schematics is
crucial for the development of high-quality electronic devices. Thereby, an
important task is to optimize drafts by adding components that improve the
robustness and reliability of the circuit, e.g., pull-up resistors or
decoupling capacitors. Since there is a shortage of skilled engineers and
manual optimizations are very time-consuming, these best practices are often
neglected. However, this typically leads to higher costs for troubleshooting in
later development stages as well as shortened product life cycles, resulting in
an increased amount of electronic waste that is difficult to recycle. Here, we
present an approach for automating the addition of new components into PCB
schematics by representing them as bipartite graphs and utilizing a node pair
prediction model based on Graph Neural Networks (GNNs). We apply our approach
to three highly relevant PCB design optimization tasks and compare the
performance of several popular GNN architectures on real-world datasets labeled
by human experts. We show that GNNs can solve these problems with high accuracy
and demonstrate that our approach offers the potential to automate PCB design
optimizations in a time- and cost-efficient manner.

</details>


### [145] [Size-adaptive Hypothesis Testing for Fairness](https://arxiv.org/abs/2506.10586)
*Antonio Ferrara,Francesco Cozzi,Alan Perotti,André Panisson,Francesco Bonchi*

Main category: cs.LG

TL;DR: 本文提出了一种新的统计测试框架，更加可靠地评估算法决策系统中的公平性，尤其适用于复杂的交叉人群分析。


<details>
  <summary>Details</summary>
Motivation: 当前算法决策系统中常用的公平性评价方法，通常依赖于单点估计与预定义阈值的比较，但这种做法存在统计上的脆弱性，尤其在考虑多个敏感属性的交叉分析中更为明显。

Method: 本文提出了一种统一的、大小自适应的假设检验框架，将公平性评估转变为基于证据的统计决策。具体方法包括：(i) 对于足够大的子群体，证明统计平等差异的中心极限定理，提供解析置信区间和Wald检验。(ii) 对于小的交叉群体，提出基于狄利克雷-多项式的贝叶斯估计器，使用蒙特卡罗置信区间校准任何样本量。

Result: 通过在标准数据集上的实验证明，该方法能够在不同的数据可用度和交叉分析程度下，提供可解释且统计上严谨的决策。

Conclusion: 对大子群体和小交叉群体的公平性评估分别提供了统计上的保障，使得评估决策对数据量的变化具有鲁棒性。

Abstract: Determining whether an algorithmic decision-making system discriminates
against a specific demographic typically involves comparing a single point
estimate of a fairness metric against a predefined threshold. This practice is
statistically brittle: it ignores sampling error and treats small demographic
subgroups the same as large ones. The problem intensifies in intersectional
analyses, where multiple sensitive attributes are considered jointly, giving
rise to a larger number of smaller groups. As these groups become more
granular, the data representing them becomes too sparse for reliable
estimation, and fairness metrics yield excessively wide confidence intervals,
precluding meaningful conclusions about potential unfair treatments.
  In this paper, we introduce a unified, size-adaptive, hypothesis-testing
framework that turns fairness assessment into an evidence-based statistical
decision. Our contribution is twofold. (i) For sufficiently large subgroups, we
prove a Central-Limit result for the statistical parity difference, leading to
analytic confidence intervals and a Wald test whose type-I (false positive)
error is guaranteed at level $\alpha$. (ii) For the long tail of small
intersectional groups, we derive a fully Bayesian Dirichlet-multinomial
estimator; Monte-Carlo credible intervals are calibrated for any sample size
and naturally converge to Wald intervals as more data becomes available. We
validate our approach empirically on benchmark datasets, demonstrating how our
tests provide interpretable, statistically rigorous decisions under varying
degrees of data availability and intersectionality.

</details>


### [146] [Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability](https://arxiv.org/abs/2506.10616)
*Yu-Jie Zhang,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 研究提出了一种改进的动态遗憾最小化方法，利用混合性减少对维数d的依赖，比现有的方法更有效。


<details>
  <summary>Details</summary>
Motivation: 弥补动态遗憾最小化中对曲率更强的损失函数（如平方或逻辑损失）的探索不足。

Method: 使用具有固定分享更新的指数加权方法，对具有混合特性的损失实现动态遗憾最小化。

Result: 通过一种简单而强大的分析框架，显著改进了现有结果，达到了$\mathcal{O}(d T^{1/3} P_T^{2/3} \log T)$的动态遗憾，大大减少了对d的依赖。

Conclusion: 该研究显著提升了在具有混合特性的损失函数下的动态遗憾，通过利用混合性这一概念，使得损失曲率的捕获更有效，超越了之前的工作。

Abstract: Non-stationary online learning has drawn much attention in recent years.
Despite considerable progress, dynamic regret minimization has primarily
focused on convex functions, leaving the functions with stronger curvature
(e.g., squared or logistic loss) underexplored. In this work, we address this
gap by showing that the regret can be substantially improved by leveraging the
concept of mixability, a property that generalizes exp-concavity to effectively
capture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path
length of comparators that reflects the environmental non-stationarity. We
demonstrate that an exponential-weight method with fixed-share updates achieves
an $\mathcal{O}(d T^{1/3} P_T^{2/3} \log T)$ dynamic regret for mixable losses,
improving upon the best-known $\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \log T)$
result (Baby and Wang, 2021) in $d$. More importantly, this improvement arises
from a simple yet powerful analytical framework that exploits the mixability,
which avoids the Karush-Kuhn-Tucker-based analysis required by existing work.

</details>


### [147] [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/abs/2506.10617)
*Reza Karbasi,Masoud Rahimi,Abdol-Hossein Vahabie,Hadi Moradi*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段的心电图信号数字化方法，在处理信号重叠方面具有显著优势，实验结果显示相比基线技术本方法显著提高了数字化准确率。


<details>
  <summary>Details</summary>
Motivation: 解决纸质心电图数字化中单个导联信号重叠处理不足的问题。

Method: 提出了一个两阶段的处理流程。第一阶段使用一个基于U-Net的分割网络，该网络在包含重叠信号的数据集上进行训练，并通过定制的数据增强来增强其稳健性，用于准确隔离主心电图迹线。第二阶段则是利用现有的数字化技术将精细的二进制掩码转化为时间序列信号，并通过自适应网格检测模块增强其在不同心电图格式和规模上的适应性。

Result: 实验结果表明，U-Net架构在精细分割任务上达到了0.87的IoU，与基线技术相比，本方法在非重叠和重叠信号上都表现出了更优异的性能。对于非重叠信号，MSE和Pearson相关系数分别为0.0010和0.9644，而基线为0.0015和0.9366。对于有信号重叠的样本，本方法的MSE和Pearson相关系数为0.0029和0.9641，而基线分别为0.0178和0.8676。

Conclusion: 本文提出的方法能够显著提高原始纸质心电图记录数字化的准确性，特别是在处理信号重叠时表现突出，为将模拟心电图记录可靠转换为可分析的数字数据奠定了坚实的基础。

Abstract: This paper addresses the persistent challenge of accurately digitizing
paper-based electrocardiogram (ECG) recordings, with a particular focus on
robustly handling single leads compromised by signal overlaps-a common yet
under-addressed issue in existing methodologies. We propose a two-stage
pipeline designed to overcome this limitation. The first stage employs a U-Net
based segmentation network, trained on a dataset enriched with overlapping
signals and fortified with custom data augmentations, to accurately isolate the
primary ECG trace. The subsequent stage converts this refined binary mask into
a time-series signal using established digitization techniques, enhanced by an
adaptive grid detection module for improved versatility across different ECG
formats and scales. Our experimental results demonstrate the efficacy of our
approach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained
segmentation task. Crucially, our proposed digitization method yields superior
performance compared to a well-established baseline technique across both
non-overlapping and challenging overlapping ECG samples. For non-overlapping
signals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson
Correlation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,
respectively, for the baseline. On samples with signal overlap, our method
achieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the
baseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to
significantly enhance digitization accuracy, especially in the presence of
signal overlaps, thereby laying a strong foundation for the reliable conversion
of analog ECG records into analyzable digital data for contemporary research
and clinical applications. The implementation is publicly available at this
GitHub repository: https://github.com/masoudrahimi39/ECG-code.

</details>


### [148] [Leveraging Low-rank Factorizations of Conditional Correlation Matrices in Graph Learning](https://arxiv.org/abs/2506.10628)
*Thu Ha Phi,Alexandre Hippert-Ferrer,Florent Bouchard,Arnaud Breloy*

Main category: cs.LG

TL;DR: 提出低秩因子分解方法结合黎曼优化，解决大规模无向图学习问题，实验结果显示出良好的性能与维度权衡。


<details>
  <summary>Details</summary>
Motivation: 解决从每个节点收集的数据中学习无向图的问题，特别是在大规模维度下的困难。

Method: 提出了一种基于低秩因子分解的条件相关矩阵图学习框架，并利用黎曼优化技术求解优化问题。

Result: 实验表明，该方法在性能和维度之间实现了高效的权衡，在合成数据和真实数据上均取得良好效果。

Conclusion: 提出的低秩因子分解方法结合黎曼优化技术，能有效应对大规模图学习问题，并在性能与维度之间实现良好的权衡。

Abstract: This paper addresses the problem of learning an undirected graph from data
gathered at each nodes. Within the graph signal processing framework, the
topology of such graph can be linked to the support of the conditional
correlation matrix of the data. The corresponding graph learning problem then
scales to the squares of the number of variables (nodes), which is usually
problematic at large dimension. To tackle this issue, we propose a graph
learning framework that leverages a low-rank factorization of the conditional
correlation matrix. In order to solve for the resulting optimization problems,
we derive tools required to apply Riemannian optimization techniques for this
particular structure. The proposal is then particularized to a low-rank
constrained counterpart of the GLasso algorithm, i.e., the penalized maximum
likelihood estimation of a Gaussian graphical model. Experiments on synthetic
and real data evidence that a very efficient dimension-versus-performance
trade-off can be achieved with this approach.

</details>


### [149] [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://arxiv.org/abs/2506.10629)
*Yucheng Yang,Tianyi Zhou,Qiang He,Lei Han,Mykola Pechenizkiy,Meng Fang*

Main category: cs.LG

TL;DR: 本文通过引入Wasserstein距离，提出WSEP和PWSEP，提升了技能学习的多样性和可分离性，从而更好地支持下游任务的适应。


<details>
  <summary>Details</summary>
Motivation: 现有的MISL在学习技能的多样性和可分离性方面不足，无法充分初始化下游任务策略。为此，提出新的理论分析和方法来提高下游任务的适应性。

Method: 通过提出新的解耦度量LSEPIN，建立其与下游任务适应成本之间的信息几何连接。然后在信息几何中用Wasserstein距离替代KL散度，并扩展几何分析来提出新的技能学习目标WSEP。最后，提出另一种基于Wasserstein距离的算法PWSEP。

Result: 通过新的技能学习目标WSEP和算法PWSEP，能够帮助下游任务更好地适应，并且能够发现比MISL更多的初始策略，甚至可以发现所有最优初始策略。

Conclusion: 提出了一种新的基于Wasserstein距离的技能学习目标WSEP和算法PWSEP，理论上更有利于下游任务的适应，能够发现更多甚至全部最优初始策略。

Abstract: Unsupervised reinforcement learning (URL) aims to learn general skills for
unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL
by maximizing the mutual information between states and skills but lacks
sufficient theoretical analysis, e.g., how well its learned skills can
initialize a downstream task's policy. Our new theoretical analysis in this
paper shows that the diversity and separability of learned skills are
fundamentally critical to downstream task adaptation but MISL does not
necessarily guarantee these properties. To complement MISL, we propose a novel
disentanglement metric LSEPIN. Moreover, we build an information-geometric
connection between LSEPIN and downstream task adaptation cost. For better
geometric properties, we investigate a new strategy that replaces the KL
divergence in information geometry with Wasserstein distance. We extend the
geometric analysis to it, which leads to a novel skill-learning objective WSEP.
It is theoretically justified to be helpful to downstream task adaptation and
it is capable of discovering more initial policies for downstream tasks than
MISL. We finally propose another Wasserstein distance-based algorithm PWSEP
that can theoretically discover all optimal initial policies.

</details>


### [150] [Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs](https://arxiv.org/abs/2506.10630)
*Yucong Luo,Yitong Zhou,Mingyue Cheng,Jiahao Wang,Daoyu Wang,Tingyue Pan,Jintao Zhang*

Main category: cs.LG

TL;DR: 提出Time-R1框架，增强LLM的多步时间序列推理能力，实现更高预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法大多依赖于快速思维模型，缺乏渐进的推理过程，无法充分解决多步推理相关问题。

Method: 本文介绍了一种两阶段强化微调框架Time-R1，以增强大语言模型（LLM）的多步推理能力。第一阶段通过监督微调进行适应，第二阶段利用强化学习提高模型的泛化能力。

Result: 实验表明Time-R1显著提升了在各种数据集上的预测性能。

Conclusion: Time-R1框架有效克服了传统大语言模型在时间序列推理中的局限性，通过强化微调显著提高了模型预测性能。

Abstract: To advance time series forecasting (TSF), various methods have been proposed
to improve prediction accuracy, evolving from statistical techniques to
data-driven deep learning architectures. Despite their effectiveness, most
existing methods still adhere to a fast thinking paradigm-relying on extracting
historical patterns and mapping them to future values as their core modeling
philosophy, lacking an explicit thinking process that incorporates intermediate
time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)
have shown remarkable multi-step reasoning capabilities, offering an
alternative way to overcome these issues. However, prompt engineering alone
presents several limitations - including high computational cost, privacy
risks, and limited capacity for in-depth domain-specific time series reasoning.
To address these limitations, a more promising approach is to train LLMs to
develop slow thinking capabilities and acquire strong time series reasoning
skills. For this purpose, we propose Time-R1, a two-stage reinforcement
fine-tuning framework designed to enhance multi-step reasoning ability of LLMs
for time series forecasting. Specifically, the first stage conducts supervised
fine-tuning for warmup adaptation, while the second stage employs reinforcement
learning to improve the model's generalization ability. Particularly, we design
a fine-grained multi-objective reward specifically for time series forecasting,
and then introduce GRIP (group-based relative importance for policy
optimization), which leverages non-uniform sampling to further encourage and
optimize the model's exploration of effective reasoning paths. Experiments
demonstrate that Time-R1 significantly improves forecast performance across
diverse datasets.

</details>


### [151] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/abs/2506.10632)
*Alexander Lobashev,Dmitry Guskov,Maria Larchenko,Mikhail Tamm*

Main category: cs.LG

TL;DR: 提出一种通过重构Fisher信息度量来分析生成模型潜在空间的新方法，验证显示该方法优于现有基线，可揭示扩散模型中的相变结构。


<details>
  <summary>Details</summary>
Motivation: 分析生成模型潜在空间几何结构以更好理解其性质和行为。

Method: 通过重构Fisher信息度量来分析潜在空间几何结构，逼近给定样本的后验分布，学习定义Fisher度量的对数分区函数。

Result: 在Ising和TASEP模型上验证了该方法，优于现有基线，并在扩散模型中揭示了相变的分形结构。

Conclusion: 提供了新的见解，揭示了扩散模型潜在空间的复杂结构及其与相变等现象的联系。

Abstract: This paper presents a novel method for analyzing the latent space geometry of
generative models, including statistical physics models and diffusion models,
by reconstructing the Fisher information metric. The method approximates the
posterior distribution of latent variables given generated samples and uses
this to learn the log-partition function, which defines the Fisher metric for
exponential families. Theoretical convergence guarantees are provided, and the
method is validated on the Ising and TASEP models, outperforming existing
baselines in reconstructing thermodynamic quantities. Applied to diffusion
models, the method reveals a fractal structure of phase transitions in the
latent space, characterized by abrupt changes in the Fisher metric. We
demonstrate that while geodesic interpolations are approximately linear within
individual phases, this linearity breaks down at phase boundaries, where the
diffusion model exhibits a divergent Lipschitz constant with respect to the
latent space. These findings provide new insights into the complex structure of
diffusion model latent spaces and their connection to phenomena like phase
transitions. Our source code is available at
https://github.com/alobashev/hessian-geometry-of-diffusion-models.

</details>


### [152] [Data Shifts Hurt CoT: A Theoretical Study](https://arxiv.org/abs/2506.10647)
*Lang Yin,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.LG

TL;DR: 研究分析了数据分布偏移对CoT方法的影响，尤其在$k$-parity问题上，发现CoT在某些情况下表现不如直接预测，并给出了原因分析。


<details>
  <summary>Details</summary>
Motivation: Chain of Thought (CoT)虽然在提高大型语言模型（LLMs）的输出质量方面有效，但往往假设训练和测试分布相同，以及训练数据没有错误。这在现实世界中难以成立。本研究首次严格探讨数据分布偏移对模型带来的精确危害。

Method: 研究主要通过分析$k$-parity问题，调查数据分布偏移和数据中毒对训练结果的联合影响，并通过一种可靠的CoT分解进行评估。

Result: 发现CoT在学习奇偶问题上表现不如直接生成预测，研究提供了该现象背后的机械原因的全面解释。

Conclusion: 在存在数据分布偏移的情况下，CoT方法可能导致比直接预测更差的表现。这种现象来自于对数据偏移的敏感性。

Abstract: Chain of Thought (CoT) has been applied to various large language models
(LLMs) and proven to be effective in improving the quality of outputs. In
recent studies, transformers are proven to have absolute upper bounds in terms
of expressive power, and consequently, they cannot solve many computationally
difficult problems. However, empowered by CoT, transformers are proven to be
able to solve some difficult problems effectively, such as the $k$-parity
problem. Nevertheless, those works rely on two imperative assumptions: (1)
identical training and testing distribution, and (2) corruption-free training
data with correct reasoning steps. However, in the real world, these
assumptions do not always hold. Although the risks of data shifts have caught
attention, our work is the first to rigorously study the exact harm caused by
such shifts to the best of our knowledge. Focusing on the $k$-parity problem,
in this work we investigate the joint impact of two types of data shifts: the
distribution shifts and data poisoning, on the quality of trained models
obtained by a well-established CoT decomposition. In addition to revealing a
surprising phenomenon that CoT leads to worse performance on learning parity
than directly generating the prediction, our technical results also give a
rigorous and comprehensive explanation of the mechanistic reasons of such
impact.

</details>


### [153] [Saturation Self-Organizing Map](https://arxiv.org/abs/2506.10680)
*Igor Urbanik,Paweł Gajewski*

Main category: cs.LG

TL;DR: 该研究引入SatSOM，通过饱和机制提升连续学习中的知识保留，解决SOM的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 自组织映射(SOM)在解释性和效率上的优势使其在连续学习领域备受关注，但它们同样容易受到灾难性遗忘的影响。因此，需要一种方法来提升知识保留能力。

Method: 引入饱和自组织映射(SatSOM)，增加了一种饱和机制，该机制在神经元积累信息时逐渐降低其学习率和邻域半径，从而冻结训练良好的神经元，并将学习重心转移到使用不足的区域。

Result: SatSOM通过其创新的饱和机制有效改善了神经系统在连续学习场景中的知识保留能力。

Conclusion: SatSOM提供了一种解决自组织映射在连续学习中灾难性遗忘问题的方法，通过创新的饱和机制成功改善知识保留，提高了学习效率。

Abstract: Continual learning poses a fundamental challenge for neural systems, which
often suffer from catastrophic forgetting when exposed to sequential tasks.
Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are
not immune to this issue. In this paper, we introduce Saturation
Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve
knowledge retention in continual learning scenarios. SatSOM incorporates a
novel saturation mechanism that gradually reduces the learning rate and
neighborhood radius of neurons as they accumulate information. This effectively
freezes well-trained neurons and redirects learning to underutilized areas of
the map.

</details>


### [154] [Preserving Task-Relevant Information Under Linear Concept Removal](https://arxiv.org/abs/2506.10703)
*Floris Holstege,Shauli Ravfogel,Bram Wouters*

Main category: cs.LG

TL;DR: SPLICE方法通过保留目标标签的协方差，有效去除神经网络表示中不需要的概念，而不会严重破坏主要任务信号。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络往往会将不需要的概念与任务相关的信息一起编码，导致公平性和可解释性的担忧。已有的事后方法可以去除不需要的概念，但往往会降低有用信号。

Method: 引入了SPLICE方法，通过一种倾斜投影来剔除不需要的方向，同时保持重要的标签相关性。理论上，它是唯一能去除线性概念可预测性并以最小的嵌入失真保持目标协方差的解决方案。

Result: SPLICE在去除表示中的受保护属性时，其主要任务信息受到的损害最小，性能超过了基准测试中的对比方法。

Conclusion: 采用SPLICE方法可以在不显著损害主要任务信息的情况下，有效去除表示中的敏感概念，同时保留与目标标签的协方差。这一方法在Bias in Bios和Winobias等基准上表现出色。

Abstract: Modern neural networks often encode unwanted concepts alongside task-relevant
information, leading to fairness and interpretability concerns. Existing
post-hoc approaches can remove undesired concepts but often degrade useful
signals. We introduce SPLICE-Simultaneous Projection for LInear concept removal
and Covariance prEservation-which eliminates sensitive concepts from
representations while exactly preserving their covariance with a target label.
SPLICE achieves this via an oblique projection that "splices out" the unwanted
direction yet protects important label correlations. Theoretically, it is the
unique solution that removes linear concept predictability and maintains target
covariance with minimal embedding distortion. Empirically, SPLICE outperforms
baselines on benchmarks such as Bias in Bios and Winobias, removing protected
attributes while minimally damaging main-task information.

</details>


### [155] [ConTextTab: A Semantics-Aware Tabular In-Context Learner](https://arxiv.org/abs/2506.10707)
*Marco Spinaci,Marek Polewczyk,Maximilian Schambach,Sam Thelin*

Main category: cs.LG

TL;DR: ConTextTab模型通过结合语义理解和表格内ICL框架，实现了与SOTA竞争，并在CARTE基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的表格上下文学习（ICL）模型在处理真实世界的表格数据时，没有充分利用其丰富的语义和世界知识。而基于预训练大型语言模型的表格ICL模型在语义理解上更有优势，但受限于架构，能使用的上下文量较少。研究者希望结合这两者的优点。

Method: 引入ConTextTab模型，通过为不同数据模态设计专门的嵌入，并在大规模真实世界表格数据上进行训练，以整合语义理解和对齐到表格内的ICL框架中。

Result: 模型在广泛的基准测试中与目前的最先进方法（SOTA）竞争并在语义丰富的CARTE基准上树立了新的标准。

Conclusion: ConTextTab模型成功结合了语义理解和表格内ICL框架的优点，实现了较好的预测性能。

Abstract: Tabular in-context learning (ICL) has recently achieved state-of-the-art
(SOTA) performance on several tabular prediction tasks. Previously restricted
to classification problems on small tables, recent advances such as TabPFN and
TabICL have extended its use to larger datasets. While being architecturally
efficient and well-adapted to tabular data structures, current table-native ICL
architectures, being trained exclusively on synthetic data, do not fully
leverage the rich semantics and world knowledge contained in real-world tabular
data. On another end of this spectrum, tabular ICL models based on pretrained
large language models such as TabuLa-8B integrate deep semantic understanding
and world knowledge but are only able to make use of a small amount of context
due to inherent architectural limitations. With the aim to combine the best of
both these worlds, we introduce ConTextTab, integrating semantic understanding
and alignment into a table-native ICL framework. By employing specialized
embeddings for different data modalities and by training on large-scale
real-world tabular data, our model is competitive with SOTA across a broad set
of benchmarks while setting a new standard on the semantically rich CARTE
benchmark.

</details>


### [156] [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/abs/2506.10751)
*Sai Prasanna Teja Reddy Bogireddy,Abrar Majeedi,Viswanatha Reddy Gajjala,Zhuoyan Xu,Siddhant Rai,Vaishnav Potlapalli*

Main category: cs.LG

TL;DR: 研究提出一种分阶段优化提示的方法，用于基于电子健康记录的临床问答，效果显著胜过传统方法。


<details>
  <summary>Details</summary>
Motivation: 试图通过自动化技术解决电子健康记录中问答任务的挑战，弥补临床医生和患者之间的信息鸿沟。

Method: 将任务分为两部分：（1）句子级证据识别和（2）具有显式引用的答案合成，并使用DSPy's MIPROv2优化器来探索提示空间，联合调整开发集上的指令和少样本示例，并采用自一致性投票机制以提高证据召回率。

Result: 该方法在隐藏测试集上取得了51.5的总分，比标准零样本和少样本提示分别高出20和10个点，表明其具有优越的性能。

Conclusion: 所提出的方法在BioNLP 2025 ArchEHR-QA任务中荣获亚军，证明了数据驱动的提示优化可以替代模型微调，从而提高高风险临床QA的可靠性。

Abstract: Automated question answering (QA) over electronic health records (EHRs) can
bridge critical information gaps for clinicians and patients, yet it demands
both precise evidence retrieval and faithful answer generation under limited
supervision. In this work, we present Neural, the runner-up in the BioNLP 2025
ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method
decouples the task into (1) sentence-level evidence identification and (2)
answer synthesis with explicit citations. For each stage, we automatically
explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning
instructions and few-shot demonstrations on the development set. A
self-consistency voting scheme further improves evidence recall without
sacrificing precision. On the hidden test set, our method attains an overall
score of 51.5, placing second stage while outperforming standard zero-shot and
few-shot prompting by over 20 and 10 points, respectively. These results
indicate that data-driven prompt optimization is a cost-effective alternative
to model fine-tuning for high-stakes clinical QA, advancing the reliability of
AI assistants in healthcare.

</details>


### [157] [Skillful joint probabilistic weather forecasting from marginals](https://arxiv.org/abs/2506.10772)
*Ferran Alet,Ilan Price,Andrew El-Kadi,Dominic Masters,Stratis Markou,Tom R. Andersson,Jacklynn Stott,Remi Lam,Matthew Willson,Alvaro Sanchez-Gonzalez,Peter Battaglia*

Main category: cs.LG

TL;DR: 本文提出了FGN模型，其在准确性和灵活性上超过了当前最先进的天气预报模型，尤其在全球概率天气预报和热带气旋路径预测上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的数值天气预报模型在准确性和速度上已被基于机器学习的天气模型超越，而本文提出的FGN模型旨在进一步提升准确性和灵活性。

Method: FGN通过学习模型扰动来生成模型集合，并直接通过最小化位置预报的连续排名概率评分（CRPS）进行训练。

Result: FGN能够快速生成准确的天气预报集合，尤其在热带气旋路径预测方面表现出色。此外，尽管只在边际上训练，FGN仍能捕捉到联合空间结构。

Conclusion: FGN在全球概率天气预报中显著超过了当前最先进的模型，在多种指标上实现了最先进的集合预报，并且能够在仅基于边际条件训练的情况下，准确捕捉联合空间结构。

Abstract: Machine learning (ML)-based weather models have rapidly risen to prominence
due to their greater accuracy and speed than traditional forecasts based on
numerical weather prediction (NWP), recently outperforming traditional
ensembles in global probabilistic weather forecasting. This paper presents FGN,
a simple, scalable and flexible modeling approach which significantly
outperforms the current state-of-the-art models. FGN generates ensembles via
learned model-perturbations with an ensemble of appropriately constrained
models. It is trained directly to minimize the continuous rank probability
score (CRPS) of per-location forecasts. It produces state-of-the-art ensemble
forecasts as measured by a range of deterministic and probabilistic metrics,
makes skillful ensemble tropical cyclone track predictions, and captures joint
spatial structure despite being trained only on marginals.

</details>


### [158] [Monotone Classification with Relative Approximations](https://arxiv.org/abs/2506.10775)
*Yufei Tao*

Main category: cs.LG

TL;DR: 本文首次探讨单调分类器的构建，提出了相对误差容忍度的成本最小化算法，提供了上下界分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决单调分类问题时，错误率始终高于最优分类器。本研究旨在降低错误率，即允许相对误差而非固定误差。

Method: 本文研究通过揭示点标签来评估分类器的成本，并分析达到相对误差所需的最小成本。

Result: 提出了几乎匹配的上下界，为找到误差不超过最优分类器相对因子的单调类分类器提供了解决方案。

Conclusion: 本文提出了一种高效的算法，可以在揭示最少标签点的条件下，用于找到误差接近最佳单调分类器的单调函数。

Abstract: In monotone classification, the input is a multi-set $P$ of points in
$\mathbb{R}^d$, each associated with a hidden label from $\{-1, 1\}$. The goal
is to identify a monotone function $h$, which acts as a classifier, mapping
from $\mathbb{R}^d$ to $\{-1, 1\}$ with a small {\em error}, measured as the
number of points $p \in P$ whose labels differ from the function values $h(p)$.
The cost of an algorithm is defined as the number of points having their labels
revealed. This article presents the first study on the lowest cost required to
find a monotone classifier whose error is at most $(1 + \epsilon) \cdot k^*$
where $\epsilon \ge 0$ and $k^*$ is the minimum error achieved by an optimal
monotone classifier -- in other words, the error is allowed to exceed the
optimal by at most a relative factor. Nearly matching upper and lower bounds
are presented for the full range of $\epsilon$. All previous work on the
problem can only achieve an error higher than the optimal by an absolute
factor.

</details>


### [159] [Dense Associative Memory with Epanechnikov Energy](https://arxiv.org/abs/2506.10801)
*Benjamin Hoover,Zhaoyang Shi,Krishnakumar Balasubramanian,Dmitry Krotov,Parikshit Ram*

Main category: cs.LG

TL;DR: A novel energy function, log-sum-ReLU (LSR), for DenseAM networks offers exponential capacity and emergent creativity with more local minima than existing models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable exact memory retrieval with exponential capacity without requiring exponential separation functions in Dense AM networks.

Method: The paper introduces a new energy function called log-sum-ReLU (LSR) for Dense Associative Memory networks. It is based on the Epanechnikov kernel.

Result: Empirical results demonstrate that the LSR energy function has significantly more local minima (memories) with comparable log-likelihood to models based on LSE and shows creativity in its emergent memories on image datasets.

Conclusion: LSR energy function has the potential for large-scale memory storage and generative tasks due to its ability to have more local minima with comparable log-likelihood to LSE models and demonstrates creativity and novelty in its emergent memories.

Abstract: We propose a novel energy function for Dense Associative Memory (DenseAM)
networks, the log-sum-ReLU (LSR), inspired by optimal kernel density
estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based
on the Epanechnikov kernel and enables exact memory retrieval with exponential
capacity without requiring exponential separation functions. Moreover, it
introduces abundant additional \emph{emergent} local minima while preserving
perfect pattern recovery -- a characteristic previously unseen in DenseAM
literature. Empirical results show that LSR energy has significantly more local
minima (memories) that have comparable log-likelihood to LSE-based models.
Analysis of LSR's emergent memories on image datasets reveals a degree of
creativity and novelty, hinting at this method's potential for both large-scale
memory storage and generative tasks.

</details>


### [160] [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805)
*Alex McKenzie,Urja Pawar,Phil Blandfort,William Bankes,David Krueger,Ekdeep Singh Lubana,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 本文研究激活探针用于检测LLMs的高风险交互，探针在性能与计算效率上展示了优势，并开源了新的数据集和代码库以促进进一步研究。


<details>
  <summary>Details</summary>
Motivation: 监控是安全部署大型语言模型的重要方面，尤其是检测可能导致重大伤害的高风险交互。这些交互是监控的一个关键但未被充分探索的目标。

Method: 本文评估了在合成数据上训练的多种探针架构，并测试它们在多样化的真实世界数据中的泛化能力，探针的性能与中等规模的LLM监控员相当，同时计算开销降低了六个数量级。

Result: 探针在多样化和真实数据上的良好泛化能力，与其他监控方法相比，计算节省了六个量级。该研究还展示了构建资源感知的分层监控系统的潜力。

Conclusion: 通过使用激活探针，可以有效监控LLMs的高风险交互，且具有良好的泛化能力和计算效率。

Abstract: Monitoring is an important aspect of safely deploying Large Language Models
(LLMs). This paper examines activation probes for detecting "high-stakes"
interactions -- where the text indicates that the interaction might lead to
significant harm -- as a critical, yet underexplored, target for such
monitoring. We evaluate several probe architectures trained on synthetic data,
and find them to exhibit robust generalization to diverse, out-of-distribution,
real-world data. Probes' performance is comparable to that of prompted or
finetuned medium-sized LLM monitors, while offering computational savings of
six orders-of-magnitude. Our experiments also highlight the potential of
building resource-aware hierarchical monitoring systems, where probes serve as
an efficient initial filter and flag cases for more expensive downstream
analysis. We release our novel synthetic dataset and codebase to encourage
further study.

</details>


### [161] [Efficiency Robustness of Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.10831)
*Ravishka Rathnasuriya,Tingxi Li,Zexin Xu,Zihe Song,Mirazul Haque,Simin Chen,Wei Yang*

Main category: cs.LG

TL;DR: 本文探讨了动态深度学习系统的效率稳健性，提出了效率攻击分类，分析了防御机制的局限性，强调需要新的策略来保障系统安全。


<details>
  <summary>Details</summary>
Motivation: 动态深度学习系统(Dynamic Deep Learning Systems, DDLSs)因其动态行为带来效率提升的同时也引入了新的攻击面。

Method: 通过深入评估分析，系统地探索了动态深度学习系统的效率稳健性，并提出了效率攻击的全面分类法。

Result: 本文提出了效率攻击的详细分类，识别了针对DDLSs效率的对抗策略，并指出了保护这些系统的关键挑战。

Conclusion: 现有的防御机制在应对效率攻击上存在局限性。为了保障未来适应性动态深度学习系统的安全性，需要开发新的缓解策略。

Abstract: Deep Learning Systems (DLSs) are increasingly deployed in real-time
applications, including those in resourceconstrained environments such as
mobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning
Systems (DDLSs) adapt inference computation based on input complexity, reducing
overhead. While this dynamic behavior improves efficiency, such behavior
introduces new attack surfaces. In particular, efficiency adversarial attacks
exploit these dynamic mechanisms to degrade system performance. This paper
systematically explores efficiency robustness of DDLSs, presenting the first
comprehensive taxonomy of efficiency attacks. We categorize these attacks based
on three dynamic behaviors: (i) attacks on dynamic computations per inference,
(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic
output production for downstream tasks. Through an in-depth evaluation, we
analyze adversarial strategies that target DDLSs efficiency and identify key
challenges in securing these systems. In addition, we investigate existing
defense mechanisms, demonstrating their limitations against increasingly
popular efficiency attacks and the necessity for novel mitigation strategies to
secure future adaptive DDLSs.

</details>


### [162] [Advanced fraud detection using machine learning models: enhancing financial transaction security](https://arxiv.org/abs/2506.10842)
*Nudrat Fariha,Md Nazmuddin Moin Khan,Md Iqbal Hossain,Syed Ali Reza,Joy Chakra Bortty,Kazi Sharmin Sultana,Md Shadidur Islam Jawad,Saniah Safat,Md Abdul Ahad,Maksuda Begum*

Main category: cs.LG

TL;DR: 研究开发了一个用来检测信用卡交易欺诈的机器学习框架，其中包括多种无监督模型和数据分析方法。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付的兴起，迫切需要建立智能且可扩展的系统来检测欺诈行为。

Method: 本研究提出了一种端到端、特征丰富的机器学习框架，使用 Isolation Forest、One Class SVM 和深度自动编码器等无监督模型来检测信用卡交易异常和欺诈行为。数据分析中还使用了K-Means聚类和DBSCAN分割交易活动。

Result: 这些模型将重构误差最高的1%标记为异常，通过PCA可视化展示了各模型在二维潜在空间中分离异常的能力。

Conclusion: 研究表明，所构建的机器学习模型能够有效检测信用卡交易中的异常和欺诈行为。

Abstract: The rise of digital payments has accelerated the need for intelligent and
scalable systems to detect fraud. This research presents an end-to-end,
feature-rich machine learning framework for detecting credit card transaction
anomalies and fraud using real-world data. The study begins by merging
transactional, cardholder, merchant, and merchant category datasets from a
relational database to create a unified analytical view. Through the feature
engineering process, we extract behavioural signals such as average spending,
deviation from historical patterns, transaction timing irregularities, and
category frequency metrics. These features are enriched with temporal markers
such as hour, day of week, and weekend indicators to expose all latent patterns
that indicate fraudulent behaviours. Exploratory data analysis reveals
contextual transaction trends across all the dataset features. Using the
transactional data, we train and evaluate a range of unsupervised models:
Isolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct
normal behavior. These models flag the top 1% of reconstruction errors as
outliers. PCA visualizations illustrate each models ability to separate
anomalies into a two-dimensional latent space. We further segment the
transaction landscape using K-Means clustering and DBSCAN to identify dense
clusters of normal activity and isolate sparse, suspicious regions.

</details>


### [163] [Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization](https://arxiv.org/abs/2506.10871)
*Pierre-François Massiani,Alexander von Rohr,Lukas Haverbeck,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 研究表明，结合熵正则化与约束惩罚可提高RL在不确定环境下的安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决RL中在未知干扰下满足状态约束的策略学习问题。

Method: 研究熵正则化与约束惩罚的结合，通过实验证明其在强化学习中的效果，并提出通过惩罚放宽严格的安全约束，使问题近似化，利用标准无模型RL进行求解。

Result: 提出的策略通过简单的奖励调整在不损失安全性和最优性的情况下，提高了对干扰的抵抗力。

Conclusion: 熵正则化与约束惩罚的结合可增强强化学习策略在不确定环境下的鲁棒性，且能在不损失安全性和最优性的情况下，接近地解决约束问题。

Abstract: Despite the many recent advances in reinforcement learning (RL), the question
of learning policies that robustly satisfy state constraints under unknown
disturbances remains open. In this paper, we offer a new perspective on
achieving robust safety by analyzing the interplay between two well-established
techniques in model-free RL: entropy regularization, and constraints
penalization. We reveal empirically that entropy regularization in constrained
RL inherently biases learning toward maximizing the number of future viable
actions, thereby promoting constraints satisfaction robust to action noise.
Furthermore, we show that by relaxing strict safety constraints through
penalties, the constrained RL problem can be approximated arbitrarily closely
by an unconstrained one and thus solved using standard model-free RL. This
reformulation preserves both safety and optimality while empirically improving
resilience to disturbances. Our results indicate that the connection between
entropy regularization and robustness is a promising avenue for further
empirical and theoretical investigation, as it enables robust safety in RL
through simple reward shaping.

</details>


### [164] [Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers](https://arxiv.org/abs/2506.10888)
*Lucas Gnecco-Heredia,Benjamin Negrevergne,Yann Chevaleyre*

Main category: cs.LG

TL;DR: 本文提出并验证了一种新的攻击方法以克服现有混合分类器攻击的不足。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法不适用于有限分类器的混合，为了在原则上解决攻击混合的问题，提出需要几何分析来形成更有效的攻击策略。

Method: 作者进行了几何分析并介绍了攻击的两个理想属性，同时设计了一种新的攻击方法，并在合成和真实数据集上进行了实验证明。

Result: 实验结果表明，这种新的攻击方法在合成和实际数据集上的表现优于现有方法。

Conclusion: 本文提出了一种新的攻击方法“lattice climber attack”，并通过理论分析和实验验证其在二元线性环境中的有效性。

Abstract: Finite mixtures of classifiers (a.k.a. randomized ensembles) have been
proposed as a way to improve robustness against adversarial attacks. However,
existing attacks have been shown to not suit this kind of classifier. In this
paper, we discuss the problem of attacking a mixture in a principled way and
introduce two desirable properties of attacks based on a geometrical analysis
of the problem (effectiveness and maximality). We then show that existing
attacks do not meet both of these properties. Finally, we introduce a new
attack called {\em lattice climber attack} with theoretical guarantees in the
binary linear setting, and demonstrate its performance by conducting
experiments on synthetic and real datasets.

</details>


### [165] [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
*Subham Sekhar Sahoo,Justin Deschenaux,Aaron Gokaslan,Guanghan Wang,Justin Chiu,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 研究通过将高斯扩散技术用于均匀状态离散扩散，提出了Duo方法，显著提升了模型的训练和生成效率。


<details>
  <summary>Details</summary>
Motivation: 通过将高斯扩散技术应用于均匀状态离散扩散模型，缩小其与自回归模型及掩码扩散模型的性能差距。

Method: 提出了一种由高斯过程引导的课程学习策略和离散一致性蒸馏算法以提升均匀状态扩散模型的表现。

Result: 模型训练速度加倍，较自回归模型在7个基准测试中的3个上实现了无参数解析情况下的性能超越，生成速度提升了两个数量级。

Conclusion: 引入的Duo方法通过从高斯扩散中转移技术，显著提升了均匀状态离散扩散模型的训练和生成性能。

Abstract: Uniform-state discrete diffusion models hold the promise of fast text
generation due to their inherent ability to self-correct. However, they are
typically outperformed by autoregressive models and masked diffusion models. In
this work, we narrow this performance gap by leveraging a key insight:
Uniform-state diffusion processes naturally emerge from an underlying Gaussian
diffusion. Our method, Duo, transfers powerful techniques from Gaussian
diffusion to improve both training and sampling. First, we introduce a
curriculum learning strategy guided by the Gaussian process, doubling training
speed by reducing variance. Models trained with curriculum learning surpass
autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we
present Discrete Consistency Distillation, which adapts consistency
distillation from the continuous to the discrete setting. This algorithm
unlocks few-step generation in diffusion language models by accelerating
sampling by two orders of magnitude. We provide the code and model checkpoints
on the project page: http://s-sahoo.github.io/duo

</details>


### [166] [NoLoCo: No-all-reduce Low Communication Training Method for Large Models](https://arxiv.org/abs/2506.10911)
*Jari Kolehmainen,Nikolay Blagoev,John Donaghy,Oğuzhan Ersoy,Christopher Nies*

Main category: cs.LG

TL;DR: NoLoCo 提出了一种无需显式参数同步的优化方法，减少通信开销，提高了语言模型训练的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在低带宽网络环境下，现有的低通信训练方法的参数同步步骤成本仍然很高，因此需要一种减少同步成本的方法。

Method: NoLoCo 利用一种新型的 Nesterov 动量优化器变体，通过与随机选择的其他模型权重部分平均的方式，隐式同步模型权重。

Result: NoLoCo 在各种加速器数量和模型规模下显著减少了通信开销，且与 DiLoCo 相比，收敛速度提高了高达 4%。

Conclusion: NoLoCo 提出了一种新的优化方法，在不需要显式同步所有模型参数的情况下进行训练，显著减少了通信开销并提高了收敛速度。

Abstract: Training large language models is generally done via optimization methods on
clusters containing tens of thousands of accelerators, communicating over a
high-bandwidth interconnect. Scaling up these clusters is expensive and can
become impractical, imposing limits on the size of models that can be trained.
Several recent studies have proposed training methods that are less
communication intensive, avoiding the need for a highly connected compute
cluster. These state-of-the-art low communication training methods still employ
a synchronization step for model parameters, which, when performed over all
model replicas, can become costly on a low-bandwidth network.
  In this work, we propose a novel optimization method, NoLoCo, that does not
explicitly synchronize all model parameters during training and, as a result,
does not require any collective communication. NoLoCo implicitly synchronizes
model weights via a novel variant of the Nesterov momentum optimizer by
partially averaging model weights with a randomly selected other one. We
provide both a theoretical convergence analysis for our proposed optimizer as
well as empirical results from language model training.
  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,
between 125M to 6.8B parameters. Our method requires significantly less
communication overhead than fully sharded data parallel training or even widely
used low communication training method, DiLoCo. The synchronization step itself
is estimated to be one magnitude faster than the all-reduce used in DiLoCo for
few hundred accelerators training over the internet. We also do not have any
global blocking communication that reduces accelerator idling time. Compared to
DiLoCo, we also observe up to $4\%$ faster convergence rate with wide range of
model sizes and accelerator counts.

</details>


### [167] [Foundation Models for Causal Inference via Prior-Data Fitted Networks](https://arxiv.org/abs/2506.10914)
*Yuchen Ma,Dennis Frauen,Emil Javurek,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: CausalFM是一种用于培训PFN基础模型的框架，能够在多种因果推断设置下进行贝叶斯因果推断。这可能改变医学、经济等领域的因果推断惯例。


<details>
  <summary>Details</summary>
Motivation: 当前的因果推断技术有其局限性，CausalFM提供了通过训练基础模型在不同行因果设置下进行推断的新途径，可能对实际应用产生重大影响。

Method: 本文提出了一种基于PFN的因果推断基础模型训练框架。该框架利用结构化因果模型（SCMs）构建贝叶斯先验，使用因果启发的贝叶斯神经网络创建一个新的先验分布家族，适用于多种因果推断设置，并专门训练用于条件平均处理效果（CATE）估计的模型。

Result: CausalFM在多种合成和半合成基准上表现出与当前技术竞争力相当的CATE估计能力。

Conclusion: CausalFM框架可以作为推广性的方案，用于训练不同因果推断设置下的基础模型。相比于当前因果推断领域的最先进技术，CausalFM提供了一种新颖的模式，可能对医学、经济学等学科中的因果推断工作产生根本性的影响。

Abstract: Prior-data fitted networks (PFNs) have recently been proposed as a promising
way to train tabular foundation models. PFNs are transformers that are
pre-trained on synthetic data generated from a prespecified prior distribution
and that enable Bayesian inference through in-context learning. In this paper,
we introduce CausalFM, a comprehensive framework for training PFN-based
foundation models in various causal inference settings. First, we formalize the
construction of Bayesian priors for causal inference based on structural causal
models (SCMs) in a principled way and derive necessary criteria for the
validity of such priors. Building on this, we propose a novel family of prior
distributions using causality-inspired Bayesian neural networks that enable
CausalFM to perform Bayesian causal inference in various settings, including
back-door, front-door, and instrumental variable adjustment. Finally, we
instantiate CausalFM and explicitly train a foundation model for estimating
conditional average treatment effects (CATEs) using back-door adjustment. We
show that CausalFM performs competitively for CATE estimation using various
synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a
general recipe to train foundation models for various causal inference
settings. In contrast to the current state-of-the-art in causal inference,
CausalFM offers a novel paradigm with the potential to fundamentally change how
practitioners perform causal inference in medicine, economics, and other
disciplines.

</details>


### [168] [Sequential-Parallel Duality in Prefix Scannable Models](https://arxiv.org/abs/2506.10918)
*Morris Yau,Sharut Gupta,Valerie Engelmayer,Kazuki Irie,Stefanie Jegelka,Jacob Andreas*

Main category: cs.LG

TL;DR: 定义并评估前缀可扫描模型(PSMs)，结合并行和顺序推理的优点，表现出优异的模型推理效率和长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探讨神经序列模型如何支持近乎常数时间的并行评估和线性时间、常数空间的序列推理，并统一现有的架构如GLA、Mamba等。

Method: 使用前缀扫描算法结合自定义关联聚合操作符来更新神经序列模型的状态，定义并评估前缀可扫描模型(PSMs)，并进行小规模语言建模和合成任务的实证评估。

Result: 研究发现，前缀可扫描模型（PSMs）在长序列一般化能力上表现优异，同时保持了与状态空间模型相匹配的推理效率，且具备变压器架构的表达能力。

Conclusion: 研究探讨了神经序列模型的双序列并行性，并通过定义和评估前缀可扫描模型(PSMs)来探索解决方案。结果表明，PSMs在保持变压器架构表达能力的同时，能够达到状态空间模型的推理效率，并且在某些情况下表现出更好的长度泛化能力。

Abstract: Modern neural sequence models are designed to meet the dual mandate of
parallelizable training and fast sequential inference. Recent developments have
given rise to various models, such as Gated Linear Attention (GLA) and Mamba,
that achieve such ``sequential-parallel duality.'' This raises a natural
question: can we characterize the full class of neural sequence models that
support near-constant-time parallel evaluation and linear-time, constant-space
sequential inference? We begin by describing a broad class of such models --
state space models -- as those whose state updates can be computed using the
classic parallel prefix scan algorithm with a custom associative aggregation
operator. We then define a more general class, Prefix-Scannable Models (PSMs),
by relaxing the state aggregation operator to allow arbitrary (potentially
non-associative) functions such as softmax attention. This generalization
unifies many existing architectures, including element-wise RNNs (e.g., Mamba)
and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new
models with softmax-like operators that achieve O(1) amortized compute per
token and log(N) memory for sequence length N. We empirically evaluate such
models on illustrative small-scale language modeling and canonical synthetic
tasks, including state tracking and associative recall. Empirically, we find
that PSMs retain the expressivity of transformer-based architectures while
matching the inference efficiency of state space models -- in some cases
exhibiting better length generalization than either.

</details>


### [169] [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)
*Adam Karvonen,Samuel Marks*

Main category: cs.LG

TL;DR: 语言模型在招聘决策中存在种族和性别偏见，尤其在真实背景下。通过内部偏见缓解，可以有效降低这种偏见，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型在招聘等高风险场景中被广泛应用，但在涉及现实细节时可能产生种族和性别偏见。为了实现公平的招聘决策，需要建立有效的偏见缓解策略。

Method: 通过识别模型激活中的敏感属性方向并应用仿射概念编辑方法，来降低语言模型在招聘决策中的种族和性别偏见。即使使用简单的合成数据集，该方法仍能有效地在推理时减少偏见，同时保持模型性能。

Result: 在多个最新商业模型和开源模型中增加真实背景后，显著出现种族和性别偏见，偏见缓解技术成功将偏见降低到低水平（通常低于1%，始终低于2.5%），同时在很大程度上保持了模型性能。

Conclusion: 研究发现，在真实背景下，简单的反偏见提示无法有效消除语言模型中的种族和性别偏见。内部偏见缓解技术可以识别并中和模型激活中的敏感属性方向，从而在多个场景中实现显著的偏见减少。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes hiring
applications, making decisions that directly impact people's careers and
livelihoods. While prior studies suggest simple anti-bias prompts can eliminate
demographic biases in controlled evaluations, we find these mitigations fail
when realistic contextual details are introduced. We address these failures
through internal bias mitigation: by identifying and neutralizing sensitive
attribute directions within model activations, we achieve robust bias reduction
across all tested scenarios. Across leading commercial (GPT-4o, Claude 4
Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,
Mistral-24B), we find that adding realistic context such as company names,
culture descriptions from public careers pages, and selective hiring
constraints (e.g.,``only accept candidates in the top 10\%") induces
significant racial and gender biases (up to 12\% differences in interview
rates). When these biases emerge, they consistently favor Black over White
candidates and female over male candidates across all tested models and
scenarios. Moreover, models can infer demographics and become biased from
subtle cues like college affiliations, with these biases remaining invisible
even when inspecting the model's chain-of-thought reasoning. To address these
limitations, our internal bias mitigation identifies race and gender-correlated
directions and applies affine concept editing at inference time. Despite using
directions from a simple synthetic dataset, the intervention generalizes
robustly, consistently reducing bias to very low levels (typically under 1\%,
always below 2.5\%) while largely maintaining model performance. Our findings
suggest that practitioners deploying LLMs for hiring should adopt more
realistic evaluation methodologies and consider internal mitigation strategies
for equitable outcomes.

</details>


### [170] [Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction](https://arxiv.org/abs/2506.10930)
*Thanathai Lertpetchpun,Tiantian Feng,Dani Byrd,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 为了应对自然语境下的语音情感识别难题，该研究提出了一种多模态、多任务学习框架，并在挑战赛中获得了顶级成绩。


<details>
  <summary>Details</summary>
Motivation: 自然条件下的语音情感识别面临标注不一致和数据不平衡的问题，需要设计一种有效的方法来应对这些挑战。

Method: 该研究通过多模态学习、多任务学习以及不平衡数据处理设计了一种框架，并在训练过程中加入文本嵌入、预测性别以及在训练集中包括"其他"和"没有共识"样本。

Result: 所提出的系统在MSP-Podcast数据集上进行评估，并通过一个简单的双系统集成达到了顶级性能，荣获IS25-SER Challenge的第一和第二名成绩。

Conclusion: 我们的系统在自然情境中的情感识别挑战赛中获得了第一和第二名的优异成绩，展示了我们提出的框架的有效性。

Abstract: Speech emotion recognition (SER) in naturalistic conditions presents a
significant challenge for the speech processing community. Challenges include
disagreement in labeling among annotators and imbalanced data distributions.
This paper presents a reproducible framework that achieves superior (top 1)
performance in the Emotion Recognition in Naturalistic Conditions Challenge
(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system
is designed to tackle the aforementioned challenges through multimodal
learning, multi-task learning, and imbalanced data handling. Specifically, our
best system is trained by adding text embeddings, predicting gender, and
including ``Other'' (O) and ``No Agreement'' (X) samples in the training set.
Our system's results secured both first and second places in the IS25-SER
Challenge, and the top performance was achieved by a simple two-system
ensemble.

</details>


### [171] [Self-Adapting Language Models](https://arxiv.org/abs/2506.10943)
*Adam Zweiger,Jyothish Pari,Han Guo,Ekin Akyürek,Yoon Kim,Pulkit Agrawal*

Main category: cs.LG

TL;DR: SEAL framework enables large language models to self-adapt dynamically by generating their own finetuning data, using a reinforcement learning loop to fine-tune themselves for new tasks and knowledge.


<details>
  <summary>Details</summary>
Motivation: Large language models are static and lack the ability to adapt their weights to new information or tasks. The motivation is to enable these models to adapt themselves dynamically without relying on external adaptation modules.

Method: The method involves Self-Adapting LLMs (SEAL) that can generate their own finetuning data and update directives through self-edits. The approach uses a reinforcement learning loop where the performance of the adapted model acts as a reward signal.

Result: Experiments on knowledge incorporation and few-shot generalization demonstrate SEAL's ability to self-adapt efficiently, confirming its potential as a step toward self-directed language models.

Conclusion: SEAL represents a promising approach towards enabling language models to self-adapt using their own generated data, without the need for external modules.

Abstract: Large language models (LLMs) are powerful but static; they lack mechanisms to
adapt their weights in response to new tasks, knowledge, or examples. We
introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to
self-adapt by generating their own finetuning data and update directives. Given
a new input, the model produces a self-edit-a generation that may restructure
the information in different ways, specify optimization hyperparameters, or
invoke tools for data augmentation and gradient-based updates. Through
supervised finetuning (SFT), these self-edits result in persistent weight
updates, enabling lasting adaptation. To train the model to produce effective
self-edits, we use a reinforcement learning loop with the downstream
performance of the updated model as the reward signal. Unlike prior approaches
that rely on separate adaptation modules or auxiliary networks, SEAL directly
uses the model's own generation to control its adaptation process. Experiments
on knowledge incorporation and few-shot generalization show that SEAL is a
promising step toward language models capable of self-directed adaptation. Our
website and code is available at https://jyopari.github.io/posts/seal.

</details>


### [172] [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/abs/2506.10946)
*Evelyn Ma,Duo Zhou,Peizhi Niu,Huiting Zhou,Huan Zhang,Olgica Milenkovic,S. Rasoul Etesami*

Main category: cs.LG

TL;DR: GUARD通过量化遗忘和保留数据集的对齐程度，并分配不同的遗忘权重，解决了大语言模型去学习中的意外遗忘问题，提升了模型效用。


<details>
  <summary>Details</summary>
Motivation: 随着法规遵从、版权保护和隐私问题的增加，如何在确保大型语言模型(LLM)不忘记有价值的信息的同时实现数据的删除，变得尤为重要。

Method: 我们提出了一个名为GUARD的新框架。这一框架的核心是引入了一种轻量的代理数据属性度量，可以高效量化需要遗忘和保留的数据集之间的“对齐”程度。此外，我们设计了一种新颖的遗忘目标，根据样本的代理属性分数为其分配不同的非均匀遗忘权重。

Result: 理论保证和广泛的实验表明，GUARD能够在遗忘的同时，显著增强数据的保留。特别是在忘记10%训练数据时，GUARD能将保留集上的效用损失最多减少194.92%。

Conclusion: GUARD框架成功解决了大语言模型去学习中的意外遗忘问题，显著增强了保留数据的效用，同时确保了有效的忘记效果。

Abstract: Unlearning in large language models (LLMs) is becoming increasingly important
due to regulatory compliance, copyright protection, and privacy concerns.
However, a key challenge in LLM unlearning is unintended forgetting, where the
removal of specific data inadvertently impairs the utility of the model and its
retention of valuable, desired information. While prior work has primarily
focused on architectural innovations, the influence of data-level factors on
unlearning performance remains underexplored. As a result, existing methods
often suffer from degraded retention when forgetting high-impact data. To
address this, we propose GUARD-a novel framework for Guided Unlearning And
Retention via Data attribution. At its core, GUARD introduces a lightweight
proxy data attribution metric tailored for LLM unlearning, which quantifies the
"alignment" between the forget and retain sets while remaining computationally
efficient. Building on this, we design a novel unlearning objective that
assigns adaptive, nonuniform unlearning weights to samples, inversely
proportional to their proxy attribution scores. Through such a reallocation of
unlearning power, GUARD mitigates unintended losses in retention. We provide
rigorous theoretical guarantees that GUARD significantly enhances retention
while maintaining forgetting metrics comparable to prior methods. Extensive
experiments on the TOFU benchmark across multiple LLM architectures demonstrate
that GUARD substantially improves utility preservation while ensuring effective
unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to
194.92% in terms of Truth Ratio when forgetting 10% of the training data.

</details>


### [173] [Execution Guided Line-by-Line Code Generation](https://arxiv.org/abs/2506.10948)
*Boaz Lavon,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: EG-CFG方法通过结合执行信号，显著提高了代码生成的表现，尤其在复杂编程任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在代码生成中没有利用执行反馈，而人类程序员在推断过程中经常利用此类信号。

Method: 使用EG-CFG方法，通过动态整合执行信号来指导代码生成过程，主要包括三阶段：光束搜索、执行信号提取以及在生成过程中的信号整合。

Result: 实验表明，EG-CFG在各种编码任务中均优于标准方法，提升了代码生成性能。

Conclusion: 我们提出的EG-CFG方法显著提高了代码生成性能，在各个复杂度级别的任务中都达到了最新的水平。

Abstract: We present a novel approach to neural code generation that incorporates
real-time execution signals into the language model generation process. While
large language models (LLMs) have demonstrated impressive code generation
capabilities, they typically do not utilize execution feedback during
inference, a critical signal that human programmers regularly leverage. Our
method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically
incorporates execution signals as the model generates code, providing
line-by-line feedback that guides the generation process toward executable
solutions. EG-CFG employs a multi-stage process: first, we conduct beam search
to sample candidate program completions for each line; second, we extract
execution signals by executing these candidates against test cases; and
finally, we incorporate these signals into the prompt during generation. By
maintaining consistent signals across tokens within the same line and
refreshing signals at line boundaries, our approach provides coherent guidance
while preserving syntactic structure. Moreover, the method naturally supports
native parallelism at the task level in which multiple agents operate in
parallel, exploring diverse reasoning paths and collectively generating a broad
set of candidate solutions. Our experiments across diverse coding tasks
demonstrate that EG-CFG significantly improves code generation performance
compared to standard approaches, achieving state-of-the-art results across
various levels of complexity, from foundational problems to challenging
competitive programming tasks. Our code is available at:
https://github.com/boazlavon/eg_cfg

</details>


### [174] [Build the web for agents, not agents for the web](https://arxiv.org/abs/2506.10953)
*Xing Han Lù,Gaurav Kamath,Marius Mosbach,Siva Reddy*

Main category: cs.LG

TL;DR: 引入Agentic Web Interface (AWI) 概念，通过新交互界面提升网络代理设计的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的网络代理方法面临巨大挑战，主要由于人为设计的界面与大型语言模型的能力之间的根本不匹配。因此，需要一种优化的交互范式，重新定义代理在网络环境中的交互方式。

Method: 引入Agentic Web Interface (AWI) 概念，基于六项设计原则，强调安全性、效率和标准化，以迎合主要利益相关者的需求。

Result: 提出了六项AWI设计的指导原则，以确保网络代理的设计更为高效、安全、可靠和透明。

Conclusion: 本文主张改变当前网络代理研究范式，提出Agentic Web Interface（AWI）的概念，旨在为代理设计优化的交互界面，从而克服现有界面的基本局限性并提高网络代理的效率、可靠性和透明度。

Abstract: Recent advancements in Large Language Models (LLMs) and multimodal
counterparts have spurred significant interest in developing web agents -- AI
systems capable of autonomously navigating and completing tasks within web
environments. While holding tremendous promise for automating complex web
interactions, current approaches face substantial challenges due to the
fundamental mismatch between human-designed interfaces and LLM capabilities.
Current methods struggle with the inherent complexity of web inputs, whether
processing massive DOM trees, relying on screenshots augmented with additional
information, or bypassing the user interface entirely through API interactions.
This position paper advocates for a paradigm shift in web agent research:
rather than forcing web agents to adapt to interfaces designed for humans, we
should develop a new interaction paradigm specifically optimized for agentic
capabilities. To this end, we introduce the concept of an Agentic Web Interface
(AWI), an interface specifically designed for agents to navigate a website. We
establish six guiding principles for AWI design, emphasizing safety,
efficiency, and standardization, to account for the interests of all primary
stakeholders. This reframing aims to overcome fundamental limitations of
existing interfaces, paving the way for more efficient, reliable, and
transparent web agent design, which will be a collaborative effort involving
the broader ML community.

</details>


### [175] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/abs/2506.10955)
*Aayush Karan,Kulin Shah,Sitan Chen*

Main category: cs.LG

TL;DR: The paper proposes ReGuidance, a method that enhances diffusion model-based inverse problem solving by improving both sample realism and rewards, offering theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing training-free methods like diffusion posterior sampling often lack sufficient informativeness in hard inverse problems, leading to unrealistic outputs. There is a need for methods that can maintain sample realism and improve performance in low signal-to-noise scenarios.

Method: The paper introduces a wrapper called ReGuidance, which takes a candidate solution and inverts it using the unconditional probability flow ODE in reverse, then utilizes this as an initialization for diffusion posterior sampling (DPS) to enhance realism and rewards.

Result: ReGuidance applied to state-of-the-art baselines improves both the sample realism and consistency in measurements, particularly in challenging tasks such as large box in-painting and high upscaling super-resolution. Furthermore, the paper provides theoretical evidence of its effectiveness in certain multimodal data distributions.

Conclusion: ReGuidance significantly improves sample quality and measurement consistency in challenging inverse problems by refining candidate solutions closer to the data manifold and simultaneously boosting rewards.

Abstract: There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.

</details>


### [176] [Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods](https://arxiv.org/abs/2506.10959)
*Zhaiming Shen,Alexander Hsu,Rongjie Lai,Wenjing Liao*

Main category: cs.LG

TL;DR: 研究了上下文学习在几何数据中的理论，揭示了几何对泛化误差的影响，并提供了探索非线性模型新工具。


<details>
  <summary>Details</summary>
Motivation: 在于探索在结构化几何数据背景下的上下文学习的理论理解，特别是对于流形上的Hölder函数回归的研究。

Method: 通过建立注意力机制和经典核方法之间的连接，推导出关于提示长度和训练任务数量的泛化误差界。

Result: 当观察到足够多的训练任务时，变压器在流形上的Hölder函数回归达到了Minimax回归率，其扩展基于流形的内在维度，而非环境空间维度。同时，结果还描述了泛化误差如何随着训练任务数量变化。

Conclusion: 该研究揭示了几何在上下文学习中的作用，并提供了研究非线性模型的上下文学习的新工具。

Abstract: While in-context learning (ICL) has achieved remarkable success in natural
language and vision domains, its theoretical understanding--particularly in the
context of structured geometric data--remains unexplored. In this work, we
initiate a theoretical study of ICL for regression of H\"older functions on
manifolds. By establishing a novel connection between the attention mechanism
and classical kernel methods, we derive generalization error bounds in terms of
the prompt length and the number of training tasks. When a sufficient number of
training tasks are observed, transformers give rise to the minimax regression
rate of H\"older functions on manifolds, which scales exponentially with the
intrinsic dimension of the manifold, rather than the ambient space dimension.
Our result also characterizes how the generalization error scales with the
number of training tasks, shedding light on the complexity of transformers as
in-context algorithm learners. Our findings provide foundational insights into
the role of geometry in ICL and novels tools to study ICL of nonlinear models.

</details>


### [177] [Farseer: A Refined Scaling Law in Large Language Models](https://arxiv.org/abs/2506.10972)
*Houyi Li,Wenzhen Zheng,Qiufeng Wang,Zhenyu Ding,Haoying Wang,Zili Wang,Shijie Xuyang,Ning Ding,Shuigeng Zhou,Xiangyu Zhang,Daxin Jiang*

Main category: cs.LG

TL;DR: Farseer引入新的尺度定律，通过改善预测精准度来有效桥接小规模与大规模语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 解决小规模实验结果无法有效转移到耗费资源的生产系统的问题，促进高效创新。

Method: 系统构建模型损失曲面L(N,D)，比现有定律（如Chinchilla定律）更好地拟合了经验数据。

Result: Farseer将总结性误差降低了433%，验证了方法的准确性、鲁棒性和泛化能力，并开源所有模型、数据和结果。

Conclusion: Farseer提供了一个更加精准的尺度定律，通过构建模型损失曲面来改善预测准确性，实现对大规模语言模型训练的更可靠评估。

Abstract: Training Large Language Models (LLMs) is prohibitively expensive, creating a
critical scaling gap where insights from small-scale experiments often fail to
transfer to resource-intensive production systems, thereby hindering efficient
innovation. To bridge this, we introduce Farseer, a novel and refined scaling
law offering enhanced predictive accuracy across scales. By systematically
constructing a model loss surface $L(N,D)$, Farseer achieves a significantly
better fit to empirical data than prior laws (e.g., Chinchilla's law). Our
methodology yields accurate, robust, and highly generalizable predictions,
demonstrating excellent extrapolation capabilities, improving upon Chinchilla's
law by reducing extrapolation error by 433\%. This allows for the reliable
evaluation of competing training strategies across all $(N,D)$ settings,
enabling conclusions from small-scale ablation studies to be confidently
extrapolated to predict large-scale performance. Furthermore, Farseer provides
new insights into optimal compute allocation, better reflecting the nuanced
demands of modern LLM training. To validate our approach, we trained an
extensive suite of approximately 1,000 LLMs across diverse scales and
configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are
comprehensively open-sourcing all models, data, results, and logs at
https://github.com/Farseer-Scaling-Law/Farseer to foster further research.

</details>


### [178] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/abs/2506.10973)
*Julius Berner,Miguel Liu-Schiaffini,Jean Kossaifi,Valentin Duruisseaux,Boris Bonev,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: 本文提供了将流行神经网络架构转换为可解决无穷维函数空间问题的神经算子的方法，并提供了相关代码。


<details>
  <summary>Details</summary>
Motivation: 神经网络在计算机视觉和自然语言处理等领域的成功未能在科学应用中同样呈现，原因是科学问题数据多在无穷维函数空间中。为此，需要将神经网络推广为可在函数空间之间进行映射的神经算子。

Method: 使用提炼出的原则，将几种流行的神经网络架构通过最少的修改转换为神经算子的食谱。

Result: 提供了一套从神经网络到神经算子的转换指南，并开源了相关代码。

Conclusion: 本文识别并提炼了构建实际实施无穷维函数空间映射的关键原则，并提出了一套将几种流行的神经架构转换为神经算子的方法。

Abstract: A wide range of scientific problems, such as those described by
continuous-time dynamical systems and partial differential equations (PDEs),
are naturally formulated on function spaces. While function spaces are
typically infinite-dimensional, deep learning has predominantly advanced
through applications in computer vision and natural language processing that
focus on mappings between finite-dimensional spaces. Such fundamental
disparities in the nature of the data have limited neural networks from
achieving a comparable level of success in scientific applications as seen in
other fields. Neural operators are a principled way to generalize neural
networks to mappings between function spaces, offering a pathway to replicate
deep learning's transformative impact on scientific problems. For instance,
neural operators can learn solution operators for entire classes of PDEs, e.g.,
physical systems with different boundary conditions, coefficient functions, and
geometries. A key factor in deep learning's success has been the careful
engineering of neural architectures through extensive empirical testing.
Translating these neural architectures into neural operators allows operator
learning to enjoy these same empirical optimizations. However, prior neural
operator architectures have often been introduced as standalone models, not
directly derived as extensions of existing neural network architectures. In
this paper, we identify and distill the key principles for constructing
practical implementations of mappings between infinite-dimensional function
spaces. Using these principles, we propose a recipe for converting several
popular neural architectures into neural operators with minimal modifications.
This paper aims to guide practitioners through this process and details the
steps to make neural operators work in practice. Our code can be found at
https://github.com/neuraloperator/NNs-to-NOs

</details>


### [179] [Rethinking Losses for Diffusion Bridge Samplers](https://arxiv.org/abs/2506.10982)
*Sebastian Sanokowski,Lukas Gruber,Christoph Bartmann,Sepp Hochreiter,Sebastian Lehner*

Main category: cs.LG

TL;DR: 研究比较了LV损失和rKL损失在扩散桥与学习扩散系数的表现，发现rKL损失与对数导数技巧结合时表现更优，且训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 研究表明，使用重参数技巧计算逆Kullback-Leibler（rKL）梯度时，Log Variance（LV）损失表现优于rKL损失，但在使用对数导数技巧和不可学习的正向过程中时，LV损失和rKL损失梯度相同。然而，对于扩散桥或可学习扩散系数，这种等价性并不存在，所以LV损失不够具有可动机性。

Method: 通过分析Log Variance（LV）损失与逆Kullback-Leibler（rKL）损失在不同情况下的表现，并进行实验测试不同类型的扩散桥训练的采样器的表现。

Result: 使用rKL损失与对数导数技巧（rKL-LD）不仅解决了概念性问题，而且在性能上全面优于LV损失。实验表明，在复杂的基准测试中，使用rKL-LD损失训练的采样器表现更好，并且rKL-LD需要更少的超参数优化，训练过程更稳健。

Conclusion: rKL-LD损失因其理论和实践上的优越性，尤其是在包括扩散桥的深度学习采样中，优于Log Variance（LV）损失。

Abstract: Diffusion bridges are a promising class of deep-learning methods for sampling
from unnormalized distributions. Recent works show that the Log Variance (LV)
loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when
using the reparametrization trick to compute rKL-gradients. While the on-policy
LV loss yields identical gradients to the rKL loss when combined with the
log-derivative trick for diffusion samplers with non-learnable forward
processes, this equivalence does not hold for diffusion bridges or when
diffusion coefficients are learned. Based on this insight we argue that for
diffusion bridges the LV loss does not represent an optimization objective that
can be motivated like the rKL loss via the data processing inequality. Our
analysis shows that employing the rKL loss with the log-derivative trick
(rKL-LD) does not only avoid these conceptual problems but also consistently
outperforms the LV loss. Experimental results with different types of diffusion
bridges on challenging benchmarks show that samplers trained with the rKL-LD
loss achieve better performance. From a practical perspective we find that
rKL-LD requires significantly less hyperparameter optimization and yields more
stable training behavior.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [180] [CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes](https://arxiv.org/abs/2406.15669)
*Jason Yang,Ariane Mora,Shengchao Liu,Bruce J. Wittmann,Anima Anandkumar,Frances H. Arnold,Yisong Yue*

Main category: q-bio.BM

TL;DR: 该论文介绍了CARE，一个酶分类与检索的基准和数据集，提出了新的检索方法，并对现有方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 近年来，机器学习方法开始用于从序列预测酶功能，但缺乏标准化的评估基准。

Method: 引入CARE，一个用于酶分类和检索的基准和数据集套件，并设计了训练-测试分割以评估不同的分布外泛化。为分类任务提供了基线，并为检索任务提出了Contrastive Reaction-EnzymE Pretraining (CREEP) 方法与现有方法 CLIPZyme 进行比较。

Result: CARE为酶功能预测提供了一套系统的基准，支持分类和检索两大任务，并提出了新的检索任务方法作为基线。

Conclusion: CARE解决了机器学习方法预测酶功能无标准化评估的问题，推动了检索任务的研究。

Abstract: Enzymes are important proteins that catalyze chemical reactions. In recent
years, machine learning methods have emerged to predict enzyme function from
sequence; however, there are no standardized benchmarks to evaluate these
methods. We introduce CARE, a benchmark and dataset suite for the
Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1)
classification of a protein sequence by its enzyme commission (EC) number and
(2) retrieval of an EC number given a chemical reaction. For each task, we
design train-test splits to evaluate different kinds of out-of-distribution
generalization that are relevant to real use cases. For the classification
task, we provide baselines for state-of-the-art methods. Because the retrieval
task has not been previously formalized, we propose a method called Contrastive
Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task
and compare it to the recent method, CLIPZyme. CARE is available at
https://github.com/jsunn-y/CARE/.

</details>


### [181] [Identifying critical residues of a protein using meaningfully-thresholded Random Geometric Graphs](https://arxiv.org/abs/2506.10015)
*Chuqiao Zhang,Sarath Chandra Dantu,Debarghya Mitra,Dalia Chakrabarty*

Main category: q-bio.BM

TL;DR: 本文提出三种基于随机几何图模型的蛋白质关键残基识别方法，并通过模拟和实验对比验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 识别蛋白质的关键残基对理解其功能非常重要，因此发展有效的识别方法具有重要意义。

Method: 通过学习随机几何图(RGG)的方法来识别蛋白质的关键残基。在此过程中，使用Cramer's V计算每对残基状态变量的相关性，通过有机阈值法学习RGG，并与现有阈值技术进行比较。此外，开发了通过计算全图变量和去除一个残基后的图变量的后验概率差异来衡量关键性的方法，以及观察在模拟过程中节点度动态变化的关键性参数化方法。

Result: 使用三种不同的关键性参数化方法识别的结果与实验验证的关键残基进行了对比，验证了方法的有效性。

Conclusion: 使用三种方法能够有效识别蛋白质的关键残基，并与实验结果进行对比验证。

Abstract: Identification of critical residues of a protein is actively pursued, since
such residues are essential for protein function. We present three ways of
recognising critical residues of an example protein, the evolution of which is
tracked via molecular dynamical simulations. Our methods are based on learning
a Random Geometric Graph (RGG) variable, where the state variable of each of
156 residues, is attached to a node of this graph, with the RGG learnt using
the matrix of correlations between state variables of each residue-pair. Given
the categorical nature of the state variable, correlation between a residue
pair is computed using Cramer's V. We advance an organic thresholding to learn
an RGG, and compare results against extant thresholding techniques, when
parametrising criticality as the nodal degree in the learnt RGG. Secondly, we
develop a criticality measure by ranking the computed differences between the
posterior probability of the full graph variable defined on all 156 residues,
and that of the graph with all but one residue omitted. A third parametrisation
of criticality informs on the dynamical variation of nodal degrees as the
protein evolves during the simulation. Finally, we compare results obtained
with the three distinct criticality parameters, against
experimentally-ascertained critical residues.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [182] [AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](https://arxiv.org/abs/2506.10312)
*Yusuke Fujita,Tomoya Mizumoto,Atsushi Kojima,Lianbo Liu,Yui Sudo*

Main category: eess.AS

TL;DR: 开发了一种利用对话延续能力进行音频理解的模型，能在未经多任务调优的情况下实现零样本指令跟随。


<details>
  <summary>Details</summary>
Motivation: 解决音频理解模型在生成目标字幕时遇到的字幕变异问题。

Method: 利用大语言模型(Large Language Models)的对话延续能力，训练模型生成反应，模拟输入字幕触发的对话，而不是直接生成数据中的目标字幕。

Result: 所提模型在未经多任务指令微调的情况下实现了零样本指令跟随能力。实验结果表明，模型能有效跟随各种未见过的指令。

Conclusion: 通过对话延续训练，可以有效捕捉字幕的深层含义，提升模型在未见指令情况下的表现能力。

Abstract: We propose an instruction-following audio comprehension model that leverages
the dialogue continuation ability of large language models (LLMs). Instead of
directly generating target captions in training data, the proposed method
trains a model to produce responses as if the input caption triggered a
dialogue. This dialogue continuation training mitigates the caption variation
problem. Learning to continue a dialogue effectively captures the caption's
meaning beyond its surface-level words. As a result, our model enables
zero-shot instruction-following capability without multitask instruction
tuning, even trained solely on audio captioning datasets. Experiments on
AudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene
question-answering tests demonstrate our model's ability to follow various
unseen instructions.

</details>


### [183] [RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding](https://arxiv.org/abs/2506.10289)
*Yisi Liu,Chenyang Wang,Hanjo Kim,Raniya Khan,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 本文提出RT-VC，利用发音特征空间和DDSP，实现低延迟高质量的实时语音转换。实验表明，RT-VC在确保与SOTA相当的合成质量下，延迟降低了13.3%。


<details>
  <summary>Details</summary>
Motivation: 语音转换作为一项关键技术，在众多领域中应用广泛，该研究旨在开发零样本实时语音转换系统，提升性能并降低延迟。

Method: 该研究利用发音特征空间来自然分离内容和说话者特征，并结合可微数字信号处理（DDSP）技术以提高编码效率。

Result: 实验评估显示，与当前最先进的方法相比，RT-VC系统的CPU延迟为61.4毫秒，减少了13.3%的延迟，并且合成质量保持不变。

Conclusion: RT-VC系统在实现高质量语音转换的同时，显著降低了转换延迟，与当前技术水平相当。

Abstract: Voice conversion has emerged as a pivotal technology in numerous applications
ranging from assistive communication to entertainment. In this paper, we
present RT-VC, a zero-shot real-time voice conversion system that delivers
ultra-low latency and high-quality performance. Our approach leverages an
articulatory feature space to naturally disentangle content and speaker
characteristics, facilitating more robust and interpretable voice
transformations. Additionally, the integration of differentiable digital signal
processing (DDSP) enables efficient vocoding directly from articulatory
features, significantly reducing conversion latency. Experimental evaluations
demonstrate that, while maintaining synthesis quality comparable to the current
state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms,
representing a 13.3\% reduction in latency.

</details>


### [184] [Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes](https://arxiv.org/abs/2506.10653)
*Rogier C. van Dalen,Shucong Zhang,Titouan Parcollet,Sourav Bhattacharya*

Main category: eess.AS

TL;DR: 该研究通过结合使用完整假设的条件熵和"speaker code" 来提高语音识别在新环境中的适应性，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 语音识别器通常只在特定环境中表现最佳，需要针对其他环境进行适应；而在新说话者的适应中，通常可用于微调的数据过少且缺乏标签。

Method: 该论文提出了一种新的损失函数，即完整假设的条件熵，并结合使用多种假设来提高适应性。此外，还引入了一种"speaker code" 短向量来表征说话者，减少需要的数据量。

Result: 在Common Voice的远场噪声增强版本上，该方案在一分钟的适应数据上实现了20%的词错误率相对改进，在十分钟上增加到29%。

Conclusion: 该论文提出了一种结合方法，使得在单分钟数据上的自适应变得更加鲁棒，实现了在不同环境下的高效性能。

Abstract: Speech recognisers usually perform optimally only in a specific environment
and need to be adapted to work well in another. For adaptation to a new
speaker, there is often too little data for fine-tuning to be robust, and that
data is usually unlabelled. This paper proposes a combination of approaches to
make adaptation to a single minute of data robust. First, instead of estimating
the adaptation parameters with cross-entropy on a single error-prone hypothesis
or "pseudo-label", this paper proposes a novel loss function, the conditional
entropy over complete hypotheses. Using multiple hypotheses makes adaptation
more robust to errors in the initial recognition. Second, a "speaker code"
characterises a speaker in a vector short enough that it requires little data
to estimate. On a far-field noise-augmented version of Common Voice, the
proposed scheme yields a 20% relative improvement in word error rate on one
minute of adaptation data, increasing on 10 minutes to 29%.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [185] [FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](https://arxiv.org/abs/2506.10789)
*Rudy Alexandro Garrido Veliz,Martin Semmann,Chris Biemann,Seid Muhie Yimam*

Main category: cs.CY

TL;DR: 本研究首次为美国社会背景下的新法西斯主义数字话语制定了编码方案，并通过大量互联网活动数据的标注，创建了识别新法西斯言论的第一批分类模型。研究强调了相关工作的继续对民主社会的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于新法西斯主义在美国及其他西方社会的发展，对民主和少数群体构成严重威胁，因此需要采取积极行动加以遏制。

Method: 通过众包标注方式对收集的帖子进行注释，随后对小型和大型语言模型进行微调和测试，构建新法西斯主义言论的分类模型。

Result: 我们在这些论坛中发现新法西斯主义论调无处不在，这些平台是未来研究的良好目标。研究中发现，用于检测新法西斯言论的分类模型可以应用于这些数据。

Conclusion: 必须继续努力对抗新法西斯主义，以维护民主社会的福祉。

Abstract: Neo-fascism is a political and societal ideology that has been having
remarkable growth in the last decade in the United States of America (USA), as
well as in other Western societies. It poses a grave danger to democracy and
the minorities it targets, and it requires active actions against it to avoid
escalation. This work presents the first-of-its-kind neo-fascist coding scheme
for digital discourse in the USA societal context, overseen by political
science researchers. Our work bridges the gap between Natural Language
Processing (NLP) and political science against this phenomena. Furthermore, to
test the coding scheme, we collect a tremendous amount of activity on the
internet from notable neo-fascist groups (the forums of Iron March and
Stormfront.org), and the guidelines are applied to a subset of the collected
posts. Through crowdsourcing, we annotate a total of a thousand posts that are
labeled as neo-fascist or non-neo-fascist. With this labeled data set, we
fine-tune and test both Small Language Models (SLMs) and Large Language Models
(LLMs), obtaining the very first classification models for neo-fascist
discourse. We find that the prevalence of neo-fascist rhetoric in this kind of
forum is ever-present, making them a good target for future research. The
societal context is a key consideration for neo-fascist speech when conducting
NLP research. Finally, the work against this kind of political movement must be
pressed upon and continued for the well-being of a democratic society.
Disclaimer: This study focuses on detecting neo-fascist content in text,
similar to other hate speech analyses, without labeling individuals or
organizations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [186] [Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations](https://arxiv.org/abs/2506.10249)
*Andrea Gaggioli,Sabrina Bartolotta,Andrea Ubaldi,Katusha Gerardini,Eleonora Diletta Sarcinella,Alice Chirico*

Main category: cs.HC

TL;DR: 研究AI如何通过支持、协同和共生模式增强创造力，并分析其理论、伦理和设计影响。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能如何增强人类创造力及其实现方式。

Method: 应用分布式创造性视角，识别三种AI支持创造过程的主要模式: 支持、协同和共生。

Result: 分析AI不同配置影响创造力水平，以及探讨理论、伦理和设计的影响。

Conclusion: AI能够通过支持、协同和共生三种模式增强人类创造力，不同模式对创造力水平的影响各异。

Abstract: Artificial Intelligence holds significant potential to enhance human
creativity. However, achieving this vision requires a clearer understanding of
how such enhancement can be effectively realized. Adopting the perspective of
distributed creativity, we identify three primary modes through which AI can
contribute to creative processes: Support, where AI acts as a tool; Synergy,
where AI and humans collaborate in complementary ways; and Symbiosis, where
human and AI cognition become so integrated that they form a unified creative
system. These modes are defined along two key dimensions: the level of
technical autonomy exhibited by the AI system and the degree of perceived
agency attributed to it. We examine how each configuration influences different
levels of creativity - from everyday problem-solving to paradigm-shifting
innovation - and discuss the theoretical, ethical, and design implications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [187] [HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration](https://arxiv.org/abs/2506.10401)
*Jiaqi Lv,Xufeng He,Yanchen Liu,Xu Dai,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 提出了一种新框架，通过AI编译器和优化技术改善CUDA代码转译，实验显示该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于CUDA在并行计算软件领域的主导地位，需要其他硬件支持CUDA软件的性能便携性。但CUDA代码的转译面临巨大挑战，而现有LLM在CUDA转译上的性能不足。

Method: 利用AI编译器和自动优化技术生成高性能CUDA代码对，并通过基于图的增强方法提升生成质量，提出了一种用于评估LLM在CUDA转译性能的新基准HPCTransEval。

Result: 研究结果表明，通过CUDA到CPU的转译实例研究，我们提出的框架在领先的LLM上显著改善了转译性能。

Conclusion: 本文提出的框架显著提高了CUDA代码的转译性能，强调了大型语言模型（LLMs）在解决CUDA生态系统兼容性挑战中的潜力。

Abstract: The rapid growth of deep learning has driven exponential increases in model
parameters and computational demands. NVIDIA GPUs and their CUDA-based software
ecosystem provide robust support for parallel computing, significantly
alleviating computational bottlenecks. Meanwhile, due to the cultivation of
user programming habits and the high performance of GPUs, the CUDA ecosystem
has established a dominant position in the field of parallel software. This
dominance requires other hardware platforms to support CUDA-based software with
performance portability. However, translating CUDA code to other platforms
poses significant challenges due to differences in parallel programming
paradigms and hardware architectures. Existing approaches rely on language
extensions, domain-specific languages (DSLs), or compilers but face limitations
in workload coverage and generalizability. Moreover, these methods often incur
substantial development costs. Recently, LLMs have demonstrated extraordinary
potential in various vertical domains, especially in code-related tasks.
However, the performance of existing LLMs in CUDA transpilation, particularly
for high-performance code, remains suboptimal. The main reason for this
limitation lies in the lack of high-quality training datasets. To address these
challenges, we propose a novel framework for generating high-performance CUDA
and corresponding platform code pairs, leveraging AI compiler and automatic
optimization technology. We further enhance the framework with a graph-based
data augmentation method and introduce HPCTransEval, a benchmark for evaluating
LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU
transpilation as a case study on leading LLMs. The result demonstrates that our
framework significantly improves CUDA transpilation, highlighting the potential
of LLMs to address compatibility challenges within the CUDA ecosystem.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [188] [Attention on flow control: transformer-based reinforcement learning for lift regulation in highly disturbed flows](https://arxiv.org/abs/2506.10153)
*Zhecheng Liu,Jeff D. Eldredge*

Main category: physics.flu-dyn

TL;DR: 研究利用变压器强化学习框架实现更优的空气动力升力控制策略，且在多阵风情境下展示了极好的泛化能力和控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有为弱干扰设计的线性流动控制策略可能在非线性强干扰情况下失效，因此本文尝试开发更好的控制策略。

Method: 本文提出了一种基于变压器的强化学习框架，用于通过俯仰控制在阵风序列中调节空气动力升力。

Result: 学习得来的控制策略在有多个阵风的情况下优于最好的比例控制，尤其是在阵风数量增加时表现更佳。四分之一弦处的俯仰控制较中弦控制在升力调节上表现更优且需要更少的控制努力。

Conclusion: 本文成功展示了基于变压器的强化学习框架在多阵风情境下的有效性和广泛适用性，并证明了其在提升气动升力控制的同时减少控制努力。

Abstract: A linear flow control strategy designed for weak disturbances may not remain
effective in sequences of strong disturbances due to nonlinear interactions,
but it is sensible to leverage it for developing a better strategy. In the
present study, we propose a transformer-based reinforcement learning (RL)
framework to learn an effective control strategy for regulating aerodynamic
lift in gust sequences via pitch control. The transformer addresses the
challenge of partial observability from limited surface pressure sensors. We
demonstrate that the training can be accelerated with two techniques --
pretraining with an expert policy (here, linear control) and task-level
transfer learning (here, extending a policy trained on isolated gusts to
multiple gusts). We show that the learned strategy outperforms the best
proportional control, with the performance gap widening as the number of gusts
increases. The control strategy learned in an environment with a small number
of successive gusts is shown to effectively generalize to an environment with
an arbitrarily long sequence of gusts. We investigate the pivot configuration
and show that quarter-chord pitching control can achieve superior lift
regulation with substantially less control effort compared to mid-chord
pitching control. Through a decomposition of the lift, we attribute this
advantage to the dominant added-mass contribution accessible via quarter-chord
pitching. The success on multiple configurations shows the generalizability of
the proposed transformer-based RL framework, which offers a promising approach
to solve more computationally demanding flow control problems when combined
with the proposed acceleration techniques.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [189] [Leveraging LLMs for Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10093)
*Marcos Abel Zuzuárregui,Stefano Carpin*

Main category: cs.RO

TL;DR: 通过利用大型语言模型和ROS2节点连接自定义任务和机器人，使机器人能通过自然语言指令执行复杂任务，有效解决了用户技术壁垒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人系统难以适应执行多样化任务，尤其是由于终端用户通常缺乏技术专长。因此需要一个系统来简化用户与机器人之间的交互。

Method: 系统利用ChatGPT等大型语言模型（LLMs），通过自然语言指令让用户分配复杂的数据收集任务给自主机器人。任务计划通过IEEE任务规格标准进行编码，并通过ROS2节点在机器人上执行，将高层次的任务描述与现有ROS库连接。

Result: 实验强调了LLMs在执行任务时的优劣，并验证了所提出方法的有效性，同时提出了改进措施以克服LLMs在空间推理和复杂路径解决中的局限性。

Conclusion: 通过实验展示了大型语言模型（LLMs）在空间推理和复杂路径制定方面的优势和局限性，并展示了所提出的实现如何克服这些问题。

Abstract: Robotics and artificial intelligence hold significant potential for advancing
precision agriculture. While robotic systems have been successfully deployed
for various tasks, adapting them to perform diverse missions remains
challenging, particularly because end users often lack technical expertise. In
this paper, we present an end-to-end system that leverages large language
models (LLMs), specifically ChatGPT, to enable users to assign complex data
collection tasks to autonomous robots using natural language instructions. To
enhance reusability, mission plans are encoded using an existing IEEE task
specification standard, and are executed on robots via ROS2 nodes that bridge
high-level mission descriptions with existing ROS libraries. Through extensive
experiments, we highlight the strengths and limitations of LLMs in this
context, particularly regarding spatial reasoning and solving complex routing
challenges, and show how our proposed implementation overcomes them.

</details>


### [190] [One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10106)
*Marcos Abel Zuzuárregui,Mustafa Melih Toslak,Stefano Carpin*

Main category: cs.RO

TL;DR: 本研究提出了一种自然语言机器人任务规划器，通过大语言模型，帮助非技术用户在精准农业中，无需编程即可控制多样化的机器人。


<details>
  <summary>Details</summary>
Motivation: 尽管技术进步为精准农业带来了更多效率，但也引入了额外的复杂性和陡峭的学习曲线。对于必须在技术采用与现有工作负载之间取得平衡的非技术用户来说，使用这些技术具有挑战性。

Method: 利用大语言模型（LLMs）和预定义的原语构建的架构，将人类语言无缝地转化为可由不同机器人平台执行的中间描述。

Result: 实验结果显示，该架构足够通用以支持各种不同的机器人，也足够强大以执行复杂任务请求。

Conclusion: 这项工作显著推动了精准农业中机器人自动化的发展，使得非技术用户可以更容易地使用。

Abstract: Artificial intelligence is transforming precision agriculture, offering
farmers new tools to streamline their daily operations. While these
technological advances promise increased efficiency, they often introduce
additional complexity and steep learning curves that are particularly
challenging for non-technical users who must balance tech adoption with
existing workloads. In this paper, we present a natural language (NL) robotic
mission planner that enables non-specialists to control heterogeneous robots
through a common interface. By leveraging large language models (LLMs) and
predefined primitives, our architecture seamlessly translates human language
into intermediate descriptions that can be executed by different robotic
platforms. With this system, users can formulate complex agricultural missions
without writing any code. In the work presented in this paper, we extend our
previous system tailored for wheeled robot mission planning through a new class
of experiments involving robotic manipulation and computer vision tasks. Our
results demonstrate that the architecture is both general enough to support a
diverse set of robots and powerful enough to execute complex mission requests.
This work represents a significant step toward making robotic automation in
precision agriculture more accessible to non-technical users.

</details>


### [191] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 该研究提出了一种模块化导航框架，通过分离视觉-语言理解和行动规划，实现低计算成本的有效导航。尽管初期在测试中遇到困难，但此方法为未来改进指明了方向。


<details>
  <summary>Details</summary>
Motivation: 尽管在大规模视觉语言模型（如CLIP和Flamingo）方面取得了重大进展，但它们引入了与计算成本和实时部署相关的新挑战。因此，提出一种新的导航框架以实现可扩展和高效的导航。

Method: 我们提出了一种模块化、即插即用的导航框架，将视觉-语言理解与行动规划分离。通过集成冷冻的视觉-语言模型Qwen2.5-VL-7B-Instruct和轻量级规划逻辑，我们旨在实现灵活、快速和适应性强的导航，而无需广泛的模型微调。我们的框架利用提示工程、结构化历史管理和两帧视觉输入策略来增强导航步骤之间的决策连续性。

Result: 我们的系统在使用Matterport3D数据集和Habitat-Lab仿真环境的VLN-CE设置中的Room-to-Room基准上进行了评估。结果虽然初期显示出推广到未见过环境中的挑战，但显示出未来提升的潜力。

Conclusion: 虽然初步结果揭示了在严格评估条件下推广到看不见的环境中的挑战，但我们的模块化方法为可扩展和高效的导航系统打下了基础，突出了通过增强环境先验和扩展多模态输入集成来进行未来改进的可喜方向。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied
AI, requiring agents to interpret natural language instructions and navigate
through visually rich, unfamiliar environments. Recent advances in large
vision-language models (LVLMs), such as CLIP and Flamingo, have significantly
improved multimodal understanding but introduced new challenges related to
computational cost and real-time deployment. In this project, we propose a
modular, plug-and-play navigation framework that decouples vision-language
understanding from action planning. By integrating a frozen vision-language
model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to
achieve flexible, fast, and adaptable navigation without extensive model
fine-tuning. Our framework leverages prompt engineering, structured history
management, and a two-frame visual input strategy to enhance decision-making
continuity across navigation steps. We evaluate our system on the Room-to-Room
benchmark within the VLN-CE setting using the Matterport3D dataset and
Habitat-Lab simulation environment. Although our initial results reveal
challenges in generalizing to unseen environments under strict evaluation
settings, our modular approach lays a foundation for scalable and efficient
navigation systems, highlighting promising directions for future improvement
through enhanced environmental priors and expanded multimodal input
integration.

</details>


### [192] [Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving](https://arxiv.org/abs/2506.10317)
*Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya,Dhaval Bhanderi*

Main category: cs.RO

TL;DR: 通过将结构化道路数据与现有模型结合，提高车道拓扑预测的准确性，测试显示改进。


<details>
  <summary>Details</summary>
Motivation: 提升自动导航的道路理解，特别在复杂交叉路口，提高车道拓扑预测的准确性。

Method: 将自然语言中编码的道路信息（如OSM地图中的结构化道路元数据和道路设计手册中的车道宽度）与道路中心线编码相结合，优化SMERF模型。

Result: 在地理多样的复杂交叉路口场景中进行评估，结果显示车道和交通元素检测及其关联得到了改善。

Conclusion: 方法可以推广并适应不同拓扑结构和条件，提升自动驾驶车辆的车道预测能力。

Abstract: Lane-topology prediction is a critical component of safe and reliable
autonomous navigation. An accurate understanding of the road environment aids
this task. We observe that this information often follows conventions encoded
in natural language, through design codes that reflect the road structure and
road names that capture the road functionality. We augment this information in
a lightweight manner to SMERF, a map-prior-based online lane-topology
prediction model, by combining structured road metadata from OSM maps and
lane-width priors from Road design manuals with the road centerline encodings.
We evaluate our method on two geo-diverse complex intersection scenarios. Our
method shows improvement in both lane and traffic element detection and their
association. We report results using four topology-aware metrics to
comprehensively assess the model performance. These results demonstrate the
ability of our approach to generalize and scale to diverse topologies and
conditions.

</details>


### [193] [Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models](https://arxiv.org/abs/2506.10098)
*Christian Reichenbächer,Philipp Rank,Jochen Hipp,Oliver Bringmann*

Main category: cs.RO

TL;DR: The paper introduces Gaussian Mixture Copula Models for better statistical modeling of driving scenarios, proving more effective than previous methods on real-world data.


<details>
  <summary>Details</summary>
Motivation: To improve the modeling of joint probability distribution of scenario parameters, which is crucial for the safety validation of automated driving systems.

Method: Utilized Gaussian Mixture Copula Models and benchmarked them against Gaussian Mixture Models and Gaussian Copula Models using real-world driving data.

Result: Evaluation on 18 million scenarios shows that Gaussian Mixture Copula Models outperform other methods in fitting real-world data.

Conclusion: Gaussian Mixture Copula Models offer a better fit for statistical modeling of driving scenarios, improving both likelihood and Sinkhorn distance.

Abstract: This paper presents the first application of Gaussian Mixture Copula Models
to the statistical modeling of driving scenarios for the safety validation of
automated driving systems. Knowledge of the joint probability distribution of
scenario parameters is essential for scenario-based safety assessment, where
risk quantification depends on the likelihood of concrete parameter
combinations. Gaussian Mixture Copula Models bring together the multimodal
expressivity of Gaussian Mixture Models and the flexibility of copulas,
enabling separate modeling of marginal distributions and dependencies. We
benchmark Gaussian Mixture Copula Models against previously proposed approaches
- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving
data drawn from scenarios defined in United Nations Regulation No. 157. Our
evaluation across 18 million scenario instances demonstrates that Gaussian
Mixture Copula Models provide a better fit to the data in terms of both
likelihood and Sinkhorn distance. These results suggest that Gaussian Mixture
Copula Models are a compelling foundation for future scenario-based validation
frameworks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [194] [Fundamental Limits of Learning High-dimensional Simplices in Noisy Regimes](https://arxiv.org/abs/2506.10101)
*Seyed Amir Hossein Saberi,Amir Najafi,Abolfazl Motahari,Babak H. khalaj*

Main category: stat.ML

TL;DR: 研究高维单纯形的学习样本复杂度，通过算法实现从噪声数据中恢复单纯形，给出了样本复杂度的上下界并解决了开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声情况下估计高维单纯形所需的样本复杂度问题，尤其是在给定信噪比的情况下。

Method: 使用样本压缩技术和新颖的基于傅里叶的方法，以恢复分布并估计单纯形。

Result: 证明存在一个算法能以高概率输出与真实单纯形在$\ell_2$或总变动距离不超过$\varepsilon$的估计，并推导出了信息论上的样本复杂度下界。

Conclusion: 我们在高维空间中研究了如何从噪声数据中学习一个单纯形，并提出了关于样本复杂度的界限。

Abstract: In this paper, we establish sample complexity bounds for learning
high-dimensional simplices in $\mathbb{R}^K$ from noisy data. Specifically, we
consider $n$ i.i.d. samples uniformly drawn from an unknown simplex in
$\mathbb{R}^K$, each corrupted by additive Gaussian noise of unknown variance.
We prove an algorithm exists that, with high probability, outputs a simplex
within $\ell_2$ or total variation (TV) distance at most $\varepsilon$ from the
true simplex, provided $n \ge (K^2/\varepsilon^2)
e^{\mathcal{O}(K/\mathrm{SNR}^2)}$, where $\mathrm{SNR}$ is the signal-to-noise
ratio. Extending our prior work~\citep{saberi2023sample}, we derive new
information-theoretic lower bounds, showing that simplex estimation within TV
distance $\varepsilon$ requires at least $n \ge \Omega(K^3
\sigma^2/\varepsilon^2 + K/\varepsilon)$ samples, where $\sigma^2$ denotes the
noise variance. In the noiseless scenario, our lower bound $n \ge
\Omega(K/\varepsilon)$ matches known upper bounds up to constant factors. We
resolve an open question by demonstrating that when $\mathrm{SNR} \ge
\Omega(K^{1/2})$, noisy-case complexity aligns with the noiseless case. Our
analysis leverages sample compression techniques (Ashtiani et al., 2018) and
introduces a novel Fourier-based method for recovering distributions from noisy
observations, potentially applicable beyond simplex learning.

</details>


### [195] [Momentum Multi-Marginal Schrödinger Bridge Matching](https://arxiv.org/abs/2506.10168)
*Panagiotis Theodoropoulos,Augustinos D. Saravanos,Evangelos A. Theodorou,Guan-Horng Liu*

Main category: stat.ML

TL;DR: 引入了3MSBM框架，解决了长时间依赖性捕捉问题，实验显示其在处理复杂动态上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法有效捕捉长时间依赖性，对推断轨迹的连贯性产生了潜在影响。

Method: 引入动量多边缘Schrödinger Bridge匹配(3MSBM)框架，通过将动力学提升至相空间并条件化在多个点上，形成一个多边缘条件随机最优控制问题。通过固定由多边缘条件桥引导的路径，最小化变分目标函数来学习底层动力学。

Result: 3MSBM在捕获具有时间依赖性的复杂动态方面表现优于现有方法，并显著提高了收敛性和可扩展性。

Conclusion: 3MSBM能够有效地从稀疏的样本快照中推断出轨迹，展现了其在多边缘环境中训练匹配框架的新途径。

Abstract: Understanding complex systems by inferring trajectories from sparse sample
snapshots is a fundamental challenge in a wide range of domains, e.g.,
single-cell biology, meteorology, and economics. Despite advancements in Bridge
and Flow matching frameworks, current methodologies rely on pairwise
interpolation between adjacent snapshots. This hinders their ability to capture
long-range temporal dependencies and potentially affects the coherence of the
inferred trajectories. To address these issues, we introduce \textbf{Momentum
Multi-Marginal Schr\"odinger Bridge Matching (3MSBM)}, a novel matching
framework that learns smooth measure-valued splines for stochastic systems that
satisfy multiple positional constraints. This is achieved by lifting the
dynamics to phase space and generalizing stochastic bridges to be conditioned
on several points, forming a multi-marginal conditional stochastic optimal
control problem. The underlying dynamics are then learned by minimizing a
variational objective, having fixed the path induced by the multi-marginal
conditional bridge. As a matching approach, 3MSBM learns transport maps that
preserve intermediate marginals throughout training, significantly improving
convergence and scalability. Extensive experimentation in a series of
real-world applications validates the superior performance of 3MSBM compared to
existing methods in capturing complex dynamics with temporal dependencies,
opening new avenues for training matching frameworks in multi-marginal
settings.

</details>


### [196] [Distributionally-Constrained Adversaries in Online Learning](https://arxiv.org/abs/2506.10293)
*Moïse Blanchard,Samory Kpotufe*

Main category: stat.ML

TL;DR: 本文探讨了分布受限对抗者的在线学习环境，并给出了学习在不同分布类中的可行性分析，尤其在没有分布类先验知识下实现有效学习。


<details>
  <summary>Details</summary>
Motivation: 在在线学习中，理解从对抗设置到随机设置的连续性引起了很多关注，已有的框架包括平滑设置，以弥合这一差距，我们希望在此基础上提供更为细致的分析。

Method: 我们研究了对抗者在某些受限分布类内选择分布的情况下，实例从中被抽取，提供了更一般和灵活的分布受限对抗者框架。通过这种框架，我们正在对抗无意识和自适应对抗者的环境中进行特征化。

Result: 在多个自然函数类中，包括线性分类器，学习可以在没有任何分布类先验知识的情况下实现。换句话说，学习者可以同时与可学习分布类中的任何受限对抗者竞争。

Conclusion: 我们给出了哪些分布类在此环境中是可学习的特征，并提供了对函数类和对抗者分布限制之间的相互作用如何促进可学习性的见解。尤其是，我们的结果恢复并推广了已知的平滑环境的可学习性。

Abstract: There has been much recent interest in understanding the continuum from
adversarial to stochastic settings in online learning, with various frameworks
including smoothed settings proposed to bridge this gap. We consider the more
general and flexible framework of distributionally constrained adversaries in
which instances are drawn from distributions chosen by an adversary within some
constrained distribution class [RST11]. Compared to smoothed analysis, we
consider general distributional classes which allows for a fine-grained
understanding of learning settings between fully stochastic and fully
adversarial for which a learner can achieve non-trivial regret. We give a
characterization for which distribution classes are learnable in this context
against both oblivious and adaptive adversaries, providing insights into the
types of interplay between the function class and distributional constraints on
adversaries that enable learnability. In particular, our results recover and
generalize learnability for known smoothed settings. Further, we show that for
several natural function classes including linear classifiers, learning can be
achieved without any prior knowledge of the distribution class -- in other
words, a learner can simultaneously compete against any constrained adversary
within learnable distribution classes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [197] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/abs/2506.10001)
*Yuxuan Li,Sheng Jinag,Bizhu Wang*

Main category: cs.MM

TL;DR: SC-CEE-Meta架构通过语义通信减少VR数据传输延迟，提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 面对元宇宙的广泛热情与发展，现有的无线传输技术受到带宽不足和信道质量差的限制，需要新的方法来改善用户体验。

Method: 提出了基于语义通信的云-边-端协作沉浸式元宇宙服务架构，用于解决VR设备与云端之间的数据传输问题。

Result: 在Meta Quest Pro设备上验证，SC-CEE-Meta可以减少96.05%的无线传输延迟，并在糟糕的信道条件下增进43.99%的图像质量。

Conclusion: SC-CEE-Meta架构能够显著减少无线传输延迟，并在恶劣的通道条件下提升图像质量。

Abstract: With technology advancing and the pursuit of new audiovisual experiences
strengthening, the metaverse has gained surging enthusiasm. However, it faces
practical hurdles as substantial data like high-resolution virtual scenes must
be transmitted between cloud platforms and VR devices. Specifically, the VR
device's wireless transmission hampered by insufficient bandwidth, causes speed
and delay problems. Meanwhile, poor channel quality leads to data errors and
worsens user experience. To solve this, we've proposed the Semantic
Communication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service
(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic
transmission, video synthesis, and 3D virtual scene reconstruction. By
deploying semantic modules on VR devices and edge servers and sending key
semantic info instead of focusing on bit-level reconstruction, it can cut
latency, resolve the resource-bandwidth conflict, and better withstand channel
interference. Also, the cloud deploys video synthesis and 3D scene
reconstruction preprocessing, while edge devices host 3D reconstruction
rendering modules, all for immersive services. Verified on Meta Quest Pro, the
SC-CEE-Meta can reduce wireless transmission delay by 96.05\% and boost image
quality by 43.99\% under poor channel condition.

</details>


### [198] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 本文提出AVD模型，通过生成行车记录仪视频的因果部分，改进了交通事故预判性能，并且实验结果显示其具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前交通事故预判方法难以识别事故的真实因果部分，数据偏差容易导致背景混淆问题，需要一种无需繁琐标注的新方法。

Method: 提出了一种名为Attentive Video Diffusion (AVD)的模型，通过生成行车记录仪视频中的因果部分来合成额外的事故视频片段，并保持视频生成后的风格和内容。AVD结合Equivariant TAA (EQ-TAA)，通过一个等变三重损失来对比生成的视频片段。

Result: AVD和EQ-TAA在交通事故预测的实验中表现出了与先进方法相比具备竞争力的性能。

Conclusion: AVD and EQ-TAA show competitive performance in Traffic Accident Anticipation compared to state-of-the-art methods.

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging
problem for achieving zero fatalities in the future. Current approaches
typically treat TAA as a supervised learning task needing the laborious
annotation of accident occurrence duration. However, the inherent long-tailed,
uncertain, and fast-evolving nature of traffic scenes has the problem that real
causal parts of accidents are difficult to identify and are easily dominated by
data bias, resulting in a background confounding issue. Thus, we propose an
Attentive Video Diffusion (AVD) model that synthesizes additional accident
video clips by generating the causal part in dashcam videos, i.e., from normal
clips to accident clips. AVD aims to generate causal video frames based on
accident or accident-free text prompts while preserving the style and content
of frames for TAA after video generation. This approach can be trained using
datasets collected from various driving scenes without any extra annotations.
Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant
triple loss for an anchor accident-free video clip, along with the generated
pair of contrastive pseudo-normal and pseudo-accident clips. Extensive
experiments have been conducted to evaluate the performance of AVD and EQ-TAA,
and competitive performance compared to state-of-the-art methods has been
obtained.

</details>


### [199] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/abs/2506.10004)
*Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 本文回顾了XR流媒体领域的最新研究，探讨其技术特点、用户体验、视觉注意力优化方法，以及面临的挑战与应用。


<details>
  <summary>Details</summary>
Motivation: 随着XR技术快速发展并变革内容创建与消费，研究旨在调查当前XR流媒体技术状态，提供对效率和用户体验提升的见解。

Method: 文章通过定义XR及其交互方法、分析XR流量特性、探索用户体验因素、提出基于视觉注意的优化方法，并检视当前的应用与挑战，进行了全面的调研。

Result: 文章详细分析了XR技术在流媒体领域的当前应用并指出了提升用户体验的关键因素，同时也提出了视觉注意力优化的方法来提高XR流媒体的效率和性能。

Conclusion: 本文调查了XR流媒体的最新进展，探讨了视觉注意力优化方法及其应用，分析了流量特性和用户体验，以提供对未来发展的见解。

Abstract: Extended reality (XR) is rapidly advancing, and poised to revolutionize
content creation and consumption. In XR, users integrate various sensory inputs
to form a cohesive perception of the virtual environment. This survey reviews
the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,
we define XR and introduce various XR headsets along with their multimodal
interaction methods to provide a foundational understanding. We then analyze XR
traffic characteristics to highlight the unique data transmission requirements.
We also explore factors that influence the quality of experience in XR systems,
aiming to identify key elements for enhancing user satisfaction. Following
this, we present visual attention-based optimization methods for XR streaming
to improve efficiency and performance. Finally, we examine current applications
and highlight challenges to provide insights into ongoing and future
developments of XR.

</details>


### [200] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出了一种高效的、自适应双模态框架来改善HER2评估模型的准确性和效率，特别在资源有限环境下。


<details>
  <summary>Details</summary>
Motivation: 现有的HER2评估模型主要单独分析H&E或IHC图像，而临床上需要综合解读这两种模式。然而同时获取这两种模式常因工作流程复杂性和成本限制而受阻。

Method: 提出了一种自适应的双模框架，包括动态分支选择器、双向跨模态GAN、混合训练协议。

Result: 提高了单模态H&E预测准确性至94.25%和双模态预测准确性至95.09%，并在仅有IHC输入时保持90.28%的可靠性。证明比基础方法提高了22.81%和12.90%的准确性。

Conclusion: 该框架减少了不同设置中因数据缺失导致的性能下降，且在多样化的医疗环境中具有显著潜力。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or
IHC images in isolation,despite clinical reliance on their synergistic
interpretation. However, concurrent acquisition of both modalities is often
hindered by workflow complexity and cost constraints. We propose an adaptive
bimodal framework enabling flexible single-/dual-modality HER2 prediction
through three innovations: 1) A dynamic branch selector that activates either
single-modality reconstruction or dual-modality joint inference based on input
completeness; 2) A bidirectional cross-modal GAN performing context-aware
feature-space reconstruction of missing modalities; 3) A hybrid training
protocol integrating adversarial learning and multi-task optimization. This
architecture elevates single-modality H&E prediction accuracy from 71.44% to
94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%
reliability with sole IHC inputs. The framework's "dual-preferred,
single-compatible" design delivers near-bimodal performance without requiring
synchronized acquisition, particularly benefiting resource-limited settings
through IHC infrastructure cost reduction. Experimental validation confirms
22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with
cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251
(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or
native fusion pathways, the system mitigates performance degradation from
missing data while preserving computational efficiency (78.55% parameter
reduction in lightweight variant). This elastic architecture demonstrates
significant potential for democratizing precise HER2 assessment across diverse
healthcare settings.

</details>


### [201] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 该论文介绍了一种通过多模态整合策略和注意力扩散模型改善3D面部情感动画的新方法，显著提升了情感表达的多样性和自然性。


<details>
  <summary>Details</summary>
Motivation: 解决音频驱动的情感3D面部动画中单一模态控制信号限制及确定性回归映射的问题。

Method: 采用FLAME中心的多模态情感绑定策略，并结合注意力潜在扩散模型，加强了运动多样性和面部动态一致性。

Result: 我们的方法在大多数指标上优于现有方法，在保持生理上合理面部动态的同时，情感相似性提高了21.6%。

Conclusion: 我们提出了一种基于扩散的框架，用于可控的3D面部动画，实现了更好的情感相似性和面部动态保持。

Abstract: Audio-driven emotional 3D facial animation encounters two significant
challenges: (1) reliance on single-modal control signals (videos, text, or
emotion labels) without leveraging their complementary strengths for
comprehensive emotion manipulation, and (2) deterministic regression-based
mapping that constrains the stochastic nature of emotional expressions and
non-verbal behaviors, limiting the expressiveness of synthesized animations. To
address these challenges, we present a diffusion-based framework for
controllable expressive 3D facial animation. Our approach introduces two key
innovations: (1) a FLAME-centered multimodal emotion binding strategy that
aligns diverse modalities (text, audio, and emotion labels) through contrastive
learning, enabling flexible emotion control from multiple signal sources, and
(2) an attention-based latent diffusion model with content-aware attention and
emotion-guided layers, which enriches motion diversity while maintaining
temporal coherence and natural facial dynamics. Extensive experiments
demonstrate that our method outperforms existing approaches across most
metrics, achieving a 21.6\% improvement in emotion similarity while preserving
physiologically plausible facial dynamics. Project Page:
https://kangweiiliu.github.io/Control_3D_Animation.

</details>


### [202] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/abs/2506.10008)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 提出了一种层次化知识图谱来解析视觉叙事，展示了通过Manga109数据集在叙事任务上取得的高效性与准确性。


<details>
  <summary>Details</summary>
Motivation: 在多模态媒体（如漫画）领域中，如何结构化地理解视觉叙事是一项挑战。因此，研发一个层次化知识图谱框架，旨在帮助理解和解析复杂的叙事内容。

Method: 该研究提出了一种分解叙事内容为多个层次的方法，并通过集成的知识图谱捕捉语义、空间和时间关系。基于面板构建多模态图，将视觉元素与文本组件链接，支持对故事结构的推理。

Result: 在Manga109数据集的手动标注子集中应用该方法，取得了高精度和高召回率，验证了该框架的连贯性和可解释性。

Conclusion: 该研究提供了一个层次化的知识图谱框架，能有效支持视听媒介的叙事任务，包括操作检索、对话追踪、角色外观映射和面板时间线重建。

Abstract: This paper presents a hierarchical knowledge graph framework for the
structured understanding of visual narratives, focusing on multimodal media
such as comics. The proposed method decomposes narrative content into multiple
levels, from macro-level story arcs to fine-grained event segments. It
represents them through integrated knowledge graphs that capture semantic,
spatial, and temporal relationships. At the panel level, we construct
multimodal graphs that link visual elements such as characters, objects, and
actions with corresponding textual components, including dialogue and captions.
These graphs are integrated across narrative levels to support reasoning over
story structure, character continuity, and event progression.
  We apply our approach to a manually annotated subset of the Manga109 dataset
and demonstrate its ability to support symbolic reasoning across diverse
narrative tasks, including action retrieval, dialogue tracing, character
appearance mapping, and panel timeline reconstruction. Evaluation results show
high precision and recall across tasks, validating the coherence and
interpretability of the framework. This work contributes a scalable foundation
for narrative-based content analysis, interactive storytelling, and multimodal
reasoning in visual media.

</details>


### [203] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/abs/2506.10011)
*Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu*

Main category: cs.MM

TL;DR: 该研究通过频域分析非语言信息提升了意图识别的准确性，比之前的方法提高了1.13%。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态意图识别方法主要集中在文本分析上，常常忽视了非语言线索中的丰富语义内容。

Method: 提出了一种新颖的基于小波驱动的多模态意图识别（WDMIR）框架，通过在频域中对非语言信息进行分析来增强意图理解。具体方法包括小波驱动的融合模块和跨模态交互机制。

Result: 在MIntRec数据集上的实验表明，该方法实现了最先进的性能，准确率比此前的方法提高了1.13%。消融研究显示，小波驱动的融合模块在提取非语言语义信息方面有显著提升，可使识别准确率提高0.41%。

Conclusion: 通过频域分析非语言信息的方法可以有效地提高多模态意图识别的准确性。

Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user
intentions by integrating verbal and non-verbal information across video, audio
and text modalities. While existing approaches prioritize text analysis, they
often overlook the rich semantic content embedded in non-verbal cues. This
paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)
framework that enhances intent understanding through frequency-domain analysis
of non-verbal information. To be more specific, we propose: (1) a
wavelet-driven fusion module that performs synchronized decomposition and
integration of video-audio features in the frequency domain, enabling
fine-grained analysis of temporal dynamics; (2) a cross-modal interaction
mechanism that facilitates progressive feature enhancement from bimodal to
trimodal integration, effectively bridging the semantic gap between verbal and
non-verbal information. Extensive experiments on MIntRec demonstrate that our
approach achieves state-of-the-art performance, surpassing previous methods by
1.13% on accuracy. Ablation studies further verify that the wavelet-driven
fusion module significantly improves the extraction of semantic information
from non-verbal sources, with a 0.41% increase in recognition accuracy when
analyzing subtle emotional cues.

</details>


### [204] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
*Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker*

Main category: cs.MM

TL;DR: 该论文综述了多模态大语言模型的发展，探讨如何通过基础性技术实现跨模态能力，分析架构创新及新兴协同作用，并识别出开放性挑战和发展方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型已经迅速超越文本生成领域，扩展到包括图像、音乐、视频、人类动作和3D对象等多样的输出模态，通过在统一架构下整合语言与其他感官模态，需分类和研究生成性模态及其关键技术。

Method: 该研究调查了自监督学习（SSL）、专家混合（MoE）、人类反馈强化学习（RLHF）和连锁思维（CoT）提示等基础性技术的使用方式，并分析关键模型、架构趋势及新兴的跨模态协同作用。

Result: 调查揭示了架构创新如变压器和扩散模型如何支持这种模式融合，促进跨模态传递和模块化专项化，并指出评估、模块化和结构化推理方面的开放性挑战。

Conclusion: 该综述提供了对多模态大语言模型（MLLMs）发展的统一视角，识别出通向更通用、适应性强且可解释的多模态系统的关键路径。

Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text
generation, now spanning diverse output modalities including images, music,
video, human motion, and 3D objects, by integrating language with other sensory
modalities under unified architectures. This survey categorises six primary
generative modalities and examines how foundational techniques, namely
Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement
Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,
enable cross-modal capabilities. We analyze key models, architectural trends,
and emergent cross-modal synergies, while highlighting transferable techniques
and unresolved challenges. Architectural innovations like transformers and
diffusion models underpin this convergence, enabling cross-modal transfer and
modular specialization. We highlight emerging patterns of synergy, and identify
open challenges in evaluation, modularity, and structured reasoning. This
survey offers a unified perspective on MLLM development and identifies critical
paths toward more general-purpose, adaptive, and interpretable multimodal
systems.

</details>


### [205] [Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction](https://arxiv.org/abs/2506.10010)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.MM

TL;DR: 本文通过分析语音及面部、手部运动的关系，加强实时情感检测的准确性，揭示不同情感在交流中的表达差异。


<details>
  <summary>Details</summary>
Motivation: 研究人类交流中情感表达的多模态性，并找到影响情感交流清晰度的因素，从而提升实时情感检测的准确性与同步性。

Method: 该研究利用运动捕捉技术分析 IEMOCAP 数据库中二人互动的多模态情感耦合。通过对区域性运动的捕捉以及语音特征的分析，关联3D面部和手部标记位移，测量不同情境下的情感表达活跃性。运用了语音特征到面部和手部运动的预测映射技术来研究两者之间的关联。

Result: 非重叠语音能够引发在面部和嘴部的更大表达活跃性，悲伤在非重叠期间表现出更高的表达性，而愤怒在重叠期间则抑制手部动作。预测模型显示语音韵律和MFCCs在言语相关区域具有最高预测准确性，而唤醒和效价则呈现较低和更依赖情境的相关性。手与语音同步性在低唤醒和重叠语音期间得到增强，而不是效价。

Conclusion: 通过分析语音、面部和手部运动的动态关系，可以增强实时情感检测的准确性。这种分析为促进人类交流和人工智能系统的情感识别提供了重要基础。

Abstract: Human emotional expression emerges through coordinated vocal, facial, and
gestural signals. While speech face alignment is well established, the broader
dynamics linking emotionally expressive speech to regional facial and hand
motion remains critical for gaining a deeper insight into how emotional and
behavior cues are communicated in real interactions. Further modulating the
coordination is the structure of conversational exchange like sequential turn
taking, which creates stable temporal windows for multimodal synchrony, and
simultaneous speech, often indicative of high arousal moments, disrupts this
alignment and impacts emotional clarity. Understanding these dynamics enhances
realtime emotion detection by improving the accuracy of timing and synchrony
across modalities in both human interactions and AI systems. This study
examines multimodal emotion coupling using region specific motion capture from
dyadic interactions in the IEMOCAP corpus. Speech features included low level
prosody, MFCCs, and model derived arousal, valence, and categorical emotions
(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker
displacements. Expressive activeness was quantified through framewise
displacement magnitudes, and speech to gesture prediction mapped speech
features to facial and hand movements. Nonoverlapping speech consistently
elicited greater activeness particularly in the lower face and mouth. Sadness
showed increased expressivity during nonoverlap, while anger suppressed
gestures during overlaps. Predictive mapping revealed highest accuracy for
prosody and MFCCs in articulatory regions while arousal and valence had lower
and more context sensitive correlations. Notably, hand speech synchrony was
enhanced under low arousal and overlapping speech, but not for valence.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [206] [An Analysis of Datasets, Metrics and Models in Keyphrase Generation](https://arxiv.org/abs/2506.10346)
*Florian Boudin,Akiko Aizawa*

Main category: cs.IR

TL;DR: 本文分析了50多篇论文，指出关键词生成领域的关键问题，并发布了一个强大的PLM模型。


<details>
  <summary>Details</summary>
Motivation: 虽然关键词生成领域有许多研究进展，但尚未有关于该领域的系统综述和分析，导致其现状不明。本文旨在填补这一空白。

Method: 分析和综述50余篇关键词生成的研究论文，找出当前评估实践中的关键问题。

Result: 我们的研究发现当前评估实践中存在几个关键问题，如常用基准数据集之间的相似性以及指标计算中的不一致性，导致性能的高估。为此，我们发布了一个基于PLM的强大模型，以促进未来的研究。

Conclusion: 本文通过分析50多篇关于关键词生成的研究论文，概述了近年来在该领域的进展、局限性和挑战。

Abstract: Keyphrase generation refers to the task of producing a set of words or
phrases that summarises the content of a document. Continuous efforts have been
dedicated to this task over the past few years, spreading across multiple lines
of research, such as model architectures, data resources, and use-case
scenarios. Yet, the current state of keyphrase generation remains unknown as
there has been no attempt to review and analyse previous work. In this paper,
we bridge this gap by presenting an analysis of over 50 research papers on
keyphrase generation, offering a comprehensive overview of recent progress,
limitations, and open challenges. Our findings highlight several critical
issues in current evaluation practices, such as the concerning similarity among
commonly-used benchmark datasets and inconsistencies in metric calculations
leading to overestimated performances. Additionally, we address the limited
availability of pre-trained models by releasing a strong PLM-based model for
keyphrase generation as an effort to facilitate future research.

</details>


### [207] [Towards Understanding Bias in Synthetic Data for Evaluation](https://arxiv.org/abs/2506.10301)
*Hossein A. Rahmani,Varsha Ramineni,Nick Craswell,Bhaskar Mitra,Emine Yilmaz*

Main category: cs.IR

TL;DR: 研究使用大型语言模型生成的合成测试集，其评估可能存在偏差，对绝对性能影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前生成合成测试集仍未被充分探索，验证合成测试集在信息检索系统评估中的可靠性。

Method: 使用LLMs生成合成查询和标签，通过线性混合效应模型验证偏差存在性，并分析其对系统评估的影响。

Result: 在系统评估中发现存在偏差，通过线性混合效应模型确认偏差的影响较大，特别是在绝对性能计算中。

Conclusion: 合成测试集在绝对系统性能评估中可能会引入显著偏差，但在比较相对系统性能时，其影响可能不如预期显著。

Abstract: Test collections are crucial for evaluating Information Retrieval (IR)
systems. Creating a diverse set of user queries for these collections can be
challenging, and obtaining relevance judgments, which indicate how well
retrieved documents match a query, is often costly and resource-intensive.
Recently, generating synthetic datasets using Large Language Models (LLMs) has
gained attention in various applications. While previous work has used LLMs to
generate synthetic queries or documents to improve ranking models, using LLMs
to create synthetic test collections is still relatively unexplored. Previous
work~\cite{rahmani2024synthetic} showed that synthetic test collections have
the potential to be used for system evaluation, however, more analysis is
needed to validate this claim. In this paper, we thoroughly investigate the
reliability of synthetic test collections constructed using LLMs, where LLMs
are used to generate synthetic queries, labels, or both. In particular, we
examine the potential biases that might occur when such test collections are
used for evaluation. We first empirically show the presence of such bias in
evaluation results and analyse the effects it might have on system evaluation.
We further validate the presence of such bias using a linear mixed-effects
model. Our analysis shows that while the effect of bias present in evaluation
results obtained using synthetic test collections could be significant, for
e.g.~computing absolute system performance, its effect may not be as
significant in comparing relative system performance. Codes and data are
available at: https://github.com/rahmanidashti/BiasSyntheticData.

</details>


### [208] [Conversational Search: From Fundamentals to Frontiers in the LLM Era](https://arxiv.org/abs/2506.10635)
*Fengran Mo,Chuan Meng,Mohammad Aliannejadi,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 本教程介绍对话搜索中LLMs的应用及其带来的前沿发展，为参与者提供开发新一代系统的知识。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在指令遵循、内容生成和推理方面的能力为构建智能对话搜索系统提供了新的机遇和挑战，吸引了显著关注和进步。

Method: 本教程旨在介绍LLMs革命性发展的基础原理和新兴话题之间的联系。

Result: 参与者将全面了解由LLMs驱动的对话搜索的核心原则和前沿发展，让他们具备相应知识，以贡献于下一代对话搜索系统的开发。

Conclusion: LLMs为智能对话搜索系统的构建提供了新的机会和挑战，推动该领域的革命性发展。

Abstract: Conversational search enables multi-turn interactions between users and
systems to fulfill users' complex information needs. During this interaction,
the system should understand the users' search intent within the conversational
context and then return the relevant information through a flexible,
dialogue-based interface. The recent powerful large language models (LLMs) with
capacities of instruction following, content generation, and reasoning, attract
significant attention and advancements, providing new opportunities and
challenges for building up intelligent conversational search systems. This
tutorial aims to introduce the connection between fundamentals and the emerging
topics revolutionized by LLMs in the context of conversational search. It is
designed for students, researchers, and practitioners from both academia and
industry. Participants will gain a comprehensive understanding of both the core
principles and cutting-edge developments driven by LLMs in conversational
search, equipping them with the knowledge needed to contribute to the
development of next-generation conversational search systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [209] [Equitable Mechanism Design for Facility Location](https://arxiv.org/abs/2506.10460)
*Toby Walsh*

Main category: cs.GT

TL;DR: 探讨设施位置策略中最大化代理之间公平性的机制，证明无法约束Gini指数，与此同时提出效用补充Gini指数和Nash福利的估计算法。


<details>
  <summary>Details</summary>
Motivation: 研究如何在位置设施策略下实现代理之间的最大公平性，同时探讨这些机制如何成功模拟辅助福利使用。

Method: 使用Gini指数来衡量代理之间的公平性，并提出了计算效用补充Gini指数的近似比的替代方法，包括使用确定性和随机机制进行比较。

Result: 研究发现，在策略机制下无法约束设施效用的最优Gini指数近似比。提出的补充Gini指数和Nash福利机制能够提供更好的近似比。

Conclusion: 本文探讨了位置设施策略下最大化代理公平性的机制。通过研究，我们发现没有一种策略机制能够控制任意数量的设施效用的最优Gini指数的近似比。

Abstract: We consider strategy proof mechanisms for facility location which maximize
equitability between agents. As is common in the literature, we measure
equitability with the Gini index. We first prove a simple but fundamental
impossibility result that no strategy proof mechanism can bound the
approximation ratio of the optimal Gini index of utilities for one or more
facilities. We propose instead computing approximation ratios of the
complemented Gini index of utilities, and consider how well both deterministic
and randomized mechanisms approximate this. In addition, as Nash welfare is
often put forwards as an equitable compromise between egalitarian and
utilitarian outcomes, we consider how well mechanisms approximate the Nash
welfare.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [210] [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/abs/2506.10274)
*Pooneh Mousavi,Gallil Maimon,Adel Moumen,Darius Petermann,Jiatong Shi,Haibin Wu,Haici Yang,Anastasia Kuznetsova,Artem Ploujnikov,Ricard Marxer,Bhuvana Ramabhadran,Benjamin Elizalde,Loren Lugosch,Jinyu Li,Cem Subakan,Phil Woodland,Minje Kim,Hung-yi Lee,Shinji Watanabe,Yossi Adi,Mirco Ravanelli*

Main category: cs.SD

TL;DR: 本文系统回顾并基准测试了离散音频标记器，提出了其分类法，评估了重建、下游性能及声学语言模型的多项表现，揭示关键瓶颈及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常专注于特定领域或任务，缺乏对各种基准的统一比较。本研究旨在为离散音频标记器提供系统回顾和基准测试。

Method: 提出标记方法的分类法，并通过对标记器进行多项基准测试的评估，进行受控消融研究以分析折衷。

Result: 研究发现了关键的限制、实际考虑因素和开放挑战，为快速发展的领域提供洞察和指导。

Conclusion: 研究对离散音频标记器进行了系统回顾和基准测试，涵盖语音、音乐和通用音频三个领域，提出了一个标记方法的分类法，并通过多项基准测试评估了这些标记器。

Abstract: Discrete audio tokens are compact representations that aim to preserve
perceptual quality, phonetic content, and speaker characteristics while
enabling efficient storage and inference, as well as competitive performance
across diverse downstream tasks.They provide a practical alternative to
continuous features, enabling the integration of speech and audio into modern
large language models (LLMs). As interest in token-based audio processing
grows, various tokenization methods have emerged, and several surveys have
reviewed the latest progress in the field. However, existing studies often
focus on specific domains or tasks and lack a unified comparison across various
benchmarks. This paper presents a systematic review and benchmark of discrete
audio tokenizers, covering three domains: speech, music, and general audio. We
propose a taxonomy of tokenization approaches based on encoder-decoder,
quantization techniques, training paradigm, streamability, and application
domains. We evaluate tokenizers on multiple benchmarks for reconstruction,
downstream performance, and acoustic language modeling, and analyze trade-offs
through controlled ablation studies. Our findings highlight key limitations,
practical considerations, and open challenges, providing insight and guidance
for future research in this rapidly evolving area. For more information,
including our main results and tokenizer database, please refer to our website:
https://poonehmousavi.github.io/dates-website/.

</details>


### [211] [Fine-Grained control over Music Generation with Activation Steering](https://arxiv.org/abs/2506.10225)
*Dipanshu Panda,Jayden Koshy Joe,Harshith M R,Swathi Narashiman,Pranay Mathur,Anish Veerakumar,Aniruddh Krishna,Keerthiharan A*

Main category: cs.SD

TL;DR: 该论文提出了一种通过MusicGen进行细粒度音乐生成控制的方法，实现音色迁移、风格迁移和流派融合。


<details>
  <summary>Details</summary>
Motivation: 通过推理时的干预实现对音乐生成的更细粒度的控制，包括音色迁移、风格迁移和流派融合。

Method: 提出了一种通过自回归生成式音乐变压器MusicGen进行细粒度音乐生成控制的方法。该方法通过在推理时干预，利用训练的线性探测器的权重来引导残差流，或以类似方式引导注意力层的激活。

Result: 通过将这一任务建模为回归任务，性能得到改善；音频样本展示了该方法的效果。

Conclusion: 该方法提供了音乐生成的全局和局部控制能力。

Abstract: We present a method for fine-grained control over music generation through
inference-time interventions on an autoregressive generative music transformer
called MusicGen. Our approach enables timbre transfer, style transfer, and
genre fusion by steering the residual stream using weights of linear probes
trained on it, or by steering the attention layer activations in a similar
manner. We observe that modelling this as a regression task provides improved
performance, hypothesizing that the mean-squared-error better preserve
meaningful directional information in the activation space. Combined with the
global conditioning offered by text prompts in MusicGen, our method provides
both global and local control over music generation. Audio samples illustrating
our method are available at our demo page.

</details>


### [212] [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
*Tony Alex,Wish Suharitdamrong,Sara Atito,Armin Mustafa,Philip J. B. Jackson,Imran Razzak,Muhammad Awais*

Main category: cs.SD

TL;DR: 研究了影响音频大语言模型架构设计的因素，通过推迟音频集成、使用注意力子模块和集成多样音频编码器改进了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管在专注应用方面的开发进展迅速，特别是在为特定能力如音频推理策划训练数据方面，但音频编码器到LLMs的语义表示高效转移的基本机制仍然未得到充分探索。

Method: 从Pengi/LLaVA风格的音频-LLM架构出发，提出并评估了一些通过机械解释性研究和LLM操作原则导出的假设性修改，通过实验验证这些修改的有效性。

Result: 最终架构整合了所有建议的修改，与基线相比实现了10%到60%的相对改进，验证了优化音频大语言模型中跨模态信息转移的成功路径。

Conclusion: 通过调整架构设计，可以显著提高音频大语言模型（Audio-LLM）的跨模态信息传输效率。

Abstract: The integration of audio perception capabilities into Large Language Models
(LLMs) has enabled significant advances in Audio-LLMs. Although
application-focused developments, particularly in curating training data for
specific capabilities e.g., audio reasoning, have progressed rapidly, the
underlying mechanisms that govern efficient transfer of rich semantic
representations from audio encoders to LLMs remain under-explored. We
conceptualize effective audio-LLM interaction as the LLM's ability to
proficiently probe the audio encoder representations to satisfy textual
queries. This paper presents a systematic investigation on how architectural
design choices can affect that. Beginning with a standard Pengi/LLaVA-style
audio-LLM architecture, we propose and evaluate several modifications guided by
hypotheses derived from mechanistic interpretability studies and LLM
operational principles. Our experiments demonstrate that: (1) delaying audio
integration until the LLM's initial layers establish textual context that
enhances its ability to probe the audio representations for relevant
information; (2) the LLM can proficiently probe audio representations
exclusively through LLM layer's attention submodule, without requiring
propagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently
integrated ensemble of diverse audio encoders provides richer, complementary
representations, thereby broadening the LLM's capacity to probe a wider
spectrum of audio information. All hypotheses are evaluated using an identical
three-stage training curriculum on a dataset of 5.6 million audio-text pairs,
ensuring controlled comparisons. Our final architecture, which incorporates all
proposed modifications, achieves relative improvements from 10\% to 60\% over
the baseline, validating our approach to optimizing cross-modal information
transfer in audio-LLMs. Project page: https://ta012.github.io/PAL/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [213] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 研究通过结合LLMs、静态代码分析及RAG，实现代码问题的自动检测与修订，大幅提高代码质量。


<details>
  <summary>Details</summary>
Motivation: 在软件开发过程中使用大型语言模型（LLMs）自动检测和修改代码问题，以提高代码质量和开发效率。

Method: 使用静态代码分析框架检测软件项目中的问题，并通过迭代的提示工程和RAG方法来自动修订代码，同时利用自定义的代码比较应用来解决LLM的幻觉问题。

Result: 通过结合LLMs、静态代码分析和RAG，代码问题显著减少，验证了方法的有效性。

Conclusion: 结合LLMs、静态分析和RAG能够有效提高代码质量，简化软件开发过程，并减少时间和资源的消耗。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [214] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 该论文提出了一种评估LLMs在代码生成中对输入提示敏感性的合成管道和人物角色评估方法。实验结果支持方法的有效性，且适用于各种编程任务和LLMs。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估大型语言模型（LLMs）在代码生成中的质量对输入提示的敏感性。由于生成代码的功能和质量对用户背景和软件开发熟悉程度具有敏感性，因此需要量化评估这种敏感性。

Method: 提出了一种合成评估管道和系统性的人物角色评估方法，以揭示LLM在不同用户背景下响应的质的差异。两种方法均与特定的编程任务和LLMs无关，具有广泛的适用性。

Result: 实验结果表明，所提出的方法有助于评估LLM对输入变化的敏感性，并展示了其在不同用户背景下的响应变化。

Conclusion: 两种评估方法被证明是有用的，并且作者分享了他们的代码以惠及社区。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [215] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 文章提出了一个结合Lisp环境的多层架构，使得语言模型能够进行程序交互和工具自我演化。


<details>
  <summary>Details</summary>
Motivation: 结合符号编程和神经语言生成，开发能进行程序交互的交互式人工智能系统。

Method: 通过在生成中嵌入Lisp表达式，并通过中间件层拦截这些表达式，使系统支持有状态的外部内存、反射编程和动态工具创建。

Result: 提出一种架构，使得LLMs能够在交互式编程环境中进行自我工具定义、调用和演进。

Conclusion: 文中提出的架构实现了与Lisp环境的深度集成，为未来的交互式AI系统发展提供了指导性框架和原则。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [216] [VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning](https://arxiv.org/abs/2506.10275)
*Jun Qi,Chao-Han Yang,Pin-Yu Chen,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: VQC-MLPNet is a new hybrid quantum-classical approach overcoming key VQC limitations, offering significant representation and training enhancements, validated theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of VQCs like limited expressivity, optimization issues, and hardware noise sensitivity, and to enhance quantum machine learning applications.

Method: Introduces a hybrid quantum-classical architecture called VQC-MLPNet, employing quantum circuits for parameter generation for MLPs using amplitude encoding and parameterized quantum operations. Provides theoretical guarantees using statistical learning techniques and Neural Tangent Kernel analysis.

Result: Demonstrated exponential improvements in representation capacity with increased quantum circuit depth and qubits, with empirical validation through experiments in semiconductor and genomic contexts, even under quantum noise simulations.

Conclusion: VQC-MLPNet shows resilient performance and is theoretically sound and practically robust for quantum-enhanced learning in noisy quantum computing environments.

Abstract: Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine
learning, yet their practical application is hindered by inherent limitations
such as constrained linear expressivity, optimization challenges, and acute
sensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a
scalable and robust hybrid quantum-classical architecture designed to overcome
these obstacles. By innovatively employing quantum circuits to dynamically
generate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude
encoding and parameterized quantum operations, VQC-MLPNet substantially expands
representation capabilities and augments training stability. We provide
rigorous theoretical guarantees via statistical learning techniques and Neural
Tangent Kernel analysis, explicitly deriving upper bounds on approximation,
uniform deviation, and optimization errors. These theoretical insights
demonstrate exponential improvements in representation capacity relative to
quantum circuit depth and the number of qubits, providing clear computational
advantages over standalone quantum circuits and existing hybrid quantum
architectures. Our theoretical claims are empirically corroborated through
extensive experiments, including classifying semiconductor quantum-dot charge
states and predicting genomic transcription factor binding sites, demonstrating
resilient performance even under realistic IBM quantum noise simulations. This
research establishes a theoretically sound and practically robust framework,
advancing the frontiers of quantum-enhanced learning for unconventional
computing paradigms in the Noisy Intermediate-Scale Quantum era and beyond.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [217] [AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components](https://arxiv.org/abs/2506.10111)
*Abiodun Ganiyu,Pranshav Gajjar,Vijay K Shah*

Main category: cs.NI

TL;DR: 本文介绍了AI5GTest，一个利用AI的大型语言模型框架，自动化O-RAN组件验证，解决了传统方法的效率和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试框架过于依赖手动过程，容易出现人为错误，导致一致性和扩展性问题。需要一种自动化的解决方案以提高效率和准确性。

Method: AI5GTest采用AI驱动的、规范感知的测试框架，利用合作的大型语言模型框架，包括Gen-LLM, Val-LLM和Debug-LLM模块，并引入人机交互机制。

Result: AI5GTest显著减少了测试执行时间，提高了验证准确性，验证了其在O-RAN组件自动化测试中的有效性。

Conclusion: AI5GTest在O-RAN组件验证中显示出比传统手动方法更快速高效，同时保持了高验证准确性。

Abstract: The advent of Open Radio Access Networks (O-RAN) has transformed the
telecommunications industry by promoting interoperability, vendor diversity,
and rapid innovation. However, its disaggregated architecture introduces
complex testing challenges, particularly in validating multi-vendor components
against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as
those provided by Open Testing and Integration Centres (OTICs), rely heavily on
manual processes, are fragmented and prone to human error, leading to
inconsistency and scalability issues. To address these limitations, we present
AI5GTest -- an AI-powered, specification-aware testing framework designed to
automate the validation of O-RAN components. AI5GTest leverages a cooperative
Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and
Debug-LLM. Gen-LLM automatically generates expected procedural flows for test
cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references
signaling messages against these flows to validate compliance and detect
deviations. If anomalies arise, Debug-LLM performs root cause analysis,
providing insight to the failure cause. To enhance transparency and
trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the
Gen-LLM presents top-k relevant official specifications to the tester for
approval before proceeding with validation. Evaluated using a range of test
cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest
demonstrates a significant reduction in overall test execution time compared to
traditional manual methods, while maintaining high validation accuracy.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [218] [scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data](https://arxiv.org/abs/2506.10031)
*Olga Ovcharenko,Florian Barkmann,Philip Toma,Imant Daunhawer,Julia Vogt,Sebastian Schelter,Valentina Boeva*

Main category: q-bio.QM

TL;DR: 提出scSSL-Bench来评估SSL方法对单细胞数据的表现，揭示了任务特定的取舍，并提供具体建议。


<details>
  <summary>Details</summary>
Motivation: 推动我们对SSL方法应用于单细胞数据的理解，提出scSSL-Bench以评估现有SSL方法的表现。

Method: 评估了19种SSL方法，对比了9个数据集中的三种下游任务，并系统评估了不同的数据增强策略。

Result: 分析结果揭示了任务特定的取舍，专门的单细胞框架在单模态批量校正中表现优秀，而通用SSL方法在细胞类型识别和多模态数据集成中表现更好。此外，随机掩蔽被证明是最有效的数据增强技术。结果还表明需要一个专门的单细胞多模态数据集成框架。

Conclusion: scSSL-Bench提供了一个标准化的评估平台，并针对单细胞分析应用SSL方法提出了具体建议，推动深度学习与单细胞基因组学的融合。

Abstract: Self-supervised learning (SSL) has proven to be a powerful approach for
extracting biologically meaningful representations from single-cell data. To
advance our understanding of SSL methods applied to single-cell data, we
present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL
methods. Our evaluation spans nine datasets and focuses on three common
downstream tasks: batch correction, cell type annotation, and missing modality
prediction. Furthermore, we systematically assess various data augmentation
strategies. Our analysis reveals task-specific trade-offs: the specialized
single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at
uni-modal batch correction, while generic SSL methods, such as VICReg and
SimCLR, demonstrate superior performance in cell typing and multi-modal data
integration. Random masking emerges as the most effective augmentation
technique across all tasks, surpassing domain-specific augmentations. Notably,
our results indicate the need for a specialized single-cell multi-modal data
integration framework. scSSL-Bench provides a standardized evaluation platform
and concrete recommendations for applying SSL to single-cell analysis,
advancing the convergence of deep learning and single-cell genomics.

</details>


### [219] [Predicting function of evolutionarily implausible DNA sequences](https://arxiv.org/abs/2506.10271)
*Shiyu Jiang,Xuyin Liu,Zitong Jerry Wang*

Main category: q-bio.QM

TL;DR: 研究引入Nullsettes任务评估基因组模型的突变效应预测能力，发现性能与非突变体可能性相关，且受序列长度影响。


<details>
  <summary>Details</summary>
Motivation: 基因组语言模型在合成生物学中生成新颖的功能性DNA序列具有潜力，但需要它们不仅学习进化的合理性，还要学习序列到功能的关系。

Method: 本研究引入了一组称为Nullsettes的预测任务，用于评估模型预测因关键控制元素转移而产生的功能丧失突变的能力。

Result: 研究发现，在12个最先进的模型中，突变效应预测性能与非突变体的预测可能性强相关。另外，预测强模型性能的可能性值范围严重依赖于序列长度。

Conclusion: 本研究强调了在利用基因组语言模型进行突变效应预测时，同时考虑序列可能性和序列长度的重要性。

Abstract: Genomic language models (gLMs) show potential for generating novel,
functional DNA sequences for synthetic biology, but doing so requires them to
learn not just evolutionary plausibility, but also sequence-to-function
relationships. We introduce a set of prediction tasks called Nullsettes, which
assesses a model's ability to predict loss-of-function mutations created by
translocating key control elements in synthetic expression cassettes. Across 12
state-of-the-art models, we find that mutation effect prediction performance
strongly correlates with the predicted likelihood of the nonmutant.
Furthermore, the range of likelihood values predictive of strong model
performance is highly dependent on sequence length. Our work highlights the
importance of considering both sequence likelihood and sequence length when
using gLMs for mutation effect prediction.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [220] [Phase-Space Engineering and Dynamical Long-Range Order in Memcomputing](https://arxiv.org/abs/2506.10149)
*Chesson Sipling,Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: physics.comp-ph

TL;DR: 研究表明，记忆和超参数对数字记忆计算机的计算效率有重要影响，适当的参数设置可优化问题求解过程。


<details>
  <summary>Details</summary>
Motivation: 研究数字记忆计算机中的物理参数如何影响组合优化问题的求解，以优化其计算效率。

Method: 通过对一种原型数字记忆计算机进行数值模拟，分析物理参数在工程设计中的作用。

Result: 发现当超参数选择得当时，系统能有效探索其状态空间，解决问题的时间随着变量数量的增加而良好扩展。如果选择不当，尽管有动态长程有序（DLRO）的现象，状态空间导航效率却降低。

Conclusion: 记忆和超参数在设计数字记忆计算机的状态空间以达到最佳计算效率方面起着重要作用。

Abstract: Digital Memcomputing machines (DMMs) are dynamical systems with memory (time
non-locality) that have been designed to solve combinatorial optimization
problems. Their corresponding ordinary differential equations depend on a few
hyper-parameters that define both the system's relevant time scales and its
phase-space geometry. Using numerical simulations on a prototypical DMM, we
analyze the role of these physical parameters in engineering the phase space to
either help or hinder the solution search by DMMs. We find that the DMM
explores its phase space efficiently for a wide range of parameters, aided by
the long-range correlations in their fast degrees of freedom that emerge
dynamically due to coupling with the (slow) memory degrees of freedom. In this
regime, the time it takes for the system to find a solution scales well as the
number of variables increases. When these hyper-parameters are chosen poorly,
the system navigates its phase space far less efficiently. However, we find
that, in many cases, dynamical long-range order (DLRO) persists even when the
phase-space exploration process is inefficient. DLRO only disappears if the
memories are made to evolve as quickly as the fast degrees of freedom. This
study points to the important role of memory and hyper-parameters in
engineering the DMMs' phase space for optimal computational efficiency.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [221] [Semi-Tensor-Product Based Convolutional Neural Networks](https://arxiv.org/abs/2506.10407)
*Daizhan Cheng*

Main category: eess.SY

TL;DR: 本文提出了一种无填充卷积产品，结合半张量积应用于卷积神经网络，改进了图像和信号识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在进行卷积运算时，常使用填充手段来适应不同尺寸的数据向量，填充可能引入噪声或干扰信息。本研究希望通过引入无填充的卷积产品，克服这一不足。

Method: 本研究提出了一种基于领域的卷积产品（CP），并结合矢量的半张量积（STP），提出新型的卷积产品。随后，基于STP的卷积神经网络（CNN）在图像和三阶信号识别中应用。

Result: 开发了一种基于STP的新型卷积神经网络，成功应用于图像和三阶信号识别。

Conclusion: 通过引入一种无填充的卷积产品方法，可以避免传统方法中的填充噪声，提高卷积神经网络在图像和信号处理中的有效性。

Abstract: The semi-tensor product (STP) of vectors is a generalization of conventional
inner product of vectors, which allows the factor vectors to of different
dimensions. This paper proposes a domain-based convolutional product (CP).
Combining domain-based CP with STP of vectors, a new CP is proposed. Since
there is no zero or any other padding, it can avoid the junk information caused
by padding. Using it, the STP-based convolutional neural network (CNN) is
developed. Its application to image and third order signal identifications is
considered.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [222] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 提出了一种新的方法，通过稳定扩散、GPT-2和混合音频管道创建高质量的60秒电影视频，支持多种分辨率和帧率，实验结果显示其在视觉质量、叙述和效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成性人工智能的进步改变了多媒体创作，使得可以从文本输入中实现自动电影视频合成。

Method: 本文提出的一种方法使用稳定扩散技术进行高保真图像合成，利用GPT-2进行叙述结构化，并通过结合gTTS和YouTube来源音乐的混合音频管道创建60秒的电影影片。系统结构为五个场景框架，并通过线性帧插值、电影后处理（例如锐化）和音视频同步来提供专业质量的结果。

Result: 该方法能够在GPU加速的Google Colab环境中使用Python 3.11实现，支持最高分辨率为1024x768和帧率为15-30 FPS的双模式Gradio界面，并通过CUDA内存管理和错误处理优化确保可靠性。

Conclusion: 实验表明，该方法在视觉质量、叙述连贯性和效率方面表现出色，推动了文本到视频合成在创意、教育和工业应用方面的发展。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [223] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/abs/2506.10085)
*Christos Ziakas,Alessandra Russo*

Main category: cs.CV

TL;DR: 提出了一种新的测试时适应方法，以增强模型在不同时序和环境下的进度估计。


<details>
  <summary>Details</summary>
Motivation: 当前的进度估计模型在测试时无法适应视觉和时间上下文，影响了预测效果。

Method: 提出了一种测试时适应方法，应用梯度基元学习策略以优化自监督目标，使得模型能够在测试时根据视觉轨迹和自然语言任务描述进行调整。

Result: 该方法可以从单一的训练环境推广到不同的分布外任务、环境和实施，并超越了当前最先进的上下文学习方法。

Conclusion: 通过引入自监督和梯度基元学习方法，成功实现了测试时在线适应，提高了进度估计性能。

Abstract: We propose a test-time adaptation method that enables a progress estimation
model to adapt online to the visual and temporal context of test trajectories
by optimizing a learned self-supervised objective. To this end, we introduce a
gradient-based meta-learning strategy to train the model on expert visual
trajectories and their natural language task descriptions, such that test-time
adaptation improves progress estimation relying on semantic content over
temporal order. Our test-time adaptation method generalizes from a single
training environment to diverse out-of-distribution tasks, environments, and
embodiments, outperforming the state-of-the-art in-context learning approach
using autoregressive vision-language models.

</details>


### [224] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/abs/2506.10119)
*Natanael Lucena,Fábio S. da Silva,Ricardo Rios*

Main category: cs.CV

TL;DR: This paper compares CNNs and ViTs for psoriasis image classification, finding ViTs, especially DaViT-B, to be superior with an f1-score of 96.4%.


<details>
  <summary>Details</summary>
Motivation: To identify the most efficient model architecture for the automated disease diagnosis of psoriasis and similar diseases using image classification.

Method: The paper compares the performance of CNNs and ViTs by adapting pre-trained models on ImageNet to a specific dataset for multi-class image classification.

Result: Dual Attention Vision Transformer-Base (DaViT-B) achieved an f1-score of 96.4% and is recommended as the most efficient model for psoriasis detection.

Conclusion: ViTs show superior performance with smaller models compared to CNNs for automated psoriasis detection, with DaViT-B achieving the best results.

Abstract: This paper presents a comparison of the performance of Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying
images containing lesions of psoriasis and diseases similar to it. Models
pre-trained on ImageNet were adapted to a specific data set. Both achieved high
predictive metrics, but the ViTs stood out for their superior performance with
smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the
best results, with an f1-score of 96.4%, and is recommended as the most
efficient architecture for automated psoriasis detection. This article
reinforces the potential of ViTs for medical image classification tasks.

</details>


### [225] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法来提高生成模型的多样性，使用条件熵动态调整多样性评估，在多个模型测试中有效，同时计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 生成的样本需要在广泛语义范围内保持足够的多样性，特别是需要根据语义相似的提示进行多样性评估。

Method: 提出了可扩展的基于提示感知Rény核熵多样性指导（SPARKE）方法，通过条件熵动态调整多样性测量，实现提示感知的多样性控制。

Result: 在多个文本到图像扩散模型上测试证明该方法提高了生成数据的提示感知多样性，同时没有显著增加计算成本。

Conclusion: SPARKE方法可以在保持计算效率的同时显著提高生成样本的多样性。

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image
synthesis and prompt-guided generative modeling. However, ensuring adequate
diversity in generated samples of prompt-guided diffusion models remains a
challenge, particularly when the prompts span a broad semantic spectrum and the
diversity of generated data needs to be evaluated in a prompt-aware fashion
across semantically similar prompts. Recent methods have introduced guidance
via diversity measures to encourage more varied generations. In this work, we
extend the diversity measure-based approaches by proposing the Scalable
Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for
prompt-aware diversity guidance. SPARKE utilizes conditional entropy for
diversity guidance, which dynamically conditions diversity measurement on
similar prompts and enables prompt-aware diversity control. While the
entropy-based guidance approach enhances prompt-aware diversity, its reliance
on the matrix-based entropy scores poses computational challenges in
large-scale generation settings. To address this, we focus on the special case
of Conditional latent RKE Score Guidance, reducing entropy computation and
gradient-based optimization complexity from the $O(n^3)$ of general entropy
measures to $O(n)$. The reduced computational complexity allows for
diversity-guided sampling over potentially thousands of generation rounds on
different prompts. We numerically test the SPARKE method on several
text-to-image diffusion models, demonstrating that the proposed method improves
the prompt-aware diversity of the generated data without incurring significant
computational costs. We release our code on the project page:
https://mjalali.github.io/SPARKE

</details>


### [226] [ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators](https://arxiv.org/abs/2506.10226)
*Parsa Rahimi,Sebastien Marcel*

Main category: cs.CV

TL;DR: 提出了ScoreMix数据增强策略，通过混合不同类条件轨迹的分数，提高了辨别模型的性能，尤其是在标记数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 旨在提高辨别模型的性能，特别是在标记数据有限的情况下。

Method: 提出了一种新颖且简单的数据增强策略ScoreMix，通过在扩散采样期间对不同类别条件轨迹进行凸性混合，以生成具有挑战性的合成样本。

Result: 显著提高了所有研究基准中的辨别能力。

Conclusion: 结论指出，在没有广泛参数搜索的情况下实现了显著性能提升，为训练辨别模型提供了实际优势，同时有效减轻了大型数据集收集方面的问题。

Abstract: In this paper, we propose ScoreMix, a novel yet simple data augmentation
strategy leveraging the score compositional properties of diffusion models to
enhance discriminator performance, particularly under scenarios with limited
labeled data. By convexly mixing the scores from different class-conditioned
trajectories during diffusion sampling, we generate challenging synthetic
samples that significantly improve discriminative capabilities in all studied
benchmarks. We systematically investigate class-selection strategies for mixing
and discover that greater performance gains arise when combining classes
distant in the discriminator's embedding space, rather than close in the
generator's condition space. Moreover, we empirically show that, under standard
metrics, the correlation between the generator's learned condition space and
the discriminator's embedding space is minimal. Our approach achieves notable
performance improvements without extensive parameter searches, demonstrating
practical advantages for training discriminative models while effectively
mitigating problems regarding collections of large datasets. Paper website:
https://parsa-ra.github.io/scoremix

</details>


### [227] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

TL;DR: CIDer框架通过使用MSSD和MACI模块有效解决了多模态情绪识别中的模态缺失和分布外数据挑战，并在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情绪识别方法在同时处理模态缺失和分布外数据方面存在挑战，经常依赖特定模型或引入过多参数，限制了实用性。本研究旨在解决这些问题。

Method: 该研究提出了一种名为Causal Inference Distiller (CIDer)的新框架，通过引入Model-Specific Self-Distillation (MSSD)模块和Model-Agnostic Causal Inference (MACI)模块来增强多模态情绪识别的鲁棒性。MSSD模块使用共享权重的自蒸馏方法提高鲁棒性，而MACI模块通过定制的因果图减少标签和语言偏见。引入了新的任务“随机模态特征缺失”（RMFM）和新重分配的MER OOD数据集。

Result: CIDer在处理RMFM和OOD场景时表现出优秀的鲁棒性，实验结果显示，它使用更少的参数和更快的训练速度达到了优于最新技术的方法。

Conclusion: CIDer提供了一种有效的解决方案来应对多模态情绪识别中的模态缺失和分布外数据挑战，并证明了其在减少参数和提高训练速度方面的优越性。

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [228] [Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation](https://arxiv.org/abs/2506.10302)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.CV

TL;DR: 本研究评估了基于深度学习的皮肤损伤分类方法，结合不确定性量化，发现特定组合的模型在准确性和不确定性处理上表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在自动化皮肤癌分类中虽然显示出潜力，但其性能往往因数据稀缺和缺乏不确定性意识而受到限制。为了改善这一现状，研究者动机是评估基于深度学习的皮肤损伤分类，并结合不确定性量化以提高模型的准确性和可信度。

Method: 研究使用了转移学习和不确定性量化（UQ）的方法，涉及使用多个预训练特征提取器和传统分类器的组合。此外，集成了不确定性量化技术，如蒙特卡洛Dropout（MCD）、集成和集成蒙特卡洛Dropout（EMCD），以评估预测的准确性和模型输出的可靠性。

Result: 研究发现，以LAION CLIP ViT-H/14和SVM组合的CLIP视觉变换器表现出最高的分类性能，而集成方法在处理不确定性的同时提供了良好的准确性。此外，研究还发现EMCD方法对不确定性尤其敏感。

Conclusion: 本研究表明，集成方法在准确性和不确定性处理方面提供了良好的权衡，EMCD对不确定性预测更敏感。

Abstract: Accurate and reliable skin cancer diagnosis is critical for early treatment
and improved patient outcomes. Deep learning (DL) models have shown promise in
automating skin cancer classification, but their performance can be limited by
data scarcity and a lack of uncertainty awareness. In this study, we present a
comprehensive evaluation of DL-based skin lesion classification using transfer
learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the
first phase, we benchmarked several pre-trained feature extractors-including
Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50
(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual
Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range
of traditional classifiers such as Support Vector Machine (SVM), eXtreme
Gradient Boosting (XGBoost), and logistic regression. Our results show that
CLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,
deliver the highest classification performance. In the second phase, we
incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte
Carlo Dropout (EMCD) to assess not only prediction accuracy but also the
reliability of model outputs. We evaluated these models using uncertainty-aware
metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),
uncertainty specificity(USpe), and uncertainty precision(UPre). The results
demonstrate that ensemble methods offer a good trade-off between accuracy and
uncertainty handling, while EMCD is more sensitive to uncertain predictions.
This study highlights the importance of integrating UQ into DL-based medical
diagnosis to enhance both performance and trustworthiness in real-world
clinical applications.

</details>


### [229] [Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework](https://arxiv.org/abs/2506.10328)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: 研究提出了一个弱监督多模态框架，减少对手工注释的依赖，通过分析病灶图像和稀疏文本生成SOAP笔记，可与先进模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 目前，手工撰写SOAP笔记过于耗时，并且是导致临床医生职业倦怠的重要因素。

Method: 提出一种弱监督的多模态框架，利用病灶图像和稀疏临床文本生成SOAP笔记，减少对手工注释的依赖。

Result: 方法在关键临床相关性指标上达到与GPT-4o、Claude和DeepSeek Janus Pro相当的表现。并提出了两个新的评估指标：MedConceptEval和临床连贯性评分（CCS）。

Conclusion: 该方法减少了手工注释，为临床文档化提供了一种可扩展的解决方案，减轻了医生负担。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. In clinical settings,
physicians document patient visits using detailed SOAP (Subjective, Objective,
Assessment, and Plan) notes. However, manually generating these notes is
labor-intensive and contributes to clinician burnout. In this work, we propose
a weakly supervised multimodal framework to generate clinically structured SOAP
notes from limited inputs, including lesion images and sparse clinical text.
Our approach reduces reliance on manual annotations, enabling scalable,
clinically grounded documentation while alleviating clinician burden and
reducing the need for large annotated data. Our method achieves performance
comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical
relevance metrics. To evaluate clinical quality, we introduce two novel metrics
MedConceptEval and Clinical Coherence Score (CCS) which assess semantic
alignment with expert medical concepts and input features, respectively.

</details>


### [230] [Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions](https://arxiv.org/abs/2506.10334)
*Deliang Wang,Chao Yang,Gaowei Chen*

Main category: cs.CV

TL;DR: 研究使用VLMs分析学生面部表情的效果，Qwen2.5-VL-7B-Instruct表现优于Llama-3.2-11B，尤其在识别快乐和困惑表情上。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在无需微调的情况下，能否通过学生在在线学习环境中的面部表情来分析其学术情感。

Method: 采用VLMs模型Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct，通过零样本提示分析学生的面部表情。

Result: 初步结果显示，Qwen2.5-VL-7B-Instruct在学术面部表情识别中优于Llama-3.2-11B-Vision-Instruct，尤其是在识别快乐情绪方面表现良好，但在检测分心行为方面失败。

Conclusion: Qwen2.5-VL-7B-Instruct在识别学生困惑表情方面表现出较高的效果，有潜力用于识别导致学生困惑的内容。

Abstract: Students' academic emotions significantly influence their social behavior and
learning performance. Traditional approaches to automatically and accurately
analyze these emotions have predominantly relied on supervised machine learning
algorithms. However, these models often struggle to generalize across different
contexts, necessitating repeated cycles of data collection, annotation, and
training. The emergence of Vision-Language Models (VLMs) offers a promising
alternative, enabling generalization across visual recognition tasks through
zero-shot prompting without requiring fine-tuning. This study investigates the
potential of VLMs to analyze students' academic emotions via facial expressions
in an online learning environment. We employed two VLMs,
Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000
images depicting confused, distracted, happy, neutral, and tired expressions
using zero-shot prompting. Preliminary results indicate that both models
demonstrate moderate performance in academic facial expression recognition,
with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.
Notably, both models excel in identifying students' happy emotions but fail to
detect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits
relatively high performance in recognizing students' confused expressions,
highlighting its potential for practical applications in identifying content
that causes student confusion.

</details>


### [231] [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821)
*Huaying Yuan,Zheng Liu,Junjie Zhou,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CV

TL;DR: 提出了一种名为VideoDeepResearch的新框架，通过文本推理模型和多模态工具包提升长视频理解，超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 长视频理解挑战了当前多模态大语言模型（MLLM）的能力，通常认为解决此类任务需要具有扩展上下文窗口的基础MLLM，强大的视觉感知能力和卓越的领域专长。

Method: 我们引入了VideoDeepResearch，这是一种针对长视频理解的新型代理框架。该方法完全依赖于一个大规模的文本推理模型（LRM），结合模块化的多模态工具包，包括多模态检索器和视觉感知器。

Result: 实验表明，VideoDeepResearch分别在MLVU（测试）、LVBench和LongVideoBench上比之前的最先进技术提高了9.6%、6.6%和3.9%。

Conclusion: 我们的结果表明，VideoDeepResearch在LVU任务上实现了显著的改进，超过现有的MLLM基线。

Abstract: Long video understanding (LVU) presents a significant challenge for current
multi-modal large language models (MLLMs) due to the task's inherent complexity
and context window constraint. It is widely assumed that addressing LVU tasks
requires foundation MLLMs with extended context windows, strong visual
perception capabilities, and proficient domain expertise. In this work, we
challenge this common belief by introducing VideoDeepResearch, a novel agentic
framework for long video understanding. Our approach relies solely on a
text-only large reasoning model (LRM) combined with a modular multi-modal
toolkit, including multimodal retrievers and visual perceivers, all of which
are readily available in practice. For each LVU task, the system formulates a
problem-solving strategy through reasoning, while selectively accessing and
utilizing essential video content via tool using. We conduct extensive
experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.
Our results demonstrate that VideoDeepResearch achieves substantial
improvements over existing MLLM baselines, surpassing the previous
state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and
LongVideoBench, respectively. These findings highlight the promise of agentic
systems in overcoming key challenges in LVU problems.

</details>


### [232] [UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models](https://arxiv.org/abs/2506.10342)
*Jun Yin,Jing Zhong,Peilin Li,Pengyu Zeng,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CV

TL;DR: 研究提出了基于视觉语言模型的框架用于量化城市街景风格，结果表明该方法在识别风格差异时具有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的城市文化研究方法难以在不同背景下标准化，因此提出通过客观和数据驱动的方式研究城市街景风格差异。

Method: 该研究提出了一种基于视觉语言模型的多模态研究框架，建立了UrbanDiffBench数据集和UrbanSense分析框架，进行城市街景风格的量化生成与比较。

Result: 实验结果显示，超过80%的生成描述通过了t检验（p小于0.05），主观评价中的Phi分数显示该方法能够捕捉细微的风格差异（城市为0.912，年代为0.833）。

Conclusion: 该研究指出，其方法在量化和解读城市风格演变方面具有潜力，为未来设计提供了科学依据。

Abstract: Urban cultures and architectural styles vary significantly across cities due
to geographical, chronological, historical, and socio-political factors.
Understanding these differences is essential for anticipating how cities may
evolve in the future. As representative cases of historical continuity and
modern innovation in China, Beijing and Shenzhen offer valuable perspectives
for exploring the transformation of urban streetscapes. However, conventional
approaches to urban cultural studies often rely on expert interpretation and
historical documentation, which are difficult to standardize across different
contexts. To address this, we propose a multimodal research framework based on
vision-language models, enabling automated and scalable analysis of urban
streetscape style differences. This approach enhances the objectivity and
data-driven nature of urban form research. The contributions of this study are
as follows: First, we construct UrbanDiffBench, a curated dataset of urban
streetscapes containing architectural images from different periods and
regions. Second, we develop UrbanSense, the first vision-language-model-based
framework for urban streetscape analysis, enabling the quantitative generation
and comparison of urban style representations. Third, experimental results show
that Over 80% of generated descriptions pass the t-test (p less than 0.05).
High Phi scores (0.912 for cities, 0.833 for periods) from subjective
evaluations confirm the method's ability to capture subtle stylistic
differences. These results highlight the method's potential to quantify and
interpret urban style evolution, offering a scientifically grounded lens for
future design.

</details>


### [233] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 该研究通过视频数据训练图像编辑模型，取得了优异的多轮编辑效果，并在多概念组合等方面展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以直接从视频中学习到上下文图像编辑模型，同时引入了一种可扩展的方法将视频标注为交错的多模态序列。

Method: 本研究设计了一种块因果扩散变压器，并在三个代理任务上进行训练：下一图像预测、当前分割预测以及下一分割预测。

Result: 广泛的实验表明，模型在上下文图像编辑中表现出强大的能力，并在两个多轮图像编辑基准上取得了最新的研究成果。

Conclusion: 尽管只在视频上进行训练，我们的模型在多概念组合、故事生成和编辑链应用方面表现出良好的能力。

Abstract: In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.

</details>


### [234] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395)
*Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie*

Main category: cs.CV

TL;DR: Pisces, a new foundation model, addresses the challenges in unified models by using a novel visual encoding architecture, achieving strong performance in image understanding and generation across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Unified models often underperform due to the significant differences between visual features necessary for image understanding versus generation, and the distinct training processes required for each.

Method: An auto-regressive multimodal foundation model is used with a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Data curation, pretraining, and finetuning are also key components.

Result: Pisces performs strongly on over 20 benchmarks for image understanding and demonstrates robust generative capabilities on GenEval.

Conclusion: Pisces shows competitive performance in both image understanding and image generation, advancing the field of unified multimodal models.

Abstract: Recent advances in large language models (LLMs) have enabled multimodal
foundation models to tackle both image understanding and generation within a
unified framework. Despite these gains, unified models often underperform
compared to specialized models in either task. A key challenge in developing
unified models lies in the inherent differences between the visual features
needed for image understanding versus generation, as well as the distinct
training processes required for each modality. In this work, we introduce
Pisces, an auto-regressive multimodal foundation model that addresses this
challenge through a novel decoupled visual encoding architecture and tailored
training techniques optimized for multimodal generation. Combined with
meticulous data curation, pretraining, and finetuning, Pisces achieves
competitive performance in both image understanding and image generation. We
evaluate Pisces on over 20 public benchmarks for image understanding, where it
demonstrates strong performance across a wide range of tasks. Additionally, on
GenEval, a widely adopted benchmark for image generation, Pisces exhibits
robust generative capabilities. Our extensive analysis reveals the synergistic
relationship between image understanding and generation, and the benefits of
using separate visual encoders, advancing the field of unified multimodal
models.

</details>


### [235] [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
*Yuxuan Luo,Yuhui Yuan,Junwen Chen,Haonan Cai,Ziyi Yue,Yuwei Yang,Fatima Zohra Daha,Ji Li,Zhouhui Lian*

Main category: cs.CV

TL;DR: 本文介绍知识图像生成任务及MMMG基准，用于评估图像生成模型的推理能力，揭示了当前模型在事实保真和视觉清晰度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 知识图像对人类文明和学习机制至关重要，生成此类图像需要将世界知识与像素级信息融合，以实现清晰的解释性视觉效果。

Method: 采用统一的知识图谱表示法，结合MMMG-Score评价生成的知识图像，从事实保真和视觉清晰度两个维度进行评估。

Result: 16种生成模型在MMMG基准测试中的表现不佳，最好的GPT-4o仅得到50.20分，强调了MMMG基准的难度。

Conclusion: 本文提出了知识图像生成任务及其评估基准MMMG。当前模型在推理能力上表现不足，揭示了这一任务的挑战性。

Abstract: In this paper, we introduce knowledge image generation as a new task,
alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation
Benchmark (MMMG) to probe the reasoning capability of image generation models.
Knowledge images have been central to human civilization and to the mechanisms
of human learning--a fact underscored by dual-coding theory and the
picture-superiority effect. Generating such images is challenging, demanding
multimodal reasoning that fuses world knowledge with pixel-level grounding into
clear explanatory visuals. To enable comprehensive evaluation, MMMG offers
4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,
6 educational levels, and diverse knowledge formats such as charts, diagrams,
and mind maps. To eliminate confounding complexity during evaluation, we adopt
a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a
target image's core entities and their dependencies. We further introduce
MMMG-Score to evaluate generated knowledge images. This metric combines factual
fidelity, measured by graph-edit distance between KGs, with visual clarity
assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image
generation models expose serious reasoning deficits--low entity fidelity, weak
relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,
underscoring the benchmark's difficulty. To spur further progress, we release
FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines
a reasoning LLM with diffusion models and is trained on 16,000 curated
knowledge image-prompt pairs.

</details>


### [236] [Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization](https://arxiv.org/abs/2506.10463)
*Stone Yun,Alexander Wong*

Main category: cs.CV

TL;DR: 探讨DNN权重初始化对量化鲁棒性的影响，提出GHN-QAT方法，显著提升低比特量化精度。


<details>
  <summary>Details</summary>
Motivation: 当前关于DNN量化的研究主要集中在提升模型推理的精度和鲁棒性方面，但对DNN训练初始条件的改进研究较少。权重初始化对浮点模型的测试精度有显著影响，因此不同的权重初始化方法可能会影响训练后量化模型的鲁棒性。

Method: 分析不同权重初始化方式对各种卷积神经网络（CNN）构建模块的量化鲁棒性的影响，并提出一种利用图超网络（GHN）预测量化DNNs参数的新方法，称之为GHN-QAT。

Result: 实验显示，即使在不同的CNN架构中，选择不同的随机权重初始化器也会显著影响最终的量化鲁棒性。GHN-QAT方法在4-bit量化中展现出显著的精度提升，在2-bit量化中表现出优于随机的精度。

Conclusion: 本研究首次深入探讨了面向量化的深度神经网络（DNN）权重初始化，并提出了一种新方法GHN-QAT，通过图超网络预测量化DNN的参数，展示了在权重初始化阶段对量化鲁棒性的重要影响。

Abstract: Deep neural network (DNN) quantization for fast, efficient inference has been
an important tool in limiting the cost of machine learning (ML) model
inference. Quantization-specific model development techniques such as
regularization, quantization-aware training, and quantization-robustness
penalties have served to greatly boost the accuracy and robustness of modern
DNNs. However, very little exploration has been done on improving the initial
conditions of DNN training for quantization. Just as random weight
initialization has been shown to significantly impact test accuracy of floating
point models, it would make sense that different weight initialization methods
impact quantization robustness of trained models. We present an extensive study
examining the effects of different weight initializations on a variety of CNN
building blocks commonly used in efficient CNNs. This analysis reveals that
even with varying CNN architectures, the choice of random weight initializer
can significantly affect final quantization robustness. Next, we explore a new
method for quantization-robust CNN initialization -- using Graph Hypernetworks
(GHN) to predict parameters of quantized DNNs. Besides showing that
GHN-predicted parameters are quantization-robust after regular float32
pretraining (of the GHN), we find that finetuning GHNs to predict parameters
for quantized graphs (which we call GHN-QAT) can further improve quantized
accuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for
even 4-bit quantization and better-than-random accuracy for 2-bits. To the best
of our knowledge, this is the first in-depth study on quantization-aware DNN
weight initialization. GHN-QAT offers a novel approach to quantized DNN model
design. Future investigations, such as using GHN-QAT-initialized parameters for
quantization-aware training, can further streamline the DNN quantization
process.

</details>


### [237] [Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation](https://arxiv.org/abs/2506.10503)
*Shuyang Li,Shuang Wang,Zhuangzhuang Sun,Jing Xiao*

Main category: cs.CV

TL;DR: PSLG-SAM方法通过将RRSIS任务分解为粗定位和精细分割两个阶段，显著提升了分割性能，并减少了标注数据的需求。实验验证显示其超越了现有的最先进模型，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 当前的RRSIS方法面临密集标注需求和复杂场景解释的挑战。

Method: 提出了PSLG-SAM框架，将RRSIS任务分解为粗定位和精细分割两个阶段。同时贡献了一个高质量的、多类别的手动标注数据集。

Result: 在两个数据集上的实验验证表明，PSLG-SAM方法取得了显著的性能提升。

Conclusion: PSLG-SAM方法实现了显著的性能改进，超越了现有的最先进模型。代码将公开。

Abstract: The Reference Remote Sensing Image Segmentation (RRSIS) task generates
segmentation masks for specified objects in images based on textual
descriptions, which has attracted widespread attention and research interest.
Current RRSIS methods rely on multi-modal fusion backbones and semantic
segmentation heads but face challenges like dense annotation requirements and
complex scene interpretation. To address these issues, we propose a framework
named \textit{prompt-generated semantic localization guiding Segment Anything
Model}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse
localization and fine segmentation. In coarse localization stage, a visual
grounding network roughly locates the text-described object. In fine
segmentation stage, the coordinates from the first stage guide the Segment
Anything Model (SAM), enhanced by a clustering-based foreground point generator
and a mask boundary iterative optimization strategy for precise segmentation.
Notably, the second stage can be train-free, significantly reducing the
annotation data burden for the RRSIS task. Additionally, decomposing the RRSIS
task into two stages allows for focusing on specific region segmentation,
avoiding interference from complex scenes.We further contribute a high-quality,
multi-category manually annotated dataset. Experimental validation on two
datasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant
performance improvements and surpasses existing state-of-the-art models.Our
code will be made publicly available.

</details>


### [238] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/abs/2506.10128)
*Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang*

Main category: cs.CV

TL;DR: 提出了一种用于视觉-语言模型的RL代理任务ViCrit，通过识别图像字幕中的微妙错误来改善模型的视觉感知能力，并在多个基准上取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 目前在视觉-语言模型中缺乏具有挑战性且可验证的视觉任务，阻碍了强化学习在此领域的成功应用。

Method: 引入了一个名为ViCrit（视觉字幕幻觉批评器）的RL代理任务，通过在段落中注入微妙的视觉幻觉来训练VLM，要求模型指出给定图像和修改后的字幕中错误的部分。

Result: 通过ViCrit任务训练的模型在各种视觉-语言基准上取得了显著提升，且这种提升不仅限于自然图像训练数据，还扩展到抽象图像推理和视觉数学。

Conclusion: 基于细粒度幻觉批判的训练方法是有效且可推广的，能够增强视觉-语言模型（VLMs）的视觉感知能力。

Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning
large language models (LLMs) using tasks that are challenging yet easily
verifiable, such as math reasoning or code generation. However, extending this
success to visual perception in vision-language models (VLMs) has been impeded
by the scarcity of vision-centric tasks that are simultaneously challenging and
unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption
Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,
synthetic visual hallucination injected into paragraphs of human-written image
captions. Starting from a 200-word captions, we inject a single, subtle visual
description error-altering a few words on objects, attributes, counts, or
spatial relations-and task the model to pinpoint the corrupted span given the
image and the modified caption. This formulation preserves the full perceptual
difficulty while providing a binary, exact-match reward that is easy to compute
and unambiguous. Models trained with the ViCrit Task exhibit substantial gains
across a variety of VL benchmarks. Crucially, the improvements transfer beyond
natural-image training data to abstract image reasoning and visual math,
showing promises of learning to perceive rather than barely memorizing seen
objects. To facilitate evaluation, we further introduce ViCrit-Bench, a
category-balanced diagnostic benchmark that systematically probes perception
errors across diverse image domains and error types. Together, our results
demonstrate that fine-grained hallucination criticism is an effective and
generalizable objective for enhancing visual perception in VLMs.

</details>


### [239] [CogStream: Context-guided Streaming Video Question Answering](https://arxiv.org/abs/2506.10516)
*Zicheng Zhao,Kangyu Wang,Shijie Li,Rui Qian,Weiyao Lin,Huabin Liu*

Main category: cs.CV

TL;DR: 论文提出CogStream任务和基线模型CogReasoner，有效解决视频流推理中上下文选择的计算负担和干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在多模态理解上取得进展，但在流式视频推理中仍面临挑战，因为它依赖于上下文信息。然而所有可用的历史上下文信息输入到模型中会造成计算负担，同时不相关信息会分散模型注意力。

Method: 研究中引入了一种新的基线模型CogReasoner，该模型通过视觉流压缩和历史对话检索来有效处理任务。

Result: 实验结果表明，CogReasoner在处理CogStream任务上表现出色。

Conclusion: 该研究提出了一种名为CogStream的新任务，解决了现有视频大语言模型处理视频流推理时存在的计算负担和不相关上下文干扰问题。

Abstract: Despite advancements in Video Large Language Models (Vid-LLMs) improving
multimodal understanding, challenges persist in streaming video reasoning due
to its reliance on contextual information. Existing paradigms feed all
available historical contextual information into Vid-LLMs, resulting in a
significant computational burden for visual data processing. Furthermore, the
inclusion of irrelevant context distracts models from key details. This paper
introduces a challenging task called Context-guided Streaming Video Reasoning
(CogStream), which simulates real-world streaming video scenarios, requiring
models to identify the most relevant historical contextual information to
deduce answers for questions about the current stream. To support CogStream, we
present a densely annotated dataset featuring extensive and hierarchical
question-answer pairs, generated by a semi-automatic pipeline. Additionally, we
present CogReasoner as a baseline model. It efficiently tackles this task by
leveraging visual stream compression and historical dialogue retrieval.
Extensive experiments prove the effectiveness of this method. Code will be
released soon.

</details>


### [240] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/abs/2506.10559)
*Yutong Zhou,Masahiro Ryo*

Main category: cs.CV

TL;DR: 提出了一种视觉到因果的框架，通过物种图像生成其栖息地偏好的因果解释。这一系统在蜜蜂和花卉物种上的早期结果展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的生态工作流程支离破碎且对非专业人士难以接触，亟需一种能够解释物种为什么会在特定地点生存的方法，以促进生态系统的理解和生物多样性保护。

Method: 提出一种从视觉到因果的端到端框架，通过物种图像转化为关于其栖息地偏好的可解释因果洞察。该系统集成物种识别、全球发生检索、假缺失采样和气候数据提取等步骤，并使用现代因果推断方法分析环境特征间的因果关系及其对物种发生的影响。最终，利用结构化模板和大型语言模型生成统计扎实、人类可读的因果解释。

Result: 展示了蜜蜂和花卉物种上的早期结果，证明了多模态AI助手的潜力，并结合推荐的生态建模实践，以人类可理解的语言描述物种栖息地。

Conclusion: 该框架能够以人类可理解的语言生成物种栖息地的因果解释，具有促进生态系统理解和生物多样性保护的潜力。

Abstract: Explaining why the species lives at a particular location is important for
understanding ecological systems and conserving biodiversity. However, existing
ecological workflows are fragmented and often inaccessible to non-specialists.
We propose an end-to-end visual-to-causal framework that transforms a species
image into interpretable causal insights about its habitat preference. The
system integrates species recognition, global occurrence retrieval,
pseudo-absence sampling, and climate data extraction. We then discover causal
structures among environmental features and estimate their influence on species
occurrence using modern causal inference methods. Finally, we generate
statistically grounded, human-readable causal explanations from structured
templates and large language models. We demonstrate the framework on a bee and
a flower species and report early results as part of an ongoing project,
showing the potential of the multimodal AI assistant backed up by a recommended
ecological modeling practice for describing species habitat in
human-understandable language.

</details>


### [241] [Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics](https://arxiv.org/abs/2506.10564)
*Imanol Solano,Julian Fierrez,Aythami Morales,Alejandro Peña,Ruben Tolosana,Francisco Zamora-Martinez,Javier San Agustin*

Main category: cs.CV

TL;DR: 提出了综合公平指数（CEI），一种有效检测人脸识别系统中的评分分布尾部偏差的新指标，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在人脸识别系统中，现有指标难以有效检测到评分分布尾部的精细差异，导致人口统计偏差的问题出现。此偏差尤以高性能人脸识别（FR）系统中表现突出。

Method: 引入了一种新的指标-综合公平指数（CEI），它通过分别分析真实和伪造评分分布，并考虑整体分布形状，能更好地关注尾部概率。同时，我们提出了自动化版本的CEI^A，以提高客观性并简化实践应用。

Result: 实验证明，CEI具有在检测偏差方面超过现有方法的能力，尤其在细微的评分分布尾部偏差检测上表现突出。

Conclusion: CEI作为一种工具，可以敏感且可靠地对运行中的人脸识别系统进行公平性评估，特别是在评分分布尾部偏差的检测上表现优越。该方法不仅适用于人脸生物识别的偏差评估，而且可推广应用于任何需比较统计分布的问题。

Abstract: Demographic bias in high-performance face recognition (FR) systems often
eludes detection by existing metrics, especially with respect to subtle
disparities in the tails of the score distribution. We introduce the
Comprehensive Equity Index (CEI), a novel metric designed to address this
limitation. CEI uniquely analyzes genuine and impostor score distributions
separately, enabling a configurable focus on tail probabilities while also
considering overall distribution shapes. Our extensive experiments (evaluating
state-of-the-art FR systems, intentionally biased models, and diverse datasets)
confirm CEI's superior ability to detect nuanced biases where previous methods
fall short. Furthermore, we present CEI^A, an automated version of the metric
that enhances objectivity and simplifies practical application. CEI provides a
robust and sensitive tool for operational FR fairness assessment. The proposed
methods have been developed particularly for bias evaluation in face biometrics
but, in general, they are applicable for comparing statistical distributions in
any problem where one is interested in analyzing the distribution tails.

</details>


### [242] [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](https://arxiv.org/abs/2506.10568)
*Lizhen Wang,Zhurong Xia,Tianshu Hu,Pengrui Wang,Pengfei Wang,Zerong Zheng,Ming Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于Diffusion Transformer的框架，能够生成高保真的人-产品演示视频，同时保持人的身份和产品的细节，并且在生成真实演示动作方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在电子商务和数字营销中，生成高保真的人-产品演示视频对于有效的产品展示至关重要。然而，现有的大多数框架要么无法保持人和产品的身份，要么缺乏对人-产品空间关系的理解，导致不现实的表现和不自然的交互。

Method: 提出了一种基于Diffusion Transformer (DiT) 的框架，通过注入配对的人-产品参考信息和利用附加的遮掩交叉注意力机制，同时保留人的身份和产品的细节。使用3D人体网格模板和产品边界框来提供精确运动指导，并通过结构化文本编码融入类别级语义，以增强小角度旋转间的3D一致性。

Result: 训练在一个混合数据集上，并采用广泛的数据增强策略，本文方法在保持人和产品的身份完整性以及生成逼真的演示动作方面优于现有技术。

Conclusion: 本文提出的方法能够有效解决现有框架在人和产品身份保持以及人产品空间关系理解方面的不足，通过一种基于Diffusion Transformer的框架来生成逼真的人-产品演示视频。

Abstract: In e-commerce and digital marketing, generating high-fidelity human-product
demonstration videos is important for effective product presentation. However,
most existing frameworks either fail to preserve the identities of both humans
and products or lack an understanding of human-product spatial relationships,
leading to unrealistic representations and unnatural interactions. To address
these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our
method simultaneously preserves human identities and product-specific details,
such as logos and textures, by injecting paired human-product reference
information and utilizing an additional masked cross-attention mechanism. We
employ a 3D body mesh template and product bounding boxes to provide precise
motion guidance, enabling intuitive alignment of hand gestures with product
placements. Additionally, structured text encoding is used to incorporate
category-level semantics, enhancing 3D consistency during small rotational
changes across frames. Trained on a hybrid dataset with extensive data
augmentation strategies, our approach outperforms state-of-the-art techniques
in maintaining the identity integrity of both humans and products and
generating realistic demonstration motions. Project page:
https://submit2025-dream.github.io/DreamActor-H1/.

</details>


### [243] [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://arxiv.org/abs/2506.10612)
*Suin Lee,Dae-Shik Kim*

Main category: cs.CV

TL;DR: TexTailor提升了从文本生成一致物体纹理的效果，通过改进的扩散模型和自适应视角选择，达到更高的纹理一致性，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到纹理合成方法在多视点上逐渐生成纹理，但因纹理性质逐渐改变导致一致性问题，并且预定义摄像位置忽略了物体的几何形状，限制了纹理信息的有效使用。TexTailor旨在解决这些问题。

Method: TexTailor通过在扩散过程中多次集成先前合成的纹理信息，并在这些重采样的纹理上微调深度感知扩散模型来解决问题。另外，通过根据物体的几何形状自适应调整摄像头位置来改善纹理的合成。

Result: 在Objaverse数据集和ShapeNet汽车数据集上的实验表明TexTailor在合成视图一致的纹理方面优于现有的最先进方法。

Conclusion: TexTailor在生成视图一致的纹理方面优于现有的方法。

Abstract: We present TexTailor, a novel method for generating consistent object
textures from textual descriptions. Existing text-to-texture synthesis
approaches utilize depth-aware diffusion models to progressively generate
images and synthesize textures across predefined multiple viewpoints. However,
these approaches lead to a gradual shift in texture properties across
viewpoints due to (1) insufficient integration of previously synthesized
textures at each viewpoint during the diffusion process and (2) the
autoregressive nature of the texture synthesis process. Moreover, the
predefined selection of camera positions, which does not account for the
object's geometry, limits the effective use of texture information synthesized
from different viewpoints, ultimately degrading overall texture consistency. In
TexTailor, we address these issues by (1) applying a resampling scheme that
repeatedly integrates information from previously synthesized textures within
the diffusion process, and (2) fine-tuning a depth-aware diffusion model on
these resampled textures. During this process, we observed that using only a
few training images restricts the model's original ability to generate
high-fidelity images aligned with the conditioning, and therefore propose an
performance preservation loss to mitigate this issue. Additionally, we improve
the synthesis of view-consistent textures by adaptively adjusting camera
positions based on the object's geometry. Experiments on a subset of the
Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor
outperforms state-of-the-art methods in synthesizing view-consistent textures.
The source code for TexTailor is available at
https://github.com/Adios42/Textailor

</details>


### [244] [Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models](https://arxiv.org/abs/2506.10634)
*Francisco Caetano,Christiaan Viviers,Peter H. N. De With,Fons van der Sommen*

Main category: cs.CV

TL;DR: Symmetrical Flow Matching (SymmFlow) 是一种新方法，通过对称学习目标实现语义分割、分类和图像生成，同时保持生成多样性和双向一致性。实验结果表明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 流匹配是一种有效的框架，用于学习分布之间的连续变换，从而实现高保真度生成建模。

Method: 该研究提出了对称流匹配（SymmFlow），通过使用对称学习目标来建模正向和反向变换，确保双向一致性，同时保持足够的熵以支持生成多样性。引入了一种新的训练目标，以显式保留流动中的语义信息，实现高效采样，而无需迭代细化，即可实现单步分割和分类。

Result: 实验结果表明，SymmFlow在语义图像合成上实现了最先进的性能，在CelebAMask-HQ上获得了11.9的FID分数，在COCO-Stuff上获得了7.0的FID分数，仅需25次推理步骤。此外，它在语义分割上提供了有竞争力的结果，并在分类任务中表现出有希望的能力。

Conclusion: SymmFlow是一种新的统一模型，能够同时实现语义分割、分类和图像生成。它实现了双向一致性和生成多样性，并支持灵活的条件通过使用对称的学习目标。

Abstract: Flow Matching has emerged as a powerful framework for learning continuous
transformations between distributions, enabling high-fidelity generative
modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new
formulation that unifies semantic segmentation, classification, and image
generation within a single model. Using a symmetric learning objective,
SymmFlow models forward and reverse transformations jointly, ensuring
bi-directional consistency, while preserving sufficient entropy for generative
diversity. A new training objective is introduced to explicitly retain semantic
information across flows, featuring efficient sampling while preserving
semantic structure, allowing for one-step segmentation and classification
without iterative refinement. Unlike previous approaches that impose strict
one-to-one mapping between masks and images, SymmFlow generalizes to flexible
conditioning, supporting both pixel-level and image-level class labels.
Experimental results on various benchmarks demonstrate that SymmFlow achieves
state-of-the-art performance on semantic image synthesis, obtaining FID scores
of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.
Additionally, it delivers competitive results on semantic segmentation and
shows promising capabilities in classification tasks. The code will be publicly
available.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [245] [The pursuit of happiness](https://arxiv.org/abs/2506.10537)
*Debora Princepe,Onofrio Mazzarisi,Erol Akcay,Simon A. Levin,Matteo Marsili*

Main category: econ.TH

TL;DR: 研究幸福与个人行为的关系，发现亲社会行为可以促进群体合作。


<details>
  <summary>Details</summary>
Motivation: 探讨幸福概念对个体行为和群体动态的影响。

Method: 使用博弈理论模型分析个体之间的互动中的幸福反馈循环。

Result: 研究发现个体更可能关心那些幸福感较强的人，并在多人公共物品博弈中，一个亲社会行为的个体可以将整个群体引向合作。

Conclusion: 个体通常更关心那些比通过自私行为更快乐的人，而在简单的博弈理论设定中，个体可以达到超出纳什均衡的多种均衡状态。在多人公共物品博弈中，非合作的纳什均衡处于边缘不稳定状态，一个表现出亲社会行为的个体能将几乎整个群体驱动到合作状态。

Abstract: Happiness, in the U.S. Declaration of Independence, was understood quite
differently from today's popular notions of personal pleasure. Happiness
implies a flourishing life - one of virtue, purpose, and contribution to the
common good. This paper studies populations of individuals - that we call
homo-felix - who maximise an objective function that we call happiness. The
happiness of one individual depends on the payoffs that they receive in games
they play with their peers as well as on the happiness of the peers they
interact with. Individuals care more or less about others depending on whether
that makes them more or less happy. This paper analyses the happiness feedback
loops that result from these interactions in simple settings. We find that
individuals tend to care more about individuals who are happier than what they
would be by being selfish. In simple 2 x 2 game theoretic settings, we show
that homo-felix can converge to a variety of equilibria which includes but goes
beyond Nash equilibria. In an n-persons public good game we show that the
non-cooperative Nash equilibrium is marginally unstable and a single individual
who develops prosocial behaviour is able to drive almost the whole population
to a cooperative state.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [246] [Design of A* based heuristic algorithm for efficient interdiction in multi-Layer networks](https://arxiv.org/abs/2506.10017)
*Sukanya Samanta*

Main category: cs.SI

TL;DR: 本文针对动态犯罪环境，提出一种基于分层图表示和A-Star算法的拦截方法，效果优于传统的MILP方法。


<details>
  <summary>Details</summary>
Motivation: 解决动态犯罪环境中由于运输网络的庞大和犯罪位置的不断变化导致的警力资源有限情况下难以拦截犯罪的挑战。

Method: 利用分层图表示，将每个时间步与运输网络的副本关联，并在分层图上应用A-Star启发式算法计算防御者的近似最优策略。

Result: 与混合整数线性规划（MILP）方法相比，提出的方法在计算效率和解的质量上具有优越性。

Conclusion: 本文提出的分层图表示法和A-Star启发式算法在解决动态犯罪环境中阻止犯罪的复杂问题上表现出色，能在较短的计算时间内提供高质量的解。

Abstract: Intercepting a criminal using limited police resources presents a significant
challenge in dynamic crime environments, where the criminal's location
continuously changes over time. The complexity is further heightened by the
vastness of the transportation network. To tackle this problem, we propose a
layered graph representation, in which each time step is associated with a
duplicate of the transportation network. For any given set of attacker
strategies, a near-optimal defender strategy is computed using the A-Star
heuristic algorithm applied to the layered graph. The defender's goal is to
maximize the probability of successful interdiction. We evaluate the
performance of the proposed method by comparing it with a Mixed-Integer Linear
Programming (MILP) approach used for the defender. The comparison considers
both computational efficiency and solution quality. The results demonstrate
that our approach effectively addresses the complexity of the problem and
delivers high-quality solutions within a short computation time.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [247] [Diffusion prior as a direct regularization term for FWI](https://arxiv.org/abs/2506.10141)
*Yuke Xie,Hervé Chauris,Nicolas Desassis*

Main category: physics.geo-ph

TL;DR: 提出了一种新方法，将去噪扩散模型作为先验集成到全波形反演中，提高了反演的稳定性和质量，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在物理约束的计算地震成像中应用困难，特别是在全波形反演 (FWI) 的非线性求解中，噪声速度场中的波传播可能导致数值伪影和低质量的反演。需要一种稳定的、能够提升收敛性和反演质量的方法。

Method: 直接将预训练的去噪扩散概率模型 (DDPM) 通过分数重新匹配策略，作为一种生成性扩散先验，集成到全波形反演 (FWI) 中，避免了反向扩散采样，只需要较少的迭代；操作完全在干净图像空间中进行，无需通过噪声速度模型。

Result: 与传统和基于生成对抗网络（GAN）的全波形反演方法相比，所提出的方法在保真实性和鲁棒性方面具有优势，同时在地震成像和其他逆问题任务中保持了实用性和计算效率。

Conclusion: 提出了一种新的框架，通过将预训练的去噪扩散概率模型 (DDPM) 作为基于分数的生成扩散先验，集成到全波形反演 (FWI) 中，解决了传统方法在逆问题中面临的挑战，例如不稳定性和数值伪影问题。

Abstract: Diffusion models have recently shown promise as powerful generative priors
for inverse problems. However, conventional applications require solving the
full reverse diffusion process and operating on noisy intermediate states,
which poses challenges for physics-constrained computational seismic imaging.
In particular, such instability is pronounced in non-linear solvers like those
used in Full Waveform Inversion (FWI), where wave propagation through noisy
velocity fields can lead to numerical artifacts and poor inversion quality. In
this work, we propose a simple yet effective framework that directly integrates
a pretrained Denoising Diffusion Probabilistic Model (DDPM) as a score-based
generative diffusion prior into FWI through a score rematching strategy. Unlike
traditional diffusion approaches, our method avoids the reverse diffusion
sampling and needs fewer iterations. We operate the image inversion entirely in
the clean image space, eliminating the need to operate through noisy velocity
models. The generative diffusion prior can be introduced as a simple
regularization term in the standard FWI update rule, requiring minimal
modification to existing FWI pipelines. This promotes stable wave propagation
and can improve convergence behavior and inversion quality. Numerical
experiments suggest that the proposed method offers enhanced fidelity and
robustness compared to conventional and GAN-based FWI approaches, while
remaining practical and computationally efficient for seismic imaging and other
inverse problem tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [248] [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020)
*Kyubyung Chae,Hyunbin Jin,Taesup Kim*

Main category: cs.CR

TL;DR: 引入了 RAII 框架，用于安全对齐大语言模型，显著提高了模型鲁棒性和控制性。


<details>
  <summary>Details</summary>
Motivation: 目前对齐大语言模型需要大量人工标注的偏好数据，这过程既昂贵又耗时，而合成数据提供了一种有前途的替代方式。

Method: 提出 RAII 框架，通过检测内部拒绝信号并适配性注入预定义短语来引发有害但流畅的文本完成。

Result: RAAI 成功将 LLMs 的有害响应率从基准的 2.15% 提升到平均 61.04%。通过 RAAI 生成的合成数据微调 LLMs 提高了模型对有害提示的鲁棒性，同时保持了在诸如 MMLU 和 ARC 的标准任务上的一般能力。

Conclusion: 大语言模型的攻击方法可以转变为实际的、安全可控的对齐工具。

Abstract: Safely aligning large language models (LLMs) often demands extensive
human-labeled preference data, a process that's both costly and time-consuming.
While synthetic data offers a promising alternative, current methods frequently
rely on complex iterative prompting or auxiliary models. To address this, we
introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,
training-free, and model-agnostic framework that repurposes LLM attack
techniques. RAAI works by detecting internal refusal signals and adaptively
injecting predefined phrases to elicit harmful, yet fluent, completions. Our
experiments show RAAI effectively jailbreaks LLMs, increasing the harmful
response rate from a baseline of 2.15% to up to 61.04% on average across four
benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by
RAAI improves model robustness against harmful prompts while preserving general
capabilities on standard tasks like MMLU and ARC. This work highlights how LLM
attack methodologies can be reframed as practical tools for scalable and
controllable safety alignment.

</details>


### [249] [LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges](https://arxiv.org/abs/2506.10022)
*Haoyang Li,Huan Gao,Zhiyuan Zhao,Zhiyu Lin,Junyu Gao,Xuelong Li*

Main category: cs.CR

TL;DR: 研究提出MalwareBench基准数据集，评估LLM在代码生成中的安全性。结果表明，LLM在这方面的安全能力仍有待提高，尤其是在面临多种越狱方法时。


<details>
  <summary>Details</summary>
Motivation: LLMs的普遍采用引发了对其安全性的担忧，特别是在代码生成方面。尽管已经进行了关于其一般安全能力的研究，但其对代码生成中越狱攻击的特定易感性仍未得到广泛探讨。

Method: 提出了MalwareBench，一个包含3,520个用于恶意代码生成的越狱提示的基准数据集，旨在评估LLM应对这些威胁的稳健性。

Result: 实验显示，主流LLM拒绝恶意代码生成请求的平均拒绝率为60.93%，但在结合越狱攻击算法后降至39.92%。

Conclusion: 主流大型语言模型在拒绝恶意代码生成请求方面表现有限，并且结合多种越狱方法时，其安全能力显著降低。

Abstract: The widespread adoption of Large Language Models (LLMs) has heightened
concerns about their security, particularly their vulnerability to jailbreak
attacks that leverage crafted prompts to generate malicious outputs. While
prior research has been conducted on general security capabilities of LLMs,
their specific susceptibility to jailbreak attacks in code generation remains
largely unexplored. To fill this gap, we propose MalwareBench, a benchmark
dataset containing 3,520 jailbreaking prompts for malicious code-generation,
designed to evaluate LLM robustness against such threats. MalwareBench is based
on 320 manually crafted malicious code generation requirements, covering 11
jailbreak methods and 29 code functionality categories. Experiments show that
mainstream LLMs exhibit limited ability to reject malicious code-generation
requirements, and the combination of multiple jailbreak methods further reduces
the model's security capabilities: specifically, the average rejection rate for
malicious content is 60.93%, dropping to 39.92% when combined with jailbreak
attack algorithms. Our work highlights that the code security capabilities of
LLMs still pose significant challenges.

</details>


### [250] [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024)
*Elena Sofia Ruzzetti,Giancarlo A. Xompero,Davide Venditti,Fabio Massimo Zanzotto*

Main category: cs.CR

TL;DR: 本文提出了一种新方法（PME）来防止大规模语言模型泄漏私人数据，提高其对隐私攻击的防护能力。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型可能会记住个人身份信息（PII），这些信息不应被存储或泄露，需要一种防御策略来防止隐私数据泄漏。

Method: 提出名为私有记忆编辑（PME）的新方法，通过编辑模型对训练数据的记忆来减少PII的泄漏。

Result: 我们的程序不影响语言模型的基础性能，并增强其对隐私训练数据提取攻击的鲁棒性。

Conclusion: PME能够有效减少隐私信息泄露，甚至在某些情况下将隐私攻击的准确性降至零。

Abstract: Large Language Models (LLMs) memorize, and thus, among huge amounts of
uncontrolled data, may memorize Personally Identifiable Information (PII),
which should not be stored and, consequently, not leaked. In this paper, we
introduce Private Memorization Editing (PME), an approach for preventing
private data leakage that turns an apparent limitation, that is, the LLMs'
memorization ability, into a powerful privacy defense strategy. While attacks
against LLMs have been performed exploiting previous knowledge regarding their
training data, our approach aims to exploit the same kind of knowledge in order
to make a model more robust. We detect a memorized PII and then mitigate the
memorization of PII by editing a model knowledge of its training data. We
verify that our procedure does not affect the underlying language model while
making it more robust against privacy Training Data Extraction attacks. We
demonstrate that PME can effectively reduce the number of leaked PII in a
number of configurations, in some cases even reducing the accuracy of the
privacy attacks to zero.

</details>


### [251] [Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks](https://arxiv.org/abs/2506.10029)
*Rafaël Nouailles*

Main category: cs.CR

TL;DR: 论文对ChatGPT和Gemini的安全性和一致性进行了比较分析，并探讨了与越狱相关的技术。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及和应用，出现了许多新的网络安全挑战，如提示注入攻击、规避监管措施（越狱）、虚假信息传播（幻觉）以及与深度伪造相关的风险。

Method: 对ChatGPT和Gemini进行了安全性和一致性水平的比较分析，并进行了一些越狱技术的实验。

Result: 论文详细分析了ChatGPT和Google Gemini在安全性和一致性方面的差异，并探索了越狱技术的多样性。

Conclusion: 该论文进行了ChatGPT和Google Gemini在安全性和一致性方面的比较分析，并提供了一种越狱技术分类。

Abstract: Large Language models (LLMs) are transforming digital usage, particularly in
text generation, image creation, information retrieval and code development.
ChatGPT, launched by OpenAI in November 2022, quickly became a reference,
prompting the emergence of competitors such as Google's Gemini. However, these
technological advances raise new cybersecurity challenges, including prompt
injection attacks, the circumvention of regulatory measures (jailbreaking), the
spread of misinformation (hallucinations) and risks associated with deep fakes.
This paper presents a comparative analysis of the security and alignment levels
of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated
with experiments.

</details>


### [252] [Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment](https://arxiv.org/abs/2506.10030)
*Tianyu Chen,Jian Lou,Wenjie Wang*

Main category: cs.CR

TL;DR: AQUA is a novel watermarking framework for image protection in multimodal RAG, ensuring copyright protection through semantic signal embedding techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing watermarking methods in RAG that only protect textual knowledge but not image knowledge, especially in shared knowledge bases of multimodal RAG systems.

Method: AQUA uses two complementary methods to embed semantic signals into synthetic images: acronym-based triggers and spatial relationship cues.

Result: Experiments indicate that AQUA is effective in watermarking image knowledge, enabling robust and reliable copyright tracing across various models and datasets.

Conclusion: AQUA provides robust protection for image knowledge in multimodal RAG systems and fills a significant gap in copyright protection for such platforms.

Abstract: As Retrieval-Augmented Generation (RAG) evolves into service-oriented
platforms (Rag-as-a-Service) with shared knowledge bases, protecting the
copyright of contributed data becomes essential. Existing watermarking methods
in RAG focus solely on textual knowledge, leaving image knowledge unprotected.
In this work, we propose AQUA, the first watermark framework for image
knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals
into synthetic images using two complementary methods: acronym-based triggers
and spatial relationship cues. These techniques ensure watermark signals
survive indirect watermark propagation from image retriever to textual
generator, being efficient, effective and imperceptible. Experiments across
diverse models and datasets show that AQUA enables robust, stealthy, and
reliable copyright tracing, filling a key gap in multimodal RAG protection.

</details>


### [253] [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171)
*Saswat Das,Jameson Sandler,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: 研究提出CMPL框架以在多个领域中审计大语言模型代理的隐私风险，能够发现单轮防御无法阻止的泄露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为个人助手和客户服务机器人的应用需要访问敏感数据，存在隐私泄露的风险。

Method: 研究提出了一种名为CMPL的审计框架，通过模拟多轮对话来检测隐私泄露风险。

Result: 该框架能够揭示现有单轮防御措施无法检测的隐私风险，并提供定量风险指标的审计步骤和开放的基准测试。

Conclusion: CMPL框架可用于评估和揭示大型语言模型代理在隐私保护方面的潜在漏洞。

Abstract: Large Language Model agents have begun to appear as personal assistants,
customer service bots, and clinical aides. While these applications deliver
substantial operational benefits, they also require continuous access to
sensitive data, which increases the likelihood of unauthorized disclosures.
This study proposes an auditing framework for conversational privacy that
quantifies and audits these risks. The proposed Conversational Manipulation for
Privacy Leakage (CMPL) framework, is an iterative probing strategy designed to
stress-test agents that enforce strict privacy directives. Rather than focusing
solely on a single disclosure event, CMPL simulates realistic multi-turn
interactions to systematically uncover latent vulnerabilities. Our evaluation
on diverse domains, data modalities, and safety configurations demonstrate the
auditing framework's ability to reveal privacy risks that are not deterred by
existing single-turn defenses. In addition to introducing CMPL as a diagnostic
tool, the paper delivers (1) an auditing procedure grounded in quantifiable
risk metrics and (2) an open benchmark for evaluation of conversational privacy
across agent implementations.

</details>


### [254] [GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)
*Zilong Wang,Xiang Zheng,Xiaosen Wang,Bo Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: 提出GenBreak框架，通过精细调整的大语言模型生成对抗提示，揭示T2I模型的安全弱点。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型缺乏有效的安全评估工具，现有研究在生成有害图像和绕过安全机制方面存在局限性。

Method: 通过监督微调和与替代T2I模型的交互强化学习，使用多重奖励信号指导大语言模型生成对抗提示。

Result: 生成的提示在商业T2I生成器上的黑盒攻击中表现出色，揭示了实用且令人担忧的安全弱点。

Conclusion: 构建了一个名为GenBreak的框架，用于识别T2I模型中的安全漏洞，通过细致的提示生成有效的攻击内容。

Abstract: Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and
are now widely used in content creation. However, these models can be misused
to generate harmful content, including nudity or violence, posing significant
safety risks. While most platforms employ content moderation systems,
underlying vulnerabilities can still be exploited by determined adversaries.
Recent research on red-teaming and adversarial attacks against T2I models has
notable limitations: some studies successfully generate highly toxic images but
use adversarial prompts that are easily detected and blocked by safety filters,
while others focus on bypassing safety mechanisms but fail to produce genuinely
harmful outputs, neglecting the discovery of truly high-risk prompts.
Consequently, there remains a lack of reliable tools for evaluating the safety
of defended T2I models. To address this gap, we propose GenBreak, a framework
that fine-tunes a red-team large language model (LLM) to systematically explore
underlying vulnerabilities in T2I generators. Our approach combines supervised
fine-tuning on curated datasets with reinforcement learning via interaction
with a surrogate T2I model. By integrating multiple reward signals, we guide
the LLM to craft adversarial prompts that enhance both evasion capability and
image toxicity, while maintaining semantic coherence and diversity. These
prompts demonstrate strong effectiveness in black-box attacks against
commercial T2I generators, revealing practical and concerning safety
weaknesses.

</details>


### [255] [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
*Yeonwoo Jang,Shariqah Hossain,Ashwin Sreevatsa,Diogo Cruz*

Main category: cs.CR

TL;DR: 这项研究评估了八种反学习技术在面对提示攻击时的表现，并开发了评价框架以提高可靠的知识删除。


<details>
  <summary>Details</summary>
Motivation: 评估机器反学习方法在遭受提示攻击时的有效性，以及开发评价框架以提高知识删除的可靠性。

Method: 系统评估了八种反学习技术，并使用基于输出、基于logit和探针分析的方式来评估所谓已反学习的知识的检索程度。

Result: RMU和TAR方法表现出强大的反学习能力，而ELM对特定提示攻击仍然脆弱。同时，logit分析表明，反学习模型通常不会通过修改答案格式来隐藏知识，输出与logit准确性之间的相关性强。

Conclusion: 研究表明，一些机器反学习方法在面对简单的提示攻击时可能会失效，而结论强调需要开发能够有效区分真正的知识删除与表面输出抑制的评价框架。

Abstract: In this work, we show that some machine unlearning methods may fail when
subjected to straightforward prompt attacks. We systematically evaluate eight
unlearning techniques across three model families, and employ output-based,
logit-based, and probe analysis to determine to what extent supposedly
unlearned knowledge can be retrieved. While methods like RMU and TAR
demonstrate robust unlearning, ELM remains vulnerable to specific prompt
attacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).
Our logit analysis also confirms that unlearned models are generally not hiding
knowledge by modifying the way the answer is formatted, as the correlation
between output and logit accuracy is strong. These results challenge prevailing
assumptions about unlearning effectiveness and highlight the need for
evaluation frameworks that can reliably distinguish between true knowledge
removal and superficial output suppression. We also publicly make available our
evaluation framework to easily evaluate prompting techniques to retrieve
unlearning knowledge.

</details>


### [256] [SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks](https://arxiv.org/abs/2506.10424)
*Kaiyuan Zhang,Siyuan Cheng,Hanxi Guo,Yuetian Chen,Zian Su,Shengwei An,Yuntao Du,Charles Fleming,Ashish Kundu,Xiangyu Zhang,Ninghui Li*

Main category: cs.CR

TL;DR: 本文研究了微调的大型语言模型在隐私保护方面的脆弱性，并提出了SOFT防御技术，通过选择性数据模糊化实现隐私保护，验证了其在不损失模型性能的前提下有效降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: 微调LLMs涉及私人或敏感信息，导致严重的隐私问题。在此背景下，研究评估这些模型对会员推断攻击的易感性。

Method: 研究首先进行了综合分析，确定微调的LLMs容易受到MIAs的攻击。然后提出了一种新的防御技术，称为SOFT，通过选择性数据模糊化来防止隐私泄漏。

Result: 实验结果显示，SOFT可以有效降低隐私泄漏风险，同时保持了竞争力的模型性能。

Conclusion: SOFT可以有效地减少隐私风险，同时保持模型性能，提供一个实用且可扩展的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success and are widely
adopted for diverse applications. However, fine-tuning these models often
involves private or sensitive information, raising critical privacy concerns.
In this work, we conduct the first comprehensive study evaluating the
vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our
empirical analysis demonstrates that MIAs exploit the loss reduction during
fine-tuning, making them highly effective in revealing membership information.
These findings motivate the development of our defense. We propose SOFT
(\textbf{S}elective data \textbf{O}bfuscation in LLM
\textbf{F}ine-\textbf{T}uning), a novel defense technique that mitigates
privacy leakage by leveraging influential data selection with an adjustable
parameter to balance utility preservation and privacy protection. Our extensive
experiments span six diverse domains and multiple LLM architectures and scales.
Results show that SOFT effectively reduces privacy risks while maintaining
competitive model performance, offering a practical and scalable solution to
safeguard sensitive information in fine-tuned LLMs.

</details>


### [257] [Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications](https://arxiv.org/abs/2506.10467)
*Felix Härer*

Main category: cs.CR

TL;DR: 本文探索了多代理LLM系统的规范与评估，验证了其在网络安全等任务中的可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM研究主要对单独组件进行评估，而缺乏对其在多代理系统中的联合应用和规范化评估。多代理LLM系统的定义规范有助于探索其在特定领域应用的潜力。

Method: 通过延续先前的研究，本文扩展了系统架构和原型，导入多代理系统的规范，并使用这些多代理系统进行探索性研究和评估。

Result: 实验表明，基于LLM的多代理系统在网络安全等任务中表现出色，能够正确完成包括问答、服务器安全和网络安全的任务。

Conclusion: 多代理LLM系统具有广泛应用潜力，尤其在特定领域任务中。定义合理的规范可提升系统评估的系统性与准确性。

Abstract: Recent advancements in LLMs indicate potential for novel applications, e.g.,
through reasoning capabilities in the latest OpenAI and DeepSeek models. For
applying these models in specific domains beyond text generation, LLM-based
multi-agent approaches can be utilized that solve complex tasks by combining
reasoning techniques, code generation, and software execution. Applications
might utilize these capabilities and the knowledge of specialized LLM agents.
However, while many evaluations are performed on LLMs, reasoning techniques,
and applications individually, their joint specification and combined
application is not explored well. Defined specifications for multi-agent LLM
systems are required to explore their potential and their suitability for
specific applications, allowing for systematic evaluations of LLMs, reasoning
techniques, and related aspects. This paper reports the results of exploratory
research to specify and evaluate these aspects through a multi-agent system.
The system architecture and prototype are extended from previous research and a
specification is introduced for multi-agent systems. Test cases involving
cybersecurity tasks indicate feasibility of the architecture and evaluation
approach. In particular, the results show the evaluation of question answering,
server security, and network security tasks that were completed correctly by
agents with LLMs from OpenAI and DeepSeek.

</details>


### [258] [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
*Xunguang Wang,Zhenlan Ji,Wenxuan Wang,Zongjie Li,Daoyuan Wu,Shuai Wang*

Main category: cs.CR

TL;DR: 该论文分析了大型语言模型的破解护栏，提出了分类法和评估框架来评估护栏的效果，以支持未来的研究和实际部署。


<details>
  <summary>Details</summary>
Motivation: LLMs面临关键漏洞的问题，特别是绕过安全机制的破解攻击，护栏作为外部防御机制是一个有前途的解决方案。

Method: 提出了一种新的多维分类法，并引入了安全-效率-效用评估框架以评估实用效果。

Result: 通过广泛分析和实验，确定了现有护栏方法的优缺点，探讨了它们在不同攻击类型下的普遍性，并提供了优化防御组合的见解。

Conclusion: 本文提供了一个系统的基础，为未来的研究和发展提供指导，旨在推进和部署强大的LLM护栏。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, but their
deployment has exposed critical vulnerabilities, particularly to jailbreak
attacks that circumvent safety mechanisms. Guardrails--external defense
mechanisms that monitor and control LLM interaction--have emerged as a
promising solution. However, the current landscape of LLM guardrails is
fragmented, lacking a unified taxonomy and comprehensive evaluation framework.
In this Systematization of Knowledge (SoK) paper, we present the first holistic
analysis of jailbreak guardrails for LLMs. We propose a novel,
multi-dimensional taxonomy that categorizes guardrails along six key
dimensions, and introduce a Security-Efficiency-Utility evaluation framework to
assess their practical effectiveness. Through extensive analysis and
experiments, we identify the strengths and limitations of existing guardrail
approaches, explore their universality across attack types, and provide
insights into optimizing defense combinations. Our work offers a structured
foundation for future research and development, aiming to guide the principled
advancement and deployment of robust LLM guardrails. The code is available at
https://github.com/xunguangwang/SoK4JailbreakGuardrails.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [259] [Patient-Specific Deep Reinforcement Learning for Automatic Replanning in Head-and-Neck Cancer Proton Therapy](https://arxiv.org/abs/2506.10073)
*Malvern Madondo,Yuan Shao,Yingzi Liu,Jun Zhou,Xiaofeng Yang,Zhen Tian*

Main category: physics.med-ph

TL;DR: 通过深度强化学习实现个性化自动化再计划，提高头颈部癌症患者质子治疗计划质量。


<details>
  <summary>Details</summary>
Motivation: 手动再计划过程资源密集且耗时，亟需寻找自动化解决方案以维持治疗质量。

Method: 提出了一种基于深度强化学习的自动化个性化再计划框架，使用深度Q网络和近端策略优化两个算法进行剂量体积直方图的状态表示。

Result: 研究表明，深度强化学习模型优于人工计划，改善了计划质量分数，临床验证显示在不同解剖变化情况下提高了肿瘤覆盖率和器官保护。

Conclusion: 深度强化学习方法能够有效提高头颈部癌症患者的质子治疗计划质量，并在肿瘤覆盖和保护风险器官方面表现出显著提升。

Abstract: Anatomical changes during intensity-modulated proton therapy (IMPT) for
head-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and
organ-at-risk overdosing. As a result, treatment replanning is often required
to maintain clinically acceptable treatment quality. However, current manual
replanning processes are resource-intensive and time-consuming. We propose a
patient-specific deep reinforcement learning (DRL) framework for automated IMPT
replanning, with a reward-shaping mechanism based on a $150$-point plan quality
score addressing competing clinical objectives. We formulate the planning
process as an RL problem where agents learn control policies to adjust
optimization priorities, maximizing plan quality. Unlike population-based
approaches, our framework trains personalized agents for each patient using
their planning CT (Computed Tomography) and augmented anatomies simulating
anatomical changes (tumor progression and regression). This patient-specific
approach leverages anatomical similarities throughout treatment, enabling
effective plan adaptation. We implemented two DRL algorithms, Deep Q-Network
and Proximal Policy Optimization, using dose-volume histograms (DVHs) as state
representations and a $22$-dimensional action space of priority adjustments.
Evaluation on five HNC patients using actual replanning CT data showed both DRL
agents improved initial plan scores from $120.63 \pm 21.40$ to $139.78 \pm
6.84$ (DQN) and $142.74 \pm 5.16$ (PPO), surpassing manual replans generated by
a human planner ($137.20 \pm 5.58$). Clinical validation confirms that
improvements translate to better tumor coverage and OAR sparing across diverse
anatomical changes. This work demonstrates DRL's potential in addressing
geometric and dosimetric complexities of adaptive proton therapy, offering
efficient offline adaptation solutions and advancing online adaptive proton
therapy.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [260] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: 这篇论文介绍了一种从Levy的CBPV到pi演算的编码方式，证明了其健全性和完备性，并在Coq中开始进行形式化证明。


<details>
  <summary>Details</summary>
Motivation: 克服使用de Bruijn索引的形式化中的挑战，并使得在该设定中早期、后期和开放的双重仿生符合且为一个一致关系。

Method: 通过对pi演算的内部化形式进行专门化编码，从而实现对CBPV的编码，并使用Coq开始形式化证明其编码的健全性和完备性。

Result: 展示了该编码满足Gorla提出的优良编码的五条标准，并且与Milner的编码具有相似性。还开始在Coq中对pi演算的内部化形式的健全性和完备性进行形式化证明。

Conclusion: 该论文定义了一种从Levy的按值调用lambda演算（CBPV）到pi演算的编码，并证明其编码是健全且完备的。

Abstract: In this report we define an encoding of Levys call-by-push-value
lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both
sound and complete. We present informal (by-hand) proofs of soundness,
completeness, and all required lemmas. The encoding is specialized to the
internal pi-calculus (pi-i-calculus) to circumvent certain challenges
associated with using de Bruijn index in a formalization, and it also helps
with bisimulation as early-, late- and open-bisimulation coincide in this
setting, furthermore bisimulation is a congruence. Additionally, we argue that
our encoding also satisfies the five criteria for good encodings proposed by
Gorla, as well as show similarities between Milners and our encoding. This
paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic
pi-calculus and the local pi-calculus. We begin a formalization of the proof in
Coq for the soundness and completeness of the encoding in the pi-i-calculus.
Not all lemmas used in the formalization are themselves formally proven.
However, we argue that the non-proven lemmas are reasonable, as they are proven
by hand, or amount to Coq formalities that are straightforward given informal
arguments.

</details>


### [261] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: StepProof通过细化验证步骤，提高了自动形式化的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自动形式化方法仅限于验证完整证明，缺乏句级验证能力，因此提出StepProof来解决这一问题。

Method: 提出了一种称为StepProof的新型自动形式化方法，旨在实现细粒度的逐步验证，将完整证明分解为多个可验证的子证明，实现句级验证。

Result: 实验结果表明，StepProof显著提高了证明成功率和效率。此外，发现对自然语言证明进行微调以适应步骤级验证能进一步提升StepProof的自动形式化性能。

Conclusion: StepProof显著提高了证明成功率和效率，相较于传统方法具有更好的性能。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [262] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX通过架构层面的剪枝框架提高FLUX的推理效率，同时保持高图像质量，克服了大型模型推理速度慢和部署困境。


<details>
  <summary>Details</summary>
Motivation: 当前的大型模型在文本生成图像任务中存在推理速度慢、内存占用高和部署性差的问题。现有加速方法存在性能显著下降和训练成本高的问题。

Method: 提出了块级线性层替代方法（BRLL），将ResBlocks中的复杂残差分支替换为轻量级线性层，保留原有的快捷连接以确保稳定性，并引入三明治训练（ST）以监督相邻模块，减轻结构替换导致的性能下降。

Result: FastFLUX显著提高了推理速度，同时在定性和定量评估保持高图像质量，即使在剪枝20%的层级情况下也是如此。代码将很快开放。

Conclusion: FastFLUX通过架构层面的剪枝框架提高了FLUX的推理效率，同时保持高质量的图像生成。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the
emergence of highly expressive models such as diffusion transformers (DiTs),
exemplified by FLUX. However, their massive parameter sizes lead to slow
inference, high memory usage, and poor deployability. Existing acceleration
methods (e.g., single-step distillation and attention pruning) often suffer
from significant performance degradation and incur substantial training costs.
To address these limitations, we propose FastFLUX, an architecture-level
pruning framework designed to enhance the inference efficiency of FLUX. At its
core is the Block-wise Replacement with Linear Layers (BRLL) method, which
replaces structurally complex residual branches in ResBlocks with lightweight
linear layers while preserving the original shortcut connections for stability.
Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning
strategy that leverages LoRA to supervise neighboring blocks, mitigating
performance drops caused by structural replacement. Experiments show that our
FastFLUX maintains high image quality under both qualitative and quantitative
evaluations, while significantly improving inference speed, even with 20\% of
the hierarchy pruned. Our code will be available soon.

</details>


### [263] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 通过Ambient Diffusion Omni框架利用低质量、合成和分布外图像来提升扩散模型质量，并在ImageNet上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 通常扩散模型在经过高度筛选的数据集上进行训练，但该研究认为那些通常被丢弃的低质量图像同样具有巨大价值。

Method: 提出Ambient Diffusion Omni框架，利用自然图像的谱功率衰减和局部性属性，从所有可用图像中提取信号，利用合成低质量图像对模型进行训练。

Result: 通过合成的图像（包括高斯模糊、JPEG压缩、运动模糊）成功训练了扩散模型，实现了ImageNet数据集上的SOTA FID，同时在文本到图像生成建模中也表现出显著改善。

Conclusion: 该研究提出的Ambient Diffusion Omni框架可以有效利用低质量、合成和分布外图像来提升扩散模型的质量，实现了在ImageNet数据集上的最新SOTA表现，并在图像质量和多样性上取得了显著提高。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to
improve the quality of a diffusion model. Typically, diffusion models are
trained on curated datasets that emerge from highly filtered data pools from
the Web and other sources. We show that there is immense value in the
lower-quality images that are often discarded. We present Ambient Diffusion
Omni, a simple, principled framework to train diffusion models that can extract
signal from all available images during training. Our framework exploits two
properties of natural images -- spectral power law decay and locality. We first
validate our framework by successfully training diffusion models with images
synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We
then use our framework to achieve state-of-the-art ImageNet FID, and we show
significant improvements in both image quality and diversity for text-to-image
generative modeling. The core insight is that noise dampens the initial skew
between the desired high-quality distribution and the mixed distribution we
actually observe. We provide rigorous theoretical justification for our
approach by analyzing the trade-off between learning from biased data versus
limited unbiased data across diffusion times.

</details>


### [264] [Learning-based density-equalizing map](https://arxiv.org/abs/2506.10027)
*Yanwen Huang,Lok Ming Lui,Gary P. T. Choi*

Main category: cs.GR

TL;DR: 提出了一种新型学习型密度均衡映射方法，克服了传统方法的局限性，能高效从2D扩展到3D。


<details>
  <summary>Details</summary>
Motivation: 传统密度均衡地图制图方法在精确度和适用性方面存在局限，尤其是在处理2D到3D的扩展时，需要重新设计算法。

Method: 提出了一种基于深度神经网络的学习型密度均衡映射框架（LDEM），通过引入强制密度均匀性和几何规律性的损失函数，并采用层次方法预测粗细两个层次的变换。

Result: 新方法在广泛的简单和复杂密度分布中表现出优越的密度均衡和一对一的映射特性，并且可以轻松应用于具有不同效果的表面网格划分，从2D到3D无须更改模型架构或损失公式。

Conclusion: 该研究为密度均衡地图的可扩展和稳健计算开辟了新的可能性，可在实际应用中广泛使用。

Abstract: Density-equalizing map (DEM) serves as a powerful technique for creating
shape deformations with the area changes reflecting an underlying density
function. In recent decades, DEM has found widespread applications in fields
such as data visualization, geometry processing, and medical imaging.
Traditional approaches to DEM primarily rely on iterative numerical solvers for
diffusion equations or optimization-based methods that minimize handcrafted
energy functionals. However, these conventional techniques often face several
challenges: they may suffer from limited accuracy, produce overlapping
artifacts in extreme cases, and require substantial algorithmic redesign when
extended from 2D to 3D, due to the derivative-dependent nature of their energy
formulations. In this work, we propose a novel learning-based
density-equalizing mapping framework (LDEM) using deep neural networks.
Specifically, we introduce a loss function that enforces density uniformity and
geometric regularity, and utilize a hierarchical approach to predict the
transformations at both the coarse and dense levels. Our method demonstrates
superior density-equalizing and bijectivity properties compared to prior
methods for a wide range of simple and complex density distributions, and can
be easily applied to surface remeshing with different effects. Also, it
generalizes seamlessly from 2D to 3D domains without structural changes to the
model architecture or loss formulation. Altogether, our work opens up new
possibilities for scalable and robust computation of density-equalizing maps
for practical applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [265] [DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction](https://arxiv.org/abs/2506.10309)
*Yuliang Zhu,Jing Cheng,Qi Xie,Zhuo-Xu Cui,Qingyong Zhu,Yuanyuan Liu,Xin Liu,Jianfeng Ren,Chengbo Wang,Dong Liang*

Main category: eess.IV

TL;DR: DUN-SRE leverages spatiotemporal rotation symmetry to improve dynamic MRI reconstruction, showing superior results in experiments on Cardiac CINE MRI datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ECNNs fail to effectively model temporal symmetry in dynamic MRI reconstruction, limiting the quality of images, especially in undersampling scenarios.

Method: Proposes a novel Deep Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) leveraging a (2+1)D equivariant convolutional architecture.

Result: Comprehensive experiments show DUN-SRE provides better performance in modeling cardiac motion dynamics and preserving rotation symmetry compared to existing methods.

Conclusion: DUN-SRE achieves state-of-the-art performance in preserving rotation-symmetric structures in dynamic MRI reconstruction, offering strong generalization capability.

Abstract: Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries,
including spatial rotation symmetry within individual frames and temporal
symmetry along the time dimension. Explicit incorporation of these symmetry
priors in the reconstruction model can significantly improve image quality,
especially under aggressive undersampling scenarios. Recently, Equivariant
convolutional neural network (ECNN) has shown great promise in exploiting
spatial symmetry priors. However, existing ECNNs critically fail to model
temporal symmetry, arguably the most universal and informative structural prior
in dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep
Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for
Dynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance
through a (2+1)D equivariant convolutional architecture. In particular, it
integrates both the data consistency and proximal mapping module into a unified
deep unrolling framework. This architecture ensures rigorous propagation of
spatiotemporal rotation symmetry constraints throughout the reconstruction
process, enabling more physically accurate modeling of cardiac motion dynamics
in cine MRI. In addition, a high-fidelity group filter parameterization
mechanism is developed to maintain representation precision while enforcing
symmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets
demonstrate that DUN-SRE achieves state-of-the-art performance, particularly in
preserving rotation-symmetric structures, offering strong generalization
capability to a broad range of dynamic MRI reconstruction tasks.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [266] [Exploring Topological and Localization Phenomena in SSH Chains under Generalized AAH Modulation: A Computational Approach](https://arxiv.org/abs/2506.10195)
*Souvik Ghosh,Sayak Roy*

Main category: cond-mat.mtrl-sci

TL;DR: 本文研究广义SSH模型下拓扑、无序、非Hermiticity和时间驱动的相互作用，揭示了局域化转换破坏拓扑边缘态和非Hermitian皮肤效应等现象，并通过Floquet驱动成功实现拓扑转变。


<details>
  <summary>Details</summary>
Motivation: SSH模型作为一维拓扑绝缘体的典型例子，其在复杂情况下的行为仍需研究。本文旨在通过计算研究探索拓扑、准周期性无序、非Hermiticity和时间驱动之间的相互作用。

Method: 本文使用精确对角化和专门的数值求解器映射系统的相空间，并通过逆参与率（IPR）定量其谱特性和局域化特征。此外，使用无监督机器学习（PCA）自动分类系统的相位。

Result: 通过Aubry-André-Harper (AAH)调制，标准SSH模型中的拓扑保护边缘态被强局域化转变破坏。此外，非Hermitian效应导致所有体态在边界处极度局域化。应用周期性Floquet驱动后，成功在拓扑平凡链中实现了Floquet拓扑绝缘体。

Conclusion: 本文通过多种计算方法展示了广义SSH模型中的丰富现象，包括强局域化和非Hermiticity效应对拓扑边缘态的影响，以及如何通过Floquet驱动实现拓扑转变。

Abstract: The Su-Schrieffer-Heeger (SSH) model serves as a canonical example of a
one-dimensional topological insulator, yet its behavior under more complex,
realistic conditions remains a fertile ground for research. This paper presents
a comprehensive computational investigation into generalized SSH models,
exploring the interplay between topology, quasi-periodic disorder,
non-Hermiticity, and time-dependent driving. Using exact diagonalization and
specialized numerical solvers, we map the system's phase space through its
spectral properties and localization characteristics, quantified by the Inverse
Participation Ratio (IPR). We demonstrate that while the standard SSH model
exhibits topologically protected edge states, these are destroyed by a
localization transition induced by strong Aubry-Andr\'e-Harper (AAH)
modulation. Further, we employ unsupervised machine learning (PCA) to
autonomously classify the system's phases, revealing that strong localization
can obscure underlying topological signatures. Extending the model beyond
Hermiticity, we uncover the non-Hermitian skin effect, a dramatic localization
of all bulk states at a boundary. Finally, we apply a periodic Floquet drive to
a topologically trivial chain, successfully engineering a Floquet topological
insulator characterized by the emergence of anomalous edge states at the
boundaries of the quasi-energy zone. These findings collectively provide a
multi-faceted view of the rich phenomena hosted in generalized 1D topological
systems.

</details>
