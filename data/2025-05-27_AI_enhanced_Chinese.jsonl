{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86\u5229\u7528NLP\u548c\u793e\u533a\u53c2\u4e0e\u6765\u4fdd\u62a4\u6fd2\u5371\u8bed\u8a00\u79d1\u66fc\u5947\uff0c\u901a\u8fc7\u5c11\u91cf\u5b9e\u4f8b\u63d0\u793a\u63d0\u9ad8\u4e86\u8bed\u8a00\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u6fd2\u5371\u8bed\u8a00\u5728\u6570\u5b57\u9886\u57df\u7684\u6392\u65a5\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u8bed\u8a00\u590d\u5174\u5de5\u4f5c\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u624b\u52a8\u6574\u7406\u7684412\u4e2a\u77ed\u8bed\u6570\u636e\u96c6\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5e76\u5bf9GPT-4o\u548cGPT-4o-mini\u8fdb\u884c\u8bed\u8a00\u8bc6\u522b\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5728\u96f6\u6837\u672c\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u5bf9\u79d1\u66fc\u5947\u8bed\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u63d0\u793a\uff08\u4ec5\u4e94\u4e2a\u4f8b\u5b50\uff09\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5c11\u91cf\u5b9e\u4f8b\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u79d1\u66fc\u5947\u8bed\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u8bc1\u660e\u4e86NLP\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0BERT\u7c7b\u6a21\u578b\u5728\u4e00\u4e9b\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eLLMs\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u7684\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u3002", "motivation": "\u8d28\u7591\u5f53\u524d\u6d41\u884c\u7684'\u4ee5LLM\u4e3a\u4e2d\u5fc3'\u7684\u8d8b\u52bf\uff0c\u5c55\u793a\u4f20\u7edfBERT\u7c7b\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u6f5c\u5728\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5728\u516d\u4e2a\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u4e09\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u5305\u62ec\u5fae\u8c03BERT\u6a21\u578b\u3001LLM\u5185\u90e8\u72b6\u6001\u5229\u7528\u548c\u96f6\u6837\u672c\u63a8\u7406\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cBERT\u7c7b\u6a21\u578b\u5728\u6a21\u5f0f\u9a71\u52a8\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u800cLLMs\u5219\u5728\u9700\u8981\u6df1\u5ea6\u8bed\u4e49\u6216\u4e16\u754c\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u5360\u4f18\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTaMAS\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "\u4f20\u7edf\u7684BERT\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u4f18\u4e8eLLMs\uff0c\u5c24\u5176\u662f\u5728\u6a21\u5f0f\u9a71\u52a8\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors.", "AI": {"tldr": "CoMet\u6846\u67b6\u63d0\u5347LLMs\u5904\u7406\u9690\u55bb\u80fd\u529b\uff0c\u589e\u5f3a\u6218\u7565\u6c9f\u901a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4ee3\u7406\u8bed\u8a00\u6e38\u620f\u4e2d\u5904\u7406\u9690\u55bb\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u5176\u6218\u7565\u6027\u6c9f\u901a\u7684\u6548\u7387\u3002", "method": "CoMet\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u5047\u8bbe\u7684\u9690\u55bb\u63a8\u7406\u5668\u548c\u9690\u55bb\u751f\u6210\u5668\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u77e5\u8bc6\u6574\u5408\u8fdb\u884c\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCoMet\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u5728\u201cUndercover\u201d\u548c\u201cAdversarial Taboo\u201d\u6e38\u620f\u4e2d\u4f7f\u7528\u9690\u55bb\u8fdb\u884c\u6218\u7565\u6c9f\u901a\u7684\u80fd\u529b\u3002", "conclusion": "CoMet\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4ee3\u7406\u8bed\u8a00\u6e38\u620f\u4e2d\u4f7f\u7528\u9690\u55bb\u8fdb\u884c\u6218\u7565\u6027\u4ea4\u6d41\u7684\u80fd\u529b\u3002"}}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning.", "AI": {"tldr": "\u5f15\u5165IDA-Bench\u4ee5\u8bc4\u4f30LLM\u5728\u591a\u8f6e\u6570\u636e\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728<50%\u7684\u4efb\u52a1\u4e2d\u6210\u529f\uff0c\u9700\u6539\u8fdb\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u4e86\u6570\u636e\u5206\u6790\u9886\u57df\u4e2d\u7684\u8fed\u4ee3\u6027\u8d28\uff0c\u4e13\u5bb6\u7684\u51b3\u7b56\u968f\u7740\u5bf9\u6570\u636e\u96c6\u7684\u6df1\u5165\u7406\u89e3\u800c\u6f14\u53d8\u3002\u6b64\u9879\u5de5\u4f5c\u7684\u76ee\u7684\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165IDA-Bench\uff0c\u8bc4\u4ef7LLM\u4ee3\u7406\u5728\u591a\u8f6e\u4e92\u52a8\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002\u4efb\u52a1\u7531\u590d\u6742Kaggle\u7b14\u8bb0\u672c\u6d3e\u751f\uff0c\u5e76\u901a\u8fc7LLM\u6a21\u62df\u7528\u6237\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u3002\u901a\u8fc7\u5c06\u4ee3\u7406\u6700\u7ec8\u6570\u503c\u8f93\u51fa\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u6765\u5224\u65ad\u5176\u8868\u73b0\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude-3.7-thinking\uff09\uff0c\u5728<50%\u7684\u4efb\u52a1\u4e2d\u6210\u529f\uff0c\u7a81\u663e\u4e86\u5728\u5355\u8f6e\u6d4b\u8bd5\u4e2d\u672a\u73b0\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LLMs\u9700\u8981\u63d0\u9ad8\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\u4ee5\u6210\u4e3a\u66f4\u53ef\u9760\u7684\u6570\u636e\u5206\u6790\u4ee3\u7406\uff0c\u9700\u8981\u5728\u6307\u4ee4\u9075\u5faa\u548c\u63a8\u7406\u4e4b\u95f4\u8fbe\u5230\u5e73\u8861\u3002"}}
{"id": "2505.18277", "pdf": "https://arxiv.org/pdf/2505.18277", "abs": "https://arxiv.org/abs/2505.18277", "authors": ["Joshua S. Rule", "Steven T. Piantadosi"], "title": "The end of radical concept nativism", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Though humans seem to be remarkable learners, arguments in cognitive science\nand philosophy of mind have long maintained that learning something\nfundamentally new is impossible. Specifically, Jerry Fodor's arguments for\nradical concept nativism hold that most, if not all, concepts are innate and\nthat what many call concept learning never actually leads to the acquisition of\nnew concepts. These arguments have deeply affected cognitive science, and many\nbelieve that the counterarguments to radical concept nativism have been either\nunsuccessful or only apply to a narrow class of concepts. This paper first\nreviews the features and limitations of prior arguments. We then identify three\ncritical points - related to issues of expressive power, conceptual structure,\nand concept possession - at which the arguments in favor of radical concept\nnativism diverge from describing actual human cognition. We use ideas from\ncomputer science and information theory to formalize the relevant ideas in ways\nthat are arguably more scientifically productive. We conclude that, as a\nresult, there is an important sense in which people do indeed learn new\nconcepts.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6bd4\u6fc0\u8fdb\u6982\u5ff5\u5929\u8d4b\u8bba\u7684\u7f3a\u9677\uff0c\u6211\u4eec\u8bc1\u660e\u5728\u67d0\u79cd\u610f\u4e49\u4e0a\uff0c\u4eba\u7c7b\u786e\u5b9e\u80fd\u5b66\u4e60\u65b0\u6982\u5ff5\u3002", "motivation": "\u5c3d\u7ba1\u8ba4\u77e5\u79d1\u5b66\u548c\u5fc3\u7075\u54f2\u5b66\u7684\u4e89\u8bba\u957f\u4e45\u4ee5\u6765\u8ba4\u4e3a\u5b66\u4e60\u65b0\u7684\u6982\u5ff5\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a\u6709\u5fc5\u8981\u91cd\u65b0\u5ba1\u89c6\u5e76\u6311\u6218\u8fd9\u79cd\u89c2\u70b9\uff0c\u5c24\u5176\u662f\u5728\u53cd\u9a73\u6fc0\u8fdb\u6982\u5ff5\u5929\u8d4b\u8bba\u7684\u8bba\u70b9\u4e0a\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u7406\u8bba\u7684\u89c2\u70b9\uff0c\u5c06\u76f8\u5173\u89c2\u70b9\u5f62\u5f0f\u5316\uff0c\u4ee5\u671f\u5728\u79d1\u5b66\u4e0a\u66f4\u5177\u6210\u6548\u3002", "result": "\u6211\u4eec\u786e\u5b9a\u4e86\u6709\u5173\u8868\u8ff0\u80fd\u529b\u3001\u6982\u5ff5\u7ed3\u6784\u548c\u6982\u5ff5\u5360\u6709\u7684\u4e09\u4e2a\u5173\u952e\u70b9\uff0c\u8fd9\u4e9b\u70b9\u662f\u6fc0\u8fdb\u6982\u5ff5\u5929\u8d4b\u8bba\u4e0e\u5b9e\u9645\u4eba\u7c7b\u8ba4\u77e5\u63cf\u7ed8\u76f8\u5206\u6b67\u7684\u5730\u65b9\u3002", "conclusion": "\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u4e8b\u5b9e\u4e0a\uff0c\u4eba\u4eec\u786e\u5b9e\u5728\u67d0\u79cd\u91cd\u8981\u7684\u610f\u4e49\u4e0a\u5b66\u4e60\u4e86\u65b0\u6982\u5ff5\u3002"}}
{"id": "2505.18164", "pdf": "https://arxiv.org/pdf/2505.18164", "abs": "https://arxiv.org/abs/2505.18164", "authors": ["Davide Macario", "Hulya Seferoglu", "Erdem Koyuncu"], "title": "Model-Distributed Inference for Large Language Models at the Edge", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),\na novel framework designed to facilitate the deployment of state-of-the-art\nlarge-language models (LLMs) across low-power devices at the edge. This is\naccomplished by dividing the model into multiple partitions, which are then\nassigned to different devices/nodes within the network. These nodes exchange\nintermediate activation vectors via device-to-device links, enabling\ncollaborative computation. To enhance the efficiency of this process, we\npropose the \"recurrent pipeline parallelism\" technique, which reduces idle time\non each device and facilitates parallel inference during the generation of\nmultiple text sequences. By leveraging the combined computational resources of\nmultiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the\nmemory capacity of individual devices, making it possible to perform inference\non low-cost hardware. Furthermore, as the number of participating devices\nincreases, MDI-LLM boosts token generation throughput and reduces memory\nconsumption per device.", "AI": {"tldr": "MDI-LLM\u901a\u8fc7\u6a21\u578b\u5206\u5272\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u90e8\u7f72LLM\uff0c\u4f7f\u7528\u5faa\u73af\u6d41\u6c34\u7ebf\u5e76\u884c\u6280\u672f\u63d0\u9ad8\u6548\u7387\uff0c\u968f\u7740\u8bbe\u5907\u589e\u52a0\u63d0\u5347\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u65b0\u6846\u67b6\u4ee5\u4fc3\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u8fb9\u7f18\u7684\u4f4e\u529f\u7387\u8bbe\u5907\u4e0a\u8fdb\u884c\u90e8\u7f72\u3002", "method": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u90e8\u5206\uff0c\u5e76\u5c06\u8fd9\u4e9b\u90e8\u5206\u5206\u914d\u7ed9\u7f51\u7edc\u4e2d\u7684\u4e0d\u540c\u8bbe\u5907/\u8282\u70b9\u6765\u5b9e\u73b0\u3002\u8282\u70b9\u4e4b\u95f4\u901a\u8fc7\u8bbe\u5907\u95f4\u94fe\u63a5\u4ea4\u6362\u4e2d\u95f4\u6fc0\u6d3b\u5411\u91cf\uff0c\u5b9e\u73b0\u534f\u540c\u8ba1\u7b97\u3002\u4e3a\u4e86\u63d0\u5347\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5faa\u73af\u6d41\u6c34\u7ebf\u5e76\u884c\u201d\u6280\u672f\uff0c\u51cf\u5c11\u6bcf\u4e2a\u8bbe\u5907\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u5e76\u5728\u751f\u6210\u591a\u4e2a\u6587\u672c\u5e8f\u5217\u65f6\u5b9e\u73b0\u5e76\u884c\u63a8\u7406\u3002", "result": "MDI-LLM\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f7f\u5f97\u6bd4\u5355\u53f0\u8bbe\u5907\u5bb9\u91cf\u5927\u7684LLM\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u5f97\u4ee5\u90e8\u7f72\uff0c\u5e76\u968f\u7740\u8bbe\u5907\u6570\u91cf\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u964d\u4f4e\u6bcf\u4e2a\u8bbe\u5907\u7684\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "MDI-LLM\u80fd\u591f\u5229\u7528\u591a\u4e2a\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f7f\u5f97\u5355\u53f0\u8bbe\u5907\u65e0\u6cd5\u5bb9\u7eb3\u7684LLM\u5f97\u4ee5\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u540c\u65f6\uff0c\u968f\u7740\u53c2\u4e0e\u8bbe\u5907\u6570\u91cf\u7684\u589e\u52a0\uff0cMDI-LLM\u63d0\u5347\u4e86\u4ee4\u724c\u751f\u6210\u7684\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u4e86\u5355\u4e2a\u8bbe\u5907\u7684\u5185\u5b58\u6d88\u8017\u3002"}}
{"id": "2505.19034", "pdf": "https://arxiv.org/pdf/2505.19034", "abs": "https://arxiv.org/abs/2505.19034", "authors": ["Teng Liu", "Andreas Morr", "Sebastian Bathiany", "Lana L. Blaschke", "Zhen Qian", "Chan Diao", "Taylor Smith", "Niklas Boers"], "title": "The influence of data gaps and outliers on resilience indicators", "categories": ["nlin.AO", "nlin.CD", "physics.data-an", "physics.geo-ph"], "comment": null, "summary": "The resilience, or stability, of major Earth system components is\nincreasingly threatened by anthropogenic pressures, demanding reliable early\nwarning signals for abrupt and irreversible regime shifts. Widely used\ndata-driven resilience indicators based on variance and autocorrelation detect\n`critical slowing down', a signature of decreasing stability. However, the\ninterpretation of these indicators is hampered by poorly understood\ninterdependencies and their susceptibility to common data issues such as\nmissing values and outliers. Here, we establish a rigorous mathematical\nanalysis of the statistical dependency between variance- and\nautocorrelation-based resilience indicators, revealing that their agreement is\nfundamentally driven by the time series' initial data point. Using synthetic\nand empirical data, we demonstrate that missing values substantially weaken\nindicator agreement, while outliers introduce systematic biases that lead to\noverestimation of resilience based on temporal autocorrelation. Our results\nprovide a necessary and rigorous foundation for preprocessing strategies and\naccuracy assessments across the growing number of disciplines that use\nreal-world data to infer changes in system resilience.", "AI": {"tldr": "\u6587\u7ae0\u63a2\u8ba8\u4e86\u5173\u952e\u97e7\u6027\u6307\u6807\u7684\u6570\u5b66\u4f9d\u8d56\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u7f3a\u5931\u6570\u636e\u548c\u5f02\u5e38\u503c\u5bf9\u6307\u6807\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5730\u7403\u7cfb\u7edf\u7ec4\u4ef6\u7684\u7a33\u5b9a\u6027\u53d7\u5230\u4eba\u7c7b\u6d3b\u52a8\u538b\u529b\u7684\u5a01\u80c1\uff0c\u9700\u8981\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u6765\u9884\u6d4b\u7a81\u7136\u4e14\u4e0d\u53ef\u9006\u7684\u4f53\u5236\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5408\u6210\u53ca\u5b9e\u8bc1\u6570\u636e\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u7f3a\u5931\u503c\u53ca\u5f02\u5e38\u503c\u5bf9\u97e7\u6027\u6307\u6807\u4e00\u81f4\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u6210\u679c\u4e3a\u5236\u5b9a\u6570\u636e\u9884\u5904\u7406\u7b56\u7565\u548c\u7cbe\u786e\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u591a\u4e2a\u5b66\u79d1\u9886\u57df\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u63a8\u65ad\u7cfb\u7edf\u97e7\u6027\u7684\u53d8\u5316\u3002", "conclusion": "\u6211\u4eec\u901a\u8fc7\u6570\u5b66\u5206\u6790\u63ed\u793a\u4e86\u65b9\u5dee\u548c\u81ea\u76f8\u5173\u6027\u6307\u6807\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\uff0c\u5f3a\u8c03\u5b83\u4eec\u7684\u4e00\u81f4\u6027\u4e3b\u8981\u53d7\u65f6\u95f4\u5e8f\u5217\u7684\u521d\u59cb\u6570\u636e\u70b9\u9a71\u52a8\u3002\u7f3a\u5931\u6570\u636e\u548c\u5f02\u5e38\u503c\u5bf9\u6307\u6807\u7684\u4e00\u81f4\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5f02\u5e38\u503c\u4f1a\u5bfc\u81f4\u901a\u8fc7\u65f6\u95f4\u81ea\u76f8\u5173\u6027\u7684\u97e7\u6027\u8fc7\u9ad8\u4f30\u8ba1\u3002"}}
{"id": "2505.18228", "pdf": "https://arxiv.org/pdf/2505.18228", "abs": "https://arxiv.org/abs/2505.18228", "authors": ["Timotheus Kampik"], "title": "Implementing Agents in JavaScript", "categories": ["cs.MA"], "comment": "This chapter will eventually by published in the book \"Agents and\n  Multi-Agent Systems Development -- Platforms, Toolkits, Technologies\", edited\n  by Collier, Mascardi, and Ricci", "summary": "This chapter gives an introduction to agent-oriented programming in\nJavaScript. It provides an example-based walk-through of how to implement\nabstractions for reasoning loop agents in vanilla JavaScript. The initial\nexample is used as a stepping stone for explaining how to implement slightly\nmore advanced agents and multi-agent systems using JS-son, a JavaScript library\nfor agent-oriented programming. In this context, the chapter also explains how\nto integrate reasoning loop agents with generative AI\ntechnologies--specifically, large language models. Finally, application\nscenarios in several technology ecosystems and future research directions are\nsketched.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86JavaScript\u4e2d\u7684\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\u53ca\u5176\u5728\u591a\u4e2a\u6280\u672f\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5f15\u5165\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\uff0c\u901a\u8fc7JavaScript\u5b9e\u73b0\u63a8\u7406\u5faa\u73af\u4ee3\u7406\u7684\u62bd\u8c61\u3002", "method": "\u8fdb\u884c\u57fa\u4e8e\u5b9e\u4f8b\u7684\u8bb2\u89e3\uff0c\u4f7f\u7528JS-son\u5e93\u6765\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u5f0fAI\u6280\u672f\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u5728\u591a\u4e2a\u6280\u672f\u751f\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "conclusion": "JavaScript\u7684\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\u5bf9\u4e8e\u7cfb\u7edf\u8bbe\u8ba1\u548cAI\u6574\u5408\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u94fe\u8d8a\u957f\u4fe1\u606f\u504f\u5dee\u8d8a\u5927\u4e14\u4fe1\u606f\u589e\u76ca\u8d8a\u5c0f\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u52a8\u6001\u505c\u6b62\u63a8\u7406\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u7136\u5728\u591a\u6b65\u63a8\u7406\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u662f\u901a\u5e38\u751f\u6210\u7684\u63a8\u7406\u94fe\u8fc7\u957f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7528\u4e8e\u91cf\u5316\u4fe1\u606f\u504f\u5dee\u548c\u4fe1\u606f\u589e\u76ca\u7684\u6307\u6807\uff1aInfoBias\u548cInfoGain\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u601d\u7ef4\u7b56\u7565\uff0c\u52a8\u6001\u505c\u6b62\u63a8\u7406\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5728QwQ-32B\u4e0a\u7684\u516d\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\uff0c\u5c06\u6807\u8bb0\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8650.80\uff05\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e861.10\uff05\u3002", "conclusion": "\u71b5\u57fa\u81ea\u9002\u5e94\u601d\u7ef4\u7b56\u7565\u5728\u63d0\u9ad8\u591a\u6b65\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u51cf\u5c11\u751f\u6210\u7684\u63a8\u7406\u94fe\u957f\u5ea6\u3002\u4e0e\u9ed8\u8ba4\u6a21\u5f0f\u76f8\u6bd4\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u8be5\u7b56\u7565\u63d0\u9ad8\u4e861.10%\u7684\u5e73\u5747\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4e8650.80%\u7684\u6807\u8bb0\u4f7f\u7528\u91cf\u3002"}}
