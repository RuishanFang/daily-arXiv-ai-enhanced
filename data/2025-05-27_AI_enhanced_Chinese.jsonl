{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86\u5229\u7528NLP\u548c\u793e\u533a\u53c2\u4e0e\u6765\u4fdd\u62a4\u6fd2\u5371\u8bed\u8a00\u79d1\u66fc\u5947\uff0c\u901a\u8fc7\u5c11\u91cf\u5b9e\u4f8b\u63d0\u793a\u63d0\u9ad8\u4e86\u8bed\u8a00\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u6fd2\u5371\u8bed\u8a00\u5728\u6570\u5b57\u9886\u57df\u7684\u6392\u65a5\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u8bed\u8a00\u590d\u5174\u5de5\u4f5c\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u624b\u52a8\u6574\u7406\u7684412\u4e2a\u77ed\u8bed\u6570\u636e\u96c6\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5e76\u5bf9GPT-4o\u548cGPT-4o-mini\u8fdb\u884c\u8bed\u8a00\u8bc6\u522b\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5728\u96f6\u6837\u672c\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u5bf9\u79d1\u66fc\u5947\u8bed\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u63d0\u793a\uff08\u4ec5\u4e94\u4e2a\u4f8b\u5b50\uff09\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5c11\u91cf\u5b9e\u4f8b\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u79d1\u66fc\u5947\u8bed\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u8bc1\u660e\u4e86NLP\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0BERT\u7c7b\u6a21\u578b\u5728\u4e00\u4e9b\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eLLMs\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u7684\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u3002", "motivation": "\u8d28\u7591\u5f53\u524d\u6d41\u884c\u7684'\u4ee5LLM\u4e3a\u4e2d\u5fc3'\u7684\u8d8b\u52bf\uff0c\u5c55\u793a\u4f20\u7edfBERT\u7c7b\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u6f5c\u5728\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5728\u516d\u4e2a\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u4e09\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u5305\u62ec\u5fae\u8c03BERT\u6a21\u578b\u3001LLM\u5185\u90e8\u72b6\u6001\u5229\u7528\u548c\u96f6\u6837\u672c\u63a8\u7406\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cBERT\u7c7b\u6a21\u578b\u5728\u6a21\u5f0f\u9a71\u52a8\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u800cLLMs\u5219\u5728\u9700\u8981\u6df1\u5ea6\u8bed\u4e49\u6216\u4e16\u754c\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u5360\u4f18\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTaMAS\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "\u4f20\u7edf\u7684BERT\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u4f18\u4e8eLLMs\uff0c\u5c24\u5176\u662f\u5728\u6a21\u5f0f\u9a71\u52a8\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors.", "AI": {"tldr": "CoMet\u6846\u67b6\u63d0\u5347LLMs\u5904\u7406\u9690\u55bb\u80fd\u529b\uff0c\u589e\u5f3a\u6218\u7565\u6c9f\u901a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4ee3\u7406\u8bed\u8a00\u6e38\u620f\u4e2d\u5904\u7406\u9690\u55bb\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u5176\u6218\u7565\u6027\u6c9f\u901a\u7684\u6548\u7387\u3002", "method": "CoMet\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u5047\u8bbe\u7684\u9690\u55bb\u63a8\u7406\u5668\u548c\u9690\u55bb\u751f\u6210\u5668\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u77e5\u8bc6\u6574\u5408\u8fdb\u884c\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCoMet\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u5728\u201cUndercover\u201d\u548c\u201cAdversarial Taboo\u201d\u6e38\u620f\u4e2d\u4f7f\u7528\u9690\u55bb\u8fdb\u884c\u6218\u7565\u6c9f\u901a\u7684\u80fd\u529b\u3002", "conclusion": "CoMet\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4ee3\u7406\u8bed\u8a00\u6e38\u620f\u4e2d\u4f7f\u7528\u9690\u55bb\u8fdb\u884c\u6218\u7565\u6027\u4ea4\u6d41\u7684\u80fd\u529b\u3002"}}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning.", "AI": {"tldr": "\u5f15\u5165IDA-Bench\u4ee5\u8bc4\u4f30LLM\u5728\u591a\u8f6e\u6570\u636e\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728<50%\u7684\u4efb\u52a1\u4e2d\u6210\u529f\uff0c\u9700\u6539\u8fdb\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u4e86\u6570\u636e\u5206\u6790\u9886\u57df\u4e2d\u7684\u8fed\u4ee3\u6027\u8d28\uff0c\u4e13\u5bb6\u7684\u51b3\u7b56\u968f\u7740\u5bf9\u6570\u636e\u96c6\u7684\u6df1\u5165\u7406\u89e3\u800c\u6f14\u53d8\u3002\u6b64\u9879\u5de5\u4f5c\u7684\u76ee\u7684\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165IDA-Bench\uff0c\u8bc4\u4ef7LLM\u4ee3\u7406\u5728\u591a\u8f6e\u4e92\u52a8\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002\u4efb\u52a1\u7531\u590d\u6742Kaggle\u7b14\u8bb0\u672c\u6d3e\u751f\uff0c\u5e76\u901a\u8fc7LLM\u6a21\u62df\u7528\u6237\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u3002\u901a\u8fc7\u5c06\u4ee3\u7406\u6700\u7ec8\u6570\u503c\u8f93\u51fa\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u6765\u5224\u65ad\u5176\u8868\u73b0\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude-3.7-thinking\uff09\uff0c\u5728<50%\u7684\u4efb\u52a1\u4e2d\u6210\u529f\uff0c\u7a81\u663e\u4e86\u5728\u5355\u8f6e\u6d4b\u8bd5\u4e2d\u672a\u73b0\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LLMs\u9700\u8981\u63d0\u9ad8\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\u4ee5\u6210\u4e3a\u66f4\u53ef\u9760\u7684\u6570\u636e\u5206\u6790\u4ee3\u7406\uff0c\u9700\u8981\u5728\u6307\u4ee4\u9075\u5faa\u548c\u63a8\u7406\u4e4b\u95f4\u8fbe\u5230\u5e73\u8861\u3002"}}
{"id": "2505.18277", "pdf": "https://arxiv.org/pdf/2505.18277", "abs": "https://arxiv.org/abs/2505.18277", "authors": ["Joshua S. Rule", "Steven T. Piantadosi"], "title": "The end of radical concept nativism", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Though humans seem to be remarkable learners, arguments in cognitive science\nand philosophy of mind have long maintained that learning something\nfundamentally new is impossible. Specifically, Jerry Fodor's arguments for\nradical concept nativism hold that most, if not all, concepts are innate and\nthat what many call concept learning never actually leads to the acquisition of\nnew concepts. These arguments have deeply affected cognitive science, and many\nbelieve that the counterarguments to radical concept nativism have been either\nunsuccessful or only apply to a narrow class of concepts. This paper first\nreviews the features and limitations of prior arguments. We then identify three\ncritical points - related to issues of expressive power, conceptual structure,\nand concept possession - at which the arguments in favor of radical concept\nnativism diverge from describing actual human cognition. We use ideas from\ncomputer science and information theory to formalize the relevant ideas in ways\nthat are arguably more scientifically productive. We conclude that, as a\nresult, there is an important sense in which people do indeed learn new\nconcepts.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6bd4\u6fc0\u8fdb\u6982\u5ff5\u5929\u8d4b\u8bba\u7684\u7f3a\u9677\uff0c\u6211\u4eec\u8bc1\u660e\u5728\u67d0\u79cd\u610f\u4e49\u4e0a\uff0c\u4eba\u7c7b\u786e\u5b9e\u80fd\u5b66\u4e60\u65b0\u6982\u5ff5\u3002", "motivation": "\u5c3d\u7ba1\u8ba4\u77e5\u79d1\u5b66\u548c\u5fc3\u7075\u54f2\u5b66\u7684\u4e89\u8bba\u957f\u4e45\u4ee5\u6765\u8ba4\u4e3a\u5b66\u4e60\u65b0\u7684\u6982\u5ff5\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a\u6709\u5fc5\u8981\u91cd\u65b0\u5ba1\u89c6\u5e76\u6311\u6218\u8fd9\u79cd\u89c2\u70b9\uff0c\u5c24\u5176\u662f\u5728\u53cd\u9a73\u6fc0\u8fdb\u6982\u5ff5\u5929\u8d4b\u8bba\u7684\u8bba\u70b9\u4e0a\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u7406\u8bba\u7684\u89c2\u70b9\uff0c\u5c06\u76f8\u5173\u89c2\u70b9\u5f62\u5f0f\u5316\uff0c\u4ee5\u671f\u5728\u79d1\u5b66\u4e0a\u66f4\u5177\u6210\u6548\u3002", "result": "\u6211\u4eec\u786e\u5b9a\u4e86\u6709\u5173\u8868\u8ff0\u80fd\u529b\u3001\u6982\u5ff5\u7ed3\u6784\u548c\u6982\u5ff5\u5360\u6709\u7684\u4e09\u4e2a\u5173\u952e\u70b9\uff0c\u8fd9\u4e9b\u70b9\u662f\u6fc0\u8fdb\u6982\u5ff5\u5929\u8d4b\u8bba\u4e0e\u5b9e\u9645\u4eba\u7c7b\u8ba4\u77e5\u63cf\u7ed8\u76f8\u5206\u6b67\u7684\u5730\u65b9\u3002", "conclusion": "\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u4e8b\u5b9e\u4e0a\uff0c\u4eba\u4eec\u786e\u5b9e\u5728\u67d0\u79cd\u91cd\u8981\u7684\u610f\u4e49\u4e0a\u5b66\u4e60\u4e86\u65b0\u6982\u5ff5\u3002"}}
{"id": "2505.18164", "pdf": "https://arxiv.org/pdf/2505.18164", "abs": "https://arxiv.org/abs/2505.18164", "authors": ["Davide Macario", "Hulya Seferoglu", "Erdem Koyuncu"], "title": "Model-Distributed Inference for Large Language Models at the Edge", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),\na novel framework designed to facilitate the deployment of state-of-the-art\nlarge-language models (LLMs) across low-power devices at the edge. This is\naccomplished by dividing the model into multiple partitions, which are then\nassigned to different devices/nodes within the network. These nodes exchange\nintermediate activation vectors via device-to-device links, enabling\ncollaborative computation. To enhance the efficiency of this process, we\npropose the \"recurrent pipeline parallelism\" technique, which reduces idle time\non each device and facilitates parallel inference during the generation of\nmultiple text sequences. By leveraging the combined computational resources of\nmultiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the\nmemory capacity of individual devices, making it possible to perform inference\non low-cost hardware. Furthermore, as the number of participating devices\nincreases, MDI-LLM boosts token generation throughput and reduces memory\nconsumption per device.", "AI": {"tldr": "MDI-LLM\u901a\u8fc7\u6a21\u578b\u5206\u5272\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u90e8\u7f72LLM\uff0c\u4f7f\u7528\u5faa\u73af\u6d41\u6c34\u7ebf\u5e76\u884c\u6280\u672f\u63d0\u9ad8\u6548\u7387\uff0c\u968f\u7740\u8bbe\u5907\u589e\u52a0\u63d0\u5347\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u65b0\u6846\u67b6\u4ee5\u4fc3\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u8fb9\u7f18\u7684\u4f4e\u529f\u7387\u8bbe\u5907\u4e0a\u8fdb\u884c\u90e8\u7f72\u3002", "method": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u90e8\u5206\uff0c\u5e76\u5c06\u8fd9\u4e9b\u90e8\u5206\u5206\u914d\u7ed9\u7f51\u7edc\u4e2d\u7684\u4e0d\u540c\u8bbe\u5907/\u8282\u70b9\u6765\u5b9e\u73b0\u3002\u8282\u70b9\u4e4b\u95f4\u901a\u8fc7\u8bbe\u5907\u95f4\u94fe\u63a5\u4ea4\u6362\u4e2d\u95f4\u6fc0\u6d3b\u5411\u91cf\uff0c\u5b9e\u73b0\u534f\u540c\u8ba1\u7b97\u3002\u4e3a\u4e86\u63d0\u5347\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5faa\u73af\u6d41\u6c34\u7ebf\u5e76\u884c\u201d\u6280\u672f\uff0c\u51cf\u5c11\u6bcf\u4e2a\u8bbe\u5907\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u5e76\u5728\u751f\u6210\u591a\u4e2a\u6587\u672c\u5e8f\u5217\u65f6\u5b9e\u73b0\u5e76\u884c\u63a8\u7406\u3002", "result": "MDI-LLM\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f7f\u5f97\u6bd4\u5355\u53f0\u8bbe\u5907\u5bb9\u91cf\u5927\u7684LLM\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u5f97\u4ee5\u90e8\u7f72\uff0c\u5e76\u968f\u7740\u8bbe\u5907\u6570\u91cf\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u964d\u4f4e\u6bcf\u4e2a\u8bbe\u5907\u7684\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "MDI-LLM\u80fd\u591f\u5229\u7528\u591a\u4e2a\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f7f\u5f97\u5355\u53f0\u8bbe\u5907\u65e0\u6cd5\u5bb9\u7eb3\u7684LLM\u5f97\u4ee5\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u540c\u65f6\uff0c\u968f\u7740\u53c2\u4e0e\u8bbe\u5907\u6570\u91cf\u7684\u589e\u52a0\uff0cMDI-LLM\u63d0\u5347\u4e86\u4ee4\u724c\u751f\u6210\u7684\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u4e86\u5355\u4e2a\u8bbe\u5907\u7684\u5185\u5b58\u6d88\u8017\u3002"}}
{"id": "2505.19034", "pdf": "https://arxiv.org/pdf/2505.19034", "abs": "https://arxiv.org/abs/2505.19034", "authors": ["Teng Liu", "Andreas Morr", "Sebastian Bathiany", "Lana L. Blaschke", "Zhen Qian", "Chan Diao", "Taylor Smith", "Niklas Boers"], "title": "The influence of data gaps and outliers on resilience indicators", "categories": ["nlin.AO", "nlin.CD", "physics.data-an", "physics.geo-ph"], "comment": null, "summary": "The resilience, or stability, of major Earth system components is\nincreasingly threatened by anthropogenic pressures, demanding reliable early\nwarning signals for abrupt and irreversible regime shifts. Widely used\ndata-driven resilience indicators based on variance and autocorrelation detect\n`critical slowing down', a signature of decreasing stability. However, the\ninterpretation of these indicators is hampered by poorly understood\ninterdependencies and their susceptibility to common data issues such as\nmissing values and outliers. Here, we establish a rigorous mathematical\nanalysis of the statistical dependency between variance- and\nautocorrelation-based resilience indicators, revealing that their agreement is\nfundamentally driven by the time series' initial data point. Using synthetic\nand empirical data, we demonstrate that missing values substantially weaken\nindicator agreement, while outliers introduce systematic biases that lead to\noverestimation of resilience based on temporal autocorrelation. Our results\nprovide a necessary and rigorous foundation for preprocessing strategies and\naccuracy assessments across the growing number of disciplines that use\nreal-world data to infer changes in system resilience.", "AI": {"tldr": "\u6587\u7ae0\u63a2\u8ba8\u4e86\u5173\u952e\u97e7\u6027\u6307\u6807\u7684\u6570\u5b66\u4f9d\u8d56\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u7f3a\u5931\u6570\u636e\u548c\u5f02\u5e38\u503c\u5bf9\u6307\u6807\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5730\u7403\u7cfb\u7edf\u7ec4\u4ef6\u7684\u7a33\u5b9a\u6027\u53d7\u5230\u4eba\u7c7b\u6d3b\u52a8\u538b\u529b\u7684\u5a01\u80c1\uff0c\u9700\u8981\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u6765\u9884\u6d4b\u7a81\u7136\u4e14\u4e0d\u53ef\u9006\u7684\u4f53\u5236\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5408\u6210\u53ca\u5b9e\u8bc1\u6570\u636e\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u7f3a\u5931\u503c\u53ca\u5f02\u5e38\u503c\u5bf9\u97e7\u6027\u6307\u6807\u4e00\u81f4\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u6210\u679c\u4e3a\u5236\u5b9a\u6570\u636e\u9884\u5904\u7406\u7b56\u7565\u548c\u7cbe\u786e\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u591a\u4e2a\u5b66\u79d1\u9886\u57df\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u63a8\u65ad\u7cfb\u7edf\u97e7\u6027\u7684\u53d8\u5316\u3002", "conclusion": "\u6211\u4eec\u901a\u8fc7\u6570\u5b66\u5206\u6790\u63ed\u793a\u4e86\u65b9\u5dee\u548c\u81ea\u76f8\u5173\u6027\u6307\u6807\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\uff0c\u5f3a\u8c03\u5b83\u4eec\u7684\u4e00\u81f4\u6027\u4e3b\u8981\u53d7\u65f6\u95f4\u5e8f\u5217\u7684\u521d\u59cb\u6570\u636e\u70b9\u9a71\u52a8\u3002\u7f3a\u5931\u6570\u636e\u548c\u5f02\u5e38\u503c\u5bf9\u6307\u6807\u7684\u4e00\u81f4\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5f02\u5e38\u503c\u4f1a\u5bfc\u81f4\u901a\u8fc7\u65f6\u95f4\u81ea\u76f8\u5173\u6027\u7684\u97e7\u6027\u8fc7\u9ad8\u4f30\u8ba1\u3002"}}
{"id": "2505.18228", "pdf": "https://arxiv.org/pdf/2505.18228", "abs": "https://arxiv.org/abs/2505.18228", "authors": ["Timotheus Kampik"], "title": "Implementing Agents in JavaScript", "categories": ["cs.MA"], "comment": "This chapter will eventually by published in the book \"Agents and\n  Multi-Agent Systems Development -- Platforms, Toolkits, Technologies\", edited\n  by Collier, Mascardi, and Ricci", "summary": "This chapter gives an introduction to agent-oriented programming in\nJavaScript. It provides an example-based walk-through of how to implement\nabstractions for reasoning loop agents in vanilla JavaScript. The initial\nexample is used as a stepping stone for explaining how to implement slightly\nmore advanced agents and multi-agent systems using JS-son, a JavaScript library\nfor agent-oriented programming. In this context, the chapter also explains how\nto integrate reasoning loop agents with generative AI\ntechnologies--specifically, large language models. Finally, application\nscenarios in several technology ecosystems and future research directions are\nsketched.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86JavaScript\u4e2d\u7684\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\u53ca\u5176\u5728\u591a\u4e2a\u6280\u672f\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5f15\u5165\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\uff0c\u901a\u8fc7JavaScript\u5b9e\u73b0\u63a8\u7406\u5faa\u73af\u4ee3\u7406\u7684\u62bd\u8c61\u3002", "method": "\u8fdb\u884c\u57fa\u4e8e\u5b9e\u4f8b\u7684\u8bb2\u89e3\uff0c\u4f7f\u7528JS-son\u5e93\u6765\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u5f0fAI\u6280\u672f\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u5728\u591a\u4e2a\u6280\u672f\u751f\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "conclusion": "JavaScript\u7684\u9762\u5411\u4ee3\u7406\u7f16\u7a0b\u5bf9\u4e8e\u7cfb\u7edf\u8bbe\u8ba1\u548cAI\u6574\u5408\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u94fe\u8d8a\u957f\u4fe1\u606f\u504f\u5dee\u8d8a\u5927\u4e14\u4fe1\u606f\u589e\u76ca\u8d8a\u5c0f\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u52a8\u6001\u505c\u6b62\u63a8\u7406\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u7136\u5728\u591a\u6b65\u63a8\u7406\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u662f\u901a\u5e38\u751f\u6210\u7684\u63a8\u7406\u94fe\u8fc7\u957f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7528\u4e8e\u91cf\u5316\u4fe1\u606f\u504f\u5dee\u548c\u4fe1\u606f\u589e\u76ca\u7684\u6307\u6807\uff1aInfoBias\u548cInfoGain\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u601d\u7ef4\u7b56\u7565\uff0c\u52a8\u6001\u505c\u6b62\u63a8\u7406\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5728QwQ-32B\u4e0a\u7684\u516d\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\uff0c\u5c06\u6807\u8bb0\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8650.80\uff05\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e861.10\uff05\u3002", "conclusion": "\u71b5\u57fa\u81ea\u9002\u5e94\u601d\u7ef4\u7b56\u7565\u5728\u63d0\u9ad8\u591a\u6b65\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u51cf\u5c11\u751f\u6210\u7684\u63a8\u7406\u94fe\u957f\u5ea6\u3002\u4e0e\u9ed8\u8ba4\u6a21\u5f0f\u76f8\u6bd4\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u8be5\u7b56\u7565\u63d0\u9ad8\u4e861.10%\u7684\u5e73\u5747\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4e8650.80%\u7684\u6807\u8bb0\u4f7f\u7528\u91cf\u3002"}}
{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u4f4e\u6210\u672c\u7684NLP\u65b9\u6cd5\u6765\u652f\u6301\u6fd2\u5371\u8bed\u8a00\u79d1\u66fc\u5947\u8bed\u7684\u4fdd\u5b58\uff0c\u901a\u8fc7\u5c11\u6837\u4f8b\u7684\u63d0\u793a\u6280\u672f\u5927\u5e45\u63d0\u9ad8\u8bed\u8a00\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6fd2\u5371\u8bed\u8a00\u5728\u6570\u5b57\u9886\u57df\u7684\u6392\u65a5\u73b0\u8c61\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u590d\u5174\u5de5\u4f5c\u3002", "method": "\u5f15\u5165\u9996\u4e2a\u5bf9\u79d1\u66fc\u5947\u8bed\u8fdb\u884c\u8ba1\u7b97\u8c03\u67e5\uff0c\u5305\u542b\u624b\u52a8\u6574\u7406\u7684412\u4e2a\u77ed\u8bed\u6570\u636e\u96c6\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5e76\u5229\u7528GPT-4o\u548cGPT-4o-mini\u8fdb\u884c\u8bed\u8a00\u8bc6\u522b\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u96be\u4ee5\u5904\u7406\u79d1\u66fc\u5947\u8bed\uff0c\u4f46\u5728\u63d0\u4f9b\u5c11\u91cf\u6837\u4f8b\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4ec5\u9700\u4e94\u4e2a\u4f8b\u5b50\u5c31\u80fd\u8fbe\u5230\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u5e94\u7528\u76ee\u6807\u660e\u786e\u7684NLP\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u5efa\u7acb\u4e86\u79d1\u66fc\u5947\u8bed\u5728NLP\u9886\u57df\u7684\u57fa\u7840\uff0c\u5e76\u63d0\u5021\u91c7\u7528\u4f18\u5148\u8003\u8651\u53ef\u8bbf\u95ee\u6027\u3001\u6587\u5316\u654f\u611f\u6027\u548c\u793e\u533a\u53c2\u4e0e\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.", "AI": {"tldr": "Study comparing BERT-like models and LLMs for text classification; BERT-like models excel in pattern tasks, LLMs in semantic/world knowledge tasks. Advocates task-driven approach over LLM-centric reliance.", "motivation": "To challenge the prevailing trend of LLM-centric approaches and highlight the potential advantages of traditional BERT-like models in text classification.", "method": "Systematic comparison of BERT-like models fine-tuning, LLM internal state utilization, and zero-shot inference across six high-difficulty datasets. PCA and probing experiments were conducted.", "result": "BERT-like models often outperform LLMs in text classification, depending on the nature of the task.", "conclusion": "BERT-like models excel in pattern-driven tasks, while LLMs dominate tasks requiring deep semantics or world knowledge. A nuanced, task-driven approach is advocated instead of relying solely on LLMs."}}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors.", "AI": {"tldr": "The CoMet framework enables better metaphor processing in LLMs, improving strategic communication in language games.", "motivation": "Many LLMs struggle with interpreting and applying metaphors, which are crucial for strategic communication. This challenge in metaphor processing hinders LLMs' performance in covert communication and semantic evasion within multi-agent language games.", "method": "CoMet integrates a hypothesis-based metaphor reasoner and a metaphor generator to improve LLM-based agents' metaphor processing through self-reflection and knowledge integration.", "result": "CoMet successfully improves the strategic and nuanced quality of interactions involving metaphors in multi-agent language games, as evidenced by enhanced performance in the Undercover and Adversarial Taboo games.", "conclusion": "CoMet significantly enhances the ability of LLM-based agents to engage in strategic communication using metaphors, as demonstrated in language games like Undercover and Adversarial Taboo."}}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning.", "AI": {"tldr": "LLMs\u5728\u591a\u8f6e\u6570\u636e\u5206\u6790\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u672a\u5145\u5206\u8bc4\u4f30\u5176\u80fd\u529b\u3002IDA-Bench\u4e3a\u8bc4\u4f30\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793aLLMs\u5728\u6b64\u9886\u57df\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u5ffd\u89c6\u4e86\u6570\u636e\u9886\u57df\u8fed\u4ee3\u7684\u6027\u8d28\uff0c\u8be5\u9886\u57df\u4e13\u5bb6\u7684\u51b3\u7b56\u968f\u7740\u5bf9\u6570\u636e\u96c6\u7684\u6df1\u5165\u7406\u89e3\u800c\u6f14\u53d8\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u70b9\uff0c\u7814\u7a76\u5f15\u5165\u4e86IDA-Bench\uff0c\u4ee5\u8bc4\u4f30LLM\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7IDA-Bench\uff0c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u4efb\u52a1\u7531\u590d\u6742\u7684Kaggle\u7b14\u8bb0\u672c\u884d\u751f\u800c\u6765\uff0c\u4ee5\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u5f62\u5f0f\u5448\u73b0\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u4ee3\u7406\u7684\u8f93\u51fa\u4e0e\u4eba\u5de5\u57fa\u7ebf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude-3.7-thinking\uff09\u5728\u4e0d\u523050%\u7684\u4efb\u52a1\u4e2d\u6210\u529f\uff0c\u7a81\u663e\u51fa\u5355\u8f6e\u6d4b\u8bd5\u4e2d\u672a\u80fd\u53d1\u73b0\u7684\u9650\u5236\u3002", "conclusion": "\u7814\u7a76\u7a81\u663e\u4e86\u63d0\u9ad8LLMs\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u5efa\u7acb\u66f4\u53ef\u9760\u7684\u6570\u636e\u5206\u6790\u4ee3\u7406\u7684\u5fc5\u8981\u6027\u3002\u5f3a\u8c03\u9700\u5728\u9075\u5faa\u6307\u4ee4\u548c\u63a8\u7406\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2505.18277", "pdf": "https://arxiv.org/pdf/2505.18277", "abs": "https://arxiv.org/abs/2505.18277", "authors": ["Joshua S. Rule", "Steven T. Piantadosi"], "title": "The end of radical concept nativism", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Though humans seem to be remarkable learners, arguments in cognitive science\nand philosophy of mind have long maintained that learning something\nfundamentally new is impossible. Specifically, Jerry Fodor's arguments for\nradical concept nativism hold that most, if not all, concepts are innate and\nthat what many call concept learning never actually leads to the acquisition of\nnew concepts. These arguments have deeply affected cognitive science, and many\nbelieve that the counterarguments to radical concept nativism have been either\nunsuccessful or only apply to a narrow class of concepts. This paper first\nreviews the features and limitations of prior arguments. We then identify three\ncritical points - related to issues of expressive power, conceptual structure,\nand concept possession - at which the arguments in favor of radical concept\nnativism diverge from describing actual human cognition. We use ideas from\ncomputer science and information theory to formalize the relevant ideas in ways\nthat are arguably more scientifically productive. We conclude that, as a\nresult, there is an important sense in which people do indeed learn new\nconcepts.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u5b9e\u8bc1\u6982\u5ff5\u5148\u5929\u8bba\uff0c\u8ba4\u4e3a\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u4fe1\u606f\u7406\u8bba\u548c\u8ba1\u7b97\u539f\u7406\u5b66\u4e60\u65b0\u7684\u6982\u5ff5\u3002", "motivation": "\u4f20\u7edf\u7684\u5b9e\u8bc1\u6982\u5ff5\u5148\u5929\u8bba\u5bf9\u8ba4\u77e5\u79d1\u5b66\u4ea7\u751f\u4e86\u6df1\u8fdc\u5f71\u54cd\uff0c\u4f46\u53cd\u5bf9\u89c2\u70b9\u9c9c\u6709\u6210\u529f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e9b\u8bba\u70b9\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u66f4\u5177\u751f\u4ea7\u529b\u7684\u79d1\u5b66\u89c6\u89d2\u6765\u5ba1\u89c6\u4eba\u7c7b\u7684\u6982\u5ff5\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u7406\u8bba\u4e2d\u7684\u601d\u60f3\uff0c\u4ee5\u66f4\u79d1\u5b66\u7684\u65b9\u5f0f\u5f62\u5f0f\u5316\u76f8\u5173\u89c2\u70b9\uff0c\u4ece\u800c\u63ed\u793a\u4e0e\u4f20\u7edf\u89c2\u5ff5\u4e0d\u540c\u7684\u5173\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e8b\u5b9e\u3002", "result": "\u8bba\u8bc1\u8868\u660e\u4eba\u7c7b\u5728\u67d0\u79cd\u91cd\u8981\u610f\u4e49\u4e0a\u80fd\u591f\u5b66\u4e60\u65b0\u7684\u6982\u5ff5\uff0c\u6311\u6218\u4e86\u5b9e\u8bc1\u6982\u5ff5\u5148\u5929\u8bba\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u63cf\u8ff0\u3002", "conclusion": "\u4eba\u7c7b\u786e\u5b9e\u53ef\u4ee5\u5b66\u4e60\u65b0\u7684\u6982\u5ff5\uff0c\u5c3d\u7ba1\u4f20\u7edf\u7684\u5b9e\u8bc1\u6982\u5ff5\u5148\u5929\u8bba\u5f3a\u8c03\u5927\u591a\u6570\u6982\u5ff5\u662f\u5148\u5929\u7684\uff0c\u8ba4\u4e3a\u6982\u5ff5\u5b66\u4e60\u4e0d\u4f1a\u5bfc\u81f4\u65b0\u6982\u5ff5\u7684\u83b7\u53d6\u3002"}}
{"id": "2505.18164", "pdf": "https://arxiv.org/pdf/2505.18164", "abs": "https://arxiv.org/abs/2505.18164", "authors": ["Davide Macario", "Hulya Seferoglu", "Erdem Koyuncu"], "title": "Model-Distributed Inference for Large Language Models at the Edge", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),\na novel framework designed to facilitate the deployment of state-of-the-art\nlarge-language models (LLMs) across low-power devices at the edge. This is\naccomplished by dividing the model into multiple partitions, which are then\nassigned to different devices/nodes within the network. These nodes exchange\nintermediate activation vectors via device-to-device links, enabling\ncollaborative computation. To enhance the efficiency of this process, we\npropose the \"recurrent pipeline parallelism\" technique, which reduces idle time\non each device and facilitates parallel inference during the generation of\nmultiple text sequences. By leveraging the combined computational resources of\nmultiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the\nmemory capacity of individual devices, making it possible to perform inference\non low-cost hardware. Furthermore, as the number of participating devices\nincreases, MDI-LLM boosts token generation throughput and reduces memory\nconsumption per device.", "AI": {"tldr": "MDI-LLM\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5206\u533a\u4e0e\u534f\u4f5c\u8ba1\u7b97\uff0c\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u6846\u67b6\u4f7f\u5f97\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u8fb9\u7f18\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u201c\u9012\u5f52\u6d41\u6c34\u7ebf\u5e76\u884c\u5316\u201d\u6280\u672f\uff0c\u51cf\u5c11\u8bbe\u5907\u7a7a\u95f2\u65f6\u95f4\u5e76\u5b9e\u73b0\u5e76\u884c\u63a8\u7406\u3002\u901a\u8fc7\u8bbe\u5907\u95f4\u4ea4\u6362\u6fc0\u6d3b\u5411\u91cf\uff0c\u8fdb\u884c\u534f\u540c\u8ba1\u7b97\u3002", "result": "MDI-LLM\u5728\u589e\u52a0\u53c2\u4e0e\u8bbe\u5907\u6570\u91cf\u65f6\uff0c\u63d0\u9ad8\u4e86\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\uff0c\u51cf\u5c11\u4e86\u6bcf\u4e2a\u8bbe\u5907\u7684\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "MDI-LLM\u63d0\u5347\u4e86\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u8bbe\u5907\u95f4\u534f\u4f5c\u5b9e\u73b0\u8d85\u8d8a\u5355\u4e00\u8bbe\u5907\u5185\u5b58\u5bb9\u91cf\u7684\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002"}}
{"id": "2505.19034", "pdf": "https://arxiv.org/pdf/2505.19034", "abs": "https://arxiv.org/abs/2505.19034", "authors": ["Teng Liu", "Andreas Morr", "Sebastian Bathiany", "Lana L. Blaschke", "Zhen Qian", "Chan Diao", "Taylor Smith", "Niklas Boers"], "title": "The influence of data gaps and outliers on resilience indicators", "categories": ["nlin.AO", "nlin.CD", "physics.data-an", "physics.geo-ph"], "comment": null, "summary": "The resilience, or stability, of major Earth system components is\nincreasingly threatened by anthropogenic pressures, demanding reliable early\nwarning signals for abrupt and irreversible regime shifts. Widely used\ndata-driven resilience indicators based on variance and autocorrelation detect\n`critical slowing down', a signature of decreasing stability. However, the\ninterpretation of these indicators is hampered by poorly understood\ninterdependencies and their susceptibility to common data issues such as\nmissing values and outliers. Here, we establish a rigorous mathematical\nanalysis of the statistical dependency between variance- and\nautocorrelation-based resilience indicators, revealing that their agreement is\nfundamentally driven by the time series' initial data point. Using synthetic\nand empirical data, we demonstrate that missing values substantially weaken\nindicator agreement, while outliers introduce systematic biases that lead to\noverestimation of resilience based on temporal autocorrelation. Our results\nprovide a necessary and rigorous foundation for preprocessing strategies and\naccuracy assessments across the growing number of disciplines that use\nreal-world data to infer changes in system resilience.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u65b9\u5dee\u548c\u81ea\u76f8\u5173\u7684\u5f39\u6027\u6307\u6807\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u53d7\u5230\u65f6\u95f4\u5e8f\u5217\u521d\u59cb\u6570\u636e\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u7f3a\u5931\u503c\u548c\u5f02\u5e38\u503c\u5bf9\u8fd9\u4e9b\u6307\u6807\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4eba\u7c7b\u6d3b\u52a8\u7684\u538b\u529b\u65e5\u76ca\u589e\u52a0\uff0c\u5730\u7403\u7cfb\u7edf\u7684\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\u7684\u7a33\u5b9a\u6027\u53d7\u5230\u5a01\u80c1\uff0c\u8feb\u5207\u9700\u8981\u53ef\u9760\u7684\u9884\u8b66\u4fe1\u53f7\u6765\u5e94\u5bf9\u7a81\u7136\u548c\u4e0d\u53ef\u9006\u8f6c\u7684\u53d8\u5316\u3002\u5f53\u524d\u7684\u6307\u6807\u7531\u4e8e\u6570\u636e\u95ee\u9898\u800c\u96be\u4ee5\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u66f4\u4e25\u8c28\u7684\u5206\u6790\u3002", "method": "\u8fdb\u884c\u4e25\u683c\u7684\u6570\u5b66\u5206\u6790\u6765\u7814\u7a76\u65b9\u5dee\u548c\u81ea\u76f8\u5173\u5f39\u6027\u6307\u6807\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u6570\u636e\u548c\u7ecf\u9a8c\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7f3a\u5931\u503c\u663e\u8457\u524a\u5f31\u4e86\u6307\u6807\u7684\u4e00\u81f4\u6027\uff0c\u800c\u5f02\u5e38\u503c\u4f1a\u5f15\u5165\u7cfb\u7edf\u504f\u5dee\uff0c\u5bfc\u81f4\u57fa\u4e8e\u65f6\u95f4\u81ea\u76f8\u5173\u7684\u5f39\u6027\u9ad8\u4f30\u3002", "conclusion": "\u901a\u8fc7\u6570\u5b66\u5206\u6790\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u65b9\u5dee\u548c\u81ea\u76f8\u5173\u7684\u5f39\u6027\u6307\u6807\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u6307\u6807\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u53d7\u5230\u65f6\u95f4\u5e8f\u5217\u521d\u59cb\u6570\u636e\u70b9\u7684\u9a71\u52a8\u3002\u8be5\u7814\u7a76\u4e3a\u5404\u79cd\u5b66\u79d1\u4f7f\u7528\u5b9e\u9645\u6570\u636e\u63a8\u65ad\u7cfb\u7edf\u5f39\u6027\u53d8\u5316\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u9884\u5904\u7406\u7b56\u7565\u548c\u51c6\u786e\u6027\u8bc4\u4f30\u4f9d\u636e\u3002"}}
{"id": "2505.18228", "pdf": "https://arxiv.org/pdf/2505.18228", "abs": "https://arxiv.org/abs/2505.18228", "authors": ["Timotheus Kampik"], "title": "Implementing Agents in JavaScript", "categories": ["cs.MA"], "comment": "This chapter will eventually by published in the book \"Agents and\n  Multi-Agent Systems Development -- Platforms, Toolkits, Technologies\", edited\n  by Collier, Mascardi, and Ricci", "summary": "This chapter gives an introduction to agent-oriented programming in\nJavaScript. It provides an example-based walk-through of how to implement\nabstractions for reasoning loop agents in vanilla JavaScript. The initial\nexample is used as a stepping stone for explaining how to implement slightly\nmore advanced agents and multi-agent systems using JS-son, a JavaScript library\nfor agent-oriented programming. In this context, the chapter also explains how\nto integrate reasoning loop agents with generative AI\ntechnologies--specifically, large language models. Finally, application\nscenarios in several technology ecosystems and future research directions are\nsketched.", "AI": {"tldr": "\u672c\u7ae0\u4ecb\u7ecd\u5982\u4f55\u5728JavaScript\u4e2d\u5b9e\u73b0\u548c\u5e94\u7528\u63a8\u7406\u5faa\u73af\u4ee3\u7406\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u6027AI\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u5982\u4f55\u5728JavaScript\u4e2d\u5b9e\u73b0\u9762\u5411\u4ee3\u7406\u7684\u7f16\u7a0b\uff0c\u5c24\u5176\u662f\u7ed3\u5408\u751f\u6210\u6027AI\u6280\u672f\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5b9e\u4f8b\u8bb2\u89e3\u5982\u4f55\u5728JavaScript\u4e2d\u5b9e\u73b0\u63a8\u7406\u5faa\u73af\u4ee3\u7406\u7684\u62bd\u8c61\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f7f\u7528JS-son\u5e93\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u5c55\u793a\u4e86\u5728\u591a\u4e2a\u6280\u672f\u751f\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u63a8\u7406\u5faa\u73af\u4ee3\u7406\u7684\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fd9\u4e00\u7ae0\u4ecb\u7ecd\u4e86\u4f7f\u7528JavaScript\u8fdb\u884c\u9762\u5411\u4ee3\u7406\u7684\u7f16\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528JS-son\u5e93\u6784\u5efa\u66f4\u9ad8\u7ea7\u7684\u4ee3\u7406\u53ca\u591a\u4ee3\u7406\u7cfb\u7edf\u3002"}}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment.", "AI": {"tldr": "\u901a\u8fc7\u71b5\u57fa\u81ea\u9002\u5e94\u601d\u8003\u7b56\u7565\u6539\u5584\u591a\u6b65\u63a8\u7406\u6548\u7387\uff0c\u5e73\u8861\u63a8\u7406\u94fe\u957f\u5ea6\u4e0e\u8bed\u4e49\u6548\u7387\uff0c\u5728\u8017\u8d39\u66f4\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u57fa\u7840\u4e0a\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5206\u6790\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u7814\u7a76\u63a8\u7406\u94fe\u957f\u5ea6\u4e0e\u8bed\u4e49\u6548\u7387\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u65e8\u5728\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\u63d0\u5347\u63a8\u7406\u8fc7\u7a0b\u7684\u6548\u7387\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u504f\u5dee\uff08InfoBias\uff09\u548c\u4fe1\u606f\u589e\u76ca\uff08InfoGain\uff09\u8fd9\u4e24\u4e2a\u6307\u6807\uff0c\u91cf\u5316\u4e0e\u7406\u60f3\u63a8\u7406\u8def\u5f84\u7684\u504f\u5dee\u53ca\u9010\u6b65\u7684\u4fe1\u606f\u8d21\u732e\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u601d\u8003\u7b56\u7565\uff0c\u52a8\u6001\u5730\u5728\u4fe1\u5fc3\u8db3\u591f\u9ad8\u65f6\u505c\u6b62\u63a8\u7406\u3002", "result": "\u76f8\u6bd4\u4e8e\u4f20\u7edf\u601d\u8003\u65b9\u5f0f\uff0c\u81ea\u9002\u5e94\u601d\u8003\u7b56\u7565\u5728\u4e0d\u540c\u7c7b\u578b\u548c\u96be\u5ea6\u7684\u516d\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cQwQ-32B\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.10%\uff0c\u6807\u8bb0\u4f7f\u7528\u91cf\u51cf\u5c1150.80%\u3002", "conclusion": "\u71b5\u57fa\u81ea\u9002\u5e94\u601d\u8003\u7b56\u7565\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8fc7\u4f20\u7edf\u8303\u5f0f\uff0c\u5c24\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u73b0\u51fa\u66f4\u4e3a\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.18325", "pdf": "https://arxiv.org/pdf/2505.18325", "abs": "https://arxiv.org/abs/2505.18325", "authors": ["Licheng Pan", "Yongqi Tong", "Xin Zhang", "Xiaolu Zhang", "Jun Zhou", "Zhixuan Chu"], "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries-a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models'safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios.We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets will be released at\nhttps://anonymous.4open.science/r/RASS-80D3.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u63d0\u51faRASS\u6846\u67b6\u901a\u8fc7\u8fb9\u754c\u5bf9\u9f50\u63d0\u793a\u6709\u6548\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u8bed\u8a00\u6269\u5c55\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u5e38\u56e0\u8fc7\u4e8e\u4fdd\u5b88\u7684\u5b89\u5168\u5bf9\u9f50\u800c\u62d2\u7edd\u56de\u7b54\u5408\u7406\u7684\u95ee\u8be2\uff0c\u8fd9\u662f\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u6a21\u578b\u7684\u5b89\u5168\u51b3\u7b56\u8fb9\u754c\uff0c\u8fd0\u7528\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5f15\u5bfc\u5411\u91cf\u7b56\u7565\u6027\u8bc6\u522b\u5e76\u751f\u6210\u8fb9\u754c\u5bf9\u9f50\u7684\u63d0\u793a\u8bed\uff0c\u4ece\u800c\u7f13\u89e3\u8fc7\u5ea6\u62d2\u7edd\u73b0\u8c61\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8fc7\u5ea6\u62d2\u7edd\u4e0e\u6a21\u578b\u5728\u5b89\u5168\u8fb9\u754c\u533a\u57df\u7684\u9519\u4f4d\u5bc6\u5207\u76f8\u5173\u3002RASS\u6846\u67b6\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u548c\u9009\u62e9\u5b89\u5168\u8fb9\u754c\u9644\u8fd1\u7684\u63d0\u793a\u8bed\u6765\u51cf\u8f7b\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u4e14\u80fd\u591f\u6269\u5c55\u5230\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RASS\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u5177\u5907\u591a\u8bed\u8a00\u9002\u5e94\u6027\u3002"}}
{"id": "2505.18166", "pdf": "https://arxiv.org/pdf/2505.18166", "abs": "https://arxiv.org/abs/2505.18166", "authors": ["Jacob Sander", "David Moe", "Achraf Cohen", "Brent Venable", "Venkat Dasari", "Brian Jalaian"], "title": "Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression", "categories": ["cs.LG"], "comment": "9 Pages, 2 Figures", "summary": "Modern foundational models are often compressed via a combination of\nstructured pruning and re-training to meet the strict compute, memory, and\nconnectivity constraints of edge deployments. While state-of-the-art pruning\nschemes target the entire Transformer, we adopt a simple, layer-wise L2-norm\npruning on only the MLP blocks as a fixed baseline. Our focus is not on\nachieving maximal compression, but on isolating the impact of the re-training\nloss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires\nlabeled data, versus (ii) Self-Distillation with KL-divergence, which leverages\nonly teacher logits (no labels) (L2PSD). We evaluate both pipelines on the\nOLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied\nconnectivity scenarios typical of edge networks. Under identical pruning\nschedules, KL-based distillation matches or exceeds CE fine-tuning in test\naccuracy, demonstrating that, even with a basic MLP-only pruning, the choice of\nloss function materially affects compressed model recovery in\nresource-constrained environments.", "AI": {"tldr": "The paper studies the effect of L2-norm pruning and re-training loss functions (Cross-Entropy vs. KL-divergence) on Transformer models, showing that KL-based self-distillation can match or exceed the accuracy of models fine-tuned with Cross-Entropy, even with simple MLP pruning.", "motivation": "The motivation is to examine the impact of different re-training loss functions on model performance after pruning, particularly focusing on resource-constrained environments like edge networks. The study aims to isolate the effects of the re-training loss function rather than achieving maximal compression.", "method": "The method involves simple, layer-wise L2-norm pruning applied solely to the MLP blocks of the Transformer model, followed by two re-training approaches: Fine-tuning with Cross-Entropy (L2PFT) and Self-Distillation with KL-divergence (L2PSD). Both methods are evaluated under identical pruning schedules on the OLMo2-7B-SFT model for CommonsenseQA.", "result": "In the conducted experiments, self-distillation with KL-divergence either matched or outperformed fine-tuning with Cross-Entropy in terms of test accuracy, highlighting the importance of the choice of loss function in recovering model performance post-pruning.", "conclusion": "KL-based self-distillation, which uses only teacher logits and does not require labeled data, matches or exceeds the test accuracy of Cross-Entropy fine-tuning. This demonstrates that the choice of loss function can significantly affect the recovery of compressed models in resource-constrained environments, even with simple MLP-only pruning."}}
{"id": "2505.19275", "pdf": "https://arxiv.org/pdf/2505.19275", "abs": "https://arxiv.org/abs/2505.19275", "authors": ["James P. Crutchfield", "Alexandra Jurgens"], "title": "Agentic Information Theory: Ergodicity and Intrinsic Semantics of Information Processes", "categories": ["cond-mat.stat-mech", "cs.IT", "cs.MA", "math.IT", "nlin.AO"], "comment": "24 pages, 10 figures, 1 appendix;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/iprocesses.htm", "summary": "We develop information theory for the temporal behavior of memoryful agents\nmoving through complex -- structured, stochastic -- environments. We introduce\ninformation processes -- stochastic processes produced by cognitive agents in\nreal-time as they interact with and interpret incoming stimuli. We provide\nbasic results on the ergodicity and semantics of the resulting time series of\nShannon information measures that monitor an agent's adapting view of\nuncertainty and structural correlation in its environment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bb0\u5fc6\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4fe1\u606f\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u9999\u519c\u4fe1\u606f\u65f6\u95f4\u5e8f\u5217\u7684\u57fa\u672c\u7ed3\u679c\uff0c\u7528\u4ee5\u76d1\u6d4b\u4ee3\u7406\u5bf9\u73af\u5883\u8ba4\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u7ed3\u6784\u5173\u8054\u6027\u3002", "motivation": "\u63ed\u793a\u8ba4\u77e5\u4ee3\u7406\u5728\u590d\u6742\u4e14\u7ed3\u6784\u5316\u7684\u968f\u673a\u73af\u5883\u4e2d\u901a\u8fc7\u65f6\u95f4\u884c\u4e3a\u8fdb\u884c\u4fe1\u606f\u89e3\u6790\u7684\u673a\u5236\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u4fe1\u606f\u8fc7\u7a0b\uff0c\u5373\u8ba4\u77e5\u4ee3\u7406\u5728\u4e0e\u73af\u5883\u4e92\u52a8\u548c\u89e3\u91ca\u523a\u6fc0\u65f6\u5b9e\u65f6\u4ea7\u751f\u7684\u968f\u673a\u8fc7\u7a0b\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u9999\u519c\u4fe1\u606f\u6d4b\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u7684\u57fa\u672c\u7ed3\u679c\uff0c\u8fd9\u4e9b\u7ed3\u679c\u76d1\u6d4b\u4ee3\u7406\u5bf9\u5176\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u7ed3\u6784\u5173\u8054\u7684\u9002\u5e94\u6027\u770b\u6cd5\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7814\u7a76\u8ba4\u77e5\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e34\u65f6\u884c\u4e3a\uff0c\u53d1\u5c55\u4e86\u4fe1\u606f\u7406\u8bba\u3002"}}
{"id": "2505.18279", "pdf": "https://arxiv.org/pdf/2505.18279", "abs": "https://arxiv.org/abs/2505.18279", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u534f\u4f5c\u8bb0\u5fc6\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u7528\u6237\u3001\u591a\u4ee3\u7406\u73af\u5883\u4e2d\u7684\u77e5\u8bc6\u5171\u4eab\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u5bf9\u79f0\u6743\u9650\u5b9e\u73b0\u5b89\u5168\u548c\u9ad8\u6548\u7684\u8de8\u7528\u6237\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u590d\u6742\u4efb\u52a1\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u59d4\u6258\u7ed9\u57fa\u4e8eLLM\u7684\u4e13\u4e1a\u4ee3\u7406\u7ec4\u5408\uff0c\u8fd9\u4e9b\u4ee3\u7406\u8fdb\u884c\u63a8\u7406\u3001\u6c9f\u901a\u548c\u534f\u8c03\u884c\u52a8\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u5047\u5b9a\u5355\u4e00\u7528\u6237\u4e0a\u4e0b\u6587\uff0c\u5ffd\u7565\u4e86\u5728\u52a8\u6001\u3001\u4e0d\u5bf9\u79f0\u6743\u9650\u4e0b\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\u7684\u76ca\u5904\u548c\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u534f\u4f5c\u8bb0\u5fc6\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u7528\u6237\u3001\u591a\u4ee3\u7406\u73af\u5883\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u4e0d\u5bf9\u79f0\u3001\u4e0d\u65ad\u53d8\u5316\u7684\u8bbf\u95ee\u63a7\u5236\u88ab\u7f16\u7801\u4e3a\u8fde\u63a5\u7528\u6237\u3001\u4ee3\u7406\u548c\u8d44\u6e90\u7684\u4e8c\u5206\u56fe\u3002\u8be5\u7cfb\u7edf\u7ef4\u62a4\u4e24\u4e2a\u5185\u5b58\u5c42\uff1a\u79c1\u4eba\u5185\u5b58\u548c\u5171\u4eab\u5185\u5b58\uff0c\u6bcf\u4e2a\u7247\u6bb5\u90fd\u6709\u4e0d\u53ef\u53d8\u7684\u6765\u6e90\u5c5e\u6027\uff0c\u4ee5\u652f\u6301\u4e8b\u540e\u8bb8\u53ef\u68c0\u67e5\u3002", "result": "\u6211\u4eec\u7684\u7cfb\u7edf\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u8bfb\u5199\u7b56\u7565\u5b9e\u73b0\u5f53\u524d\u7528\u6237-\u4ee3\u7406-\u8d44\u6e90\u7ea6\u675f\uff0c\u5e76\u5c06\u73b0\u6709\u5185\u5b58\u7247\u6bb5\u6295\u5c04\u5230\u8fc7\u6ee4\u7684\u53d8\u6362\u89c6\u56fe\u4e2d\u3002\u5199\u7b56\u7565\u786e\u5b9a\u7247\u6bb5\u7684\u4fdd\u7559\u548c\u5171\u4eab\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u53d8\u6362\u66f4\u65b0\u5185\u5b58\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u80fd\u591f\u5b89\u5168\u3001\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u5730\u5b9e\u73b0\u8de8\u7528\u6237\u77e5\u8bc6\u5171\u4eab\uff0c\u540c\u65f6\u8bc1\u660e\u9075\u5faa\u4e0d\u5bf9\u79f0\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u5185\u5b58\u64cd\u4f5c\u7684\u5b8c\u5168\u53ef\u5ba1\u8ba1\u6027\u3002"}}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240", "abs": "https://arxiv.org/abs/2505.18240", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u53c2\u8003\u8bc4\u4ef7\u65b9\u6cd5REFLEX\uff0c\u751f\u6210\u5177\u6709\u7279\u5b9a\u5ea6\u91cf\u6270\u52a8\u7684\u8d1f\u6837\u672c\u6765\u5fae\u8c03LLMs\uff0c\u4ece\u800c\u65e0\u9700\u771f\u5b9e\u6f14\u793a\u6587\u7a3f\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6f14\u793a\u5e7b\u706f\u7247\u5728\u751f\u6210\u6027AI\u65f6\u4ee3\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u7279\u5b9a\u5ea6\u91cf\u6270\u52a8\u7684\u8d1f\u9762\u6f14\u793a\u6837\u672c\uff0c\u5e76\u7528\u5b83\u4eec\u6765\u5fae\u8c03LLMs\u3002\u7136\u540e\u8bc4\u4f30\u65b9\u6cd5\u5728\u63a8\u65ad\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u65e0\u53c2\u8003\u8bc4\u4f30\u6280\u672f\u3002", "result": "\u5927\u91cf\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u5b9e\u9a8c\u8868\u660e\uff0c\u8bc4\u4f30\u65b9\u6cd5\u5728\u751f\u6210\u5206\u6570\u548c\u89e3\u91ca\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u548c\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u3002", "conclusion": "REFLEX\u8bc4\u4f30\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u548c\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u5206\u6570\u548c\u89e3\u91ca\u3002"}}
{"id": "2505.18380", "pdf": "https://arxiv.org/pdf/2505.18380", "abs": "https://arxiv.org/abs/2505.18380", "authors": ["Praphul Singh", "Charlotte Dzialo", "Jangwon Kim", "Sumana Srivatsa", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Industry Track. To appear", "summary": "Ensuring clinical data privacy while preserving utility is critical for\nAI-driven healthcare and data analytics. Existing de-identification (De-ID)\nmethods, including rule-based techniques, deep learning models, and large\nlanguage models (LLMs), often suffer from recall errors, limited\ngeneralization, and inefficiencies, limiting their real-world applicability. We\npropose a fully automated, multi-modal framework, RedactOR for de-identifying\nstructured and unstructured electronic health records, including clinical audio\nrecords. Our framework employs cost-efficient De-ID strategies, including\nintelligent routing, hybrid rule and LLM based approaches, and a two-step audio\nredaction approach. We present a retrieval-based entity relexicalization\napproach to ensure consistent substitutions of protected entities, thereby\nenhancing data coherence for downstream applications. We discuss key design\ndesiderata, de-identification and relexicalization methodology, and modular\narchitecture of RedactX and its integration with the Oracle Health Clinical AI\nsystem. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with\nstrict recall, our approach achieves competitive performance while optimizing\ntoken usage to reduce LLM costs. Finally, we discuss key lessons and insights\nfrom deployment in real-world AI- driven healthcare data pipelines.", "AI": {"tldr": "\u63d0\u51faRedactOR\u6846\u67b6\uff0c\u63d0\u9ad8\u533b\u7597\u6570\u636e\u53bb\u8bc6\u522b\u7684\u53ec\u56de\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u5316LLM\u6210\u672c\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u53bb\u8bc6\u522b\u65b9\u6cd5\u5728\u53ec\u56de\u9519\u8bef\u3001\u6709\u9650\u6cdb\u5316\u53ca\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8AI\u9a71\u52a8\u533b\u7597\u6570\u636e\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u591a\u6a21\u6001\u6846\u67b6RedactOR\uff0c\u7528\u4e8e\u53bb\u8bc6\u522b\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u5305\u62ec\u4e34\u5e8a\u97f3\u9891\u8bb0\u5f55\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8131\u654f\u7b56\u7565\uff0c\u5305\u62ec\u667a\u80fd\u8def\u7531\u3001\u6df7\u5408\u89c4\u5219\u548cLLM\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e24\u6b65\u97f3\u9891\u53bb\u8bc6\u522b\u65b9\u6cd5\u3002", "result": "\u5728i2b2 2014 De-ID\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e25\u683c\u53ec\u56de\u6807\u51c6\u548c\u6807\u51c6\u6307\u6807\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f18\u5316\u4ee4\u724c\u4f7f\u7528\u4ee5\u964d\u4f4eLLM\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u8868\u73b0\u3002", "conclusion": "RedactOR\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u73b0\u6709\u8131\u654f\u65b9\u6cd5\u4e2d\u7684\u53ec\u56de\u9519\u8bef\u3001\u6709\u9650\u6cdb\u5316\u548c\u6548\u7387\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684AI\u533b\u7597\u6570\u636e\u7ba1\u9053\u3002"}}
{"id": "2505.18168", "pdf": "https://arxiv.org/pdf/2505.18168", "abs": "https://arxiv.org/abs/2505.18168", "authors": ["Feifan Wang", "Tengfei Song", "Minggui He", "Chang Su", "Zhanglin Wu", "Hao Yang", "Wenming Zheng", "Osamu Yoshie"], "title": "Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Facial emotion perception in the vision large language model (VLLM) is\ncrucial for achieving natural human-machine interaction. However, creating\nhigh-quality annotations for both coarse- and fine-grained facial emotion\nanalysis demands costly expertise. The lack of such high-quality instruction\ndata limits the performance of VLLMs in facial emotion perception. To address\nthis, we propose a self-verification approach with emotion knowledge\nenhancement (SEKE), which generates high-quality instruction data for\nmulti-grained emotion analysis cost-effectively using closed-source VLLM. This\napproach integrates prior human knowledge to VLLM inference, guided by the\ninherent correlations between three grained levels of emotion descriptions,\ni.e., discrete expression, valence-arousal, and action unit, to reliably\ngenerate comprehensive annotations. A self-verification strategy with\nUncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to\nefficiently extract more accurate VLLM predictions, further improving\nannotation reliability. Consequently, we construct a facial emotion instruction\ndataset (FEID) containing three comprehensive descriptions, which provides\ncoarse- and fine-grained emotional information for effective model training.\nAdditionally, we introduce a facial emotion analysis benchmark (FEAB) to\nmeasure the VLLM's corresponding ability. Our method significantly outperforms\nstate-of-the-art methods on three downstream facial emotion analysis tasks.", "AI": {"tldr": "\u9762\u90e8\u60c5\u611f\u611f\u77e5\u5728\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b(VLLM)\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6ce8\u91ca\u9650\u5236\u4e86\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51faSEKE\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u9a8c\u8bc1\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u60c5\u611f\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u60c5\u611f\u5206\u6790\u6ce8\u91ca\u9700\u8981\u6602\u8d35\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u9650\u5236\u4e86VLLM\u5728\u9762\u90e8\u60c5\u611f\u611f\u77e5\u65b9\u9762\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u5229\u7528\u5c01\u95ed\u6e90VLLM\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u5206\u6790\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9a8c\u8bc1\u60c5\u611f\u77e5\u8bc6\u589e\u5f3a(SEKE)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5c01\u95ed\u6e90VLLM\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e09\u4e2a\u60c5\u611f\u63cf\u8ff0\u5c42\u7ea7\u4e4b\u95f4\u7684\u56fa\u6709\u5173\u8054\u6027\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u81ea\u9a8c\u8bc1\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8VLLM\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "result": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u9762\u90e8\u60c5\u611f\u6307\u4ee4\u6570\u636e\u96c6(FEID)\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u9762\u90e8\u60c5\u611f\u5206\u6790\u57fa\u51c6(FEAB)\uff0c\u7528\u4e8e\u6d4b\u91cfVLLM\u7684\u5bf9\u5e94\u80fd\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u79cd\u4e0b\u6e38\u9762\u90e8\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.19338", "pdf": "https://arxiv.org/pdf/2505.19338", "abs": "https://arxiv.org/abs/2505.19338", "authors": ["Adeela Bashir", "Zia Ush Shamszaman", "Zhao Song", "The Anh Han"], "title": "Co-evolutionary Dynamics of Attack and Defence in Cybersecurity", "categories": ["cs.GT", "cs.CR", "nlin.AO"], "comment": null, "summary": "In the evolving digital landscape, it is crucial to study the dynamics of\ncyberattacks and defences. This study uses an Evolutionary Game Theory (EGT)\nframework to investigate the evolutionary dynamics of attacks and defences in\ncyberspace. We develop a two-population asymmetric game between attacker and\ndefender to capture the essential factors of costs, potential benefits, and the\nprobability of successful defences. Through mathematical analysis and numerical\nsimulations, we find that systems with high defence intensities show stability\nwith minimal attack frequencies, whereas low-defence environments show\ninstability, and are vulnerable to attacks. Furthermore, we find five\nequilibria, where the strategy pair always defend and attack emerged as the\nmost likely stable state as cyber domain is characterised by a continuous\nbattle between defenders and attackers. Our theoretical findings align with\nreal-world data from past cyber incidents, demonstrating the interdisciplinary\nimpact, such as fraud detection, risk management and cybersecurity\ndecision-making. Overall, our analysis suggests that adaptive cybersecurity\nstrategies based on EGT can improve resource allocation, enhance system\nresilience, and reduce the overall risk of cyberattacks. By incorporating\nreal-world data, this study demonstrates the applicability of EGT in addressing\nthe evolving nature of cyber threats and the need for secure digital ecosystems\nthrough strategic planning and proactive defence measures.", "AI": {"tldr": "\u5229\u7528\u6f14\u5316\u535a\u5f08\u7406\u8bba\u5206\u6790\u7f51\u7edc\u653b\u51fb\u4e0e\u9632\u5fa1\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u63d0\u5347\u7f51\u7edc\u5b89\u5168\u7684\u9002\u5e94\u6027\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u9645\u6570\u636e\u9a8c\u8bc1\u4e86\u7406\u8bba\u6548\u679c\u3002", "motivation": "\u5728\u6570\u5b57\u73af\u5883\u4e0d\u65ad\u53d8\u5316\u4e2d\uff0c\u7814\u7a76\u7f51\u7edc\u653b\u51fb\u4e0e\u9632\u5fa1\u7684\u52a8\u6001\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u5e94\u5bf9\u7f51\u7edc\u5a01\u80c1\u5e76\u589e\u5f3a\u7f51\u7edc\u5b89\u5168\u3002", "method": "\u4f7f\u7528\u6f14\u5316\u535a\u5f08\u7406\u8bba\uff08EGT\uff09\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u4e4b\u95f4\u7684\u53cc\u4eba\u4e0d\u5bf9\u79f0\u535a\u5f08\uff0c\u901a\u8fc7\u6570\u5b66\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u7814\u7a76\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e0e\u653b\u51fb\u9891\u7387\u5173\u7cfb\u3002", "result": "\u5728\u9ad8\u9632\u5fa1\u5f3a\u5ea6\u7684\u7cfb\u7edf\u4e2d\uff0c\u653b\u51fb\u9891\u7387\u6781\u4f4e\u4e14\u7cfb\u7edf\u7a33\u5b9a\uff1b\u5728\u4f4e\u9632\u5fa1\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u4e14\u6613\u53d7\u653b\u51fb\u3002\u53d1\u73b0\u4e94\u4e2a\u5e73\u8861\u70b9\uff0c\u5176\u4e2d\u603b\u662f\u9632\u5fa1\u4e0e\u653b\u51fb\u7684\u7b56\u7565\u7ec4\u5408\u662f\u6700\u53ef\u80fd\u7a33\u5b9a\u7684\u72b6\u6001\u3002", "conclusion": "\u5e94\u7528\u6f14\u5316\u535a\u5f08\u7406\u8bba\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8d44\u6e90\u5206\u914d\u3001\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\uff0c\u5e76\u964d\u4f4e\u7f51\u7edc\u653b\u51fb\u98ce\u9669\u3002\u901a\u8fc7\u5b9e\u9645\u6570\u636e\u8bc1\u660e\u6f14\u5316\u535a\u5f08\u7406\u8bba\u5728\u5904\u7406\u7f51\u7edc\u5a01\u80c1\u53d8\u5316\u6027\u548c\u786e\u4fdd\u6570\u5b57\u751f\u6001\u7cfb\u7edf\u5b89\u5168\u65b9\u9762\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.18286", "pdf": "https://arxiv.org/pdf/2505.18286", "abs": "https://arxiv.org/abs/2505.18286", "authors": ["Mingyan Gao", "Yanzi Li", "Banruo Liu", "Yifan Yu", "Phillip Wang", "Ching-Yu Lin", "Fan Lai"], "title": "Single-agent or Multi-agent Systems? Why Not Both?", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to\ndifferent large language model (LLM) agents and tools. Prior studies have\nreported the superior accuracy performance of MAS across diverse domains,\nenabled by long-horizon context tracking and error correction through\nrole-specific agents. However, the design and deployment of MAS incur higher\ncomplexity and runtime cost compared to single-agent systems (SAS). Meanwhile,\nfrontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in\nlong-context reasoning, memory retention, and tool usage, mitigating many\nlimitations that originally motivated MAS designs. In this paper, we conduct an\nextensive empirical study comparing MAS and SAS across various popular agentic\napplications. We find that the benefits of MAS over SAS diminish as LLM\ncapabilities improve, and we propose efficient mechanisms to pinpoint the\nerror-prone agent in MAS. Furthermore, the performance discrepancy between MAS\nand SAS motivates our design of a hybrid agentic paradigm, request cascading\nbetween MAS and SAS, to improve both efficiency and capability. Our design\nimproves accuracy by 1.1-12% while reducing deployment costs by up to 20%\nacross various agentic applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u968f\u7740LLM\u80fd\u529b\u63d0\u9ad8\uff0cMAS\u7684\u4f18\u52bf\u51cf\u5c0f\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8303\u5f0f\u4ee5\u63d0\u5347\u667a\u80fd\u4f53\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8MAS\u4e0eSAS\u5728\u73b0\u4ee3LLM\u80fd\u529b\u8fdb\u6b65\u7684\u80cc\u666f\u4e0b\u7684\u4f18\u52a3\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u4ee5\u964d\u4f4eMAS\u7684\u590d\u6742\u6027\u548c\u6210\u672c\uff0c\u63d0\u9ad8MAS\u7684\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83MAS\u548cSAS\u5728\u5404\u79cd\u6d41\u884c\u7684\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u7cbe\u786e\u5b9a\u4f4dMAS\u4e2d\u5bb9\u6613\u51fa\u9519\u7684\u667a\u80fd\u4f53\u7684\u673a\u5236\u3002", "result": "\u8bbe\u8ba1\u7684\u6df7\u5408\u667a\u80fd\u4f53\u8303\u5f0f\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u60271.1-12%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u6700\u591a20%\u3002", "conclusion": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u7684\u63d0\u5347\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u76f8\u8f83\u4e8e\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff08SAS\uff09\u7684\u4f18\u52bf\u51cf\u5c11\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u901a\u8fc7MAS\u548cSAS\u4e4b\u95f4\u7684\u8bf7\u6c42\u7ea7\u8054\u6765\u63d0\u5347\u6548\u7387\u548c\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u3002"}}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244", "abs": "https://arxiv.org/abs/2505.18244", "authors": ["Yukin Zhang", "Qi Dong"], "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities.", "AI": {"tldr": "MSPGT\u662f\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5168\u7403\u4e0a\u4e0b\u6587\u3001\u4e2d\u95f4\u7ed3\u6784\u548c\u5c40\u90e8\u8bcd\u6c47\u9009\u62e9\u4e09\u4e2a\u8bed\u4e49\u5c3a\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u5ea6\u91cf\u6307\u6807\u8bc6\u522b\u5c3a\u5ea6\u8fb9\u754c\u3002\u8fd9\u79cd\u6846\u67b6\u5728\u89e3\u91ca\u548c\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5927\u89c4\u6a21Transformer\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u5982\u4f55\u8ba1\u5212\u3001\u6784\u5efa\u548c\u5b9e\u73b0\u6587\u672c\u65b9\u9762\u4ecd\u7136\u4e0d\u591f\u900f\u660e\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u67b6\u6784\u65e0\u5173\u89e3\u91ca\u3001\u8bca\u65ad\u548c\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u591a\u5c3a\u5ea6\u6982\u7387\u751f\u6210\u7406\u8bba\u201d\uff08MSPGT\uff09\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u8bed\u4e49\u5c3a\u5ea6\u2014\u2014\u5168\u7403\u4e0a\u4e0b\u6587\u3001\u4e2d\u95f4\u7ed3\u6784\u548c\u5c40\u90e8\u8bcd\u6c47\u9009\u62e9\uff0c\u5e76\u5c06\u6bcf\u4e2a\u5c3a\u5ea6\u4e0eTransformer\u67b6\u6784\u4e2d\u7684\u7279\u5b9a\u5c42\u8303\u56f4\u5bf9\u9f50\u3002\u4e3a\u8bc6\u522b\u5c3a\u5ea6\u8fb9\u754c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u5ea6\u91cf\u6807\u51c6\uff1a\u6ce8\u610f\u529b\u8de8\u5ea6\u9608\u503c\u548c\u5c42\u95f4\u4e92\u4fe1\u606f\u5cf0\u503c\u3002\u901a\u8fc7\u63a2\u6d4b\u4efb\u52a1\u548c\u56e0\u679c\u5e72\u9884\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5ea6\u91cf\u6807\u51c6\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u4e2d\u751f\u6210\u7a33\u5b9a\u7684\u5c40\u90e8\u3001\u4e2d\u95f4\u548c\u5168\u7403\u5212\u5206\u3002\u6211\u4eec\u53d1\u73b0\uff0cdecoder_only\u6a21\u578b\u5728\u4e2d\u95f4\u548c\u5168\u7403\u5904\u7406\u4e0a\u5206\u914d\u4e86\u66f4\u591a\u5c42\uff0c\u800cencoder_only\u6a21\u578b\u5219\u66f4\u52a0\u6ce8\u91cd\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5bf9\u5c40\u90e8\u5c3a\u5ea6\u7684\u64cd\u63a7\u4e3b\u8981\u5f71\u54cd\u8bcd\u6c47\u591a\u6837\u6027\uff0c\u4e2d\u95f4\u5c3a\u5ea6\u7684\u4fee\u6539\u4f1a\u5f71\u54cd\u53e5\u5b50\u7ed3\u6784\u548c\u957f\u5ea6\uff0c\u800c\u5168\u7403\u5c3a\u5ea6\u7684\u6270\u52a8\u5219\u5f71\u54cd\u8bdd\u8bed\u8fde\u8d2f\u6027\uff0c\u5e76\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u6548\u679c\u3002MSPGT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u67b6\u6784\u65e0\u5173\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u3001\u8bca\u65ad\u548c\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "MSPGT\u63d0\u4f9b\u4e86\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89e3\u91ca\u3001\u8bca\u65ad\u548c\u63a7\u5236\u3002\u8fd9\u4e00\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5c3a\u5ea6\u5212\u5206\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u5c42\u5206\u914d\u503e\u5411\uff0c\u5e76\u80fd\u591f\u901a\u8fc7\u64cd\u63a7\u4e0d\u540c\u5c3a\u5ea6\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u7684\u8bcd\u6c47\u591a\u6837\u6027\u3001\u53e5\u5b50\u7ed3\u6784\u548c\u8bed\u7bc7\u8fde\u8d2f\u6027\u3002"}}
{"id": "2505.18425", "pdf": "https://arxiv.org/pdf/2505.18425", "abs": "https://arxiv.org/abs/2505.18425", "authors": ["Menghua Wu", "Yujia Bao"], "title": "Advertising in AI systems: Society must be vigilant", "categories": ["cs.AI"], "comment": null, "summary": "AI systems have increasingly become our gateways to the Internet. We argue\nthat just as advertising has driven the monetization of web search and social\nmedia, so too will commercial incentives shape the content served by AI. Unlike\ntraditional media, however, the outputs of these systems are dynamic,\npersonalized, and lack clear provenance -- raising concerns for transparency\nand regulation. In this paper, we envision how commercial content could be\ndelivered through generative AI-based systems. Based on the requirements of key\nstakeholders -- advertisers, consumers, and platforms -- we propose design\nprinciples for commercially-influenced AI systems. We then outline high-level\nstrategies for end users to identify and mitigate commercial biases from model\noutputs. Finally, we conclude with open questions and a call to action towards\nthese goals.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5546\u4e1a\u52a8\u673a\u4f1a\u5982\u4f55\u5f71\u54cdAI\u7cfb\u7edf\u8f93\u51fa\uff0c\u5e76\u63d0\u51fa\u8bbe\u8ba1\u539f\u5219\u548c\u8bc6\u522b\u5546\u4e1a\u504f\u89c1\u7684\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u5a92\u4f53\u7684\u5185\u5bb9\u53d7\u5230\u5546\u4e1a\u6fc0\u52b1\u7684\u5f71\u54cd\uff0c\u800cAI\u7cfb\u7edf\u7684\u8f93\u51fa\u66f4\u5177\u52a8\u6001\u6027\u3001\u4e2a\u6027\u5316\uff0c\u4e14\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u76d1\u7ba1\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5e94\u5bf9\u5546\u4e1a\u5185\u5bb9\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u5e7f\u544a\u5546\u3001\u6d88\u8d39\u8005\u548c\u5e73\u53f0\u7b49\u5173\u952e\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u5236\u5b9a\u7b56\u7565\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u5546\u4e1a\u504f\u89c1\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u8bbe\u8ba1\u539f\u5219\u548c\u7b56\u7565\uff0c\u4ee5\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u548c\u51cf\u5c11AI\u7cfb\u7edf\u8f93\u51fa\u4e2d\u7684\u5546\u4e1a\u504f\u89c1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8bbe\u8ba1\u539f\u5219\uff0c\u65e8\u5728\u89c4\u8303\u5546\u4e1a\u5f71\u54cd\u4e0b\u7684AI\u7cfb\u7edf\uff0c\u5e76\u603b\u7ed3\u4e86\u7528\u6237\u8bc6\u522b\u548c\u51cf\u8f7b\u5546\u5546\u4e1a\u504f\u89c1\u7684\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u547c\u5401\u66f4\u591a\u7684\u7814\u7a76\u548c\u884c\u52a8\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002"}}
{"id": "2505.18169", "pdf": "https://arxiv.org/pdf/2505.18169", "abs": "https://arxiv.org/abs/2505.18169", "authors": ["Nischal Mandal"], "title": "Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Understanding and predicting human emotional and physiological states using\nwearable sensors has important applications in stress monitoring, mental health\nassessment, and affective computing. This study presents a novel Multi-Task\nPhysics-Informed Neural Network (PINN) that performs Electrodermal Activity\n(EDA) prediction and emotion classification simultaneously, using the publicly\navailable WESAD dataset. The model integrates psychological self-report\nfeatures (PANAS and SAM) with a physics-inspired differential equation\nrepresenting EDA dynamics, enforcing biophysically grounded constraints through\na custom loss function. This loss combines EDA regression, emotion\nclassification, and a physics residual term for improved interpretability.\n  The architecture supports dual outputs for both tasks and is trained under a\nunified multi-task framework. Evaluated using 5-fold cross-validation, the\nmodel achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919,\nand F1-score of 94.08 percent. These results outperform classical models such\nas SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only\nmodels.\n  In addition, the learned physical parameters including decay rate (alpha_0),\nemotional sensitivity (beta), and time scaling (gamma) are interpretable and\nstable across folds, aligning with known principles of human physiology. This\nwork is the first to introduce a multi-task PINN framework for wearable emotion\nrecognition, offering improved performance, generalizability, and model\ntransparency. The proposed system provides a foundation for future\ninterpretable and multimodal applications in healthcare and human-computer\ninteraction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u4efb\u52a1PINN\uff0c\u7528\u4e8e\u53ef\u7a7f\u6234\u60c5\u611f\u8bc6\u522b\uff0c\u7ed3\u5408\u5fc3\u7406\u7279\u5f81\u4e0e\u7269\u7406\u65b9\u7a0b\uff0c\u63d0\u5347\u4e86\u60c5\u611f\u9884\u6d4b\u7cbe\u5ea6\u548c\u6a21\u578b\u900f\u660e\u6027\uff0c\u7ed3\u679c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u5229\u7528\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7406\u89e3\u548c\u9884\u6d4b\u4eba\u4f53\u60c5\u611f\u53ca\u751f\u7406\u72b6\u6001\u5bf9\u538b\u529b\u76d1\u6d4b\u3001\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u53ca\u60c5\u611f\u8ba1\u7b97\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u901a\u8fc7\u66f4\u597d\u7684\u6a21\u578b\u6765\u63d0\u9ad8\u60c5\u611f\u548c\u751f\u7406\u72b6\u6001\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u900f\u660e\u6027\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\uff0c\u7ed3\u5408\u5fc3\u7406\u81ea\u6211\u62a5\u544a\u7279\u5f81\u4e0e\u7269\u7406\u7075\u611f\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u5b9e\u73b0EDA\u52a8\u6001\u7684\u751f\u7269\u7269\u7406\u7ea6\u675f\uff0c\u5e76\u5728\u7edf\u4e00\u591a\u4efb\u52a1\u6846\u67b6\u4e0b\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6a21\u578b\u57285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u5e73\u5747\u53d6\u5f97\u4e860.0362\u7684EDA\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u30010.9919\u7684Pearson\u76f8\u5173\u7cfb\u6570\u4ee5\u53ca94.08%\u7684F1\u5f97\u5206\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u5982SVR\u548cXGBoost\u53ca\u5355\u4efb\u52a1\u53d8\u4f53\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a1\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u540c\u65f6\u8fdb\u884cEDA\u9884\u6d4b\u548c\u60c5\u611f\u5206\u7c7b\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6a21\u578b\u900f\u660e\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.18351", "pdf": "https://arxiv.org/pdf/2505.18351", "abs": "https://arxiv.org/abs/2505.18351", "authors": ["Sola Kim", "Dongjune Chang", "Jieshu Wang"], "title": "Persona Alchemy: Designing, Evaluating, and Implementing Psychologically-Grounded LLM Agents for Diverse Stakeholder Representation", "categories": ["cs.MA", "cs.CY", "cs.DB"], "comment": null, "summary": "Despite advances in designing personas for Large Language Models (LLM),\nchallenges remain in aligning them with human cognitive processes and\nrepresenting diverse stakeholder perspectives. We introduce a Social Cognitive\nTheory (SCT) agent design framework for designing, evaluating, and implementing\npsychologically grounded LLMs with consistent behavior. Our framework\noperationalizes SCT through four personal factors (cognitive, motivational,\nbiological, and affective) for designing, six quantifiable constructs for\nevaluating, and a graph database-backed architecture for implementing\nstakeholder personas. Experiments tested agents' responses to contradicting\ninformation of varying reliability. In the highly polarized renewable energy\ntransition discourse, we design five diverse agents with distinct ideologies,\nroles, and stakes to examine stakeholder representation. The evaluation of\nthese agents in contradictory scenarios occurs through comprehensive processes\nthat implement the SCT. Results show consistent response patterns ($R^2$ range:\n$0.58-0.61$) and systematic temporal development of SCT construct effects.\nPrincipal component analysis identifies two dimensions explaining $73$% of\nvariance, validating the theoretical structure. Our framework offers improved\nexplainability and reproducibility compared to black-box approaches. This work\ncontributes to ongoing efforts to improve diverse stakeholder representation\nwhile maintaining psychological consistency in LLM personas.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba(SCT)\u4ee3\u7406\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ee5\u8bbe\u8ba1\u548c\u5b9e\u73b0\u5fc3\u7406\u4e0a\u6709\u4f9d\u636e\u4e14\u884c\u4e3a\u4e00\u81f4\u7684LLM\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u53ef\u91cd\u590d\u6027\u548c\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u4ee3\u8868\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6539\u5584\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u548c\u4ee3\u8868\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u89c2\u70b9\u65b9\u9762\u4e0a\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u5b9e\u9a8c\u901a\u8fc7\u5b9e\u65bdSCT\u6784\u5efa\u7684\u5168\u9762\u6d41\u7a0b\u6765\u8bc4\u4f30\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\u6765\u9a8c\u8bc1\u7406\u8bba\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4ee3\u7406\u5728\u77db\u76fe\u4fe1\u606f\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u53cd\u5e94\u6a21\u5f0f\uff0c\u4e3b\u6210\u5206\u5206\u6790\u8bc6\u522b\u51fa\u89e3\u91ca73%\u65b9\u5dee\u7684\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u6784\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u76f8\u6bd4\u4e8e\u9ed1\u7bb1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u63a8\u52a8\u4e86\u5728\u4fdd\u6301\u5fc3\u7406\u4e00\u81f4\u6027\u65b9\u9762\u63d0\u9ad8\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u4ee3\u8868\u6027\u7684\u52aa\u529b\u3002"}}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247", "abs": "https://arxiv.org/abs/2505.18247", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc.", "AI": {"tldr": "MetaGen Blended RAG\u65b9\u6cd5\u901a\u8fc7\u63d0\u5347\u68c0\u7d22\u5668\u6548\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684RAG\u51c6\u786e\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684RAG\u65b9\u6cd5\u5728\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u89e3\u7b54\u51c6\u786e\u6027\u5dee\uff0c\u96be\u4ee5\u5e94\u5bf9\u57df\u5185\u590d\u6742\u672f\u8bed\u53ca\u8bed\u4e49\u53d8\u5316\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f01\u4e1a\u641c\u7d22\u7cfb\u7edf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MetaGen Blended RAG\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u5173\u952e\u6982\u5ff5\u3001\u4e3b\u9898\u548c\u7f29\u7565\u8bcd\u6765\u6784\u9020\u5143\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u7136\u540e\u521b\u5efa\u5177\u6709\u589e\u5f3a\u641c\u7d22\u67e5\u8be2\u7684\u5143\u6570\u636e\u4e30\u5bcc\u7684\u6df7\u5408\u7d22\u5f15\u3002", "result": "\u5728PubMedQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8682%\u7684\u68c0\u7d22\u51c6\u786e\u7387\u548c77%\u7684RAG\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u6240\u6709\u4e4b\u524d\u672a\u8fdb\u884c\u5fae\u8c03\u7684RAG\u51c6\u786e\u6027\u7ed3\u679c\uff0c\u5e76\u4e3a\u96f6\u6837\u672c\u7ed3\u679c\u8bbe\u5b9a\u4e86\u65b0\u6807\u6746\u3002\u5176\u6027\u80fd\u751a\u81f3\u53ef\u4ee5\u5ab2\u7f8e\u5728\u8fd9\u4e00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8fc7\u5fae\u8c03\u7684\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u5185\u6709\u6548\u5730\u589e\u5f3a\u4e86RAG\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u8868\u660eMetaGen Blended RAG\u65b9\u6cd5\u5177\u5907\u826f\u597d\u7684\u8de8\u9886\u57df\u901a\u7528\u6027\u3002"}}
{"id": "2505.18457", "pdf": "https://arxiv.org/pdf/2505.18457", "abs": "https://arxiv.org/abs/2505.18457", "authors": ["Abir Ray"], "title": "EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "6 pages, 2 figures", "summary": "This paper introduces EdgeAgentX, a novel framework integrating federated\nlearning (FL), multi-agent reinforcement learning (MARL), and adversarial\ndefense mechanisms, tailored for military communication networks. EdgeAgentX\nsignificantly improves autonomous decision-making, reduces latency, enhances\nthroughput, and robustly withstands adversarial disruptions, as evidenced by\ncomprehensive simulations.", "AI": {"tldr": "EdgeAgentX integrates FL, MARL, and adversarial defenses to enhance military network performance and security.", "motivation": "The paper is motivated by the need for improved performance and robustness in military communication networks.", "method": "This paper uses federated learning, multi-agent reinforcement learning, and adversarial defense mechanisms.", "result": "EdgeAgentX significantly enhances network performance and security, as demonstrated by simulations.", "conclusion": "EdgeAgentX improves autonomous decision-making, reduces latency, enhances throughput, and withstands adversarial disruptions."}}
{"id": "2505.18171", "pdf": "https://arxiv.org/pdf/2505.18171", "abs": "https://arxiv.org/abs/2505.18171", "authors": ["Tengwei Song", "Xudong Ma", "Yang Liu", "Jie Luo"], "title": "Robust Knowledge Graph Embedding via Denoising", "categories": ["cs.LG"], "comment": null, "summary": "We focus on obtaining robust knowledge graph embedding under perturbation in\nthe embedding space. To address these challenges, we introduce a novel\nframework, Robust Knowledge Graph Embedding via Denoising, which enhances the\nrobustness of KGE models on noisy triples. By treating KGE methods as\nenergy-based models, we leverage the established connection between denoising\nand score matching, enabling the training of a robust denoising KGE model.\nFurthermore, we propose certified robustness evaluation metrics for KGE methods\nbased on the concept of randomized smoothing. Through comprehensive experiments\non benchmark datasets, our framework consistently shows superior performance\ncompared to existing state-of-the-art KGE methods when faced with perturbed\nentity embedding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6297\u566a\u58f0\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u566a\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u8ba4\u8bc1\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u56f4\u7ed5\u83b7\u5f97\u9c81\u68d2\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u6765\u5bf9\u6297\u6270\u52a8\u3002", "method": "\u5c06\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u89c6\u4e3a\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\uff0c\u5229\u7528\u53bb\u566a\u4e0e\u8bc4\u5206\u5339\u914d\u4e4b\u95f4\u7684\u5df2\u5efa\u7acb\u8054\u7cfb\uff0c\u8bad\u7ec3\u4e00\u4e2a\u9c81\u68d2\u7684\u53bb\u566aKGE\u6a21\u578b\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u968f\u673a\u5e73\u6ed1\u6982\u5ff5\u63d0\u51fa\u4e86\u7ecf\u8fc7\u8ba4\u8bc1\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728\u9762\u5bf9\u6270\u52a8\u5b9e\u4f53\u5d4c\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684KGE\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u9762\u5bf9\u6270\u52a8\u5b9e\u4f53\u5d4c\u5165\u65f6\uff0c\u8868\u73b0\u51fa\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdbKGE\u65b9\u6cd5\u7684\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2505.18397", "pdf": "https://arxiv.org/pdf/2505.18397", "abs": "https://arxiv.org/abs/2505.18397", "authors": ["Fangqiao Tian", "An Luo", "Jin Du", "Xun Xian", "Robert Specht", "Ganghua Wang", "Xuan Bi", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Rui Zhang", "Zirui Liu", "Mingyi Hong", "Jie Ding"], "title": "An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems", "categories": ["cs.MA", "cs.AI", "cs.ET", "cs.LG", "68T42 (Agent technology and artificial intelligence), 68T01 (General\n  topics in artificial intelligence), 68M14 (Distributed systems)", "I.2.11; I.2.4; I.2.6"], "comment": null, "summary": "Multi-agent AI systems (MAS) offer a promising framework for distributed\nintelligence, enabling collaborative reasoning, planning, and decision-making\nacross autonomous agents. This paper provides a systematic outlook on the\ncurrent opportunities and challenges of MAS, drawing insights from recent\nadvances in large language models (LLMs), federated optimization, and human-AI\ninteraction. We formalize key concepts including agent topology, coordination\nprotocols, and shared objectives, and identify major risks such as dependency,\nmisalignment, and vulnerabilities arising from training data overlap. Through a\nbiologically inspired simulation and comprehensive theoretical framing, we\nhighlight critical pathways for developing robust, scalable, and secure MAS in\nreal-world settings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u673a\u4f1a\u4e0e\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u751f\u7269\u6a21\u62df\u548c\u7406\u8bba\u5206\u6790\u63d0\u51fa\u5f00\u53d1\u7a33\u5065\u3001\u53ef\u6269\u5c55\u3001\u5b89\u5168\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u5206\u5e03\u5f0f\u667a\u80fd\u65b9\u9762\u7684\u673a\u4f1a\u548c\u6311\u6218\uff0c\u5e76\u501f\u9274\u6700\u65b0\u6280\u672f\u6210\u679c\u4ee5\u63d0\u5347\u5176\u80fd\u529b\u3002", "method": "\u91c7\u7528\u751f\u7269\u5b66\u542f\u53d1\u7684\u6a21\u62df\u548c\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5316\u7814\u7a76\uff0c\u8bc6\u522b\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\u8def\u5f84\u548c\u53ef\u80fd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5f00\u53d1\u53ef\u9760\u4e0e\u53ef\u6269\u5c55\u7cfb\u7edf\u7684\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5173\u952e\u6982\u5ff5\u548c\u9762\u4e34\u7684\u4e3b\u8981\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u901a\u8fc7\u751f\u7269\u6a21\u62df\u548c\u7406\u8bba\u6846\u67b6\u5f00\u53d1\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283", "abs": "https://arxiv.org/abs/2505.18283", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.", "AI": {"tldr": "\u63d0\u51faTAGS\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u7528\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u7ed3\u5408\uff0c\u4ee5\u5c42\u6b21\u5316\u68c0\u7d22\u548c\u8bc4\u4f30\u4e00\u81f4\u6027\u7684\u65b9\u5f0f\u63d0\u5347\u533b\u5b66\u95ee\u7b54\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u533b\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u6d45\u5c42\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5fae\u8c03\u7684\u533b\u5b66\u5927\u6a21\u578b\u9762\u5bf9\u5206\u5e03\u53d8\u5316\u548c\u672a\u77e5\u4e34\u5e8a\u60c5\u5883\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u6216\u53c2\u6570\u66f4\u65b0\u7684\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TAGS\u7ed3\u5408\u6cdb\u7528\u578b\u7684\u901a\u7528\u6a21\u578b\u548c\u7279\u5b9a\u9886\u57df\u7684\u4e13\u7528\u6a21\u578b\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u68c0\u7d22\u673a\u5236\u548c\u53ef\u9760\u6027\u8bc4\u5206\u6a21\u5757\uff0c\u63d0\u4f9b\u591a\u5c3a\u5ea6\u793a\u4f8b\u548c\u8bc4\u4f30\u63a8\u7406\u4e00\u81f4\u6027\uff0c\u4ee5\u652f\u6301\u7efc\u5408\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "TAGS\u5728\u4e5d\u4e2a\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86GPT-4o\u51c6\u786e\u602713.8%\uff0cDeepSeek-R1\u63d0\u534716.8%\uff0c\u4ee5\u53ca\u63d0\u5347\u4e86\u4e00\u4e2a\u7b80\u5355\u76847B\u6a21\u578b\u7684\u51c6\u786e\u6027\u4ece14.1%\u523023.9%\u3002", "conclusion": "TAGS\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u591a\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u533b\u5b66\u5927\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.18467", "pdf": "https://arxiv.org/pdf/2505.18467", "abs": "https://arxiv.org/abs/2505.18467", "authors": ["Unggi Lee", "Jaeyong Lee", "Jiyeong Bae", "Yeil Jeong", "Junbo Koh", "Gyeonggeon Lee", "Gunho Lee", "Taekyung Ahn", "Hyeoncheol Kim"], "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 5 figures, 4 tables", "summary": "Recent advances in large reasoning models (LRMs) show strong performance in\nstructured domains such as mathematics and programming; however, they often\nlack pedagogical coherence and realistic teaching behaviors. To bridge this\ngap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use\nthrough three innovations: (1) a distillation-based pipeline that filters and\nrefines model outputs for instruction-tuning, (2) the Well-balanced Educational\nBenchmark (WBEB), which evaluates performance across subject knowledge,\npedagogical knowledge, tracing, essay scoring, and teacher decision-making, and\n(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting\nteacher-style reasoning. Our mixed-method evaluation combines quantitative\nmetrics with qualitative analysis, providing the first systematic assessment of\nLRMs' pedagogical strengths and limitations.", "AI": {"tldr": "Pedagogy-R1\u662f\u4e00\u4e2a\u63d0\u9ad8LRMs\u6559\u5b66\u80fd\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7b56\u7565\u663e\u8457\u6539\u5584\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LRMs\u5728\u6559\u5b66\u4e2d\u7f3a\u4e4f\u6559\u80b2\u8fde\u8d2f\u6027\u548c\u73b0\u5b9e\u6559\u5b66\u884c\u4e3a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u6d41\u6c34\u7ebf\u3001\u521b\u5efaWBEB\u57fa\u51c6\u3001\u5e76\u4f7f\u7528CoP\u63d0\u793a\u7b56\u7565\u8fdb\u884cLRMs\u7684\u6539\u8fdb\u548c\u8bc4\u4f30\u3002", "result": "\u7efc\u5408\u5b9a\u91cf\u6307\u6807\u4e0e\u5b9a\u6027\u5206\u6790\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86LRMs\u5728\u6559\u80b2\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "Pedagogy-R1\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u7b56\u7565\u663e\u8457\u63d0\u9ad8LRMs\u5728\u8bfe\u5802\u73af\u5883\u4e2d\u7684\u6559\u5b66\u80fd\u529b\u3002"}}
{"id": "2505.18176", "pdf": "https://arxiv.org/pdf/2505.18176", "abs": "https://arxiv.org/abs/2505.18176", "authors": ["Jonathan Tammer Eweis-Labolle", "Tyler Johnson", "Xiangyu Sun", "Ramin Bostanabad"], "title": "Should We Simultaneously Calibrate Multiple Computer Models?", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In an increasing number of applications designers have access to multiple\ncomputer models which typically have different levels of fidelity and cost.\nTraditionally, designers calibrate these models one at a time against some\nhigh-fidelity data (e.g., experiments). In this paper, we question this\ntradition and assess the potential of calibrating multiple computer models at\nthe same time. To this end, we develop a probabilistic framework that is\nfounded on customized neural networks (NNs) that are designed to calibrate an\narbitrary number of computer models. In our approach, we (1) consider the fact\nthat most computer models are multi-response and that the number and nature of\ncalibration parameters may change across the models, and (2) learn a unique\nprobability distribution for each calibration parameter of each computer model,\n(3) develop a loss function that enables our NN to emulate all data sources\nwhile calibrating the computer models, and (4) aim to learn a visualizable\nlatent space where model-form errors can be identified. We test the performance\nof our approach on analytic and engineering problems to understand the\npotential advantages and pitfalls in simultaneous calibration of multiple\ncomputer models. Our method can improve predictive accuracy, however, it is\nprone to non-identifiability issues in higher-dimensional input spaces that are\nnormally constrained by underlying physics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e00\u79cd\u65b0\u65b9\u6cd5\u8d28\u7591\u4f20\u7edf\u6a21\u5f0f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\u4ee5\u540c\u65f6\u6821\u51c6\u591a\u4e2a\u8ba1\u7b97\u673a\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u53ef\u80fd\u51fa\u73b0\u4e0d\u53ef\u8fa8\u8bc6\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u8d8a\u6765\u8d8a\u591a\u7684\u5e94\u7528\u4e2d\uff0c\u8bbe\u8ba1\u8005\u62e5\u6709\u591a\u79cd\u8ba1\u7b97\u673a\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u5177\u6709\u4e0d\u540c\u7684\u4fdd\u771f\u5ea6\u548c\u6210\u672c\u3002\u4f20\u7edf\u4e0a\uff0c\u8bbe\u8ba1\u8005\u5c06\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4e00\u4e9b\u9ad8\u4fdd\u771f\u6570\u636e\uff08\u4f8b\u5982\u5b9e\u9a8c\uff09\u9010\u4e2a\u6821\u51c6\u3002\u672c\u6587\u8d28\u7591\u8fd9\u79cd\u4f20\u7edf\uff0c\u5e76\u8bc4\u4f30\u540c\u65f6\u6821\u51c6\u591a\u4e2a\u8ba1\u7b97\u673a\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9a\u5236\u795e\u7ecf\u7f51\u7edc\u7684\u6982\u7387\u6846\u67b6\uff0c\u8bbe\u8ba1\u7528\u4e8e\u540c\u65f6\u6821\u51c6\u591a\u4e2a\u8ba1\u7b97\u673a\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u8003\u8651\u5927\u591a\u6570\u8ba1\u7b97\u673a\u6a21\u578b\u662f\u591a\u54cd\u5e94\u7684\uff0c\u5e76\u4e14\u6821\u51c6\u53c2\u6570\u7684\u6570\u91cf\u548c\u6027\u8d28\u53ef\u80fd\u5728\u6a21\u578b\u95f4\u53d8\u5316\uff1b2\uff09\u4e3a\u6bcf\u4e2a\u8ba1\u7b97\u673a\u6a21\u578b\u7684\u6bcf\u4e2a\u6821\u51c6\u53c2\u6570\u5b66\u4e60\u4e00\u4e2a\u72ec\u7279\u7684\u6982\u7387\u5206\u5e03\uff1b3\uff09\u5f00\u53d1\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5728\u6821\u51c6\u8ba1\u7b97\u673a\u6a21\u578b\u7684\u540c\u65f6\u6a21\u62df\u6240\u6709\u6570\u636e\u6e90\uff1b4\uff09\u65e8\u5728\u5b66\u4e60\u4e00\u4e2a\u53ef\u89c6\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5176\u4e2d\u53ef\u4ee5\u8bc6\u522b\u6a21\u578b\u5f62\u5f0f\u8bef\u5dee\u3002", "result": "\u6211\u4eec\u5728\u5206\u6790\u548c\u5de5\u7a0b\u95ee\u9898\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4ee5\u4e86\u89e3\u5728\u540c\u65f6\u6821\u51c6\u591a\u4e2a\u8ba1\u7b97\u673a\u6a21\u578b\u65f6\u7684\u6f5c\u5728\u4f18\u52bf\u548c\u7f3a\u9677\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5728\u901a\u5e38\u53d7\u57fa\u7840\u7269\u7406\u7ea6\u675f\u7684\u9ad8\u7ef4\u8f93\u5165\u7a7a\u95f4\u4e2d\u5bb9\u6613\u51fa\u73b0\u4e0d\u53ef\u8fa8\u8bc6\u6027\u95ee\u9898\u3002"}}
{"id": "2505.18530", "pdf": "https://arxiv.org/pdf/2505.18530", "abs": "https://arxiv.org/abs/2505.18530", "authors": ["Pengyu Wang", "Shuchang Ye", "Usman Naseem", "Jinman Kim"], "title": "MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs", "categories": ["cs.MA", "cs.AI"], "comment": "10pages", "summary": "Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for\nmedical report generation. Despite Med-LVLMs producing state-of-the-art\nperformance, they exhibit a bias toward predicting all findings as normal,\nleading to reports that overlook critical abnormalities. Furthermore, these\nmodels often fail to provide comprehensive descriptions of radiologically\nrelevant regions necessary for accurate diagnosis. To address these challenges,\nwe proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent\nframework that fine-tunes specialized agents for different disease categories.\nBy curating subsets of the IU X-ray and MIMIC-CXR datasets to train\ndisease-specific agents, MRGAgents generates reports that more effectively\nbalance normal and abnormal findings while ensuring a comprehensive description\nof clinically relevant regions. Our experiments demonstrate that MRGAgents\noutperformed the state-of-the-art, improving both report comprehensiveness and\ndiagnostic utility.", "AI": {"tldr": "MRGAgents\u6846\u67b6\u901a\u8fc7\u4e13\u95e8\u4ee3\u7406\u751f\u6210\u66f4\u5e73\u8861\u548c\u5168\u9762\u7684\u533b\u7597\u62a5\u544a\uff0c\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "Med-LVLMs\u5728\u751f\u6210\u533b\u7597\u62a5\u544a\u65f6\u5b58\u5728\u4e00\u5b9a\u7684\u504f\u5dee\uff0c\u5bfc\u81f4\u5ffd\u7565\u91cd\u8981\u7684\u5f02\u5e38\u5e76\u7f3a\u4e4f\u5168\u9762\u7684\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u4e13\u95e8\u9488\u5bf9\u4e0d\u540c\u75be\u75c5\u7c7b\u522b\u7684\u4ee3\u7406\u6765\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0cMRGAgents\u80fd\u591f\u66f4\u6709\u6548\u5730\u5e73\u8861\u6b63\u5e38\u548c\u5f02\u5e38\u53d1\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e34\u5e8a\u76f8\u5173\u533a\u57df\u7684\u5168\u9762\u63cf\u8ff0\u3002", "conclusion": "MRGAgents\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u62a5\u544a\u7684\u5168\u9762\u6027\u548c\u8bca\u65ad\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.18298", "pdf": "https://arxiv.org/pdf/2505.18298", "abs": "https://arxiv.org/abs/2505.18298", "authors": ["Jinyan Su", "Claire Cardie"], "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models.", "AI": {"tldr": "\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u4f4e\u6210\u672c\u9002\u5e94\u6027\u63a8\u7406\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5956\u52b1\u663e\u8457\u51cf\u5c11\u63a8\u7406\u957f\u5ea6\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u56e0\u63a8\u7406\u8def\u5f84\u5197\u957f\u800c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u589e\u52a0\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8c03\u6574\u56fa\u5b9a\u7684\u957f\u5ea6\u60e9\u7f5a\u9879\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6f14\u53d8\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5956\u52b1\u6574\u5f62\u65b9\u6cd5\uff0c\u6839\u636e\u6a21\u578b\u6027\u80fd\u52a8\u6001\u8c03\u6574\u51c6\u786e\u6027\u4e0e\u54cd\u5e94\u957f\u5ea6\u4e4b\u95f4\u7684\u5956\u52b1\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u63a8\u7406\u957f\u5ea6\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u81ea\u9002\u5e94\u5956\u52b1\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u957f\u5ea6\u3002"}}
{"id": "2505.18470", "pdf": "https://arxiv.org/pdf/2505.18470", "abs": "https://arxiv.org/abs/2505.18470", "authors": ["Christopher J. Mungall", "Adnan Malik", "Daniel R. Korn", "Justin T. Reese", "Noel M. O'Boyle", "Noel", "Janna Hastings"], "title": "Chemical classification program synthesis using generative artificial intelligence", "categories": ["cs.AI", "q-bio.BM"], "comment": null, "summary": "Accurately classifying chemical structures is essential for cheminformatics\nand bioinformatics, including tasks such as identifying bioactive compounds of\ninterest, screening molecules for toxicity to humans, finding non-organic\ncompounds with desirable material properties, or organizing large chemical\nlibraries for drug discovery or environmental monitoring. However, manual\nclassification is labor-intensive and difficult to scale to large chemical\ndatabases. Existing automated approaches either rely on manually constructed\nclassification rules, or the use of deep learning methods that lack\nexplainability.\n  This work presents an approach that uses generative artificial intelligence\nto automatically write chemical classifier programs for classes in the Chemical\nEntities of Biological Interest (ChEBI) database. These programs can be used\nfor efficient deterministic run-time classification of SMILES structures, with\nnatural language explanations. The programs themselves constitute an\nexplainable computable ontological model of chemical class nomenclature, which\nwe call the ChEBI Chemical Class Program Ontology (C3PO).\n  We validated our approach against the ChEBI database, and compared our\nresults against state of the art deep learning models. We also demonstrate the\nuse of C3PO to classify out-of-distribution examples taken from metabolomics\nrepositories and natural product databases. We also demonstrate the potential\nuse of our approach to find systematic classification errors in existing\nchemical databases, and show how an ensemble artificial intelligence approach\ncombining generated ontologies, automated literature search, and multimodal\nvision models can be used to pinpoint potential errors requiring expert\nvalidation", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u81ea\u52a8\u751f\u6210\u5316\u5b66\u5206\u7c7b\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u5bf9\u5316\u5b66\u7ed3\u6784\u7684\u9ad8\u6548\u5206\u7c7b\u5e76\u63d0\u4f9b\u89e3\u91ca\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5316\u5b66\u5206\u7c7b\u672c\u4f53\u6a21\u578b\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u9a8c\u8bc1\u6548\u679c\u548c\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u51c6\u786e\u5206\u7c7b\u5316\u5b66\u7ed3\u6784\u5bf9\u4e8e\u5316\u5b66\u4fe1\u606f\u5b66\u548c\u751f\u7269\u4fe1\u606f\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u5e94\u5bf9\u5982\u8bc6\u522b\u751f\u7269\u6d3b\u6027\u5316\u5408\u7269\u3001\u7b5b\u9009\u5177\u6709\u6bd2\u6027\u7684\u5206\u5b50\u3001\u5bfb\u627e\u5177\u6709\u7406\u60f3\u6750\u6599\u7279\u6027\u7684\u975e\u6709\u673a\u5316\u5408\u7269\u7b49\u4efb\u52a1\u3002\u7136\u800c\uff0c\u624b\u52a8\u5206\u7c7b\u9700\u8981\u5927\u91cf\u52b3\u52a8\u6295\u5165\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u81f3\u5927\u578b\u5316\u5b66\u6570\u636e\u5e93\u3002\u73b0\u6709\u81ea\u52a8\u5316\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6784\u5efa\u7684\u5206\u7c7b\u89c4\u5219\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u6765\u81ea\u52a8\u7f16\u5199\u5316\u5b66\u5206\u7c7b\u7a0b\u5e8f\u3002\u6240\u751f\u6210\u7684\u7a0b\u5e8f\u80fd\u591f\u5bf9SMILES\u7ed3\u6784\u8fdb\u884c\u786e\u5b9a\u6027\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u7684\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9ChEBI\u6570\u636e\u5e93\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86C3PO\u5728\u4ece\u4ee3\u8c22\u7ec4\u5b66\u6570\u636e\u5e93\u548c\u5929\u7136\u4ea7\u7269\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u7684\u5206\u5e03\u5916\u6837\u672c\u4e0a\u7684\u5206\u7c7b\u6548\u679c\u3002\u6b64\u5916\uff0c\u8fd8\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u53d1\u73b0\u73b0\u6709\u5316\u5b66\u6570\u636e\u5e93\u4e2d\u7684\u7cfb\u7edf\u9519\u8bef\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u7684\u672c\u4f53\u3001\u81ea\u52a8\u6587\u732e\u68c0\u7d22\u548c\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u6765\u8bc6\u522b\u9700\u8981\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6f5c\u5728\u9519\u8bef\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u81ea\u52a8\u4e3a\u751f\u7269\u5316\u5b66\u5b9e\u4f53\u6570\u636e\u5e93\u4e2d\u7684\u5316\u5b66\u7c7b\u522b\u7f16\u5199\u5206\u7c7b\u7a0b\u5e8f\u3002\u8fd9\u4e9b\u7a0b\u5e8f\u80fd\u591f\u5b9e\u73b0\u5bf9SMILES\u7ed3\u6784\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u8fd0\u884c\u65f6\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002"}}
{"id": "2505.18177", "pdf": "https://arxiv.org/pdf/2505.18177", "abs": "https://arxiv.org/abs/2505.18177", "authors": ["Zhizhong Tan", "Jiexin Zheng", "Xingxing Yang", "Chi Zhang", "Weiping Deng", "Wenyong Wang"], "title": "FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the highly sensitive nature of certain data in cross-border sharing,\ncollaborative cross-border recommendations and data sharing are often subject\nto stringent privacy protection regulations, resulting in insufficient data for\nmodel training. Consequently, achieving efficient cross-border business\nrecommendations while ensuring privacy security poses a significant challenge.\nAlthough federated learning has demonstrated broad potential in collaborative\ntraining without exposing raw data, most existing federated learning-based GNN\ntraining methods still rely on federated averaging strategies, which perform\nsuboptimally on highly heterogeneous graph data. To address this issue, we\npropose FedGRec, a privacy-preserving federated graph learning method for\ncross-border recommendations. FedGRec captures user preferences from\ndistributed multi-domain data to enhance recommendation performance across all\ndomains without privacy leakage. Specifically, FedGRec leverages collaborative\nsignals from local subgraphs associated with users or items to enrich their\nrepresentation learning. Additionally, it employs dynamic spatiotemporal\nmodeling to integrate global and local user preferences in real time based on\nbusiness recommendation states, thereby deriving the final representations of\ntarget users and candidate items. By automatically filtering relevant\nbehaviors, FedGRec effectively mitigates noise interference from unreliable\nneighbors. Furthermore, through a personalized federated aggregation strategy,\nFedGRec adapts global preferences to heterogeneous domain data, enabling\ncollaborative learning of user preferences across multiple domains. Extensive\nexperiments on three datasets demonstrate that FedGRec consistently outperforms\ncompetitive single-domain and cross-domain baselines while effectively\npreserving data privacy in cross-border recommendations.", "AI": {"tldr": "FedGRec\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u8054\u90a6\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u4f5c\u4fe1\u53f7\u4f18\u5316\u7528\u6237\u504f\u597d\u8868\u793a\uff0c\u9002\u5e94\u5f02\u6784\u57df\u6570\u636e\uff0c\u5728\u8de8\u5883\u63a8\u8350\u4e2d\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u5728\u8de8\u5883\u5171\u4eab\u4e2d\uff0c\u7531\u4e8e\u654f\u611f\u6570\u636e\u53d7\u5230\u4e25\u683c\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u4f7f\u5f97\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff0c\u5982\u4f55\u5728\u786e\u4fdd\u9690\u79c1\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u5883\u4e1a\u52a1\u63a8\u8350\u6210\u4e3a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684GNN\u8bad\u7ec3\u65b9\u6cd5\u5728\u5f02\u8d28\u56fe\u6570\u636e\u4e0a\u4f18\u5316\u4e0d\u8db3\uff0c\u56e0\u6b64\u63d0\u51faFedGRec\u65b9\u6cd5\u4ee5\u6539\u5584\u6b64\u95ee\u9898\u3002", "method": "FedGRec\u65b9\u6cd5\u901a\u8fc7\u4ece\u7528\u6237\u6216\u9879\u76ee\u5173\u8054\u7684\u672c\u5730\u5b50\u56fe\u4e2d\u63d0\u53d6\u534f\u4f5c\u4fe1\u53f7\uff0c\u4e30\u5bcc\u5176\u8868\u793a\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u5b83\u5229\u7528\u52a8\u6001\u65f6\u7a7a\u5efa\u6a21\u5b9e\u65f6\u6574\u5408\u5168\u5c40\u548c\u672c\u5730\u7528\u6237\u504f\u597d\uff0c\u4ece\u4e1a\u52a1\u63a8\u8350\u72b6\u6001\u4e2d\u63d0\u53d6\u76ee\u6807\u7528\u6237\u548c\u5019\u9009\u9879\u76ee\u7684\u6700\u7ec8\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6ee4\u9664\u76f8\u5173\u884c\u4e3a\u51cf\u5c11\u4e0d\u53ef\u9760\u90bb\u5c45\u7684\u566a\u97f3\u5e72\u6270\u3002", "result": "FedGRec\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u5728\u5355\u57df\u548c\u8de8\u57df\u57fa\u51c6\u4e0a\u5747\u6709\u5353\u8d8a\u8868\u73b0\uff0c\u540c\u65f6\u6709\u6548\u4fdd\u8bc1\u4e86\u8de8\u5883\u63a8\u8350\u4e2d\u7684\u6570\u636e\u9690\u79c1\u3002", "conclusion": "FedGRec\u5728\u8de8\u5883\u63a8\u8350\u4e2d\uff0c\u4e00\u65b9\u9762\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u53e6\u4e00\u65b9\u9762\u6709\u6548\u5730\u4fdd\u62a4\u4e86\u6570\u636e\u9690\u79c1\u3002\u5b83\u901a\u8fc7\u4e2a\u6027\u5316\u7684\u8054\u90a6\u805a\u5408\u7b56\u7565\u9002\u5e94\u5f02\u6784\u57df\u6570\u636e\uff0c\u5b9e\u73b0\u8de8\u591a\u4e2a\u9886\u57df\u7684\u7528\u6237\u504f\u597d\u534f\u540c\u5b66\u4e60\u3002"}}
{"id": "2505.18572", "pdf": "https://arxiv.org/pdf/2505.18572", "abs": "https://arxiv.org/abs/2505.18572", "authors": ["Yifan Zhu", "Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yi Yang", "Yawei Luo"], "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit\nremarkable problem-solving and task planning capabilities across diverse\ndomains due to their specialized agentic roles and collaborative interactions.\nHowever, this also amplifies the severity of security risks under MAS attacks.\nTo address this, we introduce MASTER, a novel security research framework for\nMAS, focusing on diverse Role configurations and Topological structures across\nvarious scenarios. MASTER offers an automated construction process for\ndifferent MAS setups and an information-flow-based interaction paradigm. To\ntackle MAS security challenges in varied scenarios, we design a\nscenario-adaptive, extensible attack strategy utilizing role and topological\ninformation, which dynamically allocates targeted, domain-specific attack tasks\nfor collaborative agent execution. Our experiments demonstrate that such an\nattack, leveraging role and topological information, exhibits significant\ndestructive potential across most models. Additionally, we propose\ncorresponding defense strategies, substantially enhancing MAS resilience across\ndiverse scenarios. We anticipate that our framework and findings will provide\nvaluable insights for future research into MAS security challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MASTER\u6846\u67b6\u4ee5\u89e3\u51b3\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u7684\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u6784\u5efa\u8fc7\u7a0b\u548c\u4fe1\u606f\u6d41\u4ea4\u4e92\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86\u573a\u666f\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u76f8\u5e94\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u5728\u89e3\u51b3\u95ee\u9898\u548c\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u80fd\u529b\uff0c\u5176\u5b89\u5168\u98ce\u9669\u968f\u7740MAS\u653b\u51fb\u800c\u589e\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u6846\u67b6\u6765\u5e94\u5bf9MAS\u7684\u5b89\u5168\u6311\u6218\u3002", "method": "MASTER\u6846\u67b6\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u6784\u5efa\u4e0d\u540cMAS\u8bbe\u7f6e\u7684\u8fc7\u7a0b\u548c\u57fa\u4e8e\u4fe1\u606f\u6d41\u7684\u4ea4\u4e92\u8303\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u573a\u666f\u81ea\u9002\u5e94\u7684\u6269\u5c55\u653b\u51fb\u7b56\u7565\uff0c\u5229\u7528\u89d2\u8272\u548c\u62d3\u6251\u4fe1\u606f\u52a8\u6001\u5206\u914d\u9488\u5bf9\u6027\u7684\u3001\u4e0e\u9886\u57df\u76f8\u5173\u7684\u653b\u51fb\u4efb\u52a1\u8fdb\u884c\u534f\u4f5c\u4ee3\u7406\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u89d2\u8272\u548c\u62d3\u6251\u4fe1\u606f\u8fdb\u884c\u7684\u653b\u51fb\u5bf9\u5927\u591a\u6570\u6a21\u578b\u5177\u6709\u663e\u8457\u7834\u574f\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u7b56\u7565\u663e\u8457\u589e\u5f3a\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u7814\u7a76\u6846\u67b6MASTER\uff0c\u4e13\u6ce8\u4e8e\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u7684\u591a\u6837\u5316\u89d2\u8272\u914d\u7f6e\u548c\u62d3\u6251\u7ed3\u6784\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u89d2\u8272\u548c\u62d3\u6251\u4fe1\u606f\u53d1\u8d77\u7684\u653b\u51fb\u5bf9\u5927\u591a\u6570\u6a21\u578b\u5177\u6709\u663e\u8457\u7834\u574f\u6027\uff0c\u540c\u65f6\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5927\u5927\u589e\u5f3a\u4e86MAS\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322", "abs": "https://arxiv.org/abs/2505.18322", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base.", "AI": {"tldr": "GPT-4\u751f\u6210\u7684\u6587\u5316\u89c4\u8303\u4e0d\u592a\u5177\u4f53\uff0c\u9690\u85cf\u7684\u6587\u5316\u523b\u677f\u5370\u8c61\u6613\u6062\u590d\uff0c\u5f71\u54cd\u5176\u591a\u6837\u5316\u516c\u5e73\u670d\u52a1\u80fd\u529b\u3002", "motivation": "\u96be\u4ee5\u7f6e\u4fe1LLM\u80fd\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u5e94\u7528\u90a3\u4e9b\u4ef7\u503c\u89c2\uff0c\u65e8\u5728\u5f00\u53d1\u516c\u5e73\u670d\u52a1\u591a\u6837\u5316\u7528\u6237\u7fa4\u7684LLMs\u3002", "method": "\u91c7\u53d6\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u8981\u6c42LLM\u63a8\u7406\u4e0d\u540c\u6587\u5316\u53d9\u8ff0\u4e2d\u7684\u6587\u5316\u89c4\u8303\u3002", "result": "GPT-4\u751f\u6210\u7684\u6587\u5316\u89c4\u8303\u4e0d\u592a\u5177\u4f53\u4e14\u5bb9\u6613\u6062\u590d\u523b\u677f\u5370\u8c61\uff0c\u8fd9\u6311\u6218\u4e86LLM\u5728\u591a\u5143\u6587\u5316\u4e2d\u516c\u5e73\u5e94\u7528\u7684\u80fd\u529b\u3002", "conclusion": "GPT-4\u503e\u5411\u4e8e\u751f\u6210\u7684\u6587\u5316\u89c4\u8303\u4e0d\u592a\u5177\u4f53\uff0c\u5e76\u4e14\u867d\u7136\u907f\u514d\u4e86\u76f4\u63a5\u4ea7\u751f\u523b\u677f\u5370\u8c61\uff0c\u4f46\u8fd9\u4e9b\u523b\u677f\u5370\u8c61\u53ea\u662f\u9690\u85cf\u800c\u975e\u5b8c\u5168\u6291\u5236\uff0c\u5bb9\u6613\u88ab\u6062\u590d\u3002"}}
{"id": "2505.18483", "pdf": "https://arxiv.org/pdf/2505.18483", "abs": "https://arxiv.org/abs/2505.18483", "authors": ["Hongjia Wu", "Hongxin Zhang", "Wei Chen", "Jiazhi Xia"], "title": "Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support", "categories": ["cs.AI"], "comment": null, "summary": "Various industries have produced a large number of documents such as\nindustrial plans, technical guidelines, and regulations that are structurally\ncomplex and content-wise fragmented. This poses significant challenges for\nexperts and decision-makers in terms of retrieval and understanding. Although\nexisting LLM-based Retrieval-Augmented Generation methods can provide\ncontext-related suggestions, they lack quantitative weighting and traceable\nreasoning paths, making it difficult to offer multi-level and transparent\ndecision support. To address this issue, this paper proposes the RAD method,\nwhich integrates Multi-Criteria Decision Making with the semantic understanding\ncapabilities of LLMs. The method automatically extracts key criteria from\nindustry documents, builds a weighted hierarchical decision model, and\ngenerates structured reports under model guidance. The RAD framework introduces\nexplicit weight assignment and reasoning chains in decision generation to\nensure accuracy, completeness, and traceability. Experiments show that in\nvarious decision-making tasks, the decision reports generated by RAD\nsignificantly outperform existing methods in terms of detail, rationality, and\nstructure, demonstrating its application value and potential in complex\ndecision support scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RAD\u65b9\u6cd5\uff0c\u5c06\u591a\u51c6\u5219\u51b3\u7b56\u4e0eLLM\u7ed3\u5408\uff0c\u751f\u6210\u8be6\u7ec6\u4e14\u7ed3\u6784\u826f\u597d\u7684\u51b3\u7b56\u62a5\u544a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u63d0\u4f9b\u4e0a\u4e0b\u6587\u76f8\u5173\u5efa\u8bae\u65f6\uff0c\u7f3a\u4e4f\u5b9a\u91cf\u6743\u91cd\u548c\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8def\u5f84\uff0c\u96be\u4ee5\u63d0\u4f9b\u591a\u5c42\u6b21\u548c\u900f\u660e\u7684\u51b3\u7b56\u652f\u6301\u3002", "method": "RAD\u65b9\u6cd5\u5c06\u591a\u51c6\u5219\u51b3\u7b56\u4e0eLLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7ed3\u5408\uff0c\u7528\u4e8e\u81ea\u52a8\u63d0\u53d6\u884c\u4e1a\u6587\u4ef6\u7684\u5173\u952e\u6807\u51c6\uff0c\u5efa\u7acb\u52a0\u6743\u5c42\u6b21\u51b3\u7b56\u6a21\u578b\uff0c\u5728\u6a21\u578b\u6307\u5bfc\u4e0b\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e76\u5728\u51b3\u7b56\u751f\u6210\u4e2d\u5f15\u5165\u663e\u5f0f\u6743\u91cd\u5206\u914d\u548c\u63a8\u7406\u94fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAD\u5728\u5404\u79cd\u51b3\u7b56\u4efb\u52a1\u4e2d\u751f\u6210\u7684\u51b3\u7b56\u62a5\u544a\u5728\u7ec6\u8282\u3001\u5408\u7406\u6027\u548c\u7ed3\u6784\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAD\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u51b3\u7b56\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u548c\u6f5c\u529b\u3002"}}
{"id": "2505.18178", "pdf": "https://arxiv.org/pdf/2505.18178", "abs": "https://arxiv.org/abs/2505.18178", "authors": ["Min Namgung", "Yijun Lin", "JangHyeon Lee", "Yao-Yi Chiang"], "title": "Less is More: Multimodal Region Representation via Pairwise Inter-view Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the increasing availability of geospatial datasets, researchers have\nexplored region representation learning (RRL) to analyze complex region\ncharacteristics. Recent RRL methods use contrastive learning (CL) to capture\nshared information between two modalities but often overlook task-relevant\nunique information specific to each modality. Such modality-specific details\ncan explain region characteristics that shared information alone cannot\ncapture. Bringing information factorization to RRL can address this by\nfactorizing multimodal data into shared and unique information. However,\nexisting factorization approaches focus on two modalities, whereas RRL can\nbenefit from various geospatial data. Extending factorization beyond two\nmodalities is non-trivial because modeling high-order relationships introduces\na combinatorial number of learning objectives, increasing model complexity. We\nintroduce Cross modal Knowledge Injected Embedding, an information\nfactorization approach for RRL that captures both shared and unique\nrepresentations. CooKIE uses a pairwise inter-view learning approach that\ncaptures high-order information without modeling high-order dependency,\navoiding exhaustive combinations. We evaluate CooKIE on three regression tasks\nand a land use classification task in New York City and Delhi, India. Results\nshow that CooKIE outperforms existing RRL methods and a factorized RRL model,\ncapturing multimodal information with fewer training parameters and\nfloating-point operations per second (FLOPs). We release the code:\nhttps://github.com/MinNamgung/CooKIE.", "AI": {"tldr": "CooKIE\u65b9\u6cd5\u6539\u8fdb\u4e86\u5730\u533a\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u4fe1\u606f\u56e0\u5b50\u5316\u6355\u6349\u5171\u4eab\u548c\u72ec\u7279\u4fe1\u606f\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u56e0\u5b50\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e24\u4e2a\u6a21\u6001\uff0c\u800cRRL\u80fd\u591f\u4ece\u591a\u79cd\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e2d\u53d7\u76ca\u3002\u6269\u5c55\u56e0\u5b50\u5316\u5230\u4e24\u4e2a\u4ee5\u4e0a\u7684\u6a21\u6001\u662f\u56f0\u96be\u7684\uff0c\u56e0\u4e3a\u5efa\u6a21\u9ad8\u9636\u5173\u7cfb\u4f1a\u5f15\u5165\u4e00\u4e2a\u7ec4\u5408\u6570\u91cf\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u8de8\u6a21\u6001\u77e5\u8bc6\u6ce8\u5165\u5d4c\u5165\uff08CooKIE\uff09\uff0c\u5229\u7528\u4e00\u79cd\u6210\u5bf9\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u6765\u8fdb\u884c\u4fe1\u606f\u56e0\u5b50\u5316\uff0c\u6355\u83b7\u5171\u4eab\u548c\u72ec\u7279\u7684\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6a21\u578b\u9ad8\u9636\u4f9d\u8d56\uff0c\u907f\u514d\u4e86\u7a77\u5c3d\u7ec4\u5408\u3002", "result": "CooKIE\u901a\u8fc7\u6210\u5bf9\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u6355\u83b7\u9ad8\u9636\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u9ad8\u9636\u4f9d\u8d56\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u7ebd\u7ea6\u5e02\u548c\u5370\u5ea6\u5fb7\u91cc\u7684\u591a\u4e2a\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u8d8a\uff0c\u5e76\u4e14\u8bad\u7ec3\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "CooKIE\u5728\u7ebd\u7ea6\u5e02\u548c\u5370\u5ea6\u5fb7\u91cc\u8fdb\u884c\u7684\u4e09\u4e2a\u56de\u5f52\u4efb\u52a1\u548c\u4e00\u4e2a\u571f\u5730\u4f7f\u7528\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684RRL\u65b9\u6cd5\u548c\u4e00\u4e2a\u56e0\u5b50\u5316RRL\u6a21\u578b\uff0c\u5e76\u4ee5\u66f4\u5c11\u7684\u8bad\u7ec3\u53c2\u6570\u548c\u6d6e\u70b9\u8fd0\u7b97\u6bcf\u79d2\uff08FLOPs\uff09\u66f4\u597d\u5730\u6355\u83b7\u591a\u6a21\u6001\u4fe1\u606f\u3002"}}
{"id": "2505.19316", "pdf": "https://arxiv.org/pdf/2505.19316", "abs": "https://arxiv.org/abs/2505.19316", "authors": ["Rex Chen", "Stephanie Milani", "Zhicheng Zhang", "Norman Sadeh", "Fei Fang"], "title": "Making Teams and Influencing Agents: Efficiently Coordinating Decision Trees for Interpretable Multi-Agent Reinforcement Learning", "categories": ["cs.MA"], "comment": "16 pages; 2 tables; 11 figures", "summary": "Poor interpretability hinders the practical applicability of multi-agent\nreinforcement learning (MARL) policies. Deploying interpretable surrogates of\nuninterpretable policies enhances the safety and verifiability of MARL for\nreal-world applications. However, if these surrogates are to interact directly\nwith the environment within human supervisory frameworks, they must be both\nperformant and computationally efficient. Prior work on interpretable MARL has\neither sacrificed performance for computational efficiency or computational\nefficiency for performance. To address this issue, we propose HYDRAVIPER, a\ndecision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates\ntraining between agents based on expected team performance, and adaptively\nallocates budgets for environment interaction to improve computational\nefficiency. Experiments on standard benchmark environments for multi-agent\ncoordination and traffic signal control show that HYDRAVIPER matches the\nperformance of state-of-the-art methods using a fraction of the runtime, and\nthat it maintains a Pareto frontier of performance for different interaction\nbudgets.", "AI": {"tldr": "HYDRAVIPER\u662f\u4e00\u79cd\u65b0\u7684\u51b3\u7b56\u6811\u57fa\u4e8eMARL\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\uff0c\u5728\u6807\u51c6\u6d4b\u8bd5\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63d0\u9ad8\u5176\u5b9e\u9645\u5e94\u7528\u6027\u3002", "method": "HYDRAVIPER\u662f\u4e00\u79cd\u57fa\u4e8e\u51b3\u7b56\u6811\u7684MARL\u7b97\u6cd5\uff0c\u901a\u8fc7\u534f\u8c03\u4ee3\u7406\u95f4\u7684\u8bad\u7ec3\u5e76\u81ea\u9002\u5e94\u5206\u914d\u73af\u5883\u4ea4\u4e92\u9884\u7b97\u6765\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "HYDRAVIPER\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u6807\u51c6\u6d4b\u8bd5\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee5\u66f4\u5c11\u7684\u8fd0\u884c\u65f6\u95f4\u8fbe\u5230\u4e86\u4e0e\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u7ef4\u6301\u4e86\u4e0d\u540c\u4ea4\u4e92\u9884\u7b97\u7684\u6027\u80fd\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "HYDRAVIPER\u4e3aMARL\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u7684\u53ef\u89e3\u91ca\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331", "abs": "https://arxiv.org/abs/2505.18331", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA", "AI": {"tldr": "PerMedCQA\u662f\u9996\u4e2a\u6ce2\u65af\u8bed\u533b\u5b66\u6d88\u8d39\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b68,138\u4e2a\u95ee\u7b54\u5bf9\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u533b\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u95ee\u7b54\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u9762\u5411\u6d88\u8d39\u8005\u7684\u591a\u8bed\u8a00\u8d44\u6e90\u65b9\u9762\u4ecd\u7136\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u5728\u50cf\u6ce2\u65af\u8bed\u8fd9\u6837\u7684\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u4e2d\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PerMedCQA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u6ce2\u65af\u8bed\u7f16\u5199\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u6d88\u8d39\u8005\u751f\u6210\u7684\u533b\u5b66\u95ee\u9898\u7684\u56de\u7b54\u80fd\u529b\u3002", "method": "\u6211\u4eec\u91c7\u7528\u4e86MedJudge\uff0c\u4e00\u4e2a\u7531LLM\u8bc4\u5206\u5458\u9a71\u52a8\u7684\u65b0\u9896\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u4e0e\u4e13\u5bb6\u4eba\u5de5\u6ce8\u91ca\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u51c6\u786e\u548c\u5177\u4e0a\u4e0b\u6587\u610f\u8bc6\u7684\u533b\u7597\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "conclusion": "PerMedCQA\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d44\u6e90\uff0c\u5b83\u4e3a\u533b\u5b66\u6d88\u8d39\u95ee\u7b54\u7cfb\u7edf\u7684\u5f00\u53d1\u548cLLM\u7684\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5e76\u516c\u5f00\u63d0\u4f9b\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.18492", "pdf": "https://arxiv.org/pdf/2505.18492", "abs": "https://arxiv.org/abs/2505.18492", "authors": ["Jialiang Sun", "Yuzhi Tang", "Ao Li", "Chris J. Maddison", "Kuldeep S. Meel"], "title": "Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions", "categories": ["cs.AI"], "comment": null, "summary": "Mathematical reasoning lies at the heart of artificial intelligence,\nunderpinning applications in education, program verification, and\nresearch-level mathematical discovery. Mathematical competitions, in\nparticular, present two challenging problem types: theorem-proving, requiring\nrigorous proofs of stated conclusions, and answer-construction, involving\nhypothesizing and formally verifying mathematical objects. Large Language\nModels (LLMs) effectively generate creative candidate answers but struggle with\nformal verification, while symbolic provers ensure rigor but cannot efficiently\nhandle creative conjecture generation. We introduce the\nEnumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method\nintegrating LLM-based enumeration and pattern-driven conjecturing with formal\ntheorem proving. We present ConstructiveBench, a dataset of 3,431\nanswer-construction problems in various math competitions with verified Lean\nformalizations. On the ConstructiveBench dataset, ECP improves the accuracy of\nanswer construction from the Chain-of-Thought (CoT) baseline of 14.54% to\n45.06% with the gpt-4.1-mini model. Moreover, combining with ECP's constructed\nanswers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct\nproofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01%\naccuracy, compared to 9.86% for symbolic-only baselines. Our code and dataset\nare publicly available at GitHub and HuggingFace, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faECP\u6846\u67b6\uff0c\u7ed3\u5408LLM\u4e0e\u7b26\u53f7\u8bc1\u660e\uff0c\u63d0\u9ad8\u6784\u5efa\u95ee\u9898\u51c6\u786e\u6027\uff0c\u4ece14.54%\u63d0\u5347\u523045.06%\uff0c\u5e76\u751f\u6210\u4e8625.01%\u7cbe\u5ea6\u7684\u6b63\u786e\u8bc1\u660e\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u521b\u610f\u751f\u6210\u4e0e\u7b26\u53f7\u8bc1\u660e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u6570\u5b66\u7ade\u8d5b\u4e2d\u590d\u6742\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u5f15\u5165Enumerate-Conjecture-Prove (ECP)\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8fdb\u884c\u679a\u4e3e\u548c\u6a21\u5f0f\u9a71\u52a8\u7684\u731c\u60f3\uff0c\u7ed3\u5408\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u3002", "result": "\u5728ConstructiveBench\u6570\u636e\u96c6\u4e0a\uff0cECP\u5c06\u56de\u7b54\u6784\u5efa\u7684\u51c6\u786e\u6027\u4eceCoT\u57fa\u7ebf\u768414.54%\u63d0\u5347\u523045.06%\u3002\u7ed3\u5408ECP\u751f\u6210\u7684\u7b54\u6848\uff0cDeepSeek-Prover-V2-7B\u6a21\u578b\u5728Lean\u4e2d\u751f\u6210\u4e86858\u4e2a\u6784\u9020\u95ee\u9898\u7684\u6b63\u786e\u8bc1\u660e\uff0c\u7cbe\u5ea6\u8fbe\u523025.01%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86ECP\u6846\u67b6\uff0c\u4e00\u79cd\u96c6\u6210LLM\u548c\u7b26\u53f7\u63a8\u7406\u7684\u6a21\u5757\u5316\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6570\u5b66\u7ade\u8d5b\u4e2d\u95ee\u9898\u7684\u89e3\u51b3\u7cbe\u5ea6\u3002"}}
{"id": "2505.18179", "pdf": "https://arxiv.org/pdf/2505.18179", "abs": "https://arxiv.org/abs/2505.18179", "authors": ["Ata Akbari Asanjan", "Olivia Alexander", "Tom Berg", "Clara Zhang", "Matt Yang", "Jad Makki", "Disha Shidham", "Srija Chakraborty", "William Bender", "Stephen Peng", "Arun Ravindran", "Olivier Raiman", "David Potere", "David Bell"], "title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "We present the GAIA (Geospatial Artificial Intelligence for Atmospheres)\nFoundation Model, a novel model that combines masked autoencoders (MAE) and\nself-DIstillation with NO labels (DINO) for analyzing global atmospheric\npatterns in satellite imagery. By integrating these complementary\nself-supervised learning approaches, our model simultaneously captures both\nlocal features and global dependencies. We address two critical challenges in\nsatellite data analysis: reconstructing missing regions and estimating\nprecipitation patterns as our first downstream tasks. The model demonstrates\nsuperior temporal pattern capture compared to standard MAE approaches, while\nmaintaining robust performance in downstream tasks. Our experimental results\nshow strong gap-filling capabilities across varying mask ratios and accurate\nprecipitation estimation with limited training data, achieving a false alarm\nratio of 0.088 and structural similarity of 0.881. This work represents an\nadvancement in self-supervised learning for atmospheric science, providing a\nfoundation for improved weather monitoring and climate analysis. The trained\nmodel weights and accompanying code are publicly available as open-source on\nHugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.", "AI": {"tldr": "GAIA\u6a21\u578b\u7ed3\u5408MAE\u548cDINO\uff0c\u6210\u529f\u89e3\u51b3\u91cd\u5efa\u4e22\u5931\u533a\u57df\u548c\u964d\u6c34\u4f30\u7b97\u7684\u6311\u6218\uff0c\u8868\u73b0\u51fa\u8272\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u6570\u636e\u5206\u6790\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u91cd\u5efa\u4e22\u5931\u533a\u57df\u548c\u4f30\u7b97\u964d\u6c34\u6a21\u5f0f\u3002", "method": "GAIA\u6a21\u578b\u7ed3\u5408\u4e86\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u548c\u65e0\u6807\u7b7e\u7684\u81ea\u84b8\u998f\uff08DINO\uff09\u7528\u4e8e\u5206\u6790\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u5168\u7403\u5927\u6c14\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u5728\u586b\u8865\u4e0d\u540c\u63a9\u7801\u6bd4\u7387\u4e0a\u7684\u7a7a\u767d\u80fd\u529b\u8868\u73b0\u5f3a\u52b2\uff0c\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u7cbe\u51c6\u964d\u6c34\u4f30\u7b97\uff0c\u8fbe\u5230\u4e860.088\u7684\u9519\u8bef\u62a5\u8b66\u6bd4\u548c0.881\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5927\u6c14\u79d1\u5b66\u4e2d\u7684\u8fdb\u6b65\uff0c\u4e3a\u6539\u8fdb\u5929\u6c14\u76d1\u6d4b\u548c\u6c14\u5019\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u548c\u9644\u5e26\u4ee3\u7801\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u5728Hugging Face\u4e0a\u516c\u5f00\u3002"}}
{"id": "2505.19637", "pdf": "https://arxiv.org/pdf/2505.19637", "abs": "https://arxiv.org/abs/2505.19637", "authors": ["Byunghyun Yoo", "Younghwan Shin", "Hyunwoo Kim", "Euisok Chung", "Jeongmin Yang"], "title": "Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning", "categories": ["cs.MA", "I.2.11"], "comment": null, "summary": "In standard reinforcement learning, an episode is defined as a sequence of\ninteractions between agents and the environment, which terminates upon reaching\na terminal state or a pre-defined episode length. Setting a shorter episode\nlength enables the generation of multiple episodes with the same number of data\nsamples, thereby facilitating an exploration of diverse states. While shorter\nepisodes may limit the collection of long-term interactions, they may offer\nsignificant advantages when properly managed. For example, trajectory\ntruncation in single-agent reinforcement learning has shown how the benefits of\nshorter episodes can be leveraged despite the trade-off of reduced long-term\ninteraction experiences. However, this approach remains underexplored in MARL.\nThis paper proposes a novel MARL approach, Adaptive Episode Length Adjustment\n(AELA), where the episode length is initially limited and gradually increased\nbased on an entropy-based assessment of learning progress. By starting with\nshorter episodes, agents can focus on learning effective strategies for initial\nstates and minimize time spent in dead-end states. The use of entropy as an\nassessment metric prevents premature convergence to suboptimal policies and\nensures balanced training over varying episode lengths. We validate our\napproach using the StarCraft Multi-agent Challenge (SMAC) and a modified\npredator-prey environment, demonstrating significant improvements in both\nconvergence speed and overall performance compared to existing methods. To the\nbest of our knowledge, this is the first study to adaptively adjust episode\nlength in MARL based on learning progress.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8c03\u6574\u5267\u96c6\u957f\u5ea6\u7684MARL\u65b9\u6cd5\uff0c\u4f7f\u7528\u71b5\u8bc4\u4f30\u5b66\u4e60\u8fdb\u6b65\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u5728\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8f83\u77ed\u7684\u5267\u96c6\u957f\u5ea6\u53ef\u80fd\u9650\u5236\u957f\u671f\u4ea4\u4e92\u7684\u6536\u96c6\uff0c\u4f46\u5728\u6b63\u786e\u7ba1\u7406\u65f6\u53ef\u4ee5\u63d0\u4f9b\u663e\u8457\u4f18\u52bf\u3002\u5c24\u5176\u5728\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8f83\u77ed\u5267\u96c6\u7684\u597d\u5904\u5df2\u88ab\u8bc1\u660e\uff0c\u4f46\u5728MARL\u4e2d\u8fd9\u79cd\u65b9\u6cd5\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MARL\u65b9\u6cd5\uff0c\u5373\u81ea\u9002\u5e94\u5267\u96c6\u957f\u5ea6\u8c03\u6574\uff08Adaptive Episode Length Adjustment, AELA\uff09\uff0c\u57fa\u4e8e\u71b5\u8bc4\u4f30\u5b66\u4e60\u8fdb\u5ea6\u6765\u9010\u6e10\u589e\u52a0\u5267\u96c6\u957f\u5ea6\u3002", "result": "\u4f7f\u7528StarCraft\u591a\u667a\u80fd\u4f53\u6311\u6218\uff08SMAC\uff09\u548c\u6539\u8fdb\u7684\u6355\u98df\u8005-\u730e\u7269\u73af\u5883\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5728\u6536\u655b\u901f\u5ea6\u548c\u603b\u4f53\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u5728MARL\u4e2d\u9996\u6b21\u6839\u636e\u5b66\u4e60\u8fdb\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u5267\u96c6\u957f\u5ea6\uff0c\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2505.18343", "pdf": "https://arxiv.org/pdf/2505.18343", "abs": "https://arxiv.org/abs/2505.18343", "authors": ["Yash Kumar Atri", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "Model Editing with Graph-Based External Memory", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86HYPE\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7403\u51e0\u4f55\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6a21\u578b\u7f16\u8f91\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u548c\u8fc7\u65f6\u53c2\u6570\u77e5\u8bc6\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u8fc7\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u52a8\u6001\u66f4\u65b0\u65f6\u51fa\u73b0\u8fc7\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u4f7f\u7528\u53cc\u7a33\u5b9a\u5316\u6280\u672f\u7ed3\u5408\u8d85\u7403\u7ea7\u6570\u7684\u56fe\u6784\u5efa\u4e0eM\u00f6bius\u53d8\u6362\u66f4\u65b0\u3002", "result": "\u5728CounterFact\u3001CounterFact+\u548cMQuAKE\u7684\u5b9e\u9a8c\u4e2d\uff0cHYPE\u660e\u663e\u589e\u5f3a\u4e86\u7f16\u8f91\u7684\u7a33\u5b9a\u6027\u3001\u4e8b\u5b9e\u7684\u51c6\u786e\u6027\u548c\u591a\u8df3\u63a8\u7406\u3002", "conclusion": "HYPE\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8f91\u7684\u7a33\u5b9a\u6027\u3001\u4e8b\u5b9e\u7684\u51c6\u786e\u6027\u548c\u591a\u8df3\u63a8\u7406\u80fd\u529b\u3002"}}
