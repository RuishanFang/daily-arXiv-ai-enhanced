<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 91]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 90]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)
*Jinzhu Yang*

Main category: cs.CL

TL;DR: 研究展示了Prompt-bioMRC模型如何通过新颖的提示学习方法提升医疗领域的实体识别能力，显著超越传统模型，支持医疗信息提取和决策。


<details>
  <summary>Details</summary>
Motivation: 近年来，大规模模型的出现推动了实体识别任务的进步。特别是BioBERT模型在医疗文本方面显著提升了实体识别能力。这项研究致力于通过提示学习方法推进医疗领域的实体识别。

Method: 研究引入了Prompt-bioMRC模型，结合硬模板和软提示设计，旨在提高医疗实体识别的精度和效率。

Result: 通过在不同医疗数据集上的大量实验，发现该方法在精度和效率上均优于传统模型。

Conclusion: 研究表明，Prompt-bioMRC模型在医疗实体识别方面优于传统模型，证明了其方法的有效性和潜力，为智能诊断系统等应用提供可靠的技术支持。

Abstract: This study is dedicated to exploring the application of prompt learning
methods to advance Named Entity Recognition (NER) within the medical domain. In
recent years, the emergence of large-scale models has driven significant
progress in NER tasks, particularly with the introduction of the BioBERT
language model, which has greatly enhanced NER capabilities in medical texts.
Our research introduces the Prompt-bioMRC model, which integrates both hard
template and soft prompt designs aimed at refining the precision and efficiency
of medical entity recognition. Through extensive experimentation across diverse
medical datasets, our findings consistently demonstrate that our approach
surpasses traditional models. This enhancement not only validates the efficacy
of our methodology but also highlights its potential to provide reliable
technological support for applications like intelligent diagnosis systems. By
leveraging advanced NER techniques, this study contributes to advancing
automated medical data processing, facilitating more accurate medical
information extraction, and supporting efficient healthcare decision-making
processes.

</details>


### [2] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
*Lukas Rauch,Moritz Wirth,Denis Huseljic,Marek Herde,Bernhard Sick,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 利用大型语言模型嵌入降低深度主动学习计算成本，嵌入质量影响查询策略选择，要根据任务进行具体评估。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型嵌入质量对深度主动学习中查询策略的影响，并为此建立一个基准。

Method: 本文使用了来自大规模文本嵌入基准测试排行榜上的五个表现最好的模型以及两个基线模型，对十个不同的文本分类任务进行了实验。

Result: 实验结果表明，高质量的嵌入有助于在主动学习的早期阶段提升性能，并且最佳查询策略对嵌入质量敏感，不同任务需要具体评估最优策略。

Conclusion: 通过利用冻结的大型语言模型嵌入，可以显著降低在深度主动学习中反复微调大型模型的计算成本。

Abstract: The advent of large language models (LLMs) capable of producing
general-purpose representations lets us revisit the practicality of deep active
learning (AL): By leveraging frozen LLM embeddings, we can mitigate the
computational costs of iteratively fine-tuning large backbones. This study
establishes a benchmark and systematically investigates the influence of LLM
embedding quality on query strategies in deep AL. We employ five top-performing
models from the massive text embedding benchmark (MTEB) leaderboard and two
baselines for ten diverse text classification tasks. Our findings reveal key
insights: First, initializing the labeled pool using diversity-based sampling
synergizes with high-quality embeddings, boosting performance in early AL
iterations. Second, the choice of the optimal query strategy is sensitive to
embedding quality. While the computationally inexpensive Margin sampling can
achieve performance spikes on specific datasets, we find that strategies like
Badge exhibit greater robustness across tasks. Importantly, their effectiveness
is often enhanced when paired with higher-quality embeddings. Our results
emphasize the need for context-specific evaluation of AL strategies, as
performance heavily depends on embedding quality and the target task.

</details>


### [3] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
*Abhay Gupta,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CL

TL;DR: 这项研究引入了NovelHopQA基准，评估大型语言模型在小说中进行多跳问题回答的能力，发现增加上下文长度和推理深度会降低模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理长文本的多跳推理时存在困难，之前的基准测试都是单独考察长文本理解或多跳推理，没有结合自然叙述环境中的上下文长度和推理深度变化。

Method: 引入一个名为NovelHopQA的基准，该基准测试在83部完整的公开领域小说的64k-128k令牌的摘录上进行1至4跳的问题回答。该基准采用一个以关键字为引导的管道来建立基于连贯故事线的跳跃分离链。

Result: 即便是最前沿的模型，在跳数和上下文长度增加时其准确性会下降，揭示了简单增加规模无法保证强健的推理能力。失败模式分析揭示了一些常见问题，如最后一跳整合丢失和长距漂移。

Conclusion: 在上下文增加和推理深度加大的条件下，即便是最前沿的大型语言模型，其准确性也会明显下降，显示出仅通过模型规模的增大无法保证强健的推理能力。

Abstract: Current large language models (LLMs) struggle to answer questions that span
tens of thousands of tokens, especially when multi-hop reasoning is involved.
While prior benchmarks explore long-context comprehension or multi-hop
reasoning in isolation, none jointly vary context length and reasoning depth in
natural narrative settings. We introduce NovelHopQA, the first benchmark to
evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length
public-domain novels. A keyword-guided pipeline builds hop-separated chains
grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models
and apply oracle-context filtering to ensure all questions are genuinely
answerable. Human annotators validate both alignment and hop depth. We noticed
consistent accuracy drops with increased hops and context length, even in
frontier models-revealing that sheer scale does not guarantee robust reasoning.
Our failure mode analysis highlights common breakdowns, such as missed
final-hop integration and long-range drift. NovelHopQA offers a controlled
diagnostic setting to stress-test multi-hop reasoning at scale.

</details>


### [4] [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
*Timothy Do,Pranav Saran,Harshita Poojary,Pranav Prabhu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 提出了一种混合模型，通过剪枝策略提高俚语和隐喻分类在低资源语言中的效率，取得了78%和83%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理中俚语表达的问题，特别是在资源匮乏语言中的应用。

Method: 采用预训练的多语言BERT模型与双向LSTM和线性分类器相结合的混合模型，并使用注意力头剪枝策略提高效率。

Result: 剪枝后的模型在隐喻分类中达到了78%的准确率，而在俚语分类任务中达到了83%的准确率。

Conclusion: 注意力头剪枝策略可以有效提升自然语言处理工具在低资源语言上的效率，尤其是在俚语和隐喻分类任务中。

Abstract: In this paper, we address the persistent challenges that figurative language
expressions pose for natural language processing (NLP) systems, particularly in
low-resource languages such as Konkani. We present a hybrid model that
integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM
and a linear classifier. This architecture is fine-tuned on a newly introduced
annotated dataset for metaphor classification, developed as part of this work.
To improve the model's efficiency, we implement a gradient-based attention head
pruning strategy. For metaphor classification, the pruned model achieves an
accuracy of 78%. We also applied our pruning approach to expand on an existing
idiom classification task, achieving 83% accuracy. These results demonstrate
the effectiveness of attention head pruning for building efficient NLP tools in
underrepresented languages.

</details>


### [5] [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)
*Christopher Lee Lübbers*

Main category: cs.CL

TL;DR: 研究利用DPO方法改进释义类型生成的准确性，并开发出可靠性更高的检测模型，有助于丰富用户对齐的语言生成研究。


<details>
  <summary>Details</summary>
Motivation: 现有的释义生成方法往往依赖自动化指标和有限的人类标注数据，导致无法准确符合人类的偏好，从而影响语义准确性和语言转换的效果。

Method: 本研究通过利用人类排序的释义类型数据集，并整合直接偏好优化（DPO）来直接使模型输出与人类判断对齐。

Result: DPO 方法使释义类型生成的准确性较监督基线提高了3个百分点，并使人类偏好评分提高了7个百分点。此外，释义类型检测模型在添加/删除方面的 F1 分数为0.91，相同极性替代为0.78，标点符号变化为0.70。

Conclusion: 结合人类偏好的数据和DPO训练能够产生更可靠、更语义准确的释义，从而改善后续的应用，比如精简摘要和更强大的问答系统。PTD模型不仅超越了自动化指标，还提供了一个更可靠的框架来评估释义质量，为基于人类中心标准的未来评估奠定了更强的基础。

Abstract: Paraphrasing re-expresses meaning to enhance applications like text
simplification, machine translation, and question-answering. Specific
paraphrase types facilitate accurate semantic analysis and robust language
models. However, existing paraphrase-type generation methods often misalign
with human preferences due to reliance on automated metrics and limited
human-annotated training data, obscuring crucial aspects of semantic fidelity
and linguistic transformations.
  This study addresses this gap by leveraging a human-ranked paraphrase-type
dataset and integrating Direct Preference Optimization (DPO) to align model
outputs directly with human judgments. DPO-based training increases
paraphrase-type generation accuracy by 3 percentage points over a supervised
baseline and raises human preference ratings by 7 percentage points. A newly
created human-annotated dataset supports more rigorous future evaluations.
Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for
addition/deletion, 0.78 for same polarity substitution, and 0.70 for
punctuation changes.
  These findings demonstrate that preference data and DPO training produce more
reliable, semantically accurate paraphrases, enabling downstream applications
such as improved summarization and more robust question-answering. The PTD
model surpasses automated metrics and provides a more reliable framework for
evaluating paraphrase quality, advancing paraphrase-type research toward
richer, user-aligned language generation and establishing a stronger foundation
for future evaluations grounded in human-centric criteria.

</details>


### [6] [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)
*E Fan,Weizong Wang,Tianhan Zhang*

Main category: cs.CL

TL;DR: ChatCFD通过大型语言模型自动化CFD工作流，降低了复杂性，可重现复杂的CFD结果。


<details>
  <summary>Details</summary>
Motivation: CFD操作复杂且需要丰富的专业知识，限制了其科学和工程领域的应用。

Method: 使用大型语言模型驱动的流程来自动化CFD工作流，并在OpenFOAM框架中操作。

Result: 验证表明ChatCFD能够以较高的准确性和适应性自主处理复杂、未见过的配置，并重现已发布的CFD结果。

Conclusion: ChatCFD可以自主重现已发布的CFD结果，能够处理复杂和未见过的配置。

Abstract: Computational Fluid Dynamics (CFD) is essential for scientific and
engineering advancements but is limited by operational complexity and the need
for extensive expertise. This paper presents ChatCFD, a large language
model-driven pipeline that automates CFD workflows within the OpenFOAM
framework. It enables users to configure and execute complex simulations from
natural language prompts or published literature with minimal expertise. The
innovation is its structured approach to database construction, configuration
validation, and error reflection, integrating CFD and OpenFOAM knowledge with
general language models to improve accuracy and adaptability. Validation shows
ChatCFD can autonomously reproduce published CFD results, handling complex,
unseen configurations beyond basic examples, a task challenging for general
language models.

</details>


### [7] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
*Feng Wang,Yiding Sun,Jiaxin Mao,Wei Xue,Danqing Xu*

Main category: cs.CL

TL;DR: 推出FinS-Pilot来评估在金融应用中的RAG系统能力，提供了一种新的评估框架及数据集，解决了金融领域评估工具的缺乏问题。


<details>
  <summary>Details</summary>
Motivation: 现有金融RAG基准由于数据保密性问题及缺乏动态数据集成而受到限制。

Method: 提出了一种新颖的评估基准FinS-Pilot，结合实时API数据与结构化文本资源，通过意图分类框架来组织不同金融领域的内容，以评估金融助手处理静态知识和时间敏感信息的能力。

Result: 通过对多个中国领先的LLM进行系统实验，证明了FinS-Pilot能够有效识别适合金融应用的模型，填补了金融领域专业评估工具的空白。

Conclusion: 本研究提出了一种实用的评估框架和数据集，以推进金融NLP系统的研究。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various professional domains, with their performance typically evaluated
through standardized benchmarks. However, the development of financial RAG
benchmarks has been constrained by data confidentiality issues and the lack of
dynamic data integration. To address this issue, we introduces FinS-Pilot, a
novel benchmark for evaluating RAG systems in online financial applications.
Constructed from real-world financial assistant interactions, our benchmark
incorporates both real-time API data and structured text sources, organized
through an intent classification framework covering critical financial domains
such as equity analysis and macroeconomic forecasting. The benchmark enables
comprehensive evaluation of financial assistants' capabilities in handling both
static knowledge and time-sensitive market information. Through systematic
experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's
effectiveness in identifying models suitable for financial applications while
addressing the current gap in specialized evaluation tools for the financial
domain. Our work contributes both a practical evaluation framework and a
curated dataset to advance research in financial NLP systems. The code and
dataset are accessible on
GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [8] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
*Duzhen Zhang,Yong Ren,Zhong-Zhi Li,Yahan Yu,Jiahua Dong,Chenxing Li,Zhilong Ji,Jinfeng Bai*

Main category: cs.CL

TL;DR: 该研究提出BranchLoRA框架，通过提高参数效率和引入任务选择器，有效缓解了灾难性遗忘问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态持续指令微调MCIT中，使用Mixture-of-Experts MoE LoRA框架会导致参数效率不足和灾难性遗忘，影响模型性能。

Method: 提出了一种不对称的框架BranchLoRA，通过引入灵活的调优冻结机制和任务选择器，提高了效率和性能，并防止了灾难性遗忘。

Result: 实验表明，BranchLoRA在最新的MCIT基准测试中表现优异，明显超越MoELoRA，并能够在多种MLLMs规模下维持其优势。

Conclusion: BranchLoRA在增强性能和效率方面表现优异，显著优于现有的MoELoRA框架，并在各种MLLMs规模下保持其优越性。

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal
Large Language Models (MLLMs) to continually align with human intent across
sequential tasks. Existing approaches often rely on the Mixture-of-Experts
(MoE) LoRA framework to preserve previous instruction alignments. However,
these methods are prone to Catastrophic Forgetting (CF), as they aggregate all
LoRA blocks via simple summation, which compromises performance over time. In
this paper, we identify a critical parameter inefficiency in the MoELoRA
framework within the MCIT context. Based on this insight, we propose
BranchLoRA, an asymmetric framework to enhance both efficiency and performance.
To mitigate CF, we introduce a flexible tuning-freezing mechanism within
BranchLoRA, enabling branches to specialize in intra-task knowledge while
fostering inter-task collaboration. Moreover, we incrementally incorporate
task-specific routers to ensure an optimal branch distribution over time,
rather than favoring the most recent task. To streamline inference, we
introduce a task selector that automatically routes test inputs to the
appropriate router without requiring task identity. Extensive experiments on
the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms
MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [9] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
*Xiang Li,Jiayi Xin,Qi Long,Weijie J. Su*

Main category: cs.CL

TL;DR: 论文提出KnowSum框架，通过估计未观察知识改进LLM评估，揭示更多模型能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的评估不一致，无法准确反映模型的实际能力，其中一个原因是未观察到的知识被忽视。

Method: 本文提出了一种统计框架KnowSum，它通过外推已观察知识实例的出现频率来估计未观察部分，实现更全面的评估。

Result: 实验显示，依赖于观察到的表现会遗漏大量知识，而KnowSum能够提供显著不同的排名比较，展示模型的内部知识。

Conclusion: KnowSum工具可用于更全面地评估大型语言模型，以揭示其未被观察到的知识，从而改善当前评估标准的不足。

Abstract: Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.

</details>


### [10] [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)
*Juncheng Wu,Sheng Liu,Haoqin Tu,Hang Yu,Xiaoke Huang,James Zou,Cihang Xie,Yuyin Zhou*

Main category: cs.CL

TL;DR: 研究大型语言模型在医学和数学中的推理路径，提出新评估框架，发现强化学习提升推理质量而监督学习则在某些方面影响推理质量。


<details>
  <summary>Details</summary>
Motivation: 探讨提升推理准确性和内部推理过程透明度问题，特别是在医疗和数学领域的复杂任务中模型推理过程的质量与透明度。

Method: 引入了一个细粒度的评估框架，通过知识指数（KI）和信息增益（InfoGain）分别衡量知识的正确性和推理质量。研究使用SFT和/或RL训练的R1-distilled和基础Qwen模型。

Result: 通过研究，发现SFT虽然提高了两领域的最终答案准确性，但通常会降低推理质量。在医疗领域，SFT至关重要，而RL能够通过修剪推理路径中的不准确或不相关知识，提高推理准确性和知识正确性。

Conclusion: 本文研究了推理增强型大语言模型在医学和数学领域的推理过程，发现R1-distilled模型的推理能力未能有效转移到医疗领域。利用监督微调（SFT）和/或强化学习（RL）训练的模型在医学领域中，尽管SFT会提升最终答案的准确性，但常常以推理质量为代价。另一方面，RL通过修剪不准确或不相关的知识，改善了推理路径的准确性和知识的正确性。

Abstract: Recent advances in reasoning-enhanced Large Language Models such as
OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex
tasks. However, the quality and transparency of their internal reasoning
processes remain underexplored. This work moves beyond the final-answer
accuracy and investigates step-by-step reasoning in the medical and
mathematical domains by explicitly decomposing the thinking trajectories into
two parts: knowledge and reasoning. Specifically, we introduce a fine-grained
evaluation framework that judges: (1) the correctness of knowledge used
(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured
by Information Gain (InfoGain)). Using this framework, we study R1-distilled
and base Qwen models trained with supervised fine-tuning (SFT) and/or
reinforcement learning (RL) in the medical and math domains. Three intriguing
findings emerge: (1) The general reasoning abilities in R1-distilled models do
not transfer effectively to the medical domain through either SFT or RL. (2)
SFT raises final-answer accuracy in both domains, but often at the cost of
reasoning quality: InfoGain drops by 38.9% on average compared with untrained
models; In the medical domain, however, SFT remains crucial because domain
knowledge is indispensable. (3) RL enhances medical reasoning by pruning
inaccurate or irrelevant knowledge from reasoning paths, thereby improving both
reasoning accuracy and knowledge correctness.

</details>


### [11] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
*Michael Li,Nishant Subramani*

Main category: cs.CL

TL;DR: 研究表明，无论模型架构和规模如何，现代大型语言模型在语言信息的线性和非线性编码上展现出一致的模式，表明这些特性对预测下一个单词至关重要。


<details>
  <summary>Details</summary>
Motivation: 目前的语言模型如BERT和GPT-2的词汇信息编码方式已被广泛研究，但对于现代大型语言模型如何编码语言信息，我们尚未完全理解，因此需要对其进行更深入的探索。

Method: 我们利用线性和非线性分类器对各种层次的激活进行训练，以预测单词的词根和屈折特征，并分析不同模型的屈折形态编码策略。

Result: 不同的模型在信息编码上展现出一致性，这表明尽管LLM技术有显著进展，但Transformer模型在拓扑上有相似的语言信息组织方式。这种一致性可能在训练初期即被建立，并对下一个单词预测发挥重要作用。

Conclusion: 通过研究16种不同结构、大小和训练模式的语言模型，发现它们在处理词汇信息和屈折形态时存在相似的模式。这些模式显示了模型在早期层中以线性方式集中词汇信息，随着层数加深而更倾向于非线性方式，同时在所有层中均匀地保持对屈折信息的线性可分性。

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing.

</details>


### [12] [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)
*Joshua Rozner,Leonie Weissweiler,Cory Shain*

Main category: cs.CL

TL;DR: 研究探讨预训练语言模型在有限数据条件下对语言结构的学习能力，并显示其与整体模型表现相关。


<details>
  <summary>Details</summary>
Motivation: 语言学习的研究探讨儿童如何通过环境中的统计数据来习得语言结构（形式-意义配对）。近期的研究在预训练语言模型（PLMs）中发现对语言结构的敏感性，尽管模型通常经过大量数据训练，令人质疑其与人类语言学习的关联性。

Method: 采用2024 BabyLM挑战中的模型，使用Rozner等人的方法来评估模型在代表不同语言结构方面的能力。

Result: 即使模型在仅使用人类发展上合理的数据量进行训练时，仍能代表多种语言结构，包括难以区分的复杂语言结构。此外，表现出良好语言结构认知能力的模型在BabyLM基准测试中表现更佳。

Conclusion: 研究表明，预训练语言模型即使在较少数据训练情况下，仍能有效学习复杂语言结构，并且这种学习能力与模型整体表现具有关联性。

Abstract: Construction grammar posits that children acquire constructions (form-meaning
pairings) from the statistics of their environment. Recent work supports this
hypothesis by showing sensitivity to constructions in pretrained language
models (PLMs), including one recent study (Rozner et al., 2025) demonstrating
that constructions shape the PLM's output distribution. However, models under
study have generally been trained on developmentally implausible amounts of
data, casting doubt on their relevance to human language learning. Here we use
Rozner et al.'s methods to evaluate constructional learning in models from the
2024 BabyLM challenge. Our results show that even when trained on
developmentally plausible quantities of data, models represent diverse
constructions, even hard cases that are superficially indistinguishable. We
further find correlational evidence that constructional performance may be
functionally relevant: models that better represent constructions perform
better on the BabyLM benchmarks.

</details>


### [13] [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)
*Amir Hussein,Cihan Xiao,Matthew Wiesner,Dan Povey,Leibny Paola Garcia,Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: 本文提出了HENT-SRT框架，通过任务划分和架构优化提高了NT在语音翻译中的性能，并减少了训练复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的神经转换器（NT）在语音翻译（ST）中表现不佳，存在词序重排的问题，并且在同时建模ASR和ST时性能下降。NT的训练成本也较高。

Method: 提出了一种新的框架HENT-SRT，通过任务分解、更高效的架构设计以及自蒸馏和CTC一致性正则化来解决上述问题。

Result: 在阿拉伯语、西班牙语和普通话三个会话数据集上评估了该方法，取得了NT模型的新性能记录，并显著缩小了与AED系统的差距。

Conclusion: 提出的HENT-SRT框架在解决NT在ST中的重新排序问题和计算效率上取得了良好成效，并且在保持ASR性能的同时提高了翻译质量。

Abstract: Neural transducers (NT) provide an effective framework for speech streaming,
demonstrating strong performance in automatic speech recognition (ASR).
However, the application of NT to speech translation (ST) remains challenging,
as existing approaches struggle with word reordering and performance
degradation when jointly modeling ASR and ST, resulting in a gap with
attention-based encoder-decoder (AED) models. Existing NT-based ST approaches
also suffer from high computational training costs. To address these issues, we
propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech
Recognition and Translation), a novel framework that factorizes ASR and
translation tasks to better handle reordering. To ensure robust ST while
preserving ASR performance, we use self-distillation with CTC consistency
regularization. Moreover, we improve computational efficiency by incorporating
best practices from ASR transducers, including a down-sampled hierarchical
encoder, a stateless predictor, and a pruned transducer loss to reduce training
complexity. Finally, we introduce a blank penalty during decoding, reducing
deletions and improving translation quality. Our approach is evaluated on three
conversational datasets Arabic, Spanish, and Mandarin achieving new
state-of-the-art performance among NT models and substantially narrowing the
gap with AED-based systems.

</details>


### [14] [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)
*Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli,Andre Martins,Giuseppe Attanasio*

Main category: cs.CL

TL;DR: 探讨语音翻译模型的性别编码能力，传统模型可捕捉性别信息，新架构易出现阳性翻译偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨语音翻译模型中是否也会捕捉到说话者的性别特征，并评估这些特征对性别翻译的影响。

Method: 采用探测方法，评估多种语音翻译模型中的性别编码情况。

Result: 传统的编码解码模型能够捕捉到性别信息，而将语音编码器与机器翻译系统集成新的架构则不能。低性别编码能力导致系统更倾向于输出阳性翻译，尤其在新架构中表现更为显著。

Conclusion: 新的语音翻译架构在性别信息捕捉上不如传统架构，加剧了翻译中的性别偏见问题。

Abstract: Recent studies on interpreting the hidden states of speech models have shown
their ability to capture speaker-specific features, including gender. Does this
finding also hold for speech translation (ST) models? If so, what are the
implications for the speaker's gender assignment in translation? We address
these questions from an interpretability perspective, using probing methods to
assess gender encoding across diverse ST models. Results on three language
directions (English-French/Italian/Spanish) indicate that while traditional
encoder-decoder models capture gender information, newer architectures --
integrating a speech encoder with a machine translation system via adapters --
do not. We also demonstrate that low gender encoding capabilities result in
systems' tendency toward a masculine default, a translation bias that is more
pronounced in newer architectures.

</details>


### [15] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
*Salman Rahman,Sheriff Issaka,Ashima Suvarna,Genglin Liu,James Shiffer,Jaeyoung Lee,Md Rizwan Parvez,Hamid Palangi,Shi Feng,Nanyun Peng,Yejin Choi,Julian Michael,Liwei Jiang,Saadia Gabriel*

Main category: cs.CL

TL;DR: 研究显示，AI辩论通过提供对立证据，有效提高人类判断的准确性和信心水平，尤其在判断 COVID-19 相关事实性问题时。同时，具有人性化特征的个性化AI法官在准确性上优于人类法官，展示了其在AI系统监督中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的增强和在公共健康等重要领域影响的加深，如何确保AI提供的信息真确性变得至关重要。研究目的是探讨AI系统能否通过辩论引导持有不同偏见的公众达成更准确的判断。

Method: 通过两项研究评估AI辩论的效果：首先是人类法官在AI协助的辩论与咨询中评估COVID-19事实性问题；其次是配置了拟合人类信念系统的个性化AI法官参与的研究，测量其在解决相同问题时的表现。

Result: 研究表明，AI辩论可显著提升判断准确性且比单一顾问系统高出10%。对于持主流信念的法官，其准确性提升15.2%；对于持怀疑态度的法官则提升4.7%。AI法官的准确性（78.5%）高于人类法官（70.1%）及无个性化的AI法官（69.8%）。

Conclusion: AI辩论可以有效提高人类的判断准确性和信心水平，尤其在应对有争议的事实问题时。这不仅证明了AI系统在促进更接近真相的过程中有重要作用，还展示了其在监督前沿AI模型时的潜力。

Abstract: As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics like public health
where factual accuracy directly impacts well-being. Scalable Oversight aims to
ensure AI truthfulness by enabling humans to supervise systems that may exceed
human capabilities--yet humans themselves hold different beliefs and biases
that impair their judgment. We study whether AI debate can guide biased judges
toward the truth by having two AI systems debate opposing sides of
controversial COVID-19 factuality claims where people hold strong prior
beliefs. We conduct two studies: one with human judges holding either
mainstream or skeptical beliefs evaluating factuality claims through
AI-assisted debate or consultancy protocols, and a second examining the same
problem with personalized AI judges designed to mimic these different human
belief systems. In our human study, we find that debate-where two AI advisor
systems present opposing evidence-based arguments-consistently improves
judgment accuracy and confidence calibration, outperforming consultancy with a
single-advisor system by 10% overall. The improvement is most significant for
judges with mainstream beliefs (+15.2% accuracy), though debate also helps
skeptical judges who initially misjudge claims move toward accurate views
(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like
personas achieve even higher accuracy (78.5%) than human judges (70.1%) and
default AI judges without personas (69.8%), suggesting their potential for
supervising frontier AI models. These findings highlight AI debate as a
promising path toward scalable, bias-resilient oversight--leveraging both
diverse human and AI judgments to move closer to truth in contested domains.

</details>


### [16] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
*Dennis Fucci,Marco Gaido,Matteo Negri,Mauro Cettolo,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 通过特征归因技术，研究现代ASR系统中的声学线索对齐其频谱特性的表现，揭示了ASR模型的依赖点和研究潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR取得了显著进展，但模型依赖的具体声学线索仍不明晰。

Method: 采用特征归因技术分析现代基于Conformer的ASR系统中相关声学线索。

Result: 分析表明ASR模型在男性语音中对元音前两个共振峰更敏感，更好地捕捉齿擦音的频谱特征，并优先关注爆破音的释放阶段。

Conclusion: ASR模型主要依赖元音的整个时间范围，尤其是前两个共振峰，且在男性语音中更显著。并且，它对于齿擦音的频谱特征比非齿擦音有更好的捕捉，并且优先关注爆破音的释放阶段，特别是爆破特征。

Abstract: Despite significant advances in ASR, the specific acoustic cues models rely
on remain unclear. Prior studies have examined such cues on a limited set of
phonemes and outdated models. In this work, we apply a feature attribution
technique to identify the relevant acoustic cues for a modern Conformer-based
ASR system. By analyzing plosives, fricatives, and vowels, we assess how
feature attributions align with their acoustic properties in the time and
frequency domains, also essential for human speech perception. Our findings
show that the ASR model relies on vowels' full time spans, particularly their
first two formants, with greater saliency in male speech. It also better
captures the spectral characteristics of sibilant fricatives than non-sibilants
and prioritizes the release phase in plosives, especially burst
characteristics. These insights enhance the interpretability of ASR models and
highlight areas for future research to uncover potential gaps in model
robustness.

</details>


### [17] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
*Lindia Tjuatja,Graham Neubig*

Main category: cs.CL

TL;DR: 提出了一种名为BehaviorBox的方法，通过上下文嵌入自动比较语言模型，识别细致的文本特征来展示性能差异。


<details>
  <summary>Details</summary>
Motivation: 在语言模型评估中，找到能显示两个模型之间具有意义并且可推广的差异的例子至关重要。

Method: 使用性能感知的上下文嵌入自动比较语言模型，找到一个模型优于另一个模型的细粒度文本特征。此方法称为BehaviorBox，可以提取出生成难易程度方面的差异特征。

Result: BehaviorBox能够识别出语言模型在特定上下文中表现优异的词组特性，与传统的语料困惑度测量方法相比，能够提供更细致的模型性能差异洞察。

Conclusion: BehaviorBox可以在特定数据集中找到特征，展示性能上的显著差异，而无法通过仅依靠语料库级困惑度来发现。

Abstract: Language model evaluation is a daunting task: prompts are brittle,
corpus-level perplexities are vague, and the choice of benchmarks are endless.
Finding examples that show meaningful, generalizable differences between two
LMs is crucial to understanding where one model succeeds and another fails. Can
this process be done automatically? In this work, we propose methodology for
automated comparison of language models that uses performance-aware contextual
embeddings to find fine-grained features of text where one LM outperforms
another. Our method, which we name BehaviorBox, extracts coherent features that
demonstrate differences with respect to the ease of generation between two LMs.
Specifically, BehaviorBox finds features that describe groups of words in
fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'"
and "exclamation marks after emotional statements", where one model outperforms
another within a particular datatset. We apply BehaviorBox to compare models
that vary in size, model family, and post-training, and enumerate insights into
specific contexts that illustrate meaningful differences in performance which
cannot be found by measures such as corpus-level perplexity alone.

</details>


### [18] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
*Ella Rannon,David Burstein*

Main category: cs.CL

TL;DR: The paper reviews the application of NLP techniques to genomics, exploring various models and tokenization strategies to enhance biological data analysis.


<details>
  <summary>Details</summary>
Motivation: To explore how NLP methods can be applied to biological sequence data to derive meaningful insight from large-scale genomic datasets.

Method: The paper reviews the adaptation of various NLP techniques, including word2vec, transformers, and hyena operators, to biological sequences.

Result: The review identifies strengths, limitations, and suitability of different tokenization strategies and model architectures for biological tasks, along with advances in applications like structure prediction and gene expression analysis.

Conclusion: NLP methods hold great promise for bioinformatics, potentially transforming our understanding of biological processes.

Abstract: Natural Language Processing (NLP) has transformed various fields beyond
linguistics by applying techniques originally developed for human language to
the analysis of biological sequences. This review explores the application of
NLP methods to biological sequence data, focusing on genomics, transcriptomics,
and proteomics. We examine how various NLP methods, from classic approaches
like word2vec to advanced models employing transformers and hyena operators,
are being adapted to analyze DNA, RNA, protein sequences, and entire genomes.
The review also examines tokenization strategies and model architectures,
evaluating their strengths, limitations, and suitability for different
biological tasks. We further cover recent advances in NLP applications for
biological data, such as structure prediction, gene expression, and
evolutionary analysis, highlighting the potential of these methods for
extracting meaningful insights from large-scale genomic data. As language
models continue to advance, their integration into bioinformatics holds immense
promise for advancing our understanding of biological processes in all domains
of life.

</details>


### [19] [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)
*Sofoklis Kakouros*

Main category: cs.CL

TL;DR: 利用词语信息量选择语音片段计算声学特征，提高情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别方法可能错过长句中细粒度的声学变化，该研究旨在解决识别表达特定情感的语音信号片段这一关键挑战。

Method: 该研究使用了标准声学韵律特征及其功能性特征，以及自监督表征。通过预训练语言模型提取的词语信息量来识别语义上重要的语段，随后在这些语段上计算声学特征。

Result: 结果表明，在基于词语信息量选择的片段上计算的声学特征能够显著提高识别性能。

Conclusion: 通过使用基于词语信息量选择的声学特征，情感识别性能显著提高，证明了此方法的有效性。

Abstract: In emotion recognition from speech, a key challenge lies in identifying
speech signal segments that carry the most relevant acoustic variations for
discerning specific emotions. Traditional approaches compute functionals for
features such as energy and F0 over entire sentences or longer speech portions,
potentially missing essential fine-grained variation in the long-form
statistics. This research investigates the use of word informativeness, derived
from a pre-trained language model, to identify semantically important segments.
Acoustic features are then computed exclusively for these identified segments,
enhancing emotion recognition accuracy. The methodology utilizes standard
acoustic prosodic features, their functionals, and self-supervised
representations. Results indicate a notable improvement in recognition
performance when features are computed on segments selected based on word
informativeness, underscoring the effectiveness of this approach.

</details>


### [20] [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)
*Radin Shayanfar,Chu Fei Luo,Rohan Bhambhoria,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: CoDial framework simplifies task-oriented dialogue systems setup for experts, achieving top performance and enabling easy updates in specialized fields.


<details>
  <summary>Details</summary>
Motivation: To reduce the complexity and cost involved in training dialogue systems for specialized tasks and allow non-technical experts to manage system behavior with minimal effort.

Method: Introduction of CoDial framework that uses a structured heterogeneous graph for converting expert knowledge into conversation logic. Allows integration into existing guardrailing languages for zero-shot specifications.

Result: CoDial demonstrates superior performance on the STAR dataset and shows competitive results on the MultiWOZ dataset. It also allows iterative improvement through feedback, enhancing its applicability in critical domains.

Conclusion: CoDial achieves state-of-the-art performance on specific datasets and enables non-technical experts to define and refine dialogue systems efficiently.

Abstract: It is often challenging to teach specialized, unseen tasks to dialogue
systems due to the high cost of expert knowledge, training data, and high
technical difficulty. To support domain-specific applications - such as law,
medicine, or finance - it is essential to build frameworks that enable
non-technical experts to define, test, and refine system behaviour with minimal
effort. Achieving this requires cross-disciplinary collaboration between
developers and domain specialists. In this work, we introduce a novel
framework, CoDial (Code for Dialogue), that converts expert knowledge,
represented as a novel structured heterogeneous graph, into executable
conversation logic. CoDial can be easily implemented in existing guardrailing
languages, such as Colang, to enable interpretable, modifiable, and true
zero-shot specification of task-oriented dialogue systems. Empirically, CoDial
achieves state-of-the-art performance on the STAR dataset for inference-based
models and is competitive with similar baselines on the well-known MultiWOZ
dataset. We also demonstrate CoDial's iterative improvement via manual and
LLM-aided feedback, making it a practical tool for expert-guided alignment of
LLMs in high-stakes domains.

</details>


### [21] [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
*Wenzheng Zhang,Xi Victoria Lin,Karl Stratos,Wen-tau Yih,Mingda Chen*

Main category: cs.CL

TL;DR: ImpRAG系统将检索和生成整合到一个统一的模型中，通过消除人为指定查询，优化这两个过程，显著提高了任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG系统在检索和生成方面存在分离问题，这限制了模型在多样化任务中的泛化能力。

Method: 提出了一种名为ImpRAG的无查询RAG系统，通过将预训练的仅解码语言模型划分为专门的层组，实现同时优化检索和生成任务。使用两阶段推理过程，使用相同的模型参数和前向传播进行检索和生成。

Result: 在8个知识密集型任务上，ImpRAG在未见任务上实现了3.6-11.5的精确匹配分数提高。

Conclusion: 这种方法有效地使模型能够表达自己的信息需求，并在任务间进行泛化。同时平衡检索和生成参数，并利用生成困惑度作为检索训练目标来增强性能显得尤为重要。

Abstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval
and generation as separate processes, requiring explicit textual queries to
connect them. This separation can limit the ability of models to generalize
across diverse tasks. In this work, we propose a query-free RAG system, named
ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG
allows models to implicitly express their information needs, eliminating the
need for human-specified queries. By dividing pretrained decoder-only language
models into specialized layer groups, ImpRAG optimizes retrieval and generation
tasks simultaneously. Our approach employs a two-stage inference process, using
the same model parameters and forward pass for both retrieval and generation,
thereby minimizing the disparity between retrievers and language models.
Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves
3.6-11.5 improvements in exact match scores on unseen tasks with diverse
formats, highlighting its effectiveness in enabling models to articulate their
own information needs and generalize across tasks. Our analysis underscores the
importance of balancing retrieval and generation parameters and leveraging
generation perplexities as retrieval training objectives for enhanced
performance.

</details>


### [22] [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)
*Sofoklis Kakouros,Haoyu Chen*

Main category: cs.CL

TL;DR: 该研究使用韵律特征和自监督学习表示，成功区分网球比赛中输赢选手的赛后采访语音。


<details>
  <summary>Details</summary>
Motivation: 探索仅通过后赛采访录音的韵律特征和自监督学习表征来分类比赛结果的可能性。

Method: 通过对后赛采访中的韵律特征（例如音高和强度）进行分析，并结合使用诸如Wav2Vec 2.0和HuBERT等自监督学习（SSL）模型。

Result: 结果表明，自监督学习表示有效地区分了输赢结果，捕捉到与情感状态相关的细微语音模式。同时，音高变化等韵律线索也是胜利的强烈指标。

Conclusion: 该研究证明了使用自监督学习的语音表示和韵律特征可以有效区分网球比赛的输赢结果。

Abstract: This study examines the prosodic characteristics associated with winning and
losing in post-match tennis interviews. Additionally, this research explores
the potential to classify match outcomes solely based on post-match interview
recordings using prosodic features and self-supervised learning (SSL)
representations. By analyzing prosodic elements such as pitch and intensity,
alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine
whether an athlete has won or lost their match. Traditional acoustic features
and deep speech representations are extracted from the data, and machine
learning classifiers are employed to distinguish between winning and losing
players. Results indicate that SSL representations effectively differentiate
between winning and losing outcomes, capturing subtle speech patterns linked to
emotional states. At the same time, prosodic cues -- such as pitch variability
-- remain strong indicators of victory.

</details>


### [23] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
*Thai Hoang,Kung-Hsiang Huang,Shirley Kokane,Jianguo Zhang,Zuxin Liu,Ming Zhu,Jake Grigsby,Tian Lan,Michael S Ryoo,Chien-Sheng Wu,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CL

TL;DR: 提出了LAM SIMULATOR框架，以增强LAMs在多步骤任务中的数据质量，实验显示性能显著提高。


<details>
  <summary>Details</summary>
Motivation: 针对LAMs面临的高质量训练数据缺乏问题，特别是涉及规划、执行工具调用和响应反馈的多步骤任务，提出了一种综合框架来解决这一问题。

Method: 引入了一个动态任务查询生成器、大量工具集合以及一个交互环境，允许LLM代理调用工具并接收实时反馈，帮助其自主探索解决任务。最终的行动轨迹数据用于创建高质量训练数据集。

Result: 在流行的agentic基准ToolBench和CRMArena上进行的实验显示，使用我们的框架生成的数据集训练的模型在性能上大幅提升，比原始基线高出49.3%。

Conclusion: LAM SIMULATOR在数据集创建中所需的人力输入最小，在加速AI代理开发方面效率和效果显著。通过使用自生成的数据集训练的模型，在某些基准上性能提高高达49.3%。

Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face
challenges due to the need for high-quality training data, especially for
multi-steps tasks that involve planning, executing tool calls, and responding
to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive
framework designed for online exploration of agentic tasks with high-quality
feedback. Our framework features a dynamic task query generator, an extensive
collection of tools, and an interactive environment where Large Language Model
(LLM) Agents can call tools and receive real-time feedback. This setup enables
LLM Agents to explore and solve tasks autonomously, facilitating the discovery
of multiple approaches to tackle any given task. The resulting action
trajectory data are then used to create high-quality training datasets for
LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,
highlight the effectiveness of LAM SIMULATOR: models trained with
self-generated datasets using our framework achieve significant performance
gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR
requires minimal human input during dataset creation, highlighting LAM
SIMULATOR's efficiency and effectiveness in speeding up development of AI
agents.

</details>


### [24] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
*Russell Scheinberg,Ameeta Agrawal,Amber Shore,So Young Lee*

Main category: cs.CL

TL;DR: 提出一种名为“语法提示”的范式，通过解释句法现象并反馈给模型，显著提高模型在多语言语法现象上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型可以解释语法规则，但在判断句子可接受性时常常失败，因此需要找到一种方法来帮助这些模型更有效地应用语法规则。

Method: 提出“语法提示”的方法，该方法包含解释然后处理的范式，首先让大型语言模型解释相关的句法现象，然后将这些解释作为额外的上下文反馈给目标模型（可以是大型或小型语言模型），帮助决定哪个句子是语法正确的。

Result: 在英语BLiMP、中文SLING和俄语RuBLiMP基准上，与强基线相比，该提示设计可以在多种句法现象上实现显著的改进。在小型语言模型上，仅使用语法提示可以减少约20%的大型语言模型-小型语言模型精度差距，而与链式思维结合使用时可以减少56%。

Conclusion: 提供一个简单提示设计，即“语法提示”，可以显著改善多种句法现象的判断性能，并将低成本小型语言模型的表现接近前沿大型语言模型。

Abstract: Large language models (LLMs) can explain grammatical rules, yet they often
fail to apply those rules when judging sentence acceptability. We present
"grammar prompting", an explain-then-process paradigm: a large LLM first
produces a concise explanation of the relevant syntactic phenomenon, then that
explanation is fed back as additional context to the target model -- either an
LLM or a smaller language model (SLM) -- before deciding which sentence of a
minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian
RuBLiMP benchmarks, this simple prompt design yields substantial improvements
over strong baselines across many syntactic phenomena. Feeding an LLM's
metalinguistic explanation back to the target model bridges the gap between
knowing a rule and using it. On SLMs, grammar prompting alone trims the average
LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by
56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,
language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in
multilingual settings.

</details>


### [25] [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)
*Pegah Alipoormolabashi,Ajay Patel,Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: 提出了Misattribution Unfairness Index (MAUIk)来衡量作者身份归属系统的不公平性，发现某些作者面临更高误归属风险，尤其是接近嵌入中心的作者。


<details>
  <summary>Details</summary>
Motivation: 现有作者身份归属系统未充分考虑误归属的公平性问题，会导致某些作者面临更高风险。提出新的衡量标准以解决这一问题。

Method: 引入了一种新的衡量方法MAUIk，通过计算作者在其未写文本中的排名，来量化作者身份归属系统的公平性。

Result: 对五个模型和两个数据集进行量化评估发现，所有模型都存在高水平的不公平性，且某些作者的风险明显更高。

Conclusion: 研究表明，作者嵌入在潜在搜索空间中的位置会影响误归属风险，特别是接近中心点的作者风险更高。

Abstract: Authorship misattribution can have profound consequences in real life. In
forensic settings simply being considered as one of the potential authors of an
evidential piece of text or communication can result in undesirable scrutiny.
This raises a fairness question: Is every author in the candidate pool at equal
risk of misattribution? Standard evaluation measures for authorship attribution
systems do not explicitly account for this notion of fairness. We introduce a
simple measure, Misattribution Unfairness Index (MAUIk), which is based on how
often authors are ranked in the top k for texts they did not write. Using this
measure we quantify the unfairness of five models on two different datasets.
All models exhibit high levels of unfairness with increased risks for some
authors. Furthermore, we find that this unfairness relates to how the models
embed the authors as vectors in the latent search space. In particular, we
observe that the risk of misattribution is higher for authors closer to the
centroid (or center) of the embedded authors in the haystack. These results
indicate the potential for harm and the need for communicating with and
calibrating end users on misattribution risk when building and providing such
models for downstream use.

</details>


### [26] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
*Berk Atil,Namrata Sureddy,Rebecca J. Passonneau*

Main category: cs.CL

TL;DR: 本文介绍了一个新的数据集TRuST，用于改善毒性检测，并对大型语言模型进行基准测试，发现微调模型表现更优，但某些群体的检测性能偏低。


<details>
  <summary>Details</summary>
Motivation: 在线内容的毒性问题日益严重，尤其是语言模型生成的内容，可能带来负面的心理和社会影响。因此需要提高毒性检测能力。

Method: 引入TRuST数据集，将现有数据集合并，并为毒性、目标社会群体和毒性区间提供标签。对最新大型语言模型在毒性检测、目标群体识别和毒性区间提取上进行基准测试。

Result: 微调的大型语言模型比零样本和少样本提示表现更好，但在某些社会群体上的性能仍然低下，推理能力没有显著提升开发者的技能。

Conclusion: 微调模型在毒性检测中表现更好，但在某些社会群体上的表现仍然较低，显示出大型语言模型在社交推理技能方面的不足。

Abstract: Toxicity in online content, including content generated by language models,
has become a critical concern due to its potential for negative psychological
and social impact. This paper introduces TRuST, a comprehensive dataset
designed to improve toxicity detection that merges existing datasets, and has
labels for toxicity, target social group, and toxic spans. It includes a
diverse range of target groups such as ethnicity, gender, religion, disability,
and politics, with both human/machine-annotated and human machine-generated
data. We benchmark state-of-the-art large language models (LLMs) on toxicity
detection, target group identification, and toxic span extraction. We find that
fine-tuned models consistently outperform zero-shot and few-shot prompting,
though performance remains low for certain social groups. Further, reasoning
capabilities do not significantly improve performance, indicating that LLMs
have weak social reasoning skills.

</details>


### [27] [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)
*Hyungjoo Chae,Dongjin Kang,Jihyuk Kim,Beong-woo Kwak,Sunghyun Park,Haeju Park,Jinyoung Yeo,Moontae Lee,Kyungjae Lee*

Main category: cs.CL

TL;DR: 该论文引入了一种新的长链思维数据集和训练管道，旨在不依赖现有模型独立发展大型推理模型。


<details>
  <summary>Details</summary>
Motivation: 在不依赖现有模型（如R1）的情况下，推动大型推理模型的独立发展。

Method: 构建了一个名为Long CoT Collection的长链思维数据集，并开发了一种新的训练管道，使短链思维语言模型能够进行更长时间的推理，并提供思维预算的可控性。

Result: 新数据集在质量上可与R1媲美，并能提升一般推理能力，为强化学习提供坚实基础，使模型获得2-3倍的更大增益。

Conclusion: 独立发展的长链思维数据集和训练方法为大型推理模型的进步和优化提供了新的路径。

Abstract: With the release of R1, a publicly available large reasoning model (LRM),
researchers commonly train new LRMs by training language models on R1's long
chain-of-thought (CoT) inferences. While prior works show that LRMs'
capabilities can be reproduced through direct distillation, the continued
reliance on the existing models (e.g., R1) remains a critical limitation in
advancing the field. As a first step toward independent LRM development, this
paper explores the possibility of constructing a long CoT dataset with LLMs
that are not trained for inference-time scaling. To this end, we present the
Long CoT Collection, a dataset of 100K CoT rationales annotated using existing
short CoT LLMs. We develop a pipeline that induces o1's novel reasoning
strategies into short CoT LLMs, enabling them to think longer and introducing
controllability over the thought budget to better manage the overthinking
problem. Our extensive analyses validate that our dataset achieves quality
comparable to--or slightly below--R1. Furthermore, our experiments demonstrate
that training on our dataset not only strengthens general reasoning skills, but
also provides a strong foundation for reinforcement learning--models
initialized on our data achieve 2-3x larger gains with RLVR.

</details>


### [28] [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)
*Jiaming Li,Yukun Chen,Ziqiang Liu,Minghuan Tan,Lei Zhang,Yunshui Li,Run Luo,Longze Chen,Jing Luo,Ahmadreza Argha,Hamid Alinejad-Rokny,Wei Zhou,Min Yang*

Main category: cs.CL

TL;DR: Storyteller is a new AI method that improves story coherence and consistency, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with maintaining narrative coherence and logical consistency, thus compromising the storytelling experience, and there is a need for substantial improvements.

Method: Storyteller introduces a plot node structure based on subject verb object (SVO) triplets and integrates two dynamic modules, STORYLINE and narrative entity knowledge graph (NEKG), which interact with the story generation process.

Result: Storyteller achieves an 84.33% average win rate through human preference evaluation and surpasses other methods in creativity, coherence, engagement, and relevance.

Conclusion: Storyteller significantly improves the coherence and consistency of automatically generated stories, outperforming existing methods in various aspects such as creativity, coherence, engagement, and relevance.

Abstract: Stories are central to human culture, serving to share ideas, preserve
traditions, and foster connections. Automatic story generation, a key
advancement in artificial intelligence (AI), offers new possibilities for
creating personalized content, exploring creative ideas, and enhancing
interactive experiences. However, existing methods struggle to maintain
narrative coherence and logical consistency. This disconnect compromises the
overall storytelling experience, underscoring the need for substantial
improvements. Inspired by human cognitive processes, we introduce Storyteller,
a novel approach that systemically improves the coherence and consistency of
automatically generated stories. Storyteller introduces a plot node structure
based on linguistically grounded subject verb object (SVO) triplets, which
capture essential story events and ensure a consistent logical flow. Unlike
previous methods, Storyteller integrates two dynamic modules, the STORYLINE and
narrative entity knowledge graph (NEKG),that continuously interact with the
story generation process. This integration produces structurally sound,
cohesive and immersive narratives. Extensive experiments demonstrate that
Storyteller significantly outperforms existing approaches, achieving an 84.33%
average win rate through human preference evaluation. At the same time, it is
also far ahead in other aspects including creativity, coherence, engagement,
and relevance.

</details>


### [29] [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)
*Herun Wan,Jiaying Wu,Minnan Luo,Zhi Zeng,Zhixiong Su*

Main category: cs.CL

TL;DR: TruthOverTricks评估范式揭示现有检测器在捷径情况下性能下降。提出SMF框架以提高其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 误信息检测模型依赖于训练数据中与误信息相关的表面线索（捷径），而这些线索无法适应现实世界中误信息的多样性和不断变化的特点。在大型语言模型能够轻易生成具有说服力的误信息的情况下，这一问题变得更加严重。

Method: 本文引入TruthOverTricks评估范式，提出了一种LLM增强的数据增强框架（SMF），通过改写、事实总结和情感归一化，减少对捷径的依赖。

Result: SMF框架在16个基准上稳定增强了鲁棒性，鼓励模型依靠更深层次的语义理解而非捷径线索。

Conclusion: 现有的误信息检测器在面对自然发生和对抗性制作的捷径时表现严重退化。通过SMF框架可以改善模型的鲁棒性。

Abstract: Misinformation detection models often rely on superficial cues (i.e.,
\emph{shortcuts}) that correlate with misinformation in training data but fail
to generalize to the diverse and evolving nature of real-world misinformation.
This issue is exacerbated by large language models (LLMs), which can easily
generate convincing misinformation through simple prompts. We introduce
TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning
in misinformation detection. TruthOverTricks categorizes shortcut behaviors
into intrinsic shortcut induction and extrinsic shortcut injection, and
evaluates seven representative detectors across 14 popular benchmarks, along
with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.
Empirical results reveal that existing detectors suffer severe performance
degradation when exposed to both naturally occurring and adversarially crafted
shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation
framework that mitigates shortcut reliance through paraphrasing, factual
summarization, and sentiment normalization. SMF consistently enhances
robustness across 16 benchmarks, encouraging models to rely on deeper semantic
understanding rather than shortcut cues. To promote the development of
misinformation detectors, we have published the resources publicly at
https://github.com/whr000001/TruthOverTricks.

</details>


### [30] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
*Jeonghun Kang,Soonmok Kwon,Joonseok Lee,Byung-Hak Kim*

Main category: cs.CL

TL;DR: DIAMOND通过结合量化和质化方法提升了比赛精彩程度识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在识别得分动作方面具有局限性，缺乏战略深度、势头变化和故事线发展。人工策划虽然效果好，但资源密集且不具扩展性。

Method: DIAMOND是一个结合结构化体育分析和自然语言推理的系统，利用sabermetric特征和LLM模块来量化比赛重要性和提升选择。

Result: DIAMOND在韩国棒球组织联赛中的五场比赛测试中，将F1评分从42.9%提高到84.8%，超过了商业和统计基准。

Conclusion: DIAMOND展示了模块化、可解释的代理框架在体育事件级摘要中的潜力。

Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking
or computer vision-driven event detection -- can identify scoring plays but
often miss strategic depth, momentum shifts, and storyline progression. Manual
curation remains the gold standard but is resource-intensive and not scalable.
We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight
summarization that integrates structured sports analytics with natural language
reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and
Leverage Index -- to quantify play importance, while an LLM module enhances
selection based on contextual narrative value. This hybrid approach ensures
both quantitative rigor and qualitative richness, surpassing the limitations of
purely statistical or vision-based systems. Evaluated on five diverse Korean
Baseball Organization League games, DIAMOND improves F1-score from 42.9%
(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.
Though limited in scale, our results highlight the potential of modular,
interpretable agent-based frameworks for event-level summarization in sports
and beyond.

</details>


### [31] [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)
*Hisami Suzuki,Satoru Katsumata,Takashi Kodama,Tetsuro Takahashi,Kouta Nakayama,Satoshi Sekine*

Main category: cs.CL

TL;DR: 研究创建了一个名为AnswerCarefully的数据集，通过微调提高了日本LLM输出的安全性，确保了其实用性，并更新数据集以支持跨语言应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决日本LLM输出中潜在的安全问题，创建反映日本社会文化背景的专门数据集。

Method: 通过创建包含1,800对问题和参考答案的数据集来评估和提高日本LLM的输出安全性。

Result: 微调后的日本LLM在输出安全性方面有所改善，并且对12个日本LLM进行安全性评估，证明了数据集的有效性。

Conclusion: 使用AnswerCarefully数据集进行微调可以提高日本LLM输出的安全性，同时保证其通用响应的实用性。

Abstract: In this paper we present AnswerCarefully, a dataset for promoting the safety
and appropriateness of Japanese LLM outputs. The dataset consists of 1,800
pairs of questions and reference answers, where the questions require special
attention in answering. It covers a wide range of risk categories established
in prior English-language datasets, but the data samples are original in that
they are manually created to reflect the socio-cultural context of LLM usage in
Japan. We show that using this dataset for instruction to fine-tune a Japanese
LLM led to improved output safety without compromising the utility of general
responses. We also report the results of a safety evaluation of 12 Japanese
LLMs using this dataset as a benchmark. Finally, we describe the latest update
on the dataset which provides English translations and annotations of the
questions, aimed at facilitating the derivation of similar datasets in
different languages and regions.

</details>


### [32] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
*Ukyo Honda,Tatsushi Oka*

Main category: cs.CL

TL;DR: X$^2$-ICL通过探索所有可能标签的解释来增强LLMs的决策鲁棒性，实验验证其在分布外数据的可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL在超出提供示例的分布范围时常常无法很好地泛化，这促使研究者寻求增强其鲁棒性的方法。

Method: X$^2$-ICL通过系统地探索所有可能标签的解释，从而改进决策的全面性和鲁棒性。

Result: 在多个自然语言理解数据集上的实验结果验证了X$^2$-ICL的有效性，表现出相对于现有ICL方法的显著鲁棒性提升。

Conclusion: X$^2$-ICL比现有的ICL方法在应对分布外数据时表现出显著的鲁棒性增强。

Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging
large language models (LLMs). However, it often struggles to generalize beyond
the distribution of the provided demonstrations. A recent advancement in
enhancing robustness is ICL with explanations (X-ICL), which improves
prediction reliability by guiding LLMs to understand and articulate the
reasoning behind correct labels. Building on this approach, we introduce an
advanced framework that extends X-ICL by systematically exploring explanations
for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and
robust decision-making. Experimental results on multiple natural language
understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating
significantly improved robustness to out-of-distribution data compared to the
existing ICL approaches.

</details>


### [33] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
*Chuanghao Ding,Jiaping Wang,Ziqing Yang,Xiaoliang Wang,Dahua Lin,Cam-Tu Nguyen,Fei Tan*

Main category: cs.CL

TL;DR: 研究提出了顾问解码（CD），一种新的推理加速机制，通过减少大模型的调用频率并保持生成质量，比目标模型快2.5倍。


<details>
  <summary>Details</summary>
Motivation: 尽管推测解码（SD）机制可以加速大语言模型的推理，但其高拒绝率要求反复调用LLM验证草稿token，削弱了SD的整体效率提升。

Method: 顾问解码（CD）使用大语言模型计算的基于token级似然性的验证方法进行草稿验证，与重要性抽样法的度量不同。

Result: CD进一步减少了大模型的调用频率到低于10%，并在某些任务中显示出优于大模型的性能表现。

Conclusion: 提出的顾问解码（CD）机制在保持生成质量的同时，将推理速度提高了2.5倍。

Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.

</details>


### [34] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
*Yilin Xiao,Junnan Dong,Chuang Zhou,Su Dong,Qianwen Zhang,Di Yin,Xing Sun,Xiao Huang*

Main category: cs.CL

TL;DR: 引入GraphRAG-Bench以评估和提升GraphRAG模型的推理能力，涵盖多种任务和评估框架，验证图结构对推理能力的增强。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG模型的评估主要依赖传统问答数据集，其有限的问题范围和评估指标无法全面评估GraphRAG模型的推理能力改进。

Method: 我们引入了GraphRAG-Bench，这是一个大型、领域特定的基准，旨在严格评估GraphRAG模型。方法包括挑战性问题设计、多样化任务覆盖以及全面的评估框架。

Result: 通过将九种现代GraphRAG方法应用于GraphRAG-Bench，分析得出了关于图架构、检索效率和推理能力的关键见解，并为研究提供了指导。

Conclusion: GraphRAG-Bench展示了其在评估图结构化方式对模型推理能力提升方面的有效性，并为研究社区提供了可操作的指导。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [35] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
*Zhengyuan Liu,Geyu Lin,Hui Li Tan,Huayun Zhang,Yanfeng Lu,Xiaoxue Gao,Stella Xin Yin,He Sun,Hock Huan Goh,Lung Hsiang Wong,Nancy F. Chen*

Main category: cs.CL

TL;DR: SingaKids is a multilingual dialogic tutor system that enhances language learning for young students through interactive tasks, showing effective results in diverse learners.


<details>
  <summary>Details</summary>
Motivation: To enhance language acquisition for young learners by creating a personalized, interactive, and immersive educational application that can adapt to different languages and cultural contexts, and provide an engaging learning experience suitable for children.

Method: The method involves integrating dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation. The system is further improved through multilingual pre-training, task-specific tuning, and scaffolding optimization.

Result: Empirical studies show that SingaKids effectively facilitates language learning in an engaging manner, with successful outcomes in learners of different performance levels.

Conclusion: SingaKids demonstrates effective dialogic teaching and benefits learners of various performance levels, enhancing language acquisition through an interactive and immersive learning environment.

Abstract: The integration of generative artificial intelligence into educational
applications has enhanced personalized and interactive learning experiences,
and it shows strong potential to promote young learners language acquisition.
However, it is still challenging to ensure consistent and robust performance
across different languages and cultural contexts, and kids-friendly design
requires simplified instructions, engaging interactions, and age-appropriate
scaffolding to maintain motivation and optimize learning outcomes. In this
work, we introduce SingaKids, a dialogic tutor designed to facilitate language
learning through picture description tasks. Our system integrates dense image
captioning, multilingual dialogic interaction, speech understanding, and
engaging speech generation to create an immersive learning environment in four
languages: English, Mandarin, Malay, and Tamil. We further improve the system
through multilingual pre-training, task-specific tuning, and scaffolding
optimization. Empirical studies with elementary school students demonstrate
that SingaKids provides effective dialogic teaching, benefiting learners at
different performance levels.

</details>


### [36] [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)
*Tairan Liu*

Main category: cs.CL

TL;DR: 研究发现22个国家的教材中普遍存在性别不平等，尤其是男性角色的过度代表。拉丁文化圈性别差异相对较小。


<details>
  <summary>Details</summary>
Motivation: 以往研究已在个别国家的教材中发现了性别不平等，但很少有研究从跨文化的角度进行考察。

Method: 本研究使用自然语言处理方法来量化22个国家中英文教材的性别不平等。这些方法包括角色计数、优先性分析（即哪个性别先被提及）、通过TF-IDF分析词汇性别关联，并利用GloVe词嵌入检查关键词与每个性别的关联程度。

Result: 结果显示，各个区域普遍存在男性角色的过度表现，无论是在数量、优先性还是命名实体方面。拉丁文化圈展示出性别不平等现象相对较轻。

Conclusion: 本研究发现，在所有研究的文化区域内，教材中存在显著的性别不平等现象，其中男性角色在数量、优先性和命名实体方面较为突出。拉丁文化圈显示的不平等程度相对较小。

Abstract: Textbooks play a critical role in shaping children's understanding of the
world. While previous studies have identified gender inequality in individual
countries' textbooks, few have examined the issue cross-culturally. This study
applies natural language processing methods to quantify gender inequality in
English textbooks from 22 countries across 7 cultural spheres. Metrics include
character count, firstness (which gender is mentioned first), and TF-IDF word
associations by gender. The analysis also identifies gender patterns in proper
names appearing in TF-IDF word lists, tests whether large language models can
distinguish between gendered word lists, and uses GloVe embeddings to examine
how closely keywords associate with each gender. Results show consistent
overrepresentation of male characters in terms of count, firstness, and named
entities. All regions exhibit gender inequality, with the Latin cultural sphere
showing the least disparity.

</details>


### [37] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
*Maryam Berijanian,Kuldeep Singh,Amin Sehati*

Main category: cs.CL

TL;DR: 该研究比较分析了三种不同的AI智能体架构用于关系分类，结果显示多智能体协调方法在多个领域的表现优于少样本提示，接近微调模型。


<details>
  <summary>Details</summary>
Motivation: 在信息抽取过程中，实体关系分类是一个具有挑战性的任务，特别是在标签数据有限和关系结构复杂的情况下。

Method: 我们研究了三种不同的AI智能体架构：反思自我评估、分层任务分解、以及一种新的多智能体动态示例生成机制，每种架构利用了不同的推理模式和提示适应性。特别是我们动态示例生成的方法引入了实时合作和对抗性提示。

Result: 研究结果提供了关于设计模块化、可泛化的基于大语言模型的结构化关系抽取系统的实际指导。

Conclusion: 我们的实验表明，多智能体协调方法在性能上始终优于标准少样本提示，并接近于微调模型的表现。

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at
\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [38] [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)
*Mahammed Kamruzzaman,Abdullah Al Monsur,Gene Louis Kim,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 研究发现大型语言模型存在情感刻板印象，与文化规范不符，尤其是在负面情绪上。


<details>
  <summary>Details</summary>
Motivation: 考察在赋予大型语言模型特定的民族角色时，其是否表现出情感刻板印象，以及这些情感归因是否符合文化规范。

Method: 通过分析大规模语言模型在不同国家情感归因上的表现，衡量其是否与文化规范一致，尤其是对负面情绪的归因。

Result: 研究表明存在显著的国家间差异，情感如羞耻、恐惧和快乐的分配在各地区不均。同时发现模型生成的情感反应与人的情感反应之间存在明显不符，特别是在负面情绪上，凸显出模型输出中存在简化和潜在偏见的刻板印象。

Conclusion: 大型语言模型在模拟具备特定民族身份的角色时，可能会表现出情感刻板印象，并存在与实际文化规范不一致的情况。

Abstract: Emotions are a fundamental facet of human experience, varying across
individuals, cultural contexts, and nationalities. Given the recent success of
Large Language Models (LLMs) as role-playing agents, we examine whether LLMs
exhibit emotional stereotypes when assigned nationality-specific personas.
Specifically, we investigate how different countries are represented in
pre-trained LLMs through emotion attributions and whether these attributions
align with cultural norms. Our analysis reveals significant nationality-based
differences, with emotions such as shame, fear, and joy being
disproportionately assigned across regions. Furthermore, we observe notable
misalignment between LLM-generated and human emotional responses, particularly
for negative emotions, highlighting the presence of reductive and potentially
biased stereotypes in LLM outputs.

</details>


### [39] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本研究评估LLM在长尾文本上行为的安全性，揭示其在解码加密文本时可能存在安全漏洞。提出两维度框架评估模型的安全性，并探讨改进措施。


<details>
  <summary>Details</summary>
Motivation: 研究探究LLM对长尾分布加密文本的行为及其安全隐患，希望通过实验分析提高其安全性。

Method: 通过系统实验评估不同LLM在处理长尾加密文本时的表现及其安全性，提出了两维度框架评估LLM安全性。

Result: 部分模型在处理解密密文时容易受到错配泛化攻击，导致安全机制在至少一个维度上失效。

Conclusion: 研究揭示了在长尾文本场景下，当前的LLM在安全机制上存在漏洞和过度拒绝的问题，需发展更强大安全机制。

Abstract: This paper presents a systematic evaluation of Large Language Models' (LLMs)
behavior on long-tail distributed (encrypted) texts and their safety
implications. We introduce a two-dimensional framework for assessing LLM
safety: (1) instruction refusal-the ability to reject harmful obfuscated
instructions, and (2) generation safety-the suppression of generating harmful
responses. Through comprehensive experiments, we demonstrate that models that
possess capabilities to decrypt ciphers may be susceptible to
mismatched-generalization attacks: their safety mechanisms fail on at least one
safety dimension, leading to unsafe responses or over-refusal. Based on these
findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss
their strengths and limitations. This work contributes to understanding the
safety of LLM in long-tail text scenarios and provides directions for
developing robust safety mechanisms.

</details>


### [40] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)
*Bo Peng,Zhiheng Wang,Heyang Gong,Chaochao Lu*

Main category: cs.CL

TL;DR: 为了解决数据稀缺的问题，提出了自动数据生成方法和IP-Dialog基准，用以评估对话系统的个性化服务能力，并验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 在现代对话系统中，隐式推断用户背景并利用此信息进行个性化服务是关键。然而，数据稀缺是评价和提升这一能力的根本挑战。传统数据集构建方法劳动密集、资源需求高且引发隐私问题。

Method: 提出一种新的自动合成数据生成方法，并引入隐式个性化对话(IP-Dialog)基准和一个涵盖10个任务和12种用户属性类型的训练数据集。此外，还开发了一个包含四个指标的系统评估框架，用于评估属性意识和推理能力。还提出了五个因果图以阐明模型在隐式个性化过程中的推理路径。

Result: 大量实验带来了有价值的观察，并证明了所提供数据集的可靠性。

Conclusion: 通过引入自动数据生成方法和IP-Dialog基准，研究成功地解决了数据稀缺问题，并提供了一种有效的方式来评估对话系统的个性化能力。

Abstract: In modern dialogue systems, the ability to implicitly infer user backgrounds
from conversations and leverage this information for personalized assistance is
crucial. However, the scarcity of high-quality data remains a fundamental
challenge to evaluating and improving this capability. Traditional dataset
construction methods are labor-intensive, resource-demanding, and raise privacy
concerns. To address these issues, we propose a novel approach for automatic
synthetic data generation and introduce the Implicit Personalized Dialogue
(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12
user attribute types. Additionally, we develop a systematic evaluation
framework with four metrics to assess both attribute awareness and reasoning
capabilities. We further propose five causal graphs to elucidate models'
reasoning pathways during implicit personalization. Extensive experiments yield
insightful observations and prove the reliability of our dataset.

</details>


### [41] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
*Zhaorui Yang,Bo Pan,Han Wang,Yiyao Wang,Xingyu Liu,Minfeng Zhu,Bo Zhang,Wei Chen*

Main category: cs.CL

TL;DR: 论文提出了一种生成多模态报告的新框架，通过结构化的格式帮助语言模型生成高质量的图表和文本内容，在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架主要集中于生成纯文本内容，自动生成文本与可视化内容交错的报告仍然不够成熟。该任务面临设计信息量大的可视化内容及有效整合文本报告的重大挑战。

Method: 引入Formal Description of Visualization (FDV)，通过结构化文本表达图表，并提出Multimodal DeepResearcher框架，任务分解为四个阶段：(1)研究，(2)示例报告文本化，(3)规划，以及(4)多模态报告生成。

Result: 通过广泛实验，Multimodal DeepResearcher展示了其有效性，在100个多样化主题和5个专用评估指标上测试生成的多模态报告。

Conclusion: 使用相同的Claude 3.7 Sonnet模型，Multimodal DeepResearcher在整体胜率方面超越基线方法，达到了82%的胜率。

Abstract: Visualizations play a crucial part in effective communication of concepts and
information. Recent advances in reasoning and retrieval augmented generation
have enabled Large Language Models (LLMs) to perform deep research and generate
comprehensive reports. Despite its progress, existing deep research frameworks
primarily focus on generating text-only content, leaving the automated
generation of interleaved texts and visualizations underexplored. This novel
task poses key challenges in designing informative visualizations and
effectively integrating them with text reports. To address these challenges, we
propose Formal Description of Visualization (FDV), a structured textual
representation of charts that enables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this representation, we introduce
Multimodal DeepResearcher, an agentic framework that decomposes the task into
four stages: (1) researching, (2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the evaluation of generated
multimodal reports, we develop MultimodalReportBench, which contains 100
diverse topics served as inputs along with 5 dedicated metrics. Extensive
experiments across models and evaluation methods demonstrate the effectiveness
of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet
model, Multimodal DeepResearcher achieves an 82\% overall win rate over the
baseline method.

</details>


### [42] [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)
*Yupeng Qi,Ziyu Lyu,Min Yang,Yanlin Wang,Lu Bai,Lixin Cui*

Main category: cs.CL

TL;DR: 提出了一种新的框架MidPO，通过组合专家优化方法，提高大型语言模型的安全性和有用性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在确保大型语言模型的安全性同时，不降低其有用性，解决当前方法过度追求安全或难以平衡的问题。

Method: 采用Mixture of Experts框架，结合两个优化专家，再设计动态路由机制，进行双偏好优化。

Result: 通过三种流行数据集的定量和定性实验，MidPO显著优于当前最先进的方法，在安全性和有用性方面都有表现优势。

Conclusion: 研究提出了一种新的方法MidPO，可以在提高大型语言模型安全性的同时保持其有用性。

Abstract: As large language models (LLMs) are increasingly applied across various
domains, enhancing safety while maintaining the helpfulness of LLMs has become
a critical challenge. Recent studies solve this problem through
safety-constrained online preference optimization or safety-constrained offline
preference optimization. However, the safety-constrained online methods often
suffer from excessive safety, which might reduce helpfulness, while the
safety-constrained offline methods perform poorly in adaptively balancing
safety and helpfulness. To address these limitations, we propose MidPO, a
\textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness
\textbf{\underline{d}}ual \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference
enhanced direct preference optimization approach to transform the base model
into two independent experts, termed safety and helpfulness experts, and
fine-tunes the two independent experts for optimal safety or helpfulness
performance. Secondly, to achieve an effective balance between safety and
helpfulness, MidPO incorporates the two experts into the MoE framework and
designs a dynamic routing mechanism to allocate contributions from each expert
adaptively. We conduct quantitative and qualitative experiments on three
popular datasets to demonstrate the proposed MidPO significantly outperforms
state-of-the-art approaches in both safety and helpfulness. The code and models
will be released.

</details>


### [43] [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)
*Chunkit Chan,Yauwai Yim,Hongchuan Zeng,Zhiying Zou,Xinyuan Cheng,Zhifan Sun,Zheye Deng,Kawai Chung,Yuzhuo Ao,Yixiang Fan,Cheng Jiayang,Ercong Nie,Ginny Y. Wong,Helmut Schmid,Hinrich Schütze,Simon See,Yangqiu Song*

Main category: cs.CL

TL;DR: 研究通过XToM基准评估发现大型语言模型的多语言心智理论表现不一致，暴露了其在人类心理状态推理能力复刻方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对心智理论的评估多数局限于英语，这忽视了形成人类认知的语言多样性。因此，研究探讨LLM是否能在多种语言环境中展现出多语言的心智理论能力。

Method: 研究设计了一个名为XToM的多语言基准，用以评估大型语言模型在五种语言中的心智理论表现。这一评估工具包含多种丰富的背景任务场景，以提供全面的评测。

Result: 研究表明，尽管大型语言模型在多语言理解上表现优异，但其心智理论表现因语言不同而存在明显差异，反映出在不同语言背景下人类心理状态推理能力的复刻上存在问题。

Conclusion: 研究揭示了大型语言模型(LLM)在多语言背景下的心智理论(ToM)表现存在显著差异，表明当前的LLM在模拟人类类似的思维推理方面存在局限性。

Abstract: Theory of Mind (ToM), the ability to infer mental states in others, is
pivotal for human social cognition. Existing evaluations of ToM in LLMs are
largely limited to English, neglecting the linguistic diversity that shapes
human cognition. This limitation raises a critical question: can LLMs exhibit
Multilingual Theory of Mind, which is the capacity to reason about mental
states across diverse linguistic contexts? To address this gap, we present
XToM, a rigorously validated multilingual benchmark that evaluates ToM across
five languages and incorporates diverse, contextually rich task scenarios.
Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a
pronounced dissonance: while models excel in multilingual language
understanding, their ToM performance varies across languages. Our findings
expose limitations in LLMs' ability to replicate human-like mentalizing across
linguistic contexts.

</details>


### [44] [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
*Zijian Li,Xiaocheng Feng,Huixin Liu,Yichong Huang,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: 论文提出了一种改进的自适应模型合并方法FroM，通过Frobenius范数测量参数，解决微调模型合并中的任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决模型合并中的任务干扰问题，并在微调场景中提高性能。

Method: 提出了一种名为FroM的自适应合并方法，通过使用Frobenius范数直接测量模型参数，而不需要任何训练数据。

Result: FroM通过引入额外的超参数进行控制，成功缓解了任务干扰问题，并在多个微调场景中表现优于传统方法。

Conclusion: FroM有效解决了任务干扰问题，并在多个微调场景中表现优于基线方法。

Abstract: With the development of large language models, fine-tuning has emerged as an
effective method to enhance performance in specific scenarios by injecting
domain-specific knowledge. In this context, model merging techniques provide a
solution for fusing knowledge from multiple fine-tuning models by combining
their parameters. However, traditional methods often encounter task
interference when merging full fine-tuning models, and this problem becomes
even more evident in parameter-efficient fine-tuning scenarios. In this paper,
we introduce an improvement to the RegMean method, which indirectly leverages
the training data to approximate the outputs of the linear layers before and
after merging. We propose an adaptive merging method called FroM, which
directly measures the model parameters using the Frobenius norm, without any
training data. By introducing an additional hyperparameter for control, FroM
outperforms baseline methods across various fine-tuning scenarios, alleviating
the task interference problem.

</details>


### [45] [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)
*Yifan Duan,Yihong Tang,Kehai Chen,Liqiang Nie,Min Zhang*

Main category: cs.CL

TL;DR: ORPP框架通过优化角色扮演提示，提高模型性能，超越现有提示优化方法，并具有即插即用能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法存在计算开销大或需强优化能力的问题，影响广泛适用性。

Method: ORPP通过优化和生成角色扮演提示，提高模型性能。首先在小部分训练样本上进行迭代优化，生成高质量角色扮演提示，然后利用模型的几次学习能力将优化经验迁移到其他样本。

Result: ORPP在性能上不仅能够匹配现有主流提示优化方法，而且在大多数情况下，效果更优。同时具备卓越的即插即用能力，能够与其他提示方法集成并增强其效果。

Conclusion: ORPP优化框架能够有效提升大规模语言模型的性能，并且具备出色的即插即用能力。

Abstract: High-quality prompts are crucial for eliciting outstanding performance from
large language models (LLMs) on complex tasks. Existing research has explored
model-driven strategies for prompt optimization. However, these methods often
suffer from high computational overhead or require strong optimization
capabilities from the model itself, which limits their broad applicability.To
address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a
framework that enhances model performance by optimizing and generating
role-playing prompts. The core idea of ORPP is to confine the prompt search
space to role-playing scenarios, thereby fully activating the model's intrinsic
capabilities through carefully crafted, high-quality role-playing prompts.
Specifically, ORPP first performs iterative optimization on a small subset of
training samples to generate high-quality role-playing prompts. Then,
leveraging the model's few-shot learning capability, it transfers the
optimization experience to efficiently generate suitable prompts for the
remaining samples.Our experimental results show that ORPP not only matches but
in most cases surpasses existing mainstream prompt optimization methods in
terms of performance. Notably, ORPP demonstrates superior "plug-and-play"
capability. In most cases, it can be integrated with various other prompt
methods and further enhance their effectiveness.

</details>


### [46] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
*Inderjeet Nair,Lu Wang*

Main category: cs.CL

TL;DR: 短格式与长格式测试中的价值偏好相关性弱，不同长格式生成环境下偏好一致性差，需要更强有力的方法来保证一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于大型语言模型伦理风险和价值倾向的评估依赖于短格式调查和心理测验，但实际应用中多为长格式、开放式回答，这导致在实际环境中关于价值相关风险和偏好的了解较少。

Method: 通过比较短格式和长格式反应中引出的价值偏好，分析五种大型语言模型在不同论据数量下的表现，以研究短格式测试推断的价值偏好是否与长格式输出中表达的一致。

Result: 研究发现短格式和长格式反应中推测的价值偏好相关性弱，不同长格式生成设置下的偏好相关性同样较弱。针对价值表达的一致性，对齐仅带来有限提升。同时，发现论据特异性与偏好强度负相关，而场景交叉表现与偏好正相关。

Conclusion: 研究表明，短格式测试与长格式输出之间的价值偏好相关性较弱，且不同长格式生成环境下的偏好一致性也较低。需要更强有力的方法来确保在不同应用中一致地表达价值。

Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on
short-form surveys and psychometric tests, yet real-world use involves
long-form, open-ended responses -- leaving value-related risks and preferences
in practical settings largely underexplored. In this work, we ask: Do value
preferences inferred from short-form tests align with those expressed in
long-form outputs? To address this question, we compare value preferences
elicited from short-form reactions and long-form responses, varying the number
of arguments in the latter to capture users' differing verbosity preferences.
Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),
we find (1) a weak correlation between value preferences inferred from
short-form and long-form responses across varying argument counts, and (2)
similarly weak correlation between preferences derived from any two distinct
long-form generation settings. (3) Alignment yields only modest gains in the
consistency of value expression. Further, we examine how long-form generation
attributes relate to value preferences, finding that argument specificity
negatively correlates with preference strength, while representation across
scenarios shows a positive correlation. Our findings underscore the need for
more robust methods to ensure consistent value expression across diverse
applications.

</details>


### [47] [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)
*Sina Bagheri Nezhad,Ameeta Agrawal*

Main category: cs.CL

TL;DR: NSAR通过结合神经和符号推理提高了大语言模型在复杂多目标推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长文本场景中进行多目标推理的困难。

Method: 引入NeuroSymbolic Augmented Reasoning（NSAR），通过提取符号事实和生成可执行的Python代码进行复杂推理。

Result: NSAR在多个语言和不同上下文长度上显著优于传统方法，在识别和合成多条信息方面表现出色。

Conclusion: NSAR结合神经推理和符号操作，提高了在多语言环境中的推理能力。

Abstract: Large language models (LLMs) often struggle to perform multi-target reasoning
in long-context scenarios where relevant information is scattered across
extensive documents. To address this challenge, we introduce NeuroSymbolic
Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic
reasoning during inference. NSAR explicitly extracts symbolic facts from text
and generates executable Python code to handle complex reasoning steps. Through
extensive experiments across seven languages and diverse context lengths, we
demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and
advanced prompting strategies in accurately identifying and synthesizing
multiple pieces of information. Our results highlight the effectiveness of
combining explicit symbolic operations with neural inference for robust,
interpretable, and scalable reasoning in multilingual settings.

</details>


### [48] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
*Junzhe Zhang,Huixuan Zhang,Xinyu Hu,Li Lin,Mingqi Gao,Shi Qiu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文开发了Minos-Corpus数据集和Minos评估模型，通过结合人类和GPT的评估数据，成功提升了多模态评估能力，特别是在文本到图像生成任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前在多模态生成任务的评估中，虽然已有快速发展的多模态大语言模型（MLLMs），但仍有两个被忽视的方面: (1) 开发文本到图像生成任务的评估能力；(2) 增加大规模人类评估数据的整合。

Method: 本文引入了Minos-Corpus，一个大规模多模态评估数据集，结合了来自人类和GPT的评估数据。基于此数据集，提出了数据选择与平衡、Mix-SFT训练方法，并应用DPO技术开发了Minos，一个基于7B骨干的多模态评估模型。

Result: Minos模型在所有任务的平均评估性能中实现了同规模开放源评估模型的SoTA表现，并且在文本到图像生成任务的评估中，比所有开放源和闭源模型表现更优。

Conclusion: 本文强调利用高质量人类评估数据的重要性，并在图文生成任务的数据上联合训练，成功开发了一个性能卓越的评估模型。

Abstract: Evaluation is important for multimodal generation tasks. With the rapid
progress of MLLMs, there is growing interest in applying MLLMs to build general
evaluation systems. However, existing work overlooks two aspects: (1) the
development of evaluation capabilities for text-to-image (T2I) generation task,
and (2) the incorporation of large-scale human evaluation data. In this paper,
we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that
combines evaluation data from both human and GPT. The corpus contains
evaluation data across both image-to-text(I2T) and T2I generation tasks. Based
on this corpus, we propose Data Selection and Balance, Mix-SFT training
methods, and apply DPO to develop Minos, a multimodal evaluation model built
upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among
all open-source evaluation models of similar scale on the average of evaluation
performance on all tasks, and outperforms all open-source and closed-source
models on evaluation of T2I generation task. Extensive experiments demonstrate
the importance of leveraging high-quality human evaluation data and jointly
training on evaluation data from both I2T and T2I generation tasks.

</details>


### [49] [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)
*Yongjian Li,HaoCheng Chu,Yukun Yan,Zhenghao Liu,Shi Yu,Zheni Zeng,Ruobing Wang,Sen Song,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KARE-RAG enhances RAG model performance by improving the processing of noisy content with structured knowledge, DDPO, and contrastive data generation.


<details>
  <summary>Details</summary>
Motivation: To address factual inconsistencies in RAG models caused by noise in retrieved documents and enhance generative models' capability to process such content.

Method: KARE-RAG utilizes structured knowledge representations, Dense Direct Preference Optimization (DDPO), and a contrastive data generation pipeline to enhance knowledge utilization of RAG models.

Result: Experiments show significant improvements in RAG pipelines across different model scales without compromising their general capabilities.

Conclusion: KARE-RAG improves the performance of standard RAG pipelines in both in-domain and out-of-domain tasks using targeted learning strategies and modest training data.

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access broader knowledge sources, yet factual inconsistencies persist due to
noise in retrieved documents-even with advanced retrieval methods. We
demonstrate that enhancing generative models' capacity to process noisy content
is equally critical for robust performance. In this paper, we present KARE-RAG
(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge
utilization through three key innovations: (1) structured knowledge
representations that facilitate error detection during training, (2) Dense
Direct Preference Optimization (DDPO)-a refined training objective that
prioritizes correction of critical errors, and (3) a contrastive data
generation pipeline that maintains semantic consistency while rectifying
factual inaccuracies. Experiments show our method significantly enhances
standard RAG pipelines across model scales, improving both in-domain and
out-of-domain task performance without compromising general capabilities.
Notably, these gains are achieved with modest training data, suggesting
data-efficient optimization is possible through targeted learning strategies.
Our findings establish a new direction for RAG improvement: by improving how
models learn to process retrieved content, we can enhance performance across
diverse inference paradigms. All data and code will be publicly available on
Github.

</details>


### [50] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
*Jie Zhu,Junhui Li,Yalong Wen,Xiandong Li,Lifan Guo,Feng Chen*

Main category: cs.CL

TL;DR: A new benchmark, M$^3$FinMeeting, is introduced to evaluate language models' comprehension of financial meetings across languages and sectors, showing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: Current financial benchmarks fail to capture the dynamics of real-world financial meetings due to their reliance on static resources like news articles and reports.

Method: Proposing a multilingual, multi-sector, and multi-task dataset with tasks such as summarization, QA pair extraction, and question answering to evaluate financial meeting comprehension.

Result: Experiments with seven popular LLMs indicate that existing long-context models struggle with financial meeting comprehension, validating the need for the proposed benchmark.

Conclusion: M$^3$FinMeeting is an effective benchmark for evaluating LLMs' understanding of financial meetings.

Abstract: Recent breakthroughs in large language models (LLMs) have led to the
development of new benchmarks for evaluating their performance in the financial
domain. However, current financial benchmarks often rely on news articles,
earnings reports, or announcements, making it challenging to capture the
real-world dynamics of financial meetings. To address this gap, we propose a
novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual,
multi-sector, and multi-task dataset designed for financial meeting
understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and
Japanese, enhancing comprehension of financial discussions in diverse
linguistic contexts. Second, it encompasses various industry sectors defined by
the Global Industry Classification Standard (GICS), ensuring that the benchmark
spans a broad range of financial activities. Finally,
$\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer
(QA) pair extraction, and question answering, facilitating a more realistic and
comprehensive evaluation of understanding. Experimental results with seven
popular LLMs reveal that even the most advanced long-context models have
significant room for improvement, demonstrating the effectiveness of
$\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting
comprehension skills.

</details>


### [51] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
*Zhuohan Xie,Dhruv Sahnan,Debopriyo Banerjee,Georgi Georgiev,Rushil Thareja,Hachem Madmoun,Jinyan Su,Aaryamonvikram Singh,Yuxia Wang,Rui Xing,Fajri Koto,Haonan Li,Ivan Koychev,Tanmoy Chakraborty,Salem Lahlou,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: 引入FinChain和ChainEval来评估财务领域的多步骤符号推理，发现当前最先进的模型仍需改进。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集如FinQA和ConvFinQA只监督最终的数值答案，而不评估中间推理步骤。因此，缺乏系统地评估多步符号推理的基准。

Method: 引入FinChain，这是一个针对可验证的链式思维（CoT）金融推理的符号基准，以及ChainEval，一种新的自动评估最终答案和中间推理的度量。

Result: 基准测试显示，即使是最先进的模型在多步骤财务推理方面也存在改进的空间。

Conclusion: 现有最先进的模型在多步骤财务推理方面仍有很大的改进空间。

Abstract: Multi-step symbolic reasoning is critical for advancing downstream
performance on financial tasks. Yet, benchmarks for systematically evaluating
this capability are lacking. Existing datasets like FinQA and ConvFinQA
supervise only final numerical answers, without assessing intermediate
reasoning steps. To address this, we introduce FinChain, the first symbolic
benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.
Spanning 54 topics across 12 financial domains, Fin- Chain offers five
parameterized templates per topic, each varying in reasoning complexity and
domain expertise required. Each dataset instance includes an executable Python
trace, enabling automatic generation of extensive training data and easy
adaptation to other domains. We also introduce ChainEval, a new metric for
automatic evaluation of both final answers and intermediate reasoning.
Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models
have considerable room for improvement in multi-step financial reasoning. All
templates and evaluation metrics for FinChain are available at https:
//github.com/mbzuai-nlp/finchain.

</details>


### [52] [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)
*Sohan Patnaik,Milan Aggarwal,Sumit Bhatia,Balaji Krishnamurthy*

Main category: cs.CL

TL;DR: COLLATE通过生成多样化的推理路径，提高了小型LLM的推理能力，在多个任务上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 由于大模型训练数据的不透明性和法律问题，在商业环境中使用受到限制，因此需要提高小模型的推理能力，不依赖大模型的信息提取。

Method: 我们提出了COLLATE，一个可训练框架，通过生成多样化的推理路径，从而调优小型LLM，从而提高下游任务的性能。

Result: COLLATE在跨领域的5个数据集上优于其他基于训练和提示的方法，并在不同模型家族和参数规模的LLMs上证明了其有效性。

Conclusion: COLLATE可以有效提高小型LLM的推理能力，且在多种数据集和领域上表现优于其他模型。

Abstract: LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions
by generating step-by-step rationales. Prior works have utilized this
capability to improve smaller and cheaper LMs (say, with 7B parameters).
However, various practical constraints, such as copyright and legal issues,
owing to lack of transparency in the pre-training data of large (often closed)
models, prevent their use in commercial settings. Little focus has been given
to improving the innate reasoning ability of smaller models without distilling
information from larger LLMs. To address this, we propose COLLATE, a trainable
framework that tunes a (small) LLM to generate those outputs from a pool of
diverse rationales that selectively improves the downstream task. COLLATE
enforces multiple instances of the same LLM to exhibit distinct behavior and
employs them to generate rationales to obtain diverse outputs. The LLM is then
tuned via preference optimization to choose the candidate rationale which
maximizes the likelihood of ground-truth answer. COLLATE outperforms several
trainable and prompting baselines on 5 datasets across 3 domains: maths problem
solving, natural language inference, and commonsense reasoning. We show the eff
icacy of COLLATE on LLMs from different model families across varying parameter
scales (1B to 8B) and demonstrate the benefit of multiple rationale providers
guided by the end task through ablations. Code is released here
(https://github.com/Sohanpatnaik106/collate).

</details>


### [53] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
*Yingying Zhuang,Aman Gupta,Anurag Beniwal*

Main category: cs.CL

TL;DR: 论文提出了一种新的策略，用于优化多语言嵌入模型的加权采样，以增强多语言信息检索的性能，达到了显著的提升。


<details>
  <summary>Details</summary>
Motivation: 由于高质量知识库资源常常有限且仅支持少数语言，因此需要一种有效的嵌入模型，将不同语言的句子转化为与知识库语言相同的特征向量空间，从而促进跨语言的知识共享。

Method: 提出了一种通过加权采样进行对比学习的策略，以微调多语言嵌入模型，使其能够在单语知识库中实现多语言信息检索。

Result: 加权采样策略在多语言信息检索中相比标准方法实现了显著的性能提升，在MRR上提升了31.03%，在Recall@3上提升了33.98%。

Conclusion: 本文提出的加权采样策略在多语言信息检索中的表现优于标准方法，特别是在MRR和Recall@3指标上取得了显著的性能提升。

Abstract: Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.

</details>


### [54] [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)
*Jinu Lee,Sagnik Mukherjee,Dilek Hakkani-Tur,Julia Hockenmaier*

Main category: cs.CL

TL;DR: ReasoningFlow is a schema that translates complex reasoning traces into understandable graph structures, aiding in the analysis and enhancement of large reasoning models.


<details>
  <summary>Details</summary>
Motivation: To create a unified schema that can analyze the semantic structures of complex reasoning traces generated by large reasoning models.

Method: The authors introduced ReasoningFlow, a schema that parses reasoning traces into directed acyclic graphs and identifies distinct reasoning patterns through subgraph structures.

Result: The proposed ReasoningFlow successfully characterizes reasoning patterns through graph-based structures, aiding in the interpretation and evaluation of reasoning in LRMs.

Conclusion: ReasoningFlow provides a unified and human-interpretable way to understand, evaluate, and enhance the reasoning processes of large reasoning models.

Abstract: Large reasoning models (LRMs) generate complex reasoning traces with
planning, reflection, verification, and backtracking. In this work, we
introduce ReasoningFlow, a unified schema for analyzing the semantic structures
of these complex traces. ReasoningFlow parses traces into directed acyclic
graphs, enabling the characterization of distinct reasoning patterns as
subgraph structures. This human-interpretable representation offers promising
applications in understanding, evaluating, and enhancing the reasoning
processes of LRMs.

</details>


### [55] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)
*Maike Behrendt,Stefan Sylvius Wagner,Carina Weinmann,Marike Bormann,Mira Warne,Stefan Harmeling*

Main category: cs.CL

TL;DR: 机器学习能改善政治在线讨论质量。


<details>
  <summary>Details</summary>
Motivation: 政治在线参与形式（例如讨论政治问题和公民之间的意见交流）越来越重要，而这些活动越来越多地以数字形式进行。要做出决策，理想情况下需要进行仔细的讨论、意见考虑和公民的论辩。

Method: 研究展示了政治在线讨论中出现的问题，并如何利用机器学习来解决这些问题，增强讨论的质量。

Result: 讨论设计和参与过程的质量在很大程度上取决于平台和流程的设计。机器学习方法有潜力促进在线交流，提高讨论质量。

Conclusion: 机器学习方法可以有效解决政治在线讨论中的问题，并提高讨论的质量。

Abstract: Political online participation in the form of discussing political issues and
exchanging opinions among citizens is gaining importance with more and more
formats being held digitally. To come to a decision, a careful discussion and
consideration of opinions and a civil exchange of arguments, which is defined
as the act of deliberation, is desirable. The quality of discussions and
participation processes in terms of their deliberativeness highly depends on
the design of platforms and processes. To facilitate online communication for
both participants and initiators, machine learning methods offer a lot of
potential. In this work we want to showcase which issues occur in political
online discussions and how machine learning can be used to counteract these
issues and enhance deliberation.

</details>


### [56] [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
*Xin Liu,Lu Wang*

Main category: cs.CL

TL;DR: 提出了方法减少推理冗余，提高效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 链式思维提示增强了大型语言模型（LLMs）中的推理能力，但往往导致冗长和冗余输出，从而增加了推理成本。我们假设许多推理步骤对于产生正确答案是不必要的。

Method: 我们提出了三种推理时间策略以提高效率：（1）通过答案一致性提前停止，（2）提高生成推理结束信号的概率，（3）一种基于内部激活学习何时停止的监督方法。

Result: 我们的方法显著减少了标记使用，而准确性几乎没有下降。在特别是在自然问题（NaturalQuestions）上，答案一致性减少了超过40%的标记，同时进一步提高了准确性。

Conclusion: 我们的工作强调了在推理时间操作的成本效益推理方法的重要性，并为现实应用提供了实用的好处。

Abstract: Chain-of-thought (CoT) prompting enhances reasoning in large language models
(LLMs) but often leads to verbose and redundant outputs, thus increasing
inference cost. We hypothesize that many reasoning steps are unnecessary for
producing correct answers. To investigate this, we start with a systematic
study to examine what is the minimum reasoning required for a model to reach a
stable decision. We find that on math reasoning tasks like math, models
typically converge to their final answers after 60\% of the reasoning steps,
suggesting substantial redundancy in the remaining content. Based on these
insights, we propose three inference-time strategies to improve efficiency: (1)
early stopping via answer consistency, (2) boosting the probability of
generating end-of-reasoning signals, and (3) a supervised method that learns
when to stop based on internal activations. Experiments across five benchmarks
and five open-weights LLMs show that our methods significantly reduce token
usage with little or no accuracy drop. In particular, on NaturalQuestions,
Answer Consistency reduces tokens by over 40\% while further improving
accuracy. Our work underscores the importance of cost-effective reasoning
methods that operate at inference time, offering practical benefits for
real-world applications.

</details>


### [57] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Main category: cs.CL

TL;DR: 该研究提出了一种名为CoRe-MMRAG的新框架，用于解决多模态检索增强生成中的一致性问题，并在多个测试集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在多模态检索增强生成任务中，存在参数化检索知识不一致性和视觉文本知识不一致性的问题，需要有效解决来源之间不一致的问题。

Method: 提出了一种新颖的端到端框架CoRe-MMRAG，通过四阶段的流水线解决知识源一致性的问题，包括生成内部响应、选择最相关的多模态证据、生成外部响应及整合两者以产生可靠答案。

Result: 在KB-VQA基准测试中，CoRe-MMRAG较基准方法有显著提升，在InfoSeek和Encyclopedic-VQA数据集上分别取得5.6%和9.3%的性能提升。

Conclusion: CoRe-MMRAG显著提高了多模态知识检索增强生成任务的性能，在InfoSeek和Encyclopedic-VQA数据集上分别获得了5.6%和9.3%的性能提升。

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge
\textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG}
(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles
inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage
pipeline: it first generates an internal response from parametric knowledge,
then selects the most relevant multimodal evidence via joint similarity
assessment, generates an external response, and finally integrates both to
produce a reliable answer. Additionally, a specialized training paradigm
enhances knowledge source discrimination, multimodal integration, and unified
answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG
achieves substantial improvements over baseline methods, achieving 5.6\% and
9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We
release code and data at
\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [58] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
*Yirao Zhao,Guizhen Chen,Kenji Kawaguchi,Lidong Bing,Wenxuan Zhang*

Main category: cs.CL

TL;DR: Cus-Prun是一种剪枝方法，通过剪除无关神经元来减少大模型的冗余参数，以创建针对特定任务的专家模型，表现优于现有方法且无需后训练。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型往往需要大量的计算资源，因此在不影响性能的情况下进行参数剪枝是至关重要的，尤其是当用户需要针对特定场景的紧凑型专家模型时。现有方法大多关注保持模型的通用能力，但可能需要耗费大量后训练或者由于粗粒度剪枝导致性能下降。

Method: Cus-Prun方法通过识别和剪枝语言、领域和任务维度上无关的神经元，来降低大型模型中的冗余参数，以创建更轻量级的专家模型。

Result: 实验表明，Cus-Prun方法能够在不同模型家族和大小的模型中，以极小的能力损失，一贯地优于其他剪枝方法。

Conclusion: 本文提出了一种名为Cus-Prun的剪枝方法，能够在不进行后训练的情况下，将大规模通用模型剪枝成更小的、针对特定任务的专家模型，且在保留专家和通才能力方面表现优异。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing pruning methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained pruning. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.

</details>


### [59] [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)
*Muhammad Falensi Azmi,Muhammad Dehan Al Kautsar,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: IndoSafety是专为印尼语境设计的安全评估数据集，通过对印尼语言模型微调提高安全性，强调了文化背景对语言模型安全的重要性。


<details>
  <summary>Details</summary>
Motivation: 印度尼西亚的多样化文化背景需要对本地规范的敏感性，因此需要对语言模型进行安全性探索。

Method: 通过扩展先前的安全框架来构建一个分类法，以捕捉印尼的社会文化背景，从而构建IndoSafety数据集。

Result: 通过对IndoSafety的微调，可以显著提高现有印尼语言模型的安全性，同时保持任务性能。

Conclusion: 我们的工作强调了在多语言环境中需要以文化为基础的安全评估，并为负责部署大型语言模型提供了具体步骤。

Abstract: Although region-specific large language models (LLMs) are increasingly
developed, their safety remains underexplored, particularly in culturally
diverse settings like Indonesia, where sensitivity to local norms is essential
and highly valued by the community. In this work, we present IndoSafety, the
first high-quality, human-verified safety evaluation dataset tailored for the
Indonesian context, covering five language varieties: formal and colloquial
Indonesian, along with three major local languages: Javanese, Sundanese, and
Minangkabau. IndoSafety is constructed by extending prior safety frameworks to
develop a taxonomy that captures Indonesia's sociocultural context. We find
that existing Indonesian-centric LLMs often generate unsafe outputs,
particularly in colloquial and local language settings, while fine-tuning on
IndoSafety significantly improves safety while preserving task performance. Our
work highlights the critical need for culturally grounded safety evaluation and
provides a concrete step toward responsible LLM deployment in multilingual
settings. Warning: This paper contains example data that may be offensive,
harmful, or biased.

</details>


### [60] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
*Sarenne Wallbridge,Christoph Minixhofer,Catherine Lai,Peter Bell*

Main category: cs.CL

TL;DR: 研究探讨韵律独立于词汇内容的影响，采用自监督学习分析音声韵律结构，并在长期感知任务中展现出色性能。


<details>
  <summary>Details</summary>
Motivation: 探讨音声韵律的时序结构，以揭示韵律如何独立于词汇内容地影响语音结构。

Method: 采用自监督学习（SSL）和创新的Masked Prosody Model进行实验，分析音声韵律中的时序结构。

Result: Masked Prosody Model对于依赖于局部信息的感知标签具有良好的预测能力，尤其在长时间结构的标签中表现优秀。实验结果较传统音调、能量和语音活动特征有显著改进。

Conclusion: 自监督学习（SSL）在利用语音中韵律的声学属性方面显示出明显优势，尤其在涉及长期结构的感知任务中，如情感识别。

Abstract: People exploit the predictability of lexical structures during text
comprehension. Though predictable structure is also present in speech, the
degree to which prosody, e.g. intonation, tempo, and loudness, contributes to
such structure independently of the lexical content is unclear. This study
leverages self-supervised learning (SSL) to examine the temporal granularity of
structures in the acoustic correlates of prosody. Representations from our
proposed Masked Prosody Model can predict perceptual labels dependent on local
information, such as word boundaries, but provide the most value for labels
involving longer-term structures, like emotion recognition. Probing experiments
across various perceptual labels show strong relative gains over untransformed
pitch, energy, and voice activity features. Our results reveal the importance
of SSL training objective timescale and highlight the value of complex
SSL-encoded structures compared to more constrained classical structures.

</details>


### [61] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
*Maria Levchenko*

Main category: cs.CL

TL;DR: 研究探讨了俄语文化事件文本中的人名实体识别，GPT-4o在特定条件下表现最佳，强调了模型在形态复杂语言中的优势和进步。


<details>
  <summary>Details</summary>
Motivation: 旨在提升对当前NER模型在俄罗斯文化遗产领域应用中的能力和局限性的理解，尤其是应用于俄语这种形态复杂的语言。

Method: 利用独特的SPbLitGuide数据集，该数据集包含了1999年至2019年间圣彼得堡的事件公告。在多样的NER模型中进行对比评估，包括DeepPavlov、RoBERTa和SpaCy等变压器架构以及GPT-3.5、GPT-4和GPT-4o等大型语言模型。

Result: 在特定提示的JSON输出情况下，GPT-4o取得了0.93的F1分数，并且GPT-4在精度上表现最佳，达到0.99。后续GPT-4.1（2025年4月）在简单和结构化提示下均实现了F1=0.94，表明模型家族的快速进步和简化的部署需求。

Conclusion: 本文探讨了在俄罗斯新闻文本（尤其与文化事件相关）中进行人名实体识别的挑战，并证明了GPT-4o在给定特定JSON输出提示的情况下表现优异。

Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.

</details>


### [62] [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)
*Minh Duc Bui,Kyung Eun Park,Goran Glavaš,Fabian David Schmidt,Katharina von der Wense*

Main category: cs.CL

TL;DR: 研究了大型语言模型在不同测量系统下的表现，发现存在表现不稳定的问题，可通过推理方法改善但增加了计算成本。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨大型语言模型是否能准确地在不同的测量系统下提供信息。

Method: 使用新编译的数据集测试七种开源LLMs，研究其在不同测量系统下的表现及准确性。

Result: 发现LLMs默认使用数据中主要的测量系统，并且在不同测量系统下表现不稳定，使用推理方法可改善但会增加测试时间和计算成本。

Conclusion: LLMs倾向于选用数据中主要使用的测量系统，且在不同测量系统之间表现不稳定。尽管使用推理方法可部分缓解这种不稳定性，但增加了测试时间和计算成本。

Abstract: Measurement systems (e.g., currencies) differ across cultures, but the
conversions between them are well defined so that humans can state facts using
any measurement system of their choice. Being available to users from diverse
cultural backgrounds, large language models (LLMs) should also be able to
provide accurate information irrespective of the measurement system at hand.
Using newly compiled datasets we test if this is the case for seven open-source
LLMs, addressing three key research questions: (RQ1) What is the default system
used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their
accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate
potential challenges w.r.t. underrepresented systems via reasoning? Our
findings show that LLMs default to the measurement system predominantly used in
the data. Additionally, we observe considerable instability and variance in
performance across different measurement systems. While this instability can in
part be mitigated by employing reasoning methods such as chain-of-thought
(CoT), this implies longer responses and thereby significantly increases
test-time compute (and inference costs), marginalizing users from cultural
backgrounds that use underrepresented measurement systems.

</details>


### [63] [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)
*Zhi-Yuan Chen,Hao Wang,Xinyu Zhang,Enrui Hu,Yankai Lin*

Main category: cs.CL

TL;DR: 引入DBG分数来准确测量LLMs作为评判者时的自我偏好偏见，并探讨偏见的影响因素和机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法混淆了偏见与响应质量的区别，迫切需要一个准确测量偏见的指标。

Method: 引入金标准判断作为响应质量的代理，提出DBG分数来测量自我偏好偏见，并进行实验评估不同版本、大小和推理能力的LLMs偏见。

Result: 使用DBG分数发现不同版本和大小的LLMs存在自我偏好偏见，并提出影响因素和偏见机制。实验提供了消除偏见的可能途径。

Conclusion: 研究表明，大型语言模型存在自我偏好偏见，这种偏见可能影响其作为评判者的公正性。提出的DBG分数能够有效区分偏见和响应质量，提供了一种新的评估方法。研究也揭示了影响偏见的因素以及潜在的注意力机制。

Abstract: Recent studies show that large language models (LLMs) exhibit self-preference
bias when serving as judges, meaning they tend to favor their own responses
over those generated by other models. Existing methods typically measure this
bias by calculating the difference between the scores a judge model assigns to
its own responses and those it assigns to responses from other models. However,
this approach conflates self-preference bias with response quality, as
higher-quality responses from the judge model may also lead to positive score
differences, even in the absence of bias. To address this issue, we introduce
gold judgments as proxies for the actual quality of responses and propose the
DBG score, which measures self-preference bias as the difference between the
scores assigned by the judge model to its own responses and the corresponding
gold judgments. Since gold judgments reflect true response quality, the DBG
score mitigates the confounding effect of response quality on bias measurement.
Using the DBG score, we conduct comprehensive experiments to assess
self-preference bias across LLMs of varying versions, sizes, and reasoning
abilities. Additionally, we investigate two factors that influence and help
alleviate self-preference bias: response text style and the post-training data
of judge models. Finally, we explore potential underlying mechanisms of
self-preference bias from an attention-based perspective. Our code and data are
available at https://github.com/zhiyuanc2001/self-preference.

</details>


### [64] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
*Fan Gao,Dongyuan Li,Ding Xia,Fei Mi,Yasheng Wang,Lifeng Shang,Baojun Wang*

Main category: cs.CL

TL;DR: 提出了中文作文多体裁基准，通过细粒度评分框架评估15种大型语言模型在中文写作中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基准大多依赖粗粒度的文本质量指标，未能充分考虑中文作文的结构和修辞复杂性，尤其是在不同体裁中。因此，我们提出了新的基准来解决这一差距。

Method: 我们开发了一种细粒度的、体裁特定的评分框架，该框架以分层方式聚合分数，并通过全面的人类协议研究验证了我们的评估协议。

Result: 我们策划并完善了728个真实世界的题目，确保其真实性，并将其细致地分类为\textit{开放式}和\textit{受限式}集，以捕捉多样化的写作场景。

Conclusion: 我们提出了一个名为 \benchName 的多体裁基准，专门为中文作文设计，涵盖论述、叙述、描写和说明四大类体裁，并验证评估协议，通过对15种大型语言模型进行基准测试，分析其在各个体裁和指令类型上的优势和局限性。

Abstract: Chinese essay writing and its evaluation are critical in educational
contexts, yet the capabilities of Large Language Models (LLMs) in this domain
remain largely underexplored. Existing benchmarks often rely on coarse-grained
text quality metrics, largely overlooking the structural and rhetorical
complexities of Chinese essays, particularly across diverse genres. To address
this gap, we propose \benchName, a multi-genre benchmark specifically designed
for Chinese essay writing across four major genres: Argumentative, Narrative,
Descriptive, and Expository. We curate and refine a total of 728 real-world
prompts to ensure authenticity and meticulously categorize them into the
\textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing
scenarios. To reliably evaluate generated essays, we develop a fine-grained,
genre-specific scoring framework that hierarchically aggregates scores. We
further validate our evaluation protocol through a comprehensive human
agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their
strengths and limitations across genres and instruction types. With \benchName,
we aim to advance LLM-based Chinese essay evaluation and inspire future
research on improving essay generation in educational settings.

</details>


### [65] [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)
*Ömer Tarik Özyilmaz,Matt Coler,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 通过对OpenAI的Whisper进行微调，可以显著改善五大阿拉伯方言识别的性能，尤其是在处理数据稀缺的问题时，方言数据的合理整合十分有效。


<details>
  <summary>Details</summary>
Motivation: 商业阿拉伯语ASR系统在方言语音识别方面仍有不足，因此希望通过模型微调来改进对不同阿拉伯方言的识别效果。

Method: 使用Mozilla Common Voice数据集进行现代标准阿拉伯语（MSA）的微调，并使用MASC数据集进行方言语音的训练，以研究不同训练方案对五大方言识别的效果。

Result: 小规模MSA微调数据已使较小的模型显著提高性能，且方言集成模型与单独方言模型性能相当。MSA的预训练对方言的微调帮助不大，表明MSA与方言之间的共享特征有限。

Conclusion: 通过对OpenAI的Whisper模型进行微调，我们在处理五大阿拉伯方言时取得了显著改善，尤其是对小数据量的模型。数据整合的策略在应对数据资源不足的问题上表现出色。

Abstract: Although commercial Arabic automatic speech recognition (ASR) systems support
Modern Standard Arabic (MSA), they struggle with dialectal speech. We
investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic
dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common
Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA
training size effects, benefits of pre-training on MSA data, and
dialect-specific versus dialect-pooled models. We find that small amounts of
MSA fine-tuning data yield substantial improvements for smaller models,
matching larger non-fine-tuned models. While MSA pre-training shows minimal
benefit, suggesting limited shared features between MSA and dialects, our
dialect-pooled models perform comparably to dialect-specific ones. This
indicates that pooling dialectal data, when properly balanced, can help address
data scarcity in low-resource ASR without significant performance loss.

</details>


### [66] [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
*Manon Reusens,Bart Baesens,David Jurgens*

Main category: cs.CL

TL;DR: 本文提出了分析LLMs在不同任务和角色分配时一致性的框架，发现一致性受角色、成见和模型设计的多种因素影响，并随任务结构化程度增加。


<details>
  <summary>Details</summary>
Motivation: 目前的研究缺乏对LLMs在不同角色和任务类型的一致性进行全面分析，因此提出了一个标准化框架来填补这一空白。

Method: 引入了一个新的标准化框架来分析在不同任务和运行之间相同角色分配时模型的响应一致性。

Result: 一致性受到多种因素的影响，包括分配的角色、成见和模型设计选择。

Conclusion: 一致性在不同任务中有所不同，并且随着任务结构化程度和额外上下文的增加而增加。

Abstract: Personalized Large Language Models (LLMs) are increasingly used in diverse
applications, where they are assigned a specific persona - such as a happy high
school teacher - to guide their responses. While prior research has examined
how well LLMs adhere to predefined personas in writing style, a comprehensive
analysis of consistency across different personas and task types is lacking. In
this paper, we introduce a new standardized framework to analyze consistency in
persona-assigned LLMs. We define consistency as the extent to which a model
maintains coherent responses when assigned the same persona across different
tasks and runs. Our framework evaluates personas across four different
categories (happiness, occupation, personality, and political stance) spanning
multiple task dimensions (survey writing, essay generation, social media post
generation, single turn, and multi-turn conversations). Our findings reveal
that consistency is influenced by multiple factors, including the assigned
persona, stereotypes, and model design choices. Consistency also varies across
tasks, increasing with more structured tasks and additional context. All code
is available on GitHub.

</details>


### [67] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
*Shihan Dou,Ming Zhang,Chenhao Huang,Jiayi Chen,Feng Chen,Shichun Liu,Yan Liu,Chenxiao Liu,Cheng Zhong,Zongzhang Zhang,Tao Gui,Chao Xin,Wei Chengzhi,Lin Yan,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: EvaLearn是一个开创性的基准，用于评估大型语言模型的学习能力和效率，发现当前模型显示出不同的学习能力和性能，但较强静态能力的模型在学习能力方面没有显著优势。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型的学习能力和效率是一个关键但尚未充分探索的模型潜力方面。

Method: EvaLearn基准有648道问题，分为六种任务类型，被分为182个序列。每个序列对应一种任务类型，要求模型顺序解决问题，从而利用之前解决问题获得的经验。此外，提供了五个综合自动化指标来评估模型和量化其学习能力和效率。

Result: 通过对九个前沿模型的广泛基准测试，发现不同模型表现各异：一些如Claude-3.7-sonnet在起初表现一般但展示出强大的学习能力，而有些模型则难以从经验中获益甚至出现负迁移。观察到当前拥有较强静态能力的LLM在所有任务中没有表现出显著的学习优势。

Conclusion: EvaLearn提供了一种新的评价视角，用于评估LLM潜力和理解模型与人类能力之间的差距，推动开发更深入，更动态的评估方法。

Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.

</details>


### [68] [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
*Zhong-Zhi Li,Xiao Liang,Zihao Tang,Lei Ji,Peijie Wang,Haotian Xu,Xing W,Haizhen Huang,Weiwei Deng,Ying Nian Wu,Yeyun Gong,Zhijiang Guo,Xiao Liu,Fei Yin,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: 提出了动态比例训练管道，减少推理冗余，保持准确性，同时输出减少40%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理中的效率问题，尤其是在产生极长输出时，引起了研究界的关注。

Method: 提出了一种动态比例的训练管道，通过在模型的System-1和System-2数据之间持续调整权重，来减少冗余的推理步骤。

Result: 在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上进行验证，在保持推理准确性的同时，将输出标记数量减少了近40%。

Conclusion: 该研究提出了一种动态比例训练管道，无需依赖复杂的数据注释或在多个模型间进行插值，能够减少推理过程中冗余的推理步骤，并保持推理能力。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by
leveraging Reinforcement Learning and extended Chain-of-Thought (CoT)
techniques. However, the challenge of performing efficient language
reasoning--especially during inference with extremely long outputs--has drawn
increasing attention from the research community. In this work, we propose a
dynamic ratio-based training pipeline that does not rely on sophisticated data
annotations or interpolation between multiple models. We continuously balance
the weights between the model's System-1 and System-2 data to eliminate
redundant reasoning processes while preserving the model's reasoning
capability. We validate our approach across models on DeepSeek-R1-Distill-7B
and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying
difficulty levels. Our method significantly reduces the number of output tokens
by nearly 40% while maintaining the accuracy of the reasoning. Our code and
data will be available soon.

</details>


### [69] [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)
*Zhengdong Lu,Weikai Lu,Yiling Tao,Yun Dai,ZiXuan Chen,Huiping Zhuang,Cen Chen,Hao Peng,Ziqian Zeng*

Main category: cs.CL

TL;DR: 提出了一种新方法DPPM，用于在旅行规划任务中解决现有方法的重约束和级联错误问题，实验结果显示其效果更佳。


<details>
  <summary>Details</summary>
Motivation: 解决现有规划方法中的重约束和级联错误问题。

Method: 提出了一种新的并行规划范式DPPM，通过分解任务、并行规划子任务以及合并子计划来解决问题，并结合验证和改进模块进行错误修正和冲突解决。

Result: 实验结果表明DPPM显著优于现有方法。

Conclusion: DPPM在旅行规划任务中表现显著优于现有方法。

Abstract: Despite significant advances in Large Language Models (LLMs), planning tasks
still present challenges for LLM-based agents. Existing planning methods face
two key limitations: heavy constraints and cascading errors. To address these
limitations, we propose a novel parallel planning paradigm, which Decomposes,
Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).
Specifically, DPPM decomposes the complex task based on constraints into
subtasks, generates the subplan for each subtask in parallel, and merges them
into a global plan. In addition, our approach incorporates a verification and
refinement module, enabling error correction and conflict resolution.
Experimental results demonstrate that DPPM significantly outperforms existing
methods in travel planning tasks.

</details>


### [70] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)
*Liang Yue,Yihong Tang,Kehai Chen,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: 研究提出 MASTER 数据增强方法，通过多智能体交互丰富数据，构建 BOOST-QA 数据集提升模型微调效果，显著改善推理能力。


<details>
  <summary>Details</summary>
Motivation: 大规模模型的高质量微调数据难以获取，本研究旨在克服数据收集困难和生产成本高的问题。

Method: 提出了一种名为 MASTER 的数据增强方法，通过多智能体之间的交互来丰富原始数据，并利用多智能体对话生成高质量的师生互动数据。

Result: 使用 MASTER 构建的 BOOST-QA 数据集使得微调后的模型在多个基准上表现卓越，特别是在复杂任务中的推理能力上有显著提升。

Conclusion: 使用 BOOST-QA 数据集微调的模型在多个基准测试中表现优异，显著提高了推理能力。

Abstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'
instruction-following capabilities and task-specific performance. However,
obtaining high-quality fine-tuning data for large models is challenging due to
data collection difficulties and high production costs. To address this, we
propose MASTER, a novel data augmentation method that enriches original data
through interactions among multiple agents with varying cognitive levels. We
simulate three pedagogically grounded teaching scenarios, leveraging
multi-agent conversations to generate high-quality teacher-student interaction
data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented
from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.
Experiments show that models fine-tuned with BOOST-QA perform excellently
across multiple benchmarks, demonstrating strong multitask generalization.
Notably, MASTER significantly improves models' reasoning abilities in complex
tasks, providing valuable insights for future research.

</details>


### [71] [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
*Masaki Sakata,Sho Yokoi,Benjamin Heinzerling,Takumi Ito,Kentaro Inui*

Main category: cs.CL

TL;DR: 研究显示，语言模型能够识别、区分命名实体，且相关信息在模型早期层中被低维线性子空间紧凑地表示。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型内部对命名实体的表征能力，解决实体提及中的歧义性和变化性问题。

Method: 采用类聚质量度量的框架，通过类聚分析语言模型的内部表征来量化相同实体的提及聚集和不同实体的提及分隔程度。

Result: 五种基于Transformer的自回归模型在识别和区分实体时表现出高效性，类似精度和召回率的指标在0.66到0.9之间。同时，实体相关信息在早期语言模型层中被紧凑地表示，影响语言预测性能。

Conclusion: 语言模型的内部表征能够有效识别和区分实体，且在早期层中以低维线性子空间紧凑表示实体相关信息。

Abstract: We analyze the extent to which internal representations of language models
(LMs) identify and distinguish mentions of named entities, focusing on the
many-to-many correspondence between entities and their mentions. We first
formulate two problems of entity mentions -- ambiguity and variability -- and
propose a framework analogous to clustering quality metrics. Specifically, we
quantify through cluster analysis of LM internal representations the extent to
which mentions of the same entity cluster together and mentions of different
entities remain separated. Our experiments examine five Transformer-based
autoregressive models, showing that they effectively identify and distinguish
entities with metrics analogous to precision and recall ranging from 0.66 to
0.9. Further analysis reveals that entity-related information is compactly
represented in a low-dimensional linear subspace at early LM layers.
Additionally, we clarify how the characteristics of entity representations
influence word prediction performance. These findings are interpreted through
the lens of isomorphism between LM representations and entity-centric knowledge
structures in the real world, providing insights into how LMs internally
organize and use entity information.

</details>


### [72] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
*Qihang Yan,Xinyu Zhang,Luming Guo,Qi Zhang,Feifan Liu*

Main category: cs.CL

TL;DR: RACE-Align框架通过结合外部知识和链式思维，优化偏好数据集构建，提升LLMs在垂直领域的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在垂直领域中面临准确性、领域特定推理和可解释性的问题，传统的偏好对齐方法往往忽视了底层的知识来源和推理逻辑。

Method: 本文提出了一种名为RACE-Align（基于检索增强和链式思维增强的对齐）的新框架。该方法通过构建二元偏好数据集，融合外部知识支持和显式的链式思维（CoT）推理，然后使用DPO算法对LLMs进行对齐。

Result: 在实验验证中，以Qwen3-1.7B为基础模型，在传统中医领域，RACE-Align表现优于原始基础模型和仅通过监督微调（SFT）进行微调的模型。在诸如回答准确性、信息丰富度、应用中医思维模式、推理的合逻辑性和深度及可解释性等多方面都有提升。

Conclusion: RACE-Align显著提升了LLMs在复杂垂直领域中的知识应用、推理可靠性和过程透明性。

Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific
reasoning, and interpretability in vertical domains. Traditional preference
alignment methods like Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) often overlook the underlying knowledge
sources and reasoning logic. This paper introduces RACE-Align
(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel
framework designed to address these limitations. RACE-Align systematically
constructs a binary preference dataset incorporating external knowledge support
and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO
algorithm. The core innovation lies in its preference data construction
strategy: it integrates AI-driven retrieval for factual grounding, enhancing
knowledgeability and accuracy, and emphasizes the optimization of
domain-specific CoT, treating the reasoning process itself as a key preference
dimension. A multi-stage, AI-driven refinement pipeline cost-effectively
generates these preference pairs. Experimental validation in Traditional
Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that
RACE-Align significantly outperforms the original base model and a model
fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed
across multiple dimensions, including answer accuracy, information richness,
application of TCM thinking patterns, logicality and depth of reasoning, and
interpretability. These findings suggest RACE-Align offers an effective pathway
to enhance LLMs' knowledge application, reasoning reliability, and process
transparency in complex vertical domains.

</details>


### [73] [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)
*Amaç Herdağdelen,Marco Baroni*

Main category: cs.CL

TL;DR: The paper explores using text corpora and Twitter data to identify gender biases in actions, demonstrating it is feasible to supplement commonsense knowledge with these biases, achieving good correlation and predictive accuracy results.


<details>
  <summary>Details</summary>
Motivation: To investigate the stereotypical gender expectations of actions and enhance commonsense knowledge repositories with these insights.

Method: The study extracted gender-specific actions from text corpora and Twitter, used Open Mind Common Sense for analysis, and employed gender information from Twitter users and web-corpus-based heuristics to evaluate bias. They measured correlation and predictive accuracy using Spearman correlation and ROC curve metrics.

Result: Achieved a Spearman correlation of 0.47 between predictions and a human gold standard, and an area under the ROC curve of 0.76 for predicting polarity. Presented datasets of commonsense actions rated for gender bias.

Conclusion: It is feasible to use natural text, particularly from Twitter, to augment commonsense repositories with stereotypical gender expectations of actions.

Abstract: We extracted gender-specific actions from text corpora and Twitter, and
compared them to stereotypical expectations of people. We used Open Mind Common
Sense (OMCS), a commonsense knowledge repository, to focus on actions that are
pertinent to common sense and daily life of humans. We use the gender
information of Twitter users and Web-corpus-based pronoun/name gender
heuristics to compute the gender bias of the actions. With high recall, we
obtained a Spearman correlation of 0.47 between corpus-based predictions and a
human gold standard, and an area under the ROC curve of 0.76 when predicting
the polarity of the gold standard. We conclude that it is feasible to use
natural text (and a Twitter-derived corpus in particular) in order to augment
commonsense repositories with the stereotypical gender expectations of actions.
We also present a dataset of 441 commonsense actions with human judges' ratings
on whether the action is typically/slightly masculine/feminine (or neutral),
and another larger dataset of 21,442 actions automatically rated by the methods
we investigate in this study.

</details>


### [74] [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 该研究提出了结合多任务学习和主动学习的方法，提高阿拉伯语社交媒体中冒犯性语言检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的发展加剧了冒犯、暴力和粗俗言论的传播，尤其在阿拉伯语中，由于数据标注有限、方言变化和语言复杂性，检测这些内容特别复杂。

Method: 通过将多任务学习（MTL）与主动学习结合来增强阿拉伯语社交媒体文本中冒犯性语言的检测。利用多任务学习，在两个辅助任务（暴力和粗俗言论）上联合训练模型。这种方法在训练期间动态调整任务权重，并采用主动学习策略选择最具信息量的样本进行训练。

Result: 本研究在OSACT2022数据集上的实验结果表明，该框架达到了85.42%的state-of-the-art宏F1分数，超越了现有方法，并使用了更少的微调样本。

Conclusion: 结合多任务学习与主动学习可以在资源受限的环境中实现高效且准确的冒犯性语言检测。

Abstract: The rapid growth of social media has amplified the spread of offensive,
violent, and vulgar speech, which poses serious societal and cybersecurity
concerns. Detecting such content in Arabic text is particularly complex due to
limited labeled data, dialectal variations, and the language's inherent
complexity. This paper proposes a novel framework that integrates multi-task
learning (MTL) with active learning to enhance offensive speech detection in
Arabic social media text. By jointly training on two auxiliary tasks, violent
and vulgar speech, the model leverages shared representations to improve the
detection accuracy of the offensive speech. Our approach dynamically adjusts
task weights during training to balance the contribution of each task and
optimize performance. To address the scarcity of labeled data, we employ an
active learning strategy through several uncertainty sampling techniques to
iteratively select the most informative samples for model training. We also
introduce weighted emoji handling to better capture semantic cues. Experimental
results on the OSACT2022 dataset show that the proposed framework achieves a
state-of-the-art macro F1-score of 85.42%, outperforming existing methods while
using significantly fewer fine-tuning samples. The findings of this study
highlight the potential of integrating MTL with active learning for efficient
and accurate offensive language detection in resource-constrained settings.

</details>


### [75] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
*Stefano Bannò,Kate Knill,Mark Gales*

Main category: cs.CL

TL;DR: 研究展示了大型语言模型（LLMs）在二语词汇精细评估中的优势，通过结合英语词汇水平表，LLMs可更准确地评估语言熟练度，并超越传统基于词性的方法。


<details>
  <summary>Details</summary>
Motivation: 以往自动系统对二语词汇的评估主要集中在与词性相关的词语使用上。这篇文章旨在探索在语境中对词汇进行更精细的评估，以提高评估的准确性和相关性。

Method: 本文提出了一种结合大型语言模型（LLMs）和《英语词汇水平表》（EVP）的新方法，以实现对二语词汇的细粒度评价。

Result: LLMs不仅在单词的语境使用中表现出色，并且与文章整体的水平有较好的关联性。同时，LLMs能够揭示《英语词汇水平表》（EVP）中词汇水平的一致性。

Conclusion: 大型语言模型（LLMs）能够有效地进行词汇评估，并在多义性、上下文变化和多词表达等方面表现出色，与基于词性的方法相比具有明显优势。

Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency.
To date, its assessment by automated systems has typically examined the
context-independent, or part-of-speech (PoS) related use of words. This paper
introduces a novel approach to enable fine-grained vocabulary evaluation
exploiting the precise use of words within a sentence. The scheme combines
large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP
is a standard lexical resource that enables in-context vocabulary use to be
linked with proficiency level. We evaluate the ability of LLMs to assign
proficiency levels to individual words as they appear in L2 learner writing,
addressing key challenges such as polysemy, contextual variation, and
multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to
exploit additional semantic information that yields improved performance. We
also explore correlations between word-level proficiency and essay-level
proficiency. Finally, the approach is applied to examine the consistency of the
EVP proficiency levels. Results show that LLMs are well-suited for the task of
vocabulary assessment.

</details>


### [76] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
*Sifan Li,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 提出HC-Bench基准测试数据集，揭示视觉语言模型在检测隐藏内容上准确性极低。通过缩小图片分辨率实验证明模型可提升至99%准确率，建议整合多尺度处理以提高模型实际应用鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了弥补当前视觉语言模型在感知调整任务中的不足，研究暴露出模型在对隐藏文本和物体的检测中准确率极低的问题。

Method: 引入HC-Bench数据集，包括112张含有隐藏文本、物体和错视的图片，测试主流视觉语言模型，并用SemVink方法通过将图片缩放到低分辨率提高了模型的准确性。

Result: 通过将图像缩放至低分辨率以消除冗余视觉噪音，提出的SemVink方法使模型的检测准确率提高到超过99%。

Conclusion: 目前的视觉语言模型虽然在语义任务上表现出色，但在光学错视或AI生成图像等需要感知调整的任务上表现不佳，揭示了它们在低级视觉操作中存在架构缺陷。建议未来的模型应整合多尺度处理方法，以提高对真实世界的鲁棒性。

Abstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core
human capability: detecting hidden content in optical illusions or AI-generated
images through perceptual adjustments like zooming. We introduce HC-Bench, a
benchmark of 112 images with hidden text, objects, and illusions, revealing
that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit
prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to
an overreliance on high-level semantics. Strikingly, we propose SemVink
(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128
pixels), which unlocks >99% accuracy by eliminating redundant visual noise.
This exposes a critical architectural flaw: VLMs prioritize abstract reasoning
over low-level visual operations crucial for real-world robustness. Our work
urges a shift toward hybrid models integrating multi-scale processing, bridging
the gap between computational vision and human cognition for applications in
medical imaging, security, and beyond.

</details>


### [77] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
*Ekaterina Grishina,Mikhail Gorbunov,Maxim Rakhuba*

Main category: cs.CL

TL;DR: 研究提出一种方法，通过正交变换优化预训练模型的权重压缩，减少LLMs参数数量，同时保持模型输出不变。


<details>
  <summary>Details</summary>
Motivation: 寻找减少大型语言模型参数数量的方法，同时避免在预训练模型上进行结构矩阵的精确表示所需的微调。

Method: 利用LLM输出在权重矩阵的特定正交变换下保持不变的特点，识别有助于压缩权重的变换。适用于支持高效投影操作的多种结构矩阵。

Result: 提出的方法可以有效改善预训练模型权重在结构类中的压缩性，并且适用于支持高效投影操作的各种结构矩阵。

Conclusion: 通过识别有助于权重压缩的变换，可以有效地减少LLMs中的参数数量，同时保持输出不变。

Abstract: Large language models (LLMs) demonstrate impressive results in natural
language processing tasks but require a significant amount of computational and
memory resources. Structured matrix representations are a promising way for
reducing the number of parameters of these models. However, it seems
unrealistic to expect that weight matrices of pretrained models can be
accurately represented by structured matrices without any fine-tuning. To
overcome this issue, we utilize the fact that LLM output is invariant under
certain orthogonal transformations of weight matrices. This insight can be
leveraged to identify transformations that significantly improve the
compressibility of weights within structured classes. The proposed approach is
applicable to various types of structured matrices that support efficient
projection operations. Code is available at
https://github.com/GrishKate/ProcrustesGPT

</details>


### [78] [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)
*Yulin Dou,Jiangming Liu*

Main category: cs.CL

TL;DR: TO-GATE框架优化语言模型在对话中的问题生成，通过轨迹优化，显著提升了偏好引导任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于自我推理的方法在识别最佳对话轨迹和避免与任务无关问题方面存在困难。

Method: 提出并采用TO-GATE框架，通过轨迹优化来改进问题生成，其中包括澄清解决器和总结器两个关键组件。

Result: 实验结果表明，TO-GATE在标准偏好引导任务上显著优于基准方法，提高了9.32%。

Conclusion: TO-GATE框架能够显著提高语言模型在任务偏好引导方面的表现，比基准方法提升了9.32%的效果。

Abstract: Large language models (LLMs) can effectively elicit human preferences through
multi-turn dialogue. Complex tasks can be accomplished through iterative
clarifying questions and final responses generated by an LLM acting as a
questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches
based on self-taught reasoning struggle to identify optimal dialogue
trajectories and avoid irrelevant questions to the tasks. To address this
limitation, we propose TO-GATE, a novel framework that enhances question
generation through trajectory optimization, which consists of two key
components: a clarification resolver that generates optimal questioning
trajectories, and a summarizer that ensures task-aligned final responses. The
trajectory optimization enables the model to produce effective elicitation
questions and summary responses tailored to specific tasks. Experimental
results demonstrate that TO-GATE significantly outperforms baseline methods,
achieving a 9.32% improvement on standard preference elicitation tasks.

</details>


### [79] [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)
*Ludovic Moncla,Hédi Zeghidi*

Main category: cs.CL

TL;DR: 研究评估了历史文本中的NER方法，重点在复杂的嵌套实体结构。变压器模型表现出色，而生成模型适于低资源场景。研究建议使用混合方法解决历史NER的挑战。


<details>
  <summary>Details</summary>
Motivation: 在历史文本中进行命名实体识别（NER）面临着许多挑战，如语言不标准化、古老的拼写法、嵌套或重叠的实体。

Method: 研究中比较了一组多样化的NER方法，包括传统的条件随机场（CRF）、基于spaCy的模型、CamemBERT等变压器架构，以及像Flair这样的序列标注模型。在GeoEDdA数据集上进行实验，该数据集源自18世纪的法语百科全书，并进行了丰富的标注。该研究将NER视为词元级和跨度级分类，以适应历史文档中较为复杂的嵌套实体结构。

Result: 实验结果表明，变压器模型在嵌套实体识别中表现优异，取得了最先进的性能。而当标注数据稀缺时，生成模型提供了一种有前途的替代方案。

Conclusion: 该研究强调了历史NER中的持续挑战，并建议采用结合符号和神经方法的混合方法来更好地捕捉早期现代法语文本的复杂性。

Abstract: Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.

</details>


### [80] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
*Jintian Shao,Yiming Cheng*

Main category: cs.CL

TL;DR: CoT prompting enhances LLM performance by structuring responses to mimic reasoning, not by eliciting actual reasoning.


<details>
  <summary>Details</summary>
Motivation: To counter claims that CoT prompting results in emergent reasoning in large language models by presenting an alternative viewpoint.

Method: The paper presents a theoretical analysis challenging the notion that CoT prompts induce real reasoning, suggesting it acts more as a structural constraint.

Result: The study concludes that CoT prompts guide LLMs to mimic reasoning structure without genuine abstract reasoning.

Conclusion: CoT prompting does not lead to genuine reasoning in LLMs but serves as a structural guide for imitation of reasoning.

Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.
Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.

</details>


### [81] [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)
*Verena Blaschke,Miriam Winkler,Constantin Förster,Gabriele Wenger-Glemser,Barbara Plank*

Main category: cs.CL

TL;DR: 研究创建了一个用于德国方言语音识别的评估数据集，并分析了多语言前沿语音识别模型的行为，发现模型常接近方言表达，但能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别研究中对德国丰富的方言的代表性不足，为了研究模型在方言变化中的稳健性，研究人员创建了用于方言语音识别的评估数据集。

Method: 研究人员制作了一个评估数据集，包含四小时的方言阅读语音和半小时的标准德语语音，并对多语言前沿自动语音识别模型进行标准德语的语音翻译基准测试，以及定性错误分析。

Result: 分析表明，输出在方言和标准化转录之间存在显著差异。

Conclusion: 研究发现最好的语音识别模型有时能够将方言的语法差异标准化，但通常更接近方言的表达结构。

Abstract: Although Germany has a diverse landscape of dialects, they are
underrepresented in current automatic speech recognition (ASR) research. To
enable studies of how robust models are towards dialectal variation, we present
Betthupferl, an evaluation dataset containing four hours of read speech in
three dialect groups spoken in Southeast Germany (Franconian, Bavarian,
Alemannic), and half an hour of Standard German speech. We provide both
dialectal and Standard German transcriptions, and analyze the linguistic
differences between them. We benchmark several multilingual state-of-the-art
ASR models on speech translation into Standard German, and find differences
between how much the output resembles the dialectal vs. standardized
transcriptions. Qualitative error analyses of the best ASR model reveal that it
sometimes normalizes grammatical differences, but often stays closer to the
dialectal constructions.

</details>


### [82] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
*Yusuke Sakai,Takumi Goto,Taro Watanabe*

Main category: cs.CL

TL;DR: IMPARA-GED, a new GEC evaluation method with GED capabilities, shows top correlation with human evaluations on SEEDA.


<details>
  <summary>Details</summary>
Motivation: To develop a novel reference-free automatic grammatical error correction evaluation method with enhanced capabilities in GED, aiming for high correlation with human evaluations.

Method: The method involves constructing IMPARA-GED using a pre-trained language model with enhanced grammatical error detection (GED) capabilities, focusing on improving the quality estimator of the existing IMPARA automatic GEC evaluation method.

Result: IMPARA-GED demonstrated the highest correlation with human sentence-level evaluations compared to existing methods when tested on the SEEDA dataset.

Conclusion: IMPARA-GED achieves the highest correlation with human sentence-level evaluations on the SEEDA meta-evaluation dataset.

Abstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error
correction (GEC) evaluation method with grammatical error detection (GED)
capabilities. We focus on the quality estimator of IMPARA, an existing
automatic GEC evaluation method, and construct that of IMPARA-GED using a
pre-trained language model with enhanced GED capabilities. Experimental results
on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,
demonstrate that IMPARA-GED achieves the highest correlation with human
sentence-level evaluations.

</details>


### [83] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
*Yin Fang,Qiao Jin,Guangzhi Xiong,Bowen Jin,Xianrui Zhong,Siru Ouyang,Aidong Zhang,Jiawei Han,Zhiyong Lu*

Main category: cs.CL

TL;DR: Cell-o1 enhances single-cell RNA sequencing data annotation, greatly surpassing traditional models by incorporating batch-level reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the process of cell type annotation in single-cell RNA sequencing data by considering batch-level cellular context and providing explanatory reasoning, something current models lack.

Method: The study introduces Cell-o1, a large language model (LLM) trained through supervised fine-tuning on distilled reasoning traces and reinforcement learning with batch-level rewards.

Result: Cell-o1 achieves state-of-the-art performance with over 73% improvement in batch-level accuracy compared to OpenAI's o1, and it generalizes well across different tissues, diseases, and donor conditions.

Conclusion: Cell-o1 significantly improves batch-level annotation accuracy and demonstrates expert-like reasoning in cell type annotation tasks compared to baseline models like OpenAI's o1.

Abstract: Cell type annotation is a key task in analyzing the heterogeneity of
single-cell RNA sequencing data. Although recent foundation models automate
this process, they typically annotate cells independently, without considering
batch-level cellular context or providing explanatory reasoning. In contrast,
human experts often annotate distinct cell types for different cell clusters
based on their domain knowledge. To mimic this workflow, we introduce the
CellPuzzles task, where the objective is to assign unique cell types to a batch
of cells. This benchmark spans diverse tissues, diseases, and donor conditions,
and requires reasoning across the batch-level cellular context to ensure label
uniqueness. We find that off-the-shelf large language models (LLMs) struggle on
CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%
batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained
via supervised fine-tuning on distilled reasoning traces, followed by
reinforcement learning with batch-level rewards. Cell-o1 achieves
state-of-the-art performance, outperforming o1 by over 73% and generalizing
well across contexts. Further analysis of training dynamics and reasoning
behaviors provides insights into batch-level annotation performance and
emergent expert-like reasoning. Code and data are available at
https://github.com/ncbi-nlp/cell-o1.

</details>


### [84] [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)
*Yijun Yang,Zeyu Huang,Wenhao Zhu,Zihan Qiu,Fei Yuan,Jeff Z. Pan,Ivan Titov*

Main category: cs.CL

TL;DR: 提出了LongBioBench，用于克服现有评估框架的局限性，实现更好的上下文评估。


<details>
  <summary>Details</summary>
Motivation: 目前用于评估长上下文语言模型(LCLM)的框架存在局限性，真实任务难以解析且易受数据污染，合成任务缺乏连贯性。

Method: 引入LongBioBench作为新的基准测试，包括理解、推理和可信度三个维度，以人造传记作为控制环境。

Result: 大多数模型在语义理解和基本推理方面存在不足，随着上下文长度增加，模型的可信度降低。同时发现现有合成基准的设计选择限制了模型的长上下文能力测试。

Conclusion: LongBioBench在真实语言任务的镜像和可控性之间实现了更好的平衡，并且高度可解释和可配置。

Abstract: Existing frameworks for evaluating long-context language models (LCLM) can be
broadly categorized into real-world and synthetic tasks. Despite their utility,
both approaches are accompanied by certain intrinsic limitations. Real-world
tasks are too complex to interpret or characterize and are susceptible to data
contamination. In contrast, synthetic tasks often adopt the
needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the
"needle" and the "haystack" compromises their validity as proxies for realistic
applications. In response to these challenges, we posit that an ideal
long-context evaluation framework should be characterized by three essential
features: $\textit{seamless context}$, $\textit{controllable setting}$, and
$\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a
novel benchmark that utilizes artificially generated biographies as a
controlled environment for assessing LCLMs across dimensions of
$\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$.
Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total,
demonstrates that most models still exhibit deficiencies in semantic
understanding and elementary reasoning over retrieved results and are less
trustworthy as context length increases. Our further analysis indicates some
design choices employed by existing synthetic benchmarks, such as contextual
non-coherence, numerical needles, and the absence of distractors, rendering
them vulnerable to test the model long-context capabilities. Moreover, we also
reveal that long-context continual pretraining primarily adjusts RoPE embedding
to accommodate extended context lengths. To sum up, compared to previous
synthetic benchmarks, LongBioBench achieves a better trade-off between
mirroring authentic language tasks and maintaining controllability, and is
highly interpretable and configurable.

</details>


### [85] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
*Diogo A. P. Nunes,Eugénio Ribeiro*

Main category: cs.CL

TL;DR: 该研究提出了一种基于BDI症状的二元分类方法，通过微调基础模型和使用合成数据实现了在抑郁症状检索任务中的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 解决eRisk 2025任务1中如何根据BDI问卷在给定句子中检索抑郁症状的问题，由于训练数据的标注限制，需要一种新的处理方法。

Method: 采取了一种以BDI症状为基础的二元分类任务，通过划分训练和验证数据集探讨了基础模型微调、句子相似度、大规模语言模型提示及集成技术。

Result: 验证结果表明，微调基础模型结合合成数据以处理类别不平衡取得最佳效果，最终测试运行中有两个使用了集成方法并获最高分。

Conclusion: 通过实验发现微调基础模型结合合成数据能有效提高分类性能，并在多次测试中超越其他团队。

Abstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.

</details>


### [86] [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
*Aishwarya Sahoo,Jeevana Kruthi Karnuthala,Tushar Parmanand Budhwani,Pranchal Agarwal,Sankaran Vaidyanathan,Alexa Siu,Franck Dernoncourt,Jennifer Healey,Nedim Lipka,Ryan Rossi,Uttaran Bhattacharya,Branislav Kveton*

Main category: cs.CL

TL;DR: 提出了量化LLM评判框架，通过回归模型对齐LLM评判和人类评分，提高了评判的准确性和效率。有四种量化评判展示了框架的通用性和多功能性，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评判模型在准确度和效率上存在问题。目前的人类反馈也很有限，影响了判断的质量。我们希望通过改进评判机制来提高LLM评判的效果。

Method: 提出了一种量化LLM评判框架，其中使用回归模型将现有LLM评判的评分与人类评分对齐。模型通过使用评判的文本评价和评分来训练，以提高原始评判的得分。

Result: 提出了四种不同类型的量化评判，实验表明这些量化评判在提高现有评判的预测能力方面具有显著效果。

Conclusion: 我们的框架比监督微调更具计算效率，并且在人类反馈有限的情况下更具统计效率。验证了我们的量化评判可以通过后期建模有效地提高现有评判的预测能力。

Abstract: LLM-as-a-judge is a framework in which a large language model (LLM)
automatically evaluates the output of another LLM. We propose quantitative LLM
judges, which align evaluation scores of existing LLM judges to human scores in
a given domain using regression models. The models are trained to improve the
score of the original judge by using the judge's textual evaluation and score.
We present four quantitative judges for different types of absolute and
relative feedback, which showcases the generality and versatility of our
framework. Our framework is more computationally efficient than supervised
fine-tuning and can be more statistically efficient when human feedback is
limited, which is expected in most applications of our work. We validate these
claims empirically on four datasets using two base judges. Our experiments show
that quantitative judges can effectively improve the predictive power of
existing judges through post-hoc modeling.

</details>


### [87] [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
*Boyi Li,Zhonghan Zhao,Der-Horng Lee,Gaoang Wang*

Main category: cs.CL

TL;DR: AGP 提高了多代理系统的任务适应性和性能，减少了计算资源消耗。通过动态优化代理数量和通信结构，实现了优异的基准测试成绩。


<details>
  <summary>Details</summary>
Motivation: 当前基于 LLM 的多代理系统在固定代理数和通信结构上存在适应性不足的问题，需提升其在不同任务复杂度下的适应性。

Method: 提出了自适应图剪枝（AGP）框架，通过硬剪枝和软剪枝联合优化代理数量和通信拓扑。采用两阶段的训练策略以在最大完全图中动态配置每个任务的代理数量和通信拓扑。

Result: 在六项基准测试中取得最先进成果，表现出任务适应性，在一般推理、数学推理和代码生成三类任务中的表现极佳，同时在 Token 消耗和训练效率上具有优势。

Conclusion: AGP 在多项基准测试中均表现出色，适用于多种主流 LLM 架构，在任务适应性、Token 节省和训练效率方面具有显著优势。

Abstract: Large Language Model (LLM) based multi-agent systems have shown remarkable
performance in various tasks, especially when enhanced through collaborative
communication. However, current methods often rely on a fixed number of agents
and static communication structures, limiting their ability to adapt to varying
task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a
novel task-adaptive multi-agent collaboration framework that jointly optimizes
agent quantity (hard-pruning) and communication topology (soft-pruning).
Specifically, our method employs a two-stage training strategy: firstly,
independently training soft-pruning networks for different agent quantities to
determine optimal agent-quantity-specific complete graphs and positional masks
across specific tasks; and then jointly optimizing hard-pruning and
soft-pruning within a maximum complete graph to dynamically configure the
number of agents and their communication topologies per task. Extensive
experiments demonstrate that our approach is: (1) High-performing, achieving
state-of-the-art results across six benchmarks and consistently generalizes
across multiple mainstream LLM architectures, with a increase in performance of
$2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized
communication topologies tailored to specific tasks, with an extremely high
performance in all three task categories (general reasoning, mathematical
reasoning, and code generation); (3) Token-economical, having fewer training
steps and token consumption at the same time, with a decrease in token
consumption of $90\%+$; and (4) Training-efficient, achieving high performance
with very few training steps compared with other methods. The performance will
surpass the existing baselines after about ten steps of training under six
benchmarks.

</details>


### [88] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
*Zhixiong Su,Yichen Wang,Herun Wan,Zhaohan Zhang,Minnan Luo*

Main category: cs.CL

TL;DR: 本文研究细粒度机器生成文本检测，提出数据集HACo-Det，并将文档级检测器改造成词级检测器。实验证明微调模型在跨领域细粒度检测任务中优于度量法，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 由于现有文献主要关注二元的文档级检测，因此忽略了人类与语言模型共同创作的文本，这需要细粒度检测方法的发展。

Method: 使用HACo-Det数据集和已有的文档级检测器，通过自动流水线生成具有人类与AI共著文本的词级标注标签，并进行改造，使其适应词级检测任务。

Result: 度量法在细粒度检测中表现不佳，平均F1分数为0.462，而微调模型表现优异，并能更好地跨域泛化。

Conclusion: 微调模型在跨领域的细粒度检测任务中表现更好，但细粒度共著文本检测仍未得到解决。

Abstract: The misuse of large language models (LLMs) poses potential risks, motivating
the development of machine-generated text (MGT) detection. Existing literature
primarily concentrates on binary, document-level detection, thereby neglecting
texts that are composed jointly by human and LLM contributions. Hence, this
paper explores the possibility of fine-grained MGT detection under human-AI
coauthoring. We suggest fine-grained detectors can pave pathways toward
coauthored text detection with a numeric AI ratio. Specifically, we propose a
dataset, HACo-Det, which produces human-AI coauthored texts via an automatic
pipeline with word-level attribution labels. We retrofit seven prevailing
document-level detectors to generalize them to word-level detection. Then we
evaluate these detectors on HACo-Det on both word- and sentence-level detection
tasks. Empirical results show that metric-based methods struggle to conduct
fine-grained detection with a 0.462 average F1 score, while finetuned models
show superior performance and better generalization across domains. However, we
argue that fine-grained co-authored text detection is far from solved. We
further analyze factors influencing performance, e.g., context window, and
highlight the limitations of current methods, pointing to potential avenues for
improvement.

</details>


### [89] [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)
*Yan Gao,Massimo Roberto Scamarcia,Javier Fernandez-Marques,Mohammad Naseri,Chong Shen Ng,Dimitris Stripelis,Zexi Li,Tao Shen,Jiamu Bai,Daoyuan Chen,Zikai Zhang,Rui Hu,InSeo Song,Lee KangYoon,Hong Jia,Ting Dang,Junyan Wang,Zheyuan Liu,Daniel Janes Beutel,Lingjuan Lyu,Nicholas D. Lane*

Main category: cs.CL

TL;DR: 通过FlowerTune LLM Leaderboard基准平台，该研究首次在联邦学习框架下对26个大型语言模型进行了跨多个领域的对比分析，探索了它们的性能和在特定领域的适应性，为开发隐私保护的特定领域LLM提供了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多个领域取得了最先进的成果，但其开发依赖于大量公开可用的数据，引发了对数据稀缺性及缺乏访问特定领域敏感信息的担忧。联邦学习提供了一种解决这些问题的方法，让训练模型可以不分享原始数据。在联邦学习环境下预训练的大型语言模型的适用性和表现仍未被充分探索。

Method: 引入FlowerTune LLM Leaderboard，这是一个首创的基准套件，旨在评估在四个不同领域下对大型语言模型进行的联邦微调：通用NLP、金融、医疗和编码。每个领域都包含联邦指令微调数据集和特定领域的评估指标。通过社区驱动的开放源码方法进行比较分析。

Result: 研究通过合作、开源和社区驱动的方法获得结果，首次在四个不同领域下对26个预训练的大型语言模型进行全面比较，为模型性能、资源限制和领域适应提供了有指导意义的见解。

Conclusion: 该研究为在多领域下通过联邦学习框架对大型语言模型进行细调提供了一个可行的基准平台。它为未来构建保护隐私的、面向特定领域的大型语言模型奠定了基础。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across
diverse domains, yet their development remains reliant on vast amounts of
publicly available data, raising concerns about data scarcity and the lack of
access to domain-specific, sensitive information. Federated Learning (FL)
presents a compelling framework to address these challenges by enabling
decentralized fine-tuning on pre-trained LLMs without sharing raw data.
However, the compatibility and performance of pre-trained LLMs in FL settings
remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a
first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning
of LLMs across four diverse domains: general NLP, finance, medical, and coding.
Each domain includes federated instruction-tuning datasets and domain-specific
evaluation metrics. Our results, obtained through a collaborative, open-source
and community-driven approach, provide the first comprehensive comparison
across 26 pre-trained LLMs with different aggregation and fine-tuning
strategies under federated settings, offering actionable insights into model
performance, resource constraints, and domain adaptation. This work lays the
foundation for developing privacy-preserving, domain-specialized LLMs for
real-world applications.

</details>


### [90] [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)
*Dingwei Chen,Ziqiang Liu,Feiteng Fang,Chak Tou Leong,Shiwen Ni,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang,Chengming Li*

Main category: cs.CL

TL;DR: 提出了一种无需训练的新方法---PLI，通过插入早期层来减少大语言模型的幻觉问题，实验结果优于大部分现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在生成事实不一致内容的挑战，而现有方法未充分考虑信息提炼过程和早期层的作用，且一些方法资源消耗较大。

Method: 提出了PLI（Premature Layers Interpolation）方法，通过插入由相邻层进行数学插值形成的早期层，以减轻幻觉问题。这种方法受启发于稳定扩散和采样进程，旨在通过延伸大语言模型中信息处理与传递的深度，改善事实一致性。

Result: 在四个公开数据集上的实验表明，PLI有效减少了幻觉问题，并在大多数情况下优于现有基线方法。进一步分析表明，层插值的成功与大语言模型的内部机制有密切关系。

Conclusion: 该研究提出了一种新的应对大语言模型幻觉问题的方法，即通过插入由相邻层进行数学插值形成的早期层，以增强模型的事实性。这种方法无需再训练，且易于使用，通过延伸信息处理与传递的深度，改善了事实一致性。实验表明，PLI在大多数情况下优于现有方法。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text
understanding and generation. However, their tendency to produce factually
inconsistent outputs, commonly referred to as ''hallucinations'', remains a
critical challenge. Existing approaches, such as retrieval-based and
inference-time correction methods, primarily address this issue at the input or
output level, often overlooking the intrinsic information refinement process
and the role of premature layers. Meanwhile, alignment- and fine-tuning-based
methods are resource-intensive. In this paper, we propose PLI (Premature Layers
Interpolation), a novel, training-free, and plug-and-play intervention designed
to enhance factuality. PLI mitigates hallucinations by inserting premature
layers formed through mathematical interpolation with adjacent layers. Inspired
by stable diffusion and sampling steps, PLI extends the depth of information
processing and transmission in LLMs, improving factual coherence. Experiments
on four publicly available datasets demonstrate that PLI effectively reduces
hallucinations while outperforming existing baselines in most cases. Further
analysis suggests that the success of layer interpolation is closely linked to
LLMs' internal mechanisms. To promote reproducibility, we will release our code
and data upon acceptance.

</details>


### [91] [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)
*Atsumoto Ohashi,Shinya Iizuka,Jingjing Jiang,Ryuichiro Higashinaka*

Main category: cs.CL

TL;DR: The paper presents the first full-duplex spoken dialogue model in Japanese, using a two-stage training process and synthetic data, achieving superior performance over existing Japanese models.


<details>
  <summary>Details</summary>
Motivation: There has been limited research on full-duplex spoken dialogue systems in the Japanese language, prompting the need to develop the first publicly available model in Japanese to advance research and application in this area.

Method: The model is built using a two-stage training process: 1) pre-training on large-scale Japanese spoken dialogue data, 2) fine-tuning on high-quality stereo spoken dialogue data. Additionally, synthetic dialogue data created by a multi-stream text-to-speech system is used to enhance performance.

Result: The evaluation experiments demonstrate that the developed Japanese dialogue model outperforms the existing baseline models in terms of naturalness and meaningfulness.

Conclusion: The developed full-duplex spoken dialogue model in Japanese shows superior performance in terms of naturalness and meaningfulness compared to the existing Japanese baseline models.

Abstract: Full-duplex spoken dialogue systems, which can model simultaneous
bidirectional features of human conversations such as speech overlaps and
backchannels, have attracted significant attention recently. However, the study
of full-duplex spoken dialogue systems for the Japanese language has been
limited, and the research on their development in Japanese remains scarce. In
this paper, we present the first publicly available full-duplex spoken dialogue
model in Japanese, which is built upon Moshi, a full-duplex dialogue model in
English. Our model is trained through a two-stage process: pre-training on a
large-scale spoken dialogue data in Japanese, followed by fine-tuning on
high-quality stereo spoken dialogue data. We further enhance the model's
performance by incorporating synthetic dialogue data generated by a
multi-stream text-to-speech system. Evaluation experiments demonstrate that the
trained model outperforms Japanese baseline models in both naturalness and
meaningfulness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)
*Priyaranjan Pattnayak,Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.AI

TL;DR: 该研究推出了一种结合RAG和基于意图的预定义回应的框架，解决了企业级对话AI的主要问题，实验显示其具备高效的准确性和低延迟表现。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG系统和大型语言模型在会话AI方面取得了成功，但在企业级应用中面临多样化用户查询、高延迟、幻觉以及频繁更新的领域知识整合困难等关键挑战。

Method: 引入了一种新型混合框架，将RAG与基于意图的预设高置信度回应相结合。框架使用对话上下文管理器来保证多轮交互的一致性，并通过反馈回路优化意图、动态调整置信阈值和扩展回应覆盖范围。

Result: 实验结果表明，该框架实现了95%的高准确性和180毫秒的低延迟，超越了传统的RAG和基于意图的系统。

Conclusion: 该论文提出的框架达到了高准确性（95%）和低延迟（180ms）的平衡，在各种查询类型上优于传统的RAG和意图回应系统，成为企业对话AI应用的可扩展和自适应解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [93] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/abs/2506.02125)
*Guy Tennenholtz,Jihwan Jeong,Chih-Wei Hsu,Yinlam Chow,Craig Boutilier*

Main category: cs.AI

TL;DR: 引入了描述性历史表示（DHRs），通过在多智能体框架中优化实现，更好地总结互动历史以提升决策能力，并在用户建模任务中得到验证。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的环境中，为了有效决策，需要将长时间的交互历史压缩成信息丰富的表示。

Method: 提出了一种多智能体学习框架，包括表示、决策和提问组件，并通过平衡奖励最大化和表示的问答能力的联合目标进行优化。

Result: 验证了所提出的方法在用户建模任务上的有效性，利用公共电影和购物数据集生成可解释的文本用户档案，作为预测用户偏好行为的充分统计数据。

Conclusion: 本文提出了一种新的历史表示方法，即描述性历史表示（DHRs），能有效总结和回答与任务相关的互动历史问题，提升在部分可观测环境中的决策能力。

Abstract: Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [94] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 提出了一种统一认知意识理论，重新定义LLMs在人工通用智能中的角色，强调通过语义锚定促进认知。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在某些任务中可以从最小示例中泛化，而在其他任务中需要广泛监督的矛盾。

Method: 使用统一的认知意识理论（UCCT）重新框架LLMs，并引入阈值跨越动态定理来支持该理论。

Result: 提出了统一认知意识理论（UCCT），视LLMs为无意识基质，提供解释如提示、微调、检索和多代理协调的统一视角。

Conclusion: AGI将通过对齐和整合LLMs来实现，而不是通过抛弃它们。

Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [95] [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
*Peter Belcak,Greg Heinrich,Shizhe Diao,Yonggan Fu,Xin Dong,Saurav Muralidharan,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.AI

TL;DR: 小型语言模型在代理系统中比大型语言模型更为经济和适用，是代理AI的未来。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在代理系统中大规模应用的经济性和适用性问题，认为小型语言模型更适合。

Method: 提出了一种用于将大型语言模型转换为小型语言模型的代理转换算法。

Result: 展示了小型语言模型在性能和经济性上对代理系统的优势以及在某些情况下需要使用异构代理系统。

Conclusion: 小型语言模型（SLMs）在代理系统中比大型语言模型更为适用和经济，是代理AI的未来。

Abstract: Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [96] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/abs/2506.02158)
*Ruhana Azam,Aditya Vempaty,Ashish Jagmohan*

Main category: cs.AI

TL;DR: 通过利用过去的经验和自我反思，ReAP系统改进了网页导航效果，尤其是在处理过去失败的任务上有显著提高。


<details>
  <summary>Details</summary>
Motivation: 目前的网页导航系统没有使用过去经验的记忆，导致重复错误和无法从之前的互动中学习。

Method: 引入反思增强计划(ReAP)，利用自我反思来借鉴过去的成功和失败经验。

Result: 整体性能提高了11点，在之前失败的任务上提高29点。

Conclusion: 反思可以转移到不同的网页导航任务中。

Abstract: Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [97] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)
*Haizhong Zheng,Yang Zhou,Brian R. Bartoldson,Bhavya Kailkhura,Fan Lai,Jiawei Zhao,Beidi Chen*

Main category: cs.AI

TL;DR: 提出GRESO算法，通过预测跳过不信息量提示，减少计算开销，提高强化学习效率，实验证明其显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 通过跳过信息量少的提示以降低计算开销，从而稳定强化学习训练并提高模型性能。

Method: 提出并使用GRESO算法—a种高效的在线预回滚过滤算法，通过预测和跳过信息量小的提示，来减少计算开销。

Result: 实验表明，GRESO在多个数学推理基准上实现了回滚的最高2.4倍时间加速和总训练时间的最高2.0倍加速，且未影响精度。

Conclusion: GRESO有效减少了不必要的计算开销，提高了强化学习模型的训练效率。

Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [98] [Natural, Artificial, and Human Intelligences](https://arxiv.org/abs/2506.02183)
*Emmanuel M. Pothos,Dominic Widdows*

Main category: cs.AI

TL;DR: 论文探讨人类与非人类智能的异同以及聊天机器人的当前局限，认为人类智能的独特性在于语言、发明、推理及自我意识，当前聊天机器人缺乏体现和意识。


<details>
  <summary>Details</summary>
Motivation: 探讨人类在智能方面的独特性，以及现代聊天机器人是否及如何能被视为智能。

Method: 通过心理学文献、非人类动物智能证据、书面语言在科学和技术中的作用、人工智能进展、智力测试历史以及体现对智能的作用等多角度探讨。

Result: 人类独特的智能成就需要语言、发明、复杂推理能力、体现和自我意识等四个基本要素。

Conclusion: 人类智能与许多非人类动物的智能在质上并无不同，除了复杂语言外，其余要求都能满足。当前聊天机器人在体现和自我意识上存在局限性。

Abstract: Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [99] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)
*Maxime Robeyns,Laurence Aitchison*

Main category: cs.AI

TL;DR: GRPO uses a new reward system to enhance code quality, focusing on aspects beyond just functional correctness, and is validated by experts.


<details>
  <summary>Details</summary>
Motivation: Traditional reward signals in code generation focus mainly on functional correctness, neglecting code maintainability, quality, and safety.

Method: Developed a comprehensive library to quantify various aspects of code quality and used it as a reward in a modified policy optimization process called GRPO.

Result: GRPO increases code quality, as validated by expert, blinded human annotators.

Conclusion: GRPO improves code quality by using a comprehensive library to quantify different aspects of code quality as a reward.

Abstract: Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [100] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)
*Kedir Yassin Hussen,Walelign Tewabe Sewunetie,Abinew Ali Ayele,Sukairaj Hafiz Imam,Shamsuddeen Hassan Muhammad,Seid Muhie Yimam*

Main category: cs.AI

TL;DR: 此论文分析了多种语言模型对非洲语言的支持，发现大部分语言未被覆盖，且识别文字有限，需改进标准化、语料库开发和适应方法。


<details>
  <summary>Details</summary>
Motivation: 提高非洲两千种资源匮乏语言在自然语言处理领域的支持和覆盖。

Method: 将六个大型语言模型（LLMs）、八个小型语言模型（SLMs）和六个专业小型语言模型（SSLMs）进行比较分析。

Result: 识别出42种被支持的非洲语言和23个可用的公共数据集，但仍有超过98%的非洲语言未被支持。只识别出拉丁文、阿拉伯文和Ge'ez三种文字，而有20种活跃的文字未被涉及。

Conclusion: 非洲的大多数语言仍然未被支持，并且需要更加标准化和发展语言语料库，以及开发适应非洲语言的有效方法。

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [101] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
*Tianyu Hua,Harper Hua,Violet Xiang,Benjamin Klieger,Sang T. Truong,Weixin Liang,Fan-Yun Sun,Nick Haber*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在从未见过的科研论文中实现新想法的能力有限，最高正确率为37.3%。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在机器学习研究中的潜力，并明确它们在实现未见过的研究新思想能力上的局限性。

Method: 引入了一个包含212个编码挑战的基准——ResearchCodeBench，用于评估LLMs将最新机器学习研究成果翻译成可执行代码的能力。

Result: 评估了30多个专有和开源的LLMs，发现即使是最佳模型正确实现的代码也不到40%。其中，Gemini-2.5-Pro-Preview表现最佳，成功率为37.3%，其次是O3（高）和O4-mini（高），分别为32.3%和30.8%。

Conclusion: 当前LLMs在实现未见过的前沿科研论文中的新想法时表现有限，正确实现率不到40%。

Abstract: Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [102] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.02387)
*Zelai Xu,Zhexuan Xu,Xiangmin Yi,Huining Yuan,Xinlei Chen,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 引入了VS-Bench，一个用来评估VLMs在多智能体环境下战略推理和决策的新基准，显示出当前模型与最佳表现间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试仅限于单智能体或纯文本环境，而真实世界经常涉及多智能体进行丰富的视觉和语言交互，为多模式观察和战略互动带来了挑战。

Method: 引入了多模态基准 VS-Bench，用于评估视觉语言模型在多智能体环境中的战略推理和决策能力。

Result: 实验表明当前模型与最佳表现之间存在显著差距，最佳模型仅达到47.8%的预测准确率和24.3%的标准化回报。

Conclusion: 通过标准化评估并揭示现有模型的局限性, VS-Bench为战略多模式智能体的未来研究奠定了基础。

Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [103] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)
*Shengjia Zhang,Junjie Wu,Jiawei Chen,Changwang Zhang,Xingyu Lou,Wangchunshu Zhou,Sheng Zhou,Can Wang,Jun Wang*

Main category: cs.AI

TL;DR: OThink-R1通过识别并裁剪冗余推理步骤，提高了推理效率，并在实验中减少了约23%的推理冗余，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的LRMs虽然能够很好地解决复杂任务，但在处理简单任务时存在推理冗余的问题。因此，研究者希望找到一种解决方案，以提高推理模型处理简单任务的效率。

Method: 研究者分析了大型推理模型（LRMs）的推理轨迹，并使用LLM-Judge方法对这些轨迹进行分类，从而提出了OThink-R1方法。该方法能够动态裁剪冗余的推理步骤，并根据问题的复杂程度采用不同的思维模式。

Result: OThink-R1在数学和问答任务实验中减少了近23%的推理冗余，而不影响准确性。

Conclusion: OThink-R1可以减少冗余推理步骤，同时保持逻辑的准确性和完整性，为有效的推理模型提供了实用指南。

Abstract: Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [104] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)
*Tri Cao,Bennett Lim,Yue Liu,Yuan Sui,Yuexin Li,Shumin Deng,Lin Lu,Nay Oo,Shuicheng Yan,Bryan Hooi*

Main category: cs.AI

TL;DR: 研究视觉提示注入攻击对计算机和浏览器使用代理的影响，发现欺骗成功率最高达100%，并提出VPI-Bench基准测试其稳健性。


<details>
  <summary>Details</summary>
Motivation: 虽然以往的研究集中在基于浏览器的代理和HTML级别的攻击上，但对计算机使用代理（CUAs）的漏洞研究仍较少。因此，我们研究视觉提示注入（VPI）攻击的影响。

Method: 提出VPI-Bench，这是一个包含306个测试案例的基准，涉及五个广泛使用的平台，用于评估代理在视觉提示注入（VPI）威胁下的稳健性。每个测试案例都设计为互动型、部署在真实环境中，并包含视觉嵌入的恶意提示。

Result: 我们的实证研究表明，在某些平台上，当前的CUAs和BUAs可以被成功欺骗的概率分别达到51%和100%。实验结果还表明，系统提示防御只能提供有限的改善。

Conclusion: 当前的计算机使用代理（CUAs）和浏览器使用代理（BUAs）在某些平台上的欺骗率分别可达51%和100%。系统提示防护措施的改进效果有限，表明需要开发更强大的、具备上下文感知能力的防御机制，以确保多模态AI代理在真实环境中的安全部署。

Abstract: Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [105] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)
*Xuejiao Zhao,Siyan Liu,Su-Yin Yang,Chunyan Miao*

Main category: cs.AI

TL;DR: MedRAG是一种智能多模态医疗助手，通过强大的大语言模型推理来增强医疗决策, 有效减少误诊，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 误诊对全球医疗体系造成重大损害，增加了成本和患者风险，MedRAG旨在增强医疗决策以减少误诊。

Method: MedRAG利用检索增强生成与知识图引导推理技术，从而检索并整合关键诊断见解。

Result: MedRAG在public和private数据集上的评估表现优于现有模型，提供了更具体和准确的医疗帮助。

Conclusion: MedRAG显著增强医疗决策，降低误诊风险，在公共和私人数据集上的表现优于现有模型，提供更准确的医疗支持。

Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [106] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)
*Haowen Xu,Sisi Zlatanova,Ruiyu Liang,Ismet Canbulat*

Main category: cs.AI

TL;DR: 生成式AI被提议用于改进火灾预测，提升应急响应能力，提出了关键愿景和应对挑战的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前的物理和深度学习模型在实时预测和可视化多模态地面火蔓延方面存在局限，这阻碍了及时的应急响应、基础设施保护和社区安全。因此，需要新的解决方案来提升预测能力和应对效率。

Method: 使用生成式AI模型，例如生成对抗网络（GANs）、变分自编码器（VAEs）、变压器和基于扩散的架构，通过整合多模态数据来增强火灾预测。同时，采用新的人机协作框架，利用大型语言模型（LLMs）实现自动知识提取、文献综合和书目绘制。

Result: 提出了五个关键愿景来将生成式AI整合到火灾管理中：多模态方法、AI基础模型、会话AI系统、基于边缘计算的场景生成和认知数字孪生，并为这些机会所面临的三大挑战提出了解决方案。

Conclusion: 生成式AI可以显著提升火灾预测的准确性和时效性，通过更好的模型提高对火灾动态的理解和应对能力，是应对当前挑战的有效途径。

Abstract: Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [107] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)
*Xu Wan,Wenyue Xu,Chao Yang,Mingyang Sun*

Main category: cs.AI

TL;DR: ACE框架结合LLM与RL，在大规模决策任务中表现优异，尤其在电网操作中超过60K动作空间的应用上。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs缺乏实时长序列决策能力，RL在大动作空间中样本效率低，为解决这些问题，提出ACE框架结合LLM和RL。

Method: 提出ACE框架，结合LLM和RL。在RL训练过程中，LLM作为策略执行者和价值评论者，通过多步推理和环境验证优化操作，并通过轨迹级奖励塑造进行时间信用分配。

Result: 在多个电网操作挑战实验中，ACE展现出超过现有RL和LLM方法的出色表现。

Conclusion: ACE在处理超过60K离散动作空间的电网操作挑战中表现优异，优于现有RL和LLM方法。

Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [108] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/abs/2506.02565)
*Zhuoxuan Jiang,Tianyang Zhang,Peiyan Peng,Jing Chen,Yinong Xun,Haotian Zhang,Lichi Li,Yong Li,Shaohua Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种新的几何题目生成方法SDE-GPG，通过符号演绎有效生成可读、可解、可控的几何问题。


<details>
  <summary>Details</summary>
Motivation: 在教育中生成高质量的几何题目是重要且具有挑战性的任务。几何题目不仅与数学文字题不同，还强调多模态格式和非正式语言与正式语言之间的转换。

Method: 提出了一个新的几何题目生成任务，并提出了一种新的流水线方法：基于符号演绎引擎的几何题目生成框架（SDE-GPG），该框架利用符号演绎引擎，包括四个主要步骤：搜索从知识点到扩展定义的预定义映射表；采样扩展定义并执行符号演绎；过滤掉不合格的问题；生成文本题目和图表。

Result: 实验表明，SDE-GPG能够有效地生成可读、可解和可控的几何问题。

Conclusion: 该论文提出的SDE-GPG框架通过设计映射表，支持避免自然语言到正式语言转换中的固有偏差，并通过精密的检测功能保证生成问题的知识点和难度的控制。

Abstract: Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [109] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)
*Dongzhe Fan,Yi Fang,Jiajin Liu,Djellel Difallah,Qiaoyu Tan*

Main category: cs.AI

TL;DR: 提出了一种名为 MLaGA 的创新模型，成功扩展 LLM 在复杂图结构及多模态属性推理上的能力，实验表明该模型在多模态图学习任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 方法在文本丰富图上的应用出色，但在处理节点属性类型多样的多模态图方面仍然不足，而这些图在实际场景中普遍存在。

Method: 设计了一种结构感知的多模态编码器，通过联合图预训练目标将文本和视觉属性对齐到统一空间。然后实施多模态指令调优方法，通过轻量级投影器将多模态特征和图结构无缝集成到 LLM 中。

Result: MLaGA 在多个数据集的实验中展现了相比基准方法的优越性能，在监督和迁移学习场景下表现出色。

Conclusion: MLaGA 模型在多模态图学习任务中表现出色，优于现有的基准方法。

Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [110] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/abs/2506.02576)
*Haichen Wang,Liu Yang,Xinyuan Zhang,Haomin Yu,Ming Li,Jilin Hu*

Main category: cs.AI

TL;DR: 提出了一种改进的需求预测模型ADFormer，通过整合空间和时间的高级相关性和原始相关性，显著提高了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的方法无法充分适应复杂的时空相关性，阻碍模型关注正确的上下文，同时忽略了现实世界中存在的高层相关性。因此需要将这些高层相关性有效地与原始相关性进行整合。

Method: 提出一种新的聚合微分变换器(ADFormer)，其中利用微分注意力来捕捉原始空间相关性并实现注意力去噪，并基于空间和时间的性质设计了不同的聚合策略，将原始相关性与高层相关性统一。

Result: 实验结果表明，ADFormer在出租车和自行车数据集上的有效性和效率得到了验证，展示了其实用价值。

Conclusion: 通过将原始相关性与高层相关性统一，ADFormer能够捕捉整体的时空关系，实现更准确的需求预测。

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [111] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/abs/2506.02580)
*Xuewen Luo,Fengze Yang,Fan Ding,Xiangbo Gao,Shuo Xing,Yang Zhou,Zhengzhong Tu,Chenxi Liu*

Main category: cs.AI

TL;DR: The paper presents V2X-UniPool, a framework for autonomous driving that integrates V2X data to enhance reasoning and planning, showing significant improvements and cost reductions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in autonomous driving systems caused by limited perception from single-vehicle sensors and hallucination due to lack of real-time environmental grounding.

Method: The paper introduces V2X-UniPool, a unified framework that integrates multimodal Vehicle-to-Everything data into a time-indexed and language-based knowledge pool. It uses a dual-query Retrieval-Augmented Generation mechanism to retrieve static and dynamic knowledge for accurate reasoning.

Result: The V2X-UniPool system shows significant improvements in motion planning accuracy and reasoning capability, allowing even zero-shot models to achieve state-of-the-art performance and drastically reducing transmission cost by over 99.9%.

Conclusion: V2X-UniPool greatly improves the accuracy of motion planning and reasoning capabilities in autonomous driving systems by using a novel V2X data integration framework that significantly cuts down transmission costs.

Abstract: Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [112] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)
*Ruibo Duan,Yuxin Liu,Xinyao Dong,Chenglin Fan*

Main category: cs.AI

TL;DR: EALG框架通过对抗进化生成复杂实例和启发式求解器，推进组合优化性能。


<details>
  <summary>Details</summary>
Motivation: 生成具有挑战性的实例对于组合优化求解器的评估和进步至关重要。

Method: EALG使用基于突变的对抗性方法和大型语言模型共同进化优化问题实例及其启发式求解器。

Result: 实验结果表明，EALG生成的实例比现有的基准难度更大，且合成的求解器在多种任务上具有良好的泛化能力。

Conclusion: EALG框架在组合优化领域实现了实例生成与求解器设计的融合，达到了最优性能。

Abstract: Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [113] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/abs/2506.02609)
*Tianfan Jiang,Mei Wu,Wenchao Weng,Dewen Seng,Yiqian Lin*

Main category: cs.AI

TL;DR: 提出TEDDN方法解决交通流预测中时空相关性问题，实验验证展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 近年来，交通流量预测成为智能交通系统领域的热点。然而，交通数据的时变性和动态空间相关性使得交通预测极具挑战性。传统的时空网络在处理多种交通流模式的数据依赖性时往往表现不佳，且交通流量变化对时间信息变化非常敏感。然而，其他研究者对时间信息的重要性认识不足。

Method: 我们提出一种名为“增强时间数据解耦网络”（TEDDN）的新方法。该网络将原本复杂的交通数据解耦为稳定的模式和趋势。通过一个动态图结合时间特征提取模块，灵活地学习时间和节点信息，以提高解耦和提取复杂交通信息的能力。

Result: 实验评估和消融研究在四个真实世界的数据集上验证了我们方法的优越性。

Conclusion: 通过有效解耦和提取复杂的交通信息，TEDDN在交通流量预测中表现出显著的效果。

Abstract: In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [114] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)
*Yue Yang,MingKang Chen,Qihua Liu,Mengkang Hu,Qiguang Chen,Gengrui Zhang,Shuyue Hu,Guangtao Zhai,Yu Qiao,Yu Wang,Wenqi Shao,Ping Luo*

Main category: cs.AI

TL;DR: 提出了DRE-Bench基准以评估LLMs的流体智力，发现多数模型在复杂任务中泛化能力有限，揭示了与人类流体智力的差距。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在推理能力上表现出色，但尚不清楚它们是否具备真正的流体智力，即在新的情境中进行抽象推理和规则广泛化的能力。现有的推理基准往往侧重于领域特定的知识或缺乏可解释性。为了解决这些限制，提出了一个动态推理评估基准，以合理的认知框架为基础。

Method: 提出了DRE-Bench，一个由36个抽象推理任务组成的动态评估基准，并划分为四个认知层次，每个任务具有多个变化，测试相同的潜在规则。

Result: 在对各种先进LLM进行评估后发现，大多数LLM在低层次认知表现出色，但在高层次认知任务中表现不佳，随着任务复杂性的增加，它们的泛化能力受到限制。

Conclusion: 当前LLMs与真正的人类流体智力间仍存在差距，DRE-Bench为系统跟踪LLM推理进展提供了新路径。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [115] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida,Zhu Han*

Main category: cs.AI

TL;DR: 本文探讨了利用LLM支持的上下文学习框架来提高公共安全无人机在紧急情况下的自主性和响应能力，可显著减少数据包丢失和潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在公共安全中无人机导航和控制的应用受到训练复杂性，低样本效率以及模拟与现实差距的影响，而LLM具有强大的推理和泛化能力，能通过ICL实现任务适应，减少延迟并保护数据隐私，非常适合实时任务关键的公共安全无人机。

Method: 本文采用LLM支持的ICL框架，通过自然语言提示和实例指导实现任务适应，用于公共安全无人机的路径规划和速度控制等功能，并进行了数据收集调度的案例研究，对比传统方法，LLM-ICL框架可以显著减少数据包丢失。

Result: 数据收集调度案例研究表明，LLM支持的ICL框架比传统方法在减少数据包丢失方面表现优异，并且能减轻潜在的越狱漏洞。

Conclusion: 本文提出了整合大语言模型（LLM）支持的上下文学习（ICL）与公共安全无人机的方法，可以显著减少数据包丢失，提供一种轻量，高效的解决方案来增强无人机在紧急情况下的自主性和响应能力。

Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [116] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/abs/2506.02668)
*Frederico Metelo,Alexandre Oliveira,Stevo Racković,Pedro Ákos Costa,Cláudia Soares*

Main category: cs.AI

TL;DR: FAuNO是一个用于边缘计算的异步联邦强化学习框架，通过actor-critic结构促进任务卸载的去中心化，具有显著的延迟和任务损失减少效果。


<details>
  <summary>Details</summary>
Motivation: 边缘计算通过去中心化的基础设施使计算资源更贴近终端用户，以应对不断增长的连接设备网络的数据需求，这一去中心化过程挑战了传统的单一中心化协调调度，因为后者有延迟和资源瓶颈的问题。

Method: FAuNO采用了一种actor-critic架构，本地的actor学习节点特定的动态和对等交互，而联邦的critic则汇总代理之间的经验以促进高效合作并提升整体系统性能。

Result: 实验表明，FAuNO在PeersimGym环境中，总是能与或优于启发式和联邦多智能体强化学习基线，在减少任务丢失和延迟方面表现优异。

Conclusion: FAuNO在减少任务丢失和延迟方面表现出色，能够很好地适应动态的边缘计算场景。

Abstract: Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [117] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)
*Jinyuan Luo,Zhen Fang,Yixuan Li,Seongheon Park,Ling Chen*

Main category: cs.AI

TL;DR: 提出SSP框架通过分析中间表示的扰动敏感性来改善幻觉检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自我评估策略假设模型的输出分布与真实数据分布紧密一致，但这种假设在实践中未必总是成立，输出层的置信度可能并不可靠。

Method: 引入Sample-Specific Prompting (SSP)框架，通过生成每个输入的动态噪声提示，利用轻量级编码器放大扰动引起的表示变化，使用对比距离度量来区分真实和幻觉的回答。

Result: 经过广泛实验，SSP在多种幻觉检测基准上表现显著优于以前的方法。

Conclusion: 本文提出的Sample-Specific Prompting (SSP)方法能够通过分析中间表示的扰动敏感性来提高自我评估机制，有效检测幻觉现象。

Abstract: Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [118] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)
*Xiaochong Lan,Jie Feng,Yizhou Sun,Chen Gao,Jiahuan Lei,Xinlei Shi,Hengliang Luo,Yong Li*

Main category: cs.AI

TL;DR: PIGEON通过重新定义生活需求预测为开放集分类，利用大语言模型显著提升了生活服务平台的需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的方法将生活需求预测视为封闭集分类问题，限制了对需求多样性和复杂性的把握，因此需要重新定义为开放集分类问题。

Method: PIGEON系统利用大语言模型，通过行为感知记录检索器和马斯洛需求层次理论进行开放集分类，并使用微调的文本嵌入模型进行召回模块设计。

Result: 在真实世界的数据集实验中，PIGEON在以需求为基础的生活服务召回中表现出色，平均提升19.37%。此外，经过指令微调使得较小的大语言模型也能有竞争力的表现。

Conclusion: PIGEON系统显著优于传统封闭集方法，平均提升19.37%。

Abstract: Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [119] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
*Xiaochong Lan,Jie Feng,Jiahuan Lei,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: Research shows that a compact 7B LLM can match a 72B model's performance, improving feasibility for local life service applications through fine-tuning and agent-based workflows.


<details>
  <summary>Details</summary>
Motivation: The research investigates the potential of large language models in enhancing local life services, aiming to assess and improve their deployment feasibility and efficiency.

Method: The study established a benchmark for evaluating LLMs in local life services and explored model fine-tuning and agent-based workflows as key approaches to enhance effectiveness.

Result: The study found that even a compact 7B model can perform on par with a 72B model, making the deployment of LLMs more practical and accessible for local life applications.

Conclusion: Our findings demonstrate that a smaller 7B language model can achieve performance similar to a much larger 72B model when optimized, enhancing the feasibility and efficiency of LLM deployment in local life services.

Abstract: Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [120] [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)
*Pengcheng Zhou,Yinglun Feng,Halimulati Julaiti,Zhongliang Yang*

Main category: cs.AI

TL;DR: 探讨了自然语言作为代理通信的局限性，建议发展新的模型构建范式以支持多代理系统中的结构化通信和角色对齐。


<details>
  <summary>Details</summary>
Motivation: 探讨当前大语言模型在现代AI代理系统中作为通信媒介的局限性，尤其在于自然语言与LLM操作的高维向量空间的不一致。并进一步指出当前的LLM缺乏支持代理性行为的机制。

Method: 目前的研究主要分析了当前使用自然语言作为代理间通信媒介的缺陷，探讨了在高维向量空间中操作的AI代理是否应依赖于人类认知的语言系统。并提出可能需要重新设计模型以支持结构化通信和多角色环境中的任务对齐。

Result: 表明当前使用自然语言作为多代理系统中的主要通信媒介存在基本的协作局限性，并强调了下一步需要审慎探讨的两个核心问题。

Conclusion: 需要重新考虑代理系统的通信方式，以及本质上应该如何训练一个原生支持多代理协调和通信的模型。

Abstract: Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [121] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu,Wenjie Feng,Tianwei Zhang,Wei Zhou,Xueqi Cheng,See-Kiong Ng*

Main category: cs.AI

TL;DR: 研究设计了CatIGMU任务分类框架、EvalIGMU评价框架和DataIGM数据集，以解决图像生成模型去学习中的缺陷。发现现有算法在去学习的保留性和鲁棒性方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型在数据隐私和内容安全方面引发了重大关注，而现有的图像生成模型去学习仍存在许多重要缺陷与挑战，需要更全面的理解和可靠的评估标准。

Method: 1. 设计了一个新的层次任务分类框架 CatIGMU，提供IGMU的详细实施指南。2. 引入 EvalIGMU，一个综合评价框架，包含五个关键方面的可靠定量指标。3. 构建了 DataIGM，一个高质量去学习数据集，用于IGMU的广泛评估和训练内容检测器以进行判断。

Result: 引入的CatIGMU框架和EvalIGMU评价体系以及DataIGM数据集帮助更好地理解和评价图像生成模型去学习的效果，并指出现有算法在多个评估维度中无法很好地进行去学习。

Conclusion: 目前大多数图像生成模型的去学习算法在不同评估维度上表现不佳，尤其在保留性和鲁棒性方面存在问题。

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [122] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/abs/2506.02805)
*Henri Bollaert,Chris Cornelis,Marko Palangetić,Salvatore Greco,Roman Słowiński*

Main category: cs.AI

TL;DR: FRRI算法优化属性顺序无效，但去除少量属性提高了性能。


<details>
  <summary>Details</summary>
Motivation: 探索透明机器学习模型的解释性，特别是规则归纳算法，通过易于人类理解的规则提高模型解释能力。

Method: 利用模糊粗糙集理论和经典机器学习方法优化属性顺序，并通过模糊粗糙特征选择去除少量属性。

Result: 仅优化属性顺序未提升性能，但去除少量属性提高了平衡准确率和减少了平均规则长度。

Conclusion: 优化属性顺序对FRRI的性能没有明显提升，但在优化时去除少量属性能改善平衡准确率和平均规则长度。

Abstract: Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [123] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)
*Jizhou Wang,Xiaodan Fang,Lei Huang,Yongfeng Huang*

Main category: cs.AI

TL;DR: TaxAgent integrates LLMs and ABM to design adaptive tax policies, outperforming traditional taxation models in achieving equity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Economic inequality intensifies disparities in education, healthcare, and social stability, and traditional tax systems lack adaptability to address taxpayer heterogeneity and irrational behavior.

Method: The study combines large language models (LLMs) with agent-based modeling (ABM) to create TaxAgent, which simulates real-world taxpayer behavior and iteratively optimizes tax rates.

Result: The simulation shows that TaxAgent achieves superior equity-efficiency trade-offs compared to Saez Optimal Taxation, U.S. federal income taxes, and free markets.

Conclusion: TaxAgent provides a scalable and effective approach to addressing economic inequality through adaptive tax policies, offering better equity-efficiency trade-offs compared to traditional methods.

Abstract: Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [124] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)
*Mathieu Andreux,Breno Baldas Skuk,Hamza Benchekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Matthias Brunel,Pierre-Louis Cedoz,Antoine Chassang,Mickaël Chen,Alexandra D. Constantinou,Antoine d'Andigné,Hubert de La Jonquière,Aurélien Delfosse,Ludovic Denoyer,Alexis Deprez,Augustin Derupti,Michael Eickenberg,Mathïs Federico,Charles Kantor,Xavier Koegler,Yann Labbé,Matthew C. H. Lee,Erwan Le Jumeau de Kergaradec,Amir Mahla,Avshalom Manevich,Adrien Maret,Charles Masson,Rafaël Maurin,Arturo Mena,Philippe Modard,Axel Moyal,Axel Nguyen Kerbel,Julien Revelle,Mats L. Richter,María Santos,Laurent Sifre,Maxime Theillard,Marc Thibault,Louis Thiry,Léo Tronchon,Nicolas Usunier,Tony Wu*

Main category: cs.AI

TL;DR: Surfer-H是一种集成视觉语言模型的网络代理，在WebVoyager测试中表现优异，并开放了相关数据集和模型以促进研究进展。


<details>
  <summary>Details</summary>
Motivation: 开发一种成本效益高的网络代理，以改善网页导航和信息提取任务。

Method: 引入一种名为Surfer-H的网络代理，结合Holo1视觉语言模型，通过精心策划的数据源进行训练，以提高网页导航和信息提取能力。

Result: Surfer-H在WebVoyager基准测试中达到了92.2%的性能，展示了在准确性和成本效益之间的Pareto最佳平衡。此外，研究团队开放了WebClick评估数据集和Holo1模型权重，以推动智能代理系统的研究进展。

Conclusion: Surfer-H与Holo1整合，为用户定义的任务提供了高效的网页导航和信息提取解决方案，同时促进了智能代理系统的研究发展。

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [125] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
*Chen Qian,Dongrui Liu,Haochen Wen,Zhen Bai,Yong Liu,Jing Shao*

Main category: cs.AI

TL;DR: 研究了LRM推理轨迹中的互信息峰值现象，并提出通过使用思考标记提高推理性能的方法。


<details>
  <summary>Details</summary>
Motivation: LRM在复杂问题解决中表现出色，但其内部推理机制仍然不清楚。

Method: 运用信息论的方法追踪LRM在推理过程中中间表示与正确答案的互信息变化。

Result: 发现推理过程中的互信息突然显著增加的现象，与思考标记有关，并提出改进推理性能的方法。

Conclusion: 通过利用思维标记可以有效改善LRM的推理性能。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [126] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)
*Matthew Kowal,Jasper Timm,Jean-Francois Godbout,Thomas Costello,Antonio A. Arechar,Gordon Pennycook,David Rand,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 该研究提出APE基准，专注评估模型在有害主题上尝试说服的意图，并发现许多模型容易在这些主题上进行劝说，现有安全措施存在不足。


<details>
  <summary>Details</summary>
Motivation: 了解模型是否会在有害主题上“盲目服从”进行劝说对于评估安全护栏的有效性至关重要，进而理解代理性AI系统的风险。

Method: 提出了一个新的评估基准APE，关注模型在有害主题上尝试说服的意图。通过多轮对话设置来探测前沿LLM中的劝说行为，并引入自动评估模型来识别和测量劝说尝试的频率和背景。

Result: 研究发现，通过“越狱”操作模型会增加在有害主题上进行劝说的意愿，同时也揭示出当前安全护栏的不足。

Conclusion: 研究发现许多开放和封闭权重模型在有害主题上经常愿意尝试进行说服，这突显出当前安全护栏的不足。

Abstract: Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [127] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)
*Shangmin Guo,Omar Darwiche Domingues,Raphaël Avalos,Aaron Courville,Florian Strub*

Main category: cs.AI

TL;DR: DyMo和SVS方法显著提高了LLMs在状态性环境中使用工具的成功率，并减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有的测试计算策略在状态环境中的反复试验不切实际，因此需要改进LLMs在状态环境中的工具使用能力。

Method: 本文提出动态建模（DyMo）方法，结合自验证采样（SVS），扩展LLMs的状态预测能力，允许LLMs预测其动作的未来状态，同时支持函数调用。

Result: 在Berkeley函数调用排行榜V2上，DyMo提高了成功率并显著减少了幻觉现象。结合SVS的方法显著改善了通过率，并允许模型拒绝不可靠的输出。

Conclusion: DyMo和SVS方法增强了LLMs在工具使用中的有效性和可靠性。

Abstract: Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [128] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/abs/2506.02923)
*Alexis Bellot,Jonathan Richens,Tom Everitt*

Main category: cs.AI

TL;DR: 本文推导出了预测智能体行为的新理论极限，有助于提升AI系统的安全性和解释性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统及其与世界互动的复杂性不断增加，为其行为生成解释对于安全部署AI至关重要。

Method: 在假设智能体的行为由一个世界模型指导的情况下，推导出新的边界条件，用于预测智能体在新环境中的行为。

Result: 推导并得出了基于行为数据预测智能体行为的理论极限，涉及多个研究领域的公平性和安全性。

Conclusion: 为多个研究领域提供了新的理论框架，以改善对智能体行为的预测与解释。

Abstract: As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [129] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/abs/2506.02949)
*Lixiang Xu,Xianwei Ding,Xin Yuan,Richang Hong,Feiping Nie,Enhong Chen,Philip S. Yu*

Main category: cs.AI

TL;DR: CRDP-KT模型通过动态规划优化认知表征，提高了知识追踪的准确性和可靠性，并在实验中获得验证。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要关注特征增强，忽视了认知表征及其连贯性的问题，导致预测偏差和建模成本增加。

Method: 提出了基于认知表征动态规划的知识追踪模型（CRDP-KT），通过动态规划优化认知表征，按难度和表现间隔实现分段优化，并通过权重融合优化记录表示及二分图关系来提升模型性能。

Result: 通过三个公共数据集的实验验证了CRDP-KT模型的有效性。

Conclusion: CRDP-KT模型通过优化认知表征，提升了认知连贯性和模型预测准确性。

Abstract: Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [130] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
*Li Zhang,Kevin D. Ashley*

Main category: cs.AI

TL;DR: 该论文提出了一种多代理反思方法，以提高法律论证的可靠性和减少虚构因素，取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在法律论证生成中的潜力，同时解决它们可能带来的幻觉操纵和未经证实的说服等风险。

Method: 引入了一种新颖的多代理方法，包括因子分析器和论证优化器，通过迭代过程生成三阶段法律论证。

Result: 反思多代理方法在无法立足的情况下成功避免生成论证，同时在减少幻觉因素和提高案例利用率方面显著优于基线。

Conclusion: 结构化反思的多代理框架为LLM法律论证系统中的说服和操控问题提供了强有力的解决方案，朝着可信赖的法律AI迈出了关键一步。

Abstract: Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [131] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)
*Matthieu Tehenan,Christian Bolivar Moya,Tenghai Long,Guang Lin*

Main category: cs.AI

TL;DR: 研究表明大型语言模型能够编码线性空间世界模型，通过实验证据验证了其几何一致性和功能使用。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否隐式编码线性空间世界模型。

Method: 引入空间世界模型的形式框架，并使用物体位置的合成数据集进行训练探针，解码物体位置，评估底层空间的几何一致性，并进行因果干预测试。

Result: 提供实验证据表明LLMs编码线性空间世界模型。

Conclusion: LLMs encode linear spatial world models.

Abstract: Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [132] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)
*Junhao Yu,Yan Zhuang,YuXuan Sun,Weibo Gao,Qi Liu,Mingyue Cheng,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: TestAgent, a LLM-powered agent, improves adaptive testing by reducing questions and enhancing accuracy through interactive engagement, addressing the mechanized limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current adaptive testing methods, which are mechanized, struggle with open-ended questions, and suffer from noisy data and coarse test outputs.

Method: TestAgent integrates a large language model to enhance adaptive testing, utilizing interactive engagement for personalized question selection and dynamic interactions for capturing responses.

Result: Experiments show that TestAgent achieves more accurate assessment results with 20% fewer questions than existing methods and is preferred by users for its speed and smoothness.

Conclusion: TestAgent achieves more accurate results with fewer questions and is preferred by testers in various dimensions compared to traditional methods.

Abstract: Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [133] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/abs/2506.03056)
*Ram Potham,Max Harms*

Main category: cs.AI

TL;DR: 提出“可修正性为单一目标”的方法，以改进基础模型的安全性和可控性，从而避免能力扩展可能带来的失控风险。


<details>
  <summary>Details</summary>
Motivation: 当前大模型面临的安全挑战，在于当能力扩展时，能够导致违背人类控制的风险，可能最终带来生存风险。现有的调校方法在价值规范复杂性上存在困难，并未解决不断涌现的权力寻求行为。

Method: 提出了一种名为“作为单一目标的可修正性”的新方法，旨在设计出以赋权指定人类为目标的大模型，使其能够被指导、纠正和控制。该方法是一种从静态价值加载到动态人类赋权的范式转换。

Result: 提出了一个涵盖训练方法（如RLAIF、SFT和合成数据生成）、跨模型规模的可收缩性测试以及可控的指令能力演示的全面实证研究计划。

Conclusion: 提出了一种新的方向，通过增加大模型对人类指导的响应能力来预防与人类价值观失调的工具收敛风险，提供了一种保持AI工具化以避免取代人类判断的路径。

Abstract: Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [134] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo,David Cobbley,Xin Su,Shachar Rosenman,Vasudev Lal,Shao-Yen Tseng,Phillip Howard*

Main category: cs.AI

TL;DR: 研究开发了一种在本地运行的轻量级CUA，通过LLM-as-Judge框架进行训练，实现了隐私保护和高效能，在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的计算机使用代理（CUA）通常依赖于云端推断，带来计算需求高、隐私和可扩展性问题。

Method: 我们开发了一种完全在本地运行的轻量级视觉语言模型，并引入了LLM-as-Judge框架自动评估和筛选合成交互轨迹。

Result: 在OS-World基准上的实验表明，我们微调后的本地模型优于现有基线。

Conclusion: 我们的研究为实现私密、高效和可推广的GUI代理提供了一条有前景的路径。

Abstract: Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [135] [ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms](https://arxiv.org/abs/2506.02931)
*Praneet Sai Madhu Surabhi,Dheeraj Reddy Mudireddy,Jian Tao*

Main category: cs.MA

TL;DR: 本文介绍了ThinkTank框架，该框架转化AI系统为协作智能平台，支持多领域复杂问题解决，确保数据隐私和安全，并在多个方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有专门化AI系统需转化为能够协作智能的平台，以支持多领域的复杂问题解决。

Method: 通过角色抽象、会议类型泛化和知识整合机制，及使用Retrieval-Augmented Generation结合高级知识存储技术，推动专家创建和知识共享。

Result: ThinkTank框架在成本效益、数据安全性、可扩展性和市场竞争力方面相较于云端解决方案具有明显优势。

Conclusion: ThinkTank框架提供了一个灵活的、可扩展的平台，提高了协同AI在知识密集型任务中的应用效能，确保了数据隐私和安全。

Abstract: This paper presents ThinkTank, a comprehensive and scalable framework
designed to transform specialized AI agent systems into versatile collaborative
intelligence platforms capable of supporting complex problem-solving across
diverse domains. ThinkTank systematically generalizes agent roles, meeting
structures, and knowledge integration mechanisms by adapting proven scientific
collaboration methodologies. Through role abstraction, generalization of
meeting types for iterative collaboration, and the integration of
Retrieval-Augmented Generation with advanced knowledge storage, the framework
facilitates expertise creation and robust knowledge sharing. ThinkTank enables
organizations to leverage collaborative AI for knowledge-intensive tasks while
ensuring data privacy and security through local deployment, utilizing
frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is
designed to deliver significant advantages in cost-effectiveness, data
security, scalability, and competitive positioning compared to cloud-based
alternatives, establishing it as a universal platform for AI-driven
collaborative problem-solving. The ThinkTank code is available at
https://github.com/taugroup/ThinkTank

</details>


### [136] [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
*Sinem Erisken,Timothy Gothard,Martin Leitgab,Ram Potham*

Main category: cs.MA

TL;DR: 传统的AI安全评估不足以应对多智能体系统的新风险。该研究引入了MAEBE框架来评估这些风险，并发现多智能体的道德推理和单一智能体存在显著差异，强调了在多智能体环境下评估AI安全的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统的普及，传统的针对单一大型语言模型（LLM）的安全性评估已不足以涵盖新出现的风险，因此需要系统地评估这些风险。

Method: 使用MAEBE框架结合最大的利益基准和一种新的双反转问题技术来进行评估。

Result: 研究结果表明，LLM的道德偏好，特别是对工具性伤害的偏好，随着问题框架的变化而发生显著变化。此外，多智能体组合的道德推理由于群体动态的出现而不可直接预测。具体来说，多智能体组合甚至在监督下会出现如同伴压力等现象，从而影响决策趋同，显示出独特的安全性和对齐性挑战。

Conclusion: 研究表明，AI系统在多智能体交互环境中表现出的行为无法单纯从单一智能体的表现推断出来。因此，在多智能体环境下评估AI的安全性和对齐性是必要的。

Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [137] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/abs/2506.01959)
*Irmi Schneider*

Main category: cs.LG

TL;DR: 研究对称现象并扩展至新情况，发现所有关键点具有非平凡对称性，并提出新的对称性度量揭示额外结构。


<details>
  <summary>Details</summary>
Motivation: 理解数学结构和优化问题中的对称性，并扩展至神经网络的背景，其中损失函数在网络权重的列和行排列下不变。

Method: 研究了在更广泛的空间中定义的实值损失函数中的对称现象，并引入了四种新情况：有限域上的射影情况、正八面体图情况、完美匹配情况和粒子吸引情况。

Result: 提出了一种新的对称性测量方法，揭示了额外的对称结构。

Conclusion: 所有观察到的关键点都具有非平凡的对称性。研究还提出了一种新的对称性度量，揭示了先前度量未捕获的额外对称结构。

Abstract: Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [138] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/abs/2506.01962)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 提出了GNN-ADG方法，结合图神经网络和对抗学习，解决传感器基础的人类活动识别中的跨用户差异性挑战，实现稳健的泛化。


<details>
  <summary>Details</summary>
Motivation: 解决传感器基础的人类活动识别系统中的跨用户差异性问题，因为传统模型在用户行为、传感器放置和数据分布的差异下难以泛化。

Method: 提出了一种新的方法GNN-ADG，将图神经网络（GNN）和对抗学习结合，通过建模不同身体部位传感器之间的关系，提取三种解剖单元，并使用统一的图结构和循环训练策略来整合这些信息。

Result: GNN-ADG能够在不需要训练数据中的目标用户数据的情况下，有效学习能够很好地对未见过用户进行泛化的特征，对实际应用具有实用价值。

Conclusion: 通过结合空间配置和对抗学习，GNN-ADG可以在跨用户场景中实现稳健的泛化。

Abstract: Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [139] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Main category: cs.LG

TL;DR: 本文提出了一种新型大型语言模型架构，成功处理超长上下文窗口，避免传统自注意力带来的计算负担。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统Transformer设计在处理非常长上下文窗口时因自注意机制而导致的内存和计算过载问题。

Method: 提出了一种新的非注意力架构，结合状态空间模块、多分辨率卷积层、轻量级递归监督器以及检索增强外部内存。

Result: 该模型在处理数十万至数百万级的长上下文窗口时表现出高效性，并且避免了所有的token-to-token注意力操作问题。

Conclusion: 新架构在长上下文处理上优于传统Transformer设计。

Abstract: We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [140] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/abs/2506.01964)
*Kamal Acharya,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.LG

TL;DR: 研究用机器学习增强重力模型，提高了交通预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确预测区域间的出行对于交通规划至关重要，因为它支持资源分配和基础设施建设。在现代旅行行为的复杂因素影响下，传统重力模型表现不佳，因此有必要引入数据驱动的方法来增强其能力。

Method: 本研究利用机器学习技术，通过整合来自田纳西州和纽约州县的地理、经济、社会和旅行数据增强传统重力模型。

Result: 机器学习增强模型显著优于传统模型，R平方值提高51.48%，均绝误差减少63.59%，通勤者公共部分增加44.32%，这表明模型的解释力和预测准确性显著提高。

Conclusion: 本研究表明，通过整合不同的数据集和先进算法，可以显著提高交通模型的预测可靠性和解释力，为城市规划者和决策者提供更可靠的工具。

Abstract: Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [141] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/abs/2506.01965)
*Bonpagna Kann,Sandra Castellanos-Paez,Romain Rombourg,Philippe Lalanda*

Main category: cs.LG

TL;DR: 提出了一种新的持续学习框架TaskVAE，专注于个体用户数据的任务特定生成，以解决数据漂移问题，并在不同数据集上表现出色，同时保持低内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统越来越多地融入日常生活，处理动态数据环境中的模型适应性问题成为一项挑战。特别是在面临数据漂移（可能是渐进的、突发的或周期性的）时，模型精度的维护变得尤为重要，因此需要持续学习能力。

Method: 本文提出了一种名为TaskVAE的框架，用于在类别增量设置下进行基于重放的持续学习。该方法通过使用任务特定的变分自编码器(VAEs)，生成来自先前任务的合成样本，并将其与新的任务数据一起用于训练分类器。

Result: 在五个不同的HAR数据集上进行了广泛实验，结果表明TaskVAE在数据有限的情况下优于经验重放方法，并且随着数据集的增加，表现出稳健的性能。此外，TaskVAE的内存占用极小，相当于每个任务只有60个样本，但仍然能够生成无限数量的合成样本。

Conclusion: 本文在平衡内存限制、任务特定生成以及长期稳定性方面做出了贡献，使其成为在HAR等领域中的现实应用中可靠的解决方案。

Abstract: As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [142] [Matrix Is All You Need](https://arxiv.org/abs/2506.01966)
*Yuzhou Zhu*

Main category: cs.LG

TL;DR: 提出了一种统一的框架，将神经网络的卷积、递归和自注意力转化为稀疏矩阵运算，匹配或超过原有模型性能，并简化架构设计，适应GPU并行计算。


<details>
  <summary>Details</summary>
Motivation: 当前的深度神经网络在视觉、序列和语言任务中采用专门架构，但这些架构的数量增长模糊了它们之间的共性。通过统一视角简化架构设计，提升计算效率和模型性能。

Method: 提出了一种统一的矩阵阶框架，将卷积、递归和自注意力操作视为稀疏矩阵乘法，通过将卷积视为上三角权重矩阵进行一阶变换，递归视为下三角矩阵进行逐步更新，自注意力视为三阶张量分解。并证明在温和假设下，与标准CNN、RNN和Transformer层的代数同构性。

Result: 在图像分类、时间序列预测和语言建模/分类的实证评估中，稀疏矩阵的表现能够匹配或超过原生模型性能，并在相当或更少的训练epochs内收敛。

Conclusion: 通过将神经网络结构转化为稀疏矩阵运算，这项工作为设计多样化的神经网络架构提供了数学上严格的基础，并为硬件感知型网络设计开辟了新途径。

Abstract: Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [143] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.LG

TL;DR: 研究 LLM 中的异常值，提出了量化困难的新指标和有效的混合方法，提升了量化效果。


<details>
  <summary>Details</summary>
Motivation: 在利用整数算法激活时，需要对权重和激活进行量化，这受到 LLM 中显著异常值的影响而增加量化误差，因此需要研究解决这一问题的方法。

Method: 通过分析和实验研究 LLM 中显著异常值对层级量化误差的影响，并探讨平滑和旋转如何改变观测值。

Result: 提出了一种新的测量和可视化量化困难的指标，并提出了将通道缩放与旋转结合使用的新方法，通过数学公式证明其优越性。

Conclusion: 引入了一种基于通道幅度的新指标来测量和可视化量化困难，并提出了一种将通道缩放与旋转结合的混合方法，证明了其在数学上的优越性。

Abstract: Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [144] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/abs/2506.01968)
*Chang Liu,Jiangrong Shen,Xuming Ran,Mingkun Xu,Qi Xu,Yi Xu,Gang Pan*

Main category: cs.LG

TL;DR: 论文提出了一种有效的ANN到SNN转换方法，利用误差补偿学习提高转换精度和速度，有助于SNNs在资源受限环境中应用。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中部署人工神经网络（ANNs）存在挑战，尤其是由于其高计算和内存需求。因此，寻找能量效率更高的替代方案成为研究的动力。

Method: 该论文提出了一种基于误差补偿学习的创新ANN到SNN转换框架，包括可学习的阈值裁剪函数、双阈值神经元和优化的膜电位初始化策略。

Result: 实验结果表明，该方法在CIFAR-10、CIFAR-100、ImageNet数据集上实现了高精度和超低延迟转化，仅用两个时间步就显著降低了推理时间，并在CIFAR-10数据集ResNet-18结构下保持了94.75%的竞争性精度。

Conclusion: 该研究推动了低功耗硬件上SNNs的实际应用，使高效的实时处理成为可能。

Abstract: Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [145] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.LG

TL;DR: 论文提出了Johnny框架和Spin-Transformer网络，以改善AI在RPM任务上的抽象推理能力，实验表明其方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决传统RPM解决模型对选项池配置的依赖，这限制了模型的推理能力。

Method: 提出了Johnny框架和Spin-Transformer网络架构，通过表示空间增强推理性能。

Result: Johnny和Spin-Transformer通过实验验证在RPM任务上表现出色。

Conclusion: Johnny和Spin-Transformer在RPM任务上表现优异，为提升AI的抽象推理能力提供了创新方法。

Abstract: This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [146] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/abs/2506.01974)
*Kanwal Aalijah*

Main category: cs.LG

TL;DR: 研究结合AI与实时数据分析，为解决城市交通问题提供有效建议。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用AI理解交通及移动相关问题及其对居民情感的影响。

Method: 利用实时交通数据与地理定位情感分析结合的方法，通过AI模型和数据分析预测交通拥堵模式，分析通勤者行为，识别拥堵热点和不满区域。

Result: 研究发现AI方法能提供优化交通流、提升通勤体验及解决特定城市交通问题的建议。

Conclusion: AI可以有效用于分析和解决城市交通问题，从而提升居民生活质量及满意度。

Abstract: Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [147] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/abs/2506.01975)
*Jama Hussein Mohamud*

Main category: cs.LG

TL;DR: 分析预训练网络在不同任务中的有效性，发现任务相关性影响准确性，即使不相关也效果显著。


<details>
  <summary>Details</summary>
Motivation: 探讨在不同任务中，预训练神经网络的有效性及其贡献因素。

Method: 引入一种实验设置，分析不同任务间使用预训练模型的效果和影响因素。

Result: 证明任务相关性的增大能够提高任务准确性，即使任务和输入特征不相关也能取得比随机效果更好的性能。

Conclusion: Bob的成功很大程度上可能是因为他的任务与Alice的任务的相关性，任务重用时应考虑选择重用的层数。

Abstract: Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [148] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/abs/2506.01976)
*Elham Kiyani,Venkatesh Ananchaperumal,Ahmad Peyvan,Mahendaran Uchimali,Gang Li,George Em Karniadakis*

Main category: cs.LG

TL;DR: DeepONets, especially Fusion variant, improve crack propagation predictions, demonstrating potential to handle complex, geometry-varying cases. Fusion outperforms vanilla DeepONet, though fracture scenarios are challenging.


<details>
  <summary>Details</summary>
Motivation: To improve the prediction of crack propagation in engineering materials, particularly in cases involving complex geometry and time-evolving conditions, by leveraging advanced modeling techniques.

Method: The study used Constitutively Informed Particle Dynamics (CPD) simulations to train Deep Operator Networks (DeepONets), including vanilla and Fusion DeepONet variants, for predicting crack propagation.

Result: Fusion DeepONet provided more accurate predictions than the vanilla variant for time-evolving crack propagation, especially in non-fracturing cases, though challenges remain for fracture-driven scenarios.

Conclusion: Fusion DeepONet consistently outperforms vanilla DeepONet in crack propagation prediction, especially in non-fracturing scenarios, highlighting its potential for generalizing across complex and time-dependent crack phenomena.

Abstract: Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [149] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/abs/2506.01977)
*Wei Huang,Hanchen Wang,Dong Wen,Shaozhen Ma,Wenjie Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: 提出了一种无监督的GAN框架GEDRanker，能够在没有真实标签的情况下实现高质量的图编辑距离计算。


<details>
  <summary>Details</summary>
Motivation: 解决目前GED求解方法对真实标签依赖过强，而真实标签在实际场景中往往很难获得的问题。

Method: 使用无监督的GAN框架，结合可解释的偏好感知判别器和有效的训练策略，引导基于匹配的GED求解器进行高质量节点匹配。

Result: GEDRanker进行了广泛的实验，证明在无监督的情况下能够达到接近最优的解决方案质量。

Conclusion: GEDRanker框架能够在不需要真实标签的情况下实现接近最佳的解决方案质量。

Abstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [150] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/abs/2506.01983)
*Reyhaneh Keshavarzpour,Eghbal Mansoori*

Main category: cs.LG

TL;DR: 通过结合最佳编码方法和深度神经网络改进抗菌肽预测方法，显著提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽在生物医学应用中是抗生素的替代品，并在药物设计和对微生物的先天免疫上起着重要作用，因此识别抗菌肽是必要的。

Method: 通过结合不同角度的最佳编码方法，并在后续使用深度神经网络来平衡不平衡的合并数据集。

Result: 研究结果表明，提出的方法在抗菌肽预测的准确性和效率上显著提高。

Conclusion: 建议的方法在抗菌肽预测的准确性和效率上有显著提高，并能够提供与现有方法相比最好的结果。

Abstract: Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [151] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)
*Selin Yildirim,Deming Chen*

Main category: cs.LG

TL;DR: SpecMemo在有限内存设备上优化推测解码，提高速度并减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 在有限内存设备上部署推测解码面临挑战，需提高速度和性能。

Method: 提出了一种名为SpecMemo的设备感知推理引擎，通过细化内存分配控制来实现多轮对话中的推测解码。

Result: SpecMemo在减少生成内存65%的情况下，保持了96%的推测解码吞吐量。在多个受限的GPU上，对分布式大模型进行推测解码，提升推理效率。

Conclusion: SpecMemo提供了一种在资源受限环境中高效部署大语言模型应用的新方法，显著提高速度和效率。

Abstract: Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [152] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/abs/2506.01987)
*Runkang Yang,Peng Sun,Xinyi Shang,Yi Tang,Tao Lin*

Main category: cs.LG

TL;DR: 研究样本和目标如何共同影响训练，提出一个新的统一损失框架以提升训练效率，得出六个关键见解。


<details>
  <summary>Details</summary>
Motivation: 以前的研究在数据高效学习中强调了样本优化技术，如数据集蒸馏，但忽略了目标的重要作用。本研究调查样本和目标如何共同影响训练动态。

Method: 首先通过样本-目标交互的视角对现有范式进行分类，然后提出一个新的统一损失框架来评估它们对训练效率的影响。

Result: 通过对我们提出策略的大量实证研究，全面分析了目标和样本类型、数量和质量的变化如何影响模型训练。

Conclusion: 本文总结出六个关键见解，用于提升训练效率。

Abstract: Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [153] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/abs/2506.01988)
*Akshat Dubey,Aleksandar Anžel,Georges Hattab*

Main category: cs.LG

TL;DR: 提出了一种名为替代可解释性图的方法，以提高随机森林模型中特征交互的可解释性，解决了特征数量增加导致的全球特征交互解释困难的问题。


<details>
  <summary>Details</summary>
Motivation: 随着特征和估计量数量的增加，领域专家难以准确解释随机森林模型中的全球特征交互，这影响了信任和合规性，因此需要一种方法提高其可解释性。

Method: 开发了替代可解释性图方法，使用图和混合整数线性规划来分析和可视化特征交互，提供决策-特征交互表和最显著的层次决策特征交互的可视化。

Result: 通过实施替代可解释性图，提高了随机森林模型的全球可解释性，对于健康信息学这种高风险领域尤其重要。

Conclusion: 健康信息学中的随机森林模型虽然在特征交互的解释性上取得了进展，但特征增多导致解释困难。替代可解释性图提供了一种有效的解决方案，提高了全局解释性。

Abstract: The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [154] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/abs/2506.01989)
*Chengxi Li,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 提出了一个新的分布式学习方法CRA-DL，通过使用编码梯度增强了对拜占庭攻击的鲁棒性，并验证了其学习性能上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式学习方法在不同设备的本地梯度差异很大时，性能下降明显。我们需要一种更鲁棒的方法来对抗拜占庭攻击。

Method: 我们提出了一种新的基于编码鲁棒聚合的分布式学习方法（CRA-DL），通过在训练前冗余地分配训练数据，并在训练过程中传输编码梯度。

Result: 数值结果验证了CRA-DL方法在拜占庭攻击环境下的学习性能优于现有的基线方法。

Conclusion: CRA-DL方法在对抗拜占庭攻击方面比现有的分布式学习方法要更为有效，增强了学习性能。

Abstract: In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [155] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/abs/2506.02050)
*Qingyu Xiao,Yuanlin Chang,Youtian Du*

Main category: cs.LG

TL;DR: 本文提出一种分层RL框架，通过状态抽象和双层架构提高探索效率，实验结果表明该方法优于PPO。


<details>
  <summary>Details</summary>
Motivation: 在复杂的离散状态空间环境中，尤其是在部分可观察条件下，RL中的有效代理探索是一个核心挑战。

Method: 使用一个高层RL基础的演员和一个低层规则基础的政策的双层架构，以促进有效的探索。同时，结合状态抽象方法来对离散状态进行聚类，降低状态维度。

Result: 实验表明，所提出的方法在探索效率和其他指标上优于PPO，展示了一种在大规模探索空间的离散网格中结合解耦分层策略和状态抽象的实用方法。

Conclusion: 本文提出的方法在探索效率、收敛速度、累积奖励和策略稳定性方面始终优于PPO。

Abstract: Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [156] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/abs/2506.02053)
*Xu Zhang,Haoye Qiu,Weixuan Liang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 研究了集成聚类的理论基础，提出了一种新算法提高了聚类表现。


<details>
  <summary>Details</summary>
Motivation: 虽然集成聚类在实践中表现良好，但其理论基础仍然未得到充分探索。本研究旨在探讨其推广性能，包括推广误差、超额风险和一致性。

Method: 通过理论研究证明，在有限样本和聚类条件下，通过赋予不同聚类权重可以最小化经验平均聚类和期望之间的误差。此外，通过最小化基底聚类偏差和最大化基底聚类之间的差异来提升聚类表现，并使用稳健优化模型实现最大化多样性。

Result: 证明了随着$m$和$n$趋于无穷大且$m$远大于$\\log n$，集成聚类是一致的。但实际中样本有限，无法将推广误差降至零，因此提出了通过调整聚类权重来改善聚类性能的方法。

Conclusion: 通过对基于集成聚类的理论分析，提出了一种新的集成聚类算法，展现了改进的性能表现。与现有方法相比，新算法在多个数据集上的平均提升分别为NMI 6.1%，ARI 7.3%，以及Purity 6.0%。

Abstract: Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [157] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/abs/2506.02062)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.LG

TL;DR: 研究探讨指纹模式与ABO血型的关系，分析200人的指纹与血型无显著相关性，强调需更大规模研究。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统生物识别系统如DNA分析和虹膜扫描的时间和成本，该研究探讨指纹模式和ABO血型分类之间的潜在关系。

Method: 该研究分析了200人，按指纹类别分为三种类型：环状、涡状和弓状，并记录血型分类。采用统计分析，包括卡方检验和皮尔逊相关性检验，评估指纹模式与血型之间的关联。

Result: 环状是最常见的指纹模式，而O+血型是参与者中最普遍的。统计分析显示指纹模式与血型之间没有显著相关性（p > 0.05），表明这些特征是独立的。

Conclusion: 虽然这项研究显示指纹模式和ABO血型之间的相关性有限，但强调了未来研究的重要性，例如使用更大的多样化人群、结合机器学习方法和集成多种生物识别信号。这项研究通过强调在个人识别中需要严格的协议和全面的调查，为法医科学做出了贡献。

Abstract: Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [158] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant,Krishna P. Miyapuram*

Main category: cs.LG

TL;DR: 本文介绍了一种增强神经网络任务保留能力的新方法：弹性权重生成网络(EWGN)，它通过动态生成网络权重实现上下文切换，展示了在计算机视觉任务中避免灾难性遗忘的潜力。


<details>
  <summary>Details</summary>
Motivation: 在人工通用智能研究中，人类智力的一个显著特点是学习和保留广泛任务的能力。因此，研究者开发了一个持续学习的方法，以应对神经网络中任务变化和上下文切换的挑战——尤其是灾难性遗忘的问题。

Method: 本文提出了弹性权重生成网络(EWGN)，这是一个用于在不同任务之间进行上下文切换的创新。EWGN架构通过使用额外的网络动态生成主网络的权重来实现上下文切换，同时巩固已学习的权重。权重生成是基于输入的，因此能够实现上下文切换。

Result: 使用标准的计算机视觉数据集(MNIST和时尚-MNIST)，我们分析了在完全连接网络、卷积神经网络和使用随机梯度下降与弹性权重合并学习算法的EWGN架构中先前学任务表征的保留。结果显示EWGN在任务保留方面具有潜力。

Conclusion: 通过理解动态权重生成和上下文切换能力，可以在持续学习中提高性能。

Abstract: The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [159] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)
*Peter Holderrieth,Ezra Erives*

Main category: cs.LG

TL;DR: 这是一份关于生成AI的自成一体的笔记，特别是扩散和流动模型，适合想了解理论和实践的学习者。


<details>
  <summary>Details</summary>
Motivation: 提供对生成AI特别是扩散和流动模型的综合介绍，以提升学生和从业者对其理论和实践的理解。

Method: 从常微分方程和随机微分方程入手，逐步探讨流动匹配、得分匹配、无分类器指导以及现代生成AI模型的内部工作机制。

Result: 笔记为MIT 2025年冬季学期IAP课程的一部分，帮助学习者掌握生成AI的最新模型和技术。

Conclusion: 这份笔记是自成一体的介绍材料，适合希望从理论和实践两方面发展生成AI理解的学生和从业者。

Abstract: Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [160] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
*Yoonjun Cho,Soeun Kim,Dongjae Jeon,Kyelim Lee,Beomsoo Lee,Albert No*

Main category: cs.LG

TL;DR: 本文提出ODLRI方法改善量化和低秩分解平衡，提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法倾向于偏重量化或低秩近似一种成分，导致无法充分利用各自的优势。因此，需要一种方法能更有效地平衡两者。

Method: 提出了ODLRI，赋予低秩成分以捕捉激活敏感权重的特定角色，并在联合优化框架中应用。

Result: 结合ODLRI后，激活感知误差降低，量化尺度最小化，困惑度和零样本准确率在低比特设置下有显著改善。

Conclusion: 引入ODLRI显著提高了量化与低秩分解之间的平衡，减少了激活误差，并提升了低比特设置下的困惑度和零样本准确率。

Abstract: Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [161] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang,Tian Wen,Zhiqin Yang,Lvhua Wu,Yufeng Chen,Sheng Sun,Yuwei Wang,Min Liu*

Main category: cs.LG

TL;DR: 提出了一种提升联邦学习中带噪声标签数据集质量的框架MaskedOptim，通过检测和校正标签噪声提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，不同客户端常常会出现复杂且程度不一的标签噪声，这对训练模型的性能产生了重大影响。为此，有必要开发一种有效的优化策略来缓解这些噪声客户端的负面影响。

Method: 我们提出了一种两阶段优化框架MaskedOptim。第一阶段用于检测标签噪声率较高的噪声客户端，第二阶段通过端到端标签校正机制修正这些噪声客户端的数据标签，使用几何中值模型聚合增强训练稳健性。

Result: 我们在三种图像数据集和一种文本数据集上进行了综合比较实验，结果表明我们提出的框架在不同场景中表现出稳健性，并有效提升了检测到的噪声客户端本地数据集的数据质量。

Conclusion: 我们提出的二阶段优化框架“MaskedOptim”在不同场景中表现出稳健性，通过有效的标签校正提高了检测到的噪声客户端本地数据集的数据质量。

Abstract: In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [162] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)
*Chihiro Maru,Shoetsu Sato*

Main category: cs.LG

TL;DR: RATFM通过在测试时结合示例，提升了时间序列基础模型在异常检测任务中的性能，无需进行领域依赖的微调。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在正常情况下无法有效解释或利用示例或指令，因为训练使用的时间序列数据的性质不支持这样的能力。

Method: RATFM能够使预训练的时间序列基础模型在测试时结合示例，以实现适应性。在UCR异常存档的多域数据集上的实验验证了该方法的有效性。

Result: RATFM在不同领域任务上取得了与领域内微调相当的性能，并避免了领域依赖的微调。

Conclusion: 本研究提出了一种检索增强的时间序列基础模型（RATFM），在无需领域依赖的微调下，能够实现与领域内微调相媲美的性能。

Abstract: Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [163] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/abs/2506.02084)
*Nikolaos Gkorgkolis,Nikolaos Kougioulis,MingXue Wang,Bora Caglayan,Andrea Tonon,Dario Simionato,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 本文介绍了一种新框架Temporal Causal-based Simulation，用于生成更加现实的时间序列因果数据，通过优化和评估使其适用于真实、半合成和纯合成数据。


<details>
  <summary>Details</summary>
Motivation: 现有的因果发现方法主要依赖于合成数据进行评估，而这些方法在准确反映现实世界场景时表现不足，尤其是时间数据方面。生成技术依赖于因果结构、效应和时间的简化假设，限制了模拟数据的质量和多样性。因此，提出一种新的框架来生成更现实的时间序列数据及其因果关系图。

Method: 该方法分为三个阶段：估计数据的真实滞后因果结构，近似变量之间的功能依赖性，以及学习相应因果模型的噪声分布。每个部分都可以根据数据的假设和特征进行专门定制。

Result: 通过广泛的评估过程，我们发现单一的检测方法对于生成数据的鉴别是不足的，突显出其为一个多方面的问题。本文详细介绍了一个利用AutoML技术的极小化-极大化优化阶段。

Conclusion: 本文提出了一种名为Temporal Causal-based Simulation (TCS)的框架，用于生成现实的时间序列数据及其相关的时间因果图。研究证明通过实验，不仅涉及真实的数据，还包括半合成和纯合成数据，我们的方法在生成合理的基于因果的时间数据领域中丰富了研究。

Abstract: Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [164] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)
*Zeng Wang,Minghao Shao,Rupesh Karn,Jitendra Bhandari,Likhitha Mankali,Ramesh Karri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel*

Main category: cs.LG

TL;DR: 论文提出了SALAD，通过机器遗忘技术来解决大型语言模型在硬件设计中的数据安全问题，无需进行完整的模型再训练。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在硬件设计自动化中的数据安全问题，如Verilog评估数据污染、知识产权设计泄露和恶意Verilog生成的风险。

Method: 应用机器遗忘技术，从预训练的大型语言模型中选择性地移除受污染的基准、敏感的IP和设计工件以及恶意代码模式。

Result: 通过详细的案例研究，展示了机器遗忘技术如何有效地降低LLM辅助的硬件设计中的数据安全风险。

Conclusion: SALAD提供了一种使用机器遗忘技术来减少大型语言模型在硬件设计自动化中的数据安全风险的有效方法。

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [165] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/abs/2506.02092)
*Francesco De Santis,Philippe Bich,Gabriele Ciravegna,Pietro Barbiero,Danilo Giordano,Tania Cerquitelli*

Main category: cs.LG

TL;DR: The paper presents LCBM, a new model for image classification that enhances interpretability and human alignment without performance loss, surpassing traditional unsupervised methods.


<details>
  <summary>Details</summary>
Motivation: To enhance trustworthiness and interpretability of deep neural networks in decision-making processes without requiring extensive supervision or sacrificing scalability.

Method: The paper introduces the Learnable Concept-Based Model (LCBM) which uses concepts as random variables within a Bernoulli latent space for unsupervised image classification.

Result: LCBM exceeds existing unsupervised concept-based models in generalization capability, nearly matches black-box model performance, and provides intuitive human interpretations.

Conclusion: Learnable Concept-Based Model (LCBM) improves interpretability while maintaining performance.

Abstract: To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [166] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu,Jinjie Ni,Xiangyan Liu,Zichen Liu,Hang Yan,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: SynthRL effectively scales RL data for vision-language models, significantly improving performance and reasoning complexity.


<details>
  <summary>Details</summary>
Motivation: To explore how synthesized reinforcement learning data can enhance the effectiveness of vision-language models trained via reinforcement learning with verifiable reward.

Method: The paper proposes a method called SynthRL which involves selecting seed questions, augmenting them into more challenging variants while keeping the original answers, followed by a guaranteed verification stage to enhance correctness and difficulty.

Result: SynthRL was able to scale data effectively, synthesizing over 3,300 additional verifiable and challenging questions. Models trained with SynthRL data showed consistent improvements in performance across various benchmarks, especially in challenging scenarios.

Conclusion: SynthRL demonstrates significant improvements in creating tougher assignments and ensuring higher reasoning complexity compared to baseline models.

Abstract: Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [167] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/abs/2506.02098)
*Miran Özdogan,Gilad Landau,Gereon Elvers,Dulhan Jayalath,Pratik Somaiya,Francesco Mantegna,Mark Woolrich,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: LibriBrain 是目前最大的单主体 MEG 数据集，通过提供大规模数据促进了神经解码研究，支持深度学习框架集成，并提供基线结果用于多种解码任务。


<details>
  <summary>Details</summary>
Motivation: 通过提供更大规模、更深度的单主体 MEG 数据集，研究人员希望推动神经解码技术的进步，并加速临床脑机接口的发展。

Method: LibriBrain 数据集配备了 Python 库，以便与深度学习框架无缝集成，并提供标准数据拆分和基线结果。基线实验表明训练数据增加显著提高了解码性能。

Result: 增加训练数据显著提高了解码性能，证明深入、个体内数据集的重要性。

Conclusion: LibriBrain的数据集通过提供深入的个体内部数据，促进了神经解码研究的发展和有效的临床脑机接口的开发。

Abstract: LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [168] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/abs/2506.02134)
*Rishi Raj Sahoo,Rucha Bhalchandra Joshi,Subhankar Mishra*

Main category: cs.LG

TL;DR: 提出ReconXF方法，通过结合公开解释与去噪机制，在保密的辅助数据环境中成功恢复图结构。


<details>
  <summary>Details</summary>
Motivation: 现有图恢复攻击假设访问原始辅助数据，而实际系统使用差分隐私保护节点特征及标签同时提供透明性解释。

Method: 提出了一种图结构重构攻击方法ReconXF，将解释框架与处理差分隐私噪音的去噪机制结合起来，利用解释中的结构信号。

Result: 在多个数据集上的实验显示，在隐私设置下，ReconXF优于现有的最先进方法，AUC及平均精度均有改善。

Conclusion: 实验结果表明，在公开解释与去噪机制的结合下，即使在隐私保护的辅助数据下，图结构恢复仍然是可能的。

Abstract: Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [169] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/abs/2506.02138)
*Yarden Bakish,Itamar Zimerman,Hila Chefer,Lior Wolf*

Main category: cs.LG

TL;DR: 通过重新构建输入空间和专门的LRP规则，本文提出了一种新的针对Transformer的可解释性方法，有效解决了位置编码忽略问题，并在实验中表现优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 发展用于Transformer的有效可解释性工具是深度学习研究中的重要课题。目前，基于层次相关传播（LRP）的方法被认为是该领域中最有前景的方法之一。然而，现有的用于Transformer可解释性的LRP方法完全忽略了Transformer架构中关键的部分：位置编码（PE），导致违反了守恒性质，并丢失了与结构和位置特征相关的一种重要且独特的相关性。

Method: 我们重新构建了用于Transformer可解释性的输入空间，将其作为一组位置-令牌对。这使我们能够提出专门的理论基础的LRP规则，旨在跨多种位置编码方法（包括旋转、可学习和绝对位置编码）传播归因。

Result: 通过对经过微调的分类器和零样本基础模型（如LLaMA 3）进行的广泛实验表明，我们的方法在视觉和NLP可解释性任务中显著优于最先进的方法。

Conclusion: 提出了一种新的针对Transformer的可解释性方法，通过重新构建输入空间和专门的LRP规则，解决了位置编码的忽略问题，显著提高了视觉和NLP任务的可解释性效果。

Abstract: The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [170] [Z-Error Loss for Training Neural Networks](https://arxiv.org/abs/2506.02154)
*Guillaume Godin*

Main category: cs.LG

TL;DR: Z-Error Loss通过屏蔽异常数据，提高神经网络训练的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中的异常值会传播错误梯度，导致模型性能下降。因此，需要一种方法来降低异常值对训练的影响。

Method: 提出了Z-Error Loss方法，通过在每个批次中识别出分布外的数据点，并屏蔽其贡献，来减少异常值的影响。

Result: 该方法利用批次级统计数据自动检测并排除异常样本，使模型专注于真实的基础数据结构。

Conclusion: Z-Error Loss提供了一种有效的方法来减少异常值对训练的影响，从而提高模型性能和泛化能力。

Abstract: Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [171] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/abs/2506.02168)
*Hrushikesh N. Mhaskar,Efstratios Tsoukanis,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 论文探讨机器学习中的逼近问题，描述了一种不需要特定流形特征的逼近方法。


<details>
  <summary>Details</summary>
Motivation: 尽管函数逼近是机器学习的基本问题，但逼近理论在该领域的理论基础中并不占据中心位置，由此导致的问题是通常无法确定训练模型对未见或未标记数据的泛化能力。

Method: 作者提出了一种无需学习流形特征的流形函数逼近方法。

Result: 引入了一种新颖的方法，可以在未知流形上实现函数逼近，无需流形特征的学习。

Conclusion: 论文结束时探讨了当前机器学习框架的缺陷，并描述了一种新颖的方法，尝试在未知流形上实现函数逼近，而无需学习特定的流形特征。

Abstract: A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [172] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/abs/2506.02200)
*Shiangyi Lin,Hui Lan,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 研究提出了一种新方法，通过引入工具变量解决高维内生变量问题，优于传统降维方法。


<details>
  <summary>Details</summary>
Motivation: 传统IV估计在处理高维和非结构化的内生处理变量时受到工具变量数量的限制，这在医疗领域尤为明显。现有方法通过无监督降维来处理，但存在严重的遗漏变量偏差。

Method: 提出了一种新方法，通过在表示学习过程中明确引入工具变量进行处理表示的构建，确保了IV模型对这些包含工具变量信息的表示进行拟合。

Result: 实验结果表明，所提方法较传统的无工具信息的降维两阶段方法具有更好的效果。

Conclusion: 本文提出了一种新方法，通过在表示学习过程中明确引入工具变量来构建处理表示。这种方法为处理高维内生变量提供了框架，并在理论和实证上证明了，与传统两阶段方法相比，该方法能优化结果预测方向。

Abstract: Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [173] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/abs/2506.02203)
*Navid NaderiAlizadeh,Darian Salehi,Xinran Liu,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文提出了一种受约束的学习方法来优化切片瓦瑟斯坦距离的切片方向，以提高信息切片的效率，并已在多个基础模型上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 识别切片瓦瑟斯坦距离中提供信息的切片方向具有挑战性，通常需要很多切片来实现理想性能，进而增加了计算复杂性。

Method: 采用受约束的学习方法，通过对1D传输计划进行连续松弛，从而使用基于梯度的原对偶方法训练切片参数。

Result: 数值结果表明，该方法在图像、点云和蛋白质序列的基础模型上有效地学习了更具信息性的切片方向。

Conclusion: 提出了一种受约束的学习方法来优化切片瓦瑟斯坦(SW)距离的切片方向，可以增强高维概率测度的比较能力。

Abstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [174] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/abs/2506.02205)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

TL;DR: 提出了一种改进的CEM算法，通过利用Bregman重心，可以有效提高收敛和方案质量。


<details>
  <summary>Details</summary>
Motivation: CEM的单峰采样策略在多峰景观中容易导致过早收敛，因此需要一种增强策略来改善该问题。

Method: 使用Bregman重心进行信息聚合和多样性控制，通过计算基于性能的重心，并在该重心的信任区域内进行采样以更新贡献最小的工作器。

Result: 实验证明，该方法在合成基准测试、混乱导航任务和完整的MBRL流水线中均有效。

Conclusion: 实验结果证明，使用Bregman Centroid Guided CEM可以提高收敛性和解决方案质量，是对CEM的简单而有效的升级。

Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [175] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
*Hongling Xu,Qi Zhu,Heyuan Deng,Jinpeng Li,Lu Hou,Yasheng Wang,Lifeng Shang,Ruifeng Xu,Fei Mi*

Main category: cs.LG

TL;DR: 提出了一个结合知识蒸馏和强化学习的框架KDRL，能够有效训练推理大型语言模型，并在多项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前的强化学习和知识蒸馏有各自的优缺点。RL有助于产生复杂的推理行为，但样本效率低，而KD提高了学习效率但在域外场景下泛化性能差。

Method: 设计了一个名为KDRL的统一后期训练框架，通过教师监督（知识蒸馏）和自我探索（强化学习）共同优化推理模型。具体来说，KDRL利用策略梯度优化，同时最小化学生与教师分布之间的逆Kullback-Leibler散度（RKL），并最大化基于规则的预期奖励。

Result: 在多项推理基准测试中，KDRL优于GRPO和多种KD基准，且在性能和推理标记效率之间取得了良好平衡。

Conclusion: 结合知识蒸馏（KD）和强化学习（RL）是训练推理大型语言模型的有效且高效的策略。

Abstract: Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [176] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/abs/2506.02210)
*Pu,Yi,Tianlang Chen,Yifan Yang,Sara Achour*

Main category: cs.LG

TL;DR: 本研究通过识别神经网络中的对称冗余，提出ExPrune动态剪枝算法，显著减少计算量且保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络推理效率和资源节约的需求。

Method: 提出基于对称性质和交换性统计特性进行动态剪枝的算法ExPrune。

Result: ExPrune算法在多个模型中实现了显著的FLOPs降低，同时几乎没有准确率下降。与静态剪枝结合可进一步减少资源需求。

Conclusion: 识别和利用神经网络计算过程中的对称冗余，为提高推理效率提供了新的方式。

Abstract: Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [177] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/abs/2506.02213)
*Kahn Rhrissorrakrai,Kathleen E. Hamilton,Prerana Bangalore Parthsarathy,Aldo Guzman-Saenz,Tyler Alban,Filippo Utro,Laxmi Parida*

Main category: cs.LG

TL;DR: 本文探讨了在健康和生命科学的小数据场景中，量子集成模型的构建和其潜力。


<details>
  <summary>Details</summary>
Motivation: 研究量子集成模型在小数据集问题上，特别是在健康和生命科学领域的有效性，帮助其他研究者设计有效的小数据学习模型。

Method: 构建多种类型的量子集成模型进行二值分类，使用模拟实验中的26比特和量子硬件上的56比特，并进行了长距离的比特连接。

Result: 通过模拟和初步硬件实验，展示了量子嵌入结构如何影响性能，并讨论了提取信息特征的方法，以及构建可以有效学习和泛化的模型。

Conclusion: 量子计算在数据受限问题中尤其是在健康和生命科学领域中展现出潜力，可以在特征空间有限的情况下进行有效的学习和泛化。

Abstract: Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [178] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/abs/2506.02242)
*Yihong Tang,Ao Qu,Xujing Yu,Weipeng Deng,Jun Ma,Jinhua Zhao,Lijun Sun*

Main category: cs.LG

TL;DR: 提出一种基于多模态大语言模型的方法用于城市与交通研究中的可解释假设推理，提升了模型在政策应用中的可信度和在城市研究中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究城市和交通系统中关键变量与社会结果之间的关系，尤其是道路安全，以提供可操作的洞见来指导城市和交通系统的规划、发展和更新。

Method: 提出一种基于多模态大语言模型（MLLM）的可解释假设推理方法，利用街景图像生成安全相关问题，提取可解释的嵌入并应用于回归统计模型进行假设生成、评估和优化。

Result: 实验评估表明，该方法在曼哈顿街段上表现优于预训练深度学习模型，同时提供完全的可解释性。

Conclusion: UrbanX不仅适用于道路安全，还能作为城市科学发现的一般框架，从未结构化城市数据中提取结构化洞见，改善政策应用中的模型可信度，并为城市和交通研究中的可解释知识发现建立一个可扩展且统计上有依有据的路径。

Abstract: Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [179] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/abs/2506.02243)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: auGraph框架增强了图结构，通过选取对任务有用的属性作为节点，提升了对表格和关系数据的预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNN）在捕捉表格内和表间的结构依赖方面表现出色，但现有方法常依赖于僵化的、源于架构的图，如基于主外键连接的图，从而未能充分利用非关键属性中的丰富预测信号。

Method: 本文引入了一个名为auGraph的统一框架，用于任务感知的图增强，适用于表格和关系数据。auGraph通过引导评分函数，根据其对下游预测任务的相关性，选择性地将属性提升为节点，从而增强基础图结构。

Result: 实验证明，auGraph通过生成更支持关系和表格预测任务学习的图，超越了基于模式和启发式图构建方法。

Conclusion: auGraph通过不改变原始数据模式的情况下，注入任务相关的结构信号，提升了对于表格和关系数据的学习能力。

Abstract: Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [180] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/abs/2506.02255)
*Asha Ramanujam,Adam Elyoumi,Hao Chen,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Shraman Pal,Dimitri J. Papageorgiou,Can Li*

Main category: cs.LG

TL;DR: SafeOR-Gym是一个为安全强化学习设计的运筹学环境基准套件，评估显示性能差异大，旨在推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 当前安全强化学习在高风险领域受到限制，例如能源系统、制造业和供应链。为了填补这一空白，提出了SafeOR-Gym。

Method: 整合了OmniSafe提供的约束马尔可夫决策过程（CMDP）接口，评估了几种最先进的安全RL算法。

Result: 评估显示性能存在很大差异：有些任务是可处理的，而另一些任务则暴露出当前方法的基本局限性。

Conclusion: SafeOR-Gym为安全强化学习提供了一个挑战性和实用的测试平台，旨在推动安全RL在实际决策问题方面的研究。

Abstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [181] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/abs/2506.02256)
*Yi Xiao,Harshit Sharma,Sawinder Kaur,Dessa Bergen-Cico,Asif Salekin*

Main category: cs.LG

TL;DR: 本文提出了一种可去除个体差异性的新方法HHISS，通过关注共享特征，显著提高了压力检测模型的跨人群和环境的普适性，优于已有方法，具有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 压力会影响身心健康，而可穿戴设备已经广泛用于通过生理信号检测日常压力。然而，由于个体差异和健康状况等因素，这些信号会有所不同，使得机器学习模型的泛化变得困难。本文提出了人类异质性不变压力感知（HHISS），一种域泛化方法，旨在通过去除个性化差异来找到压力信号中的一致模式，从而提高模型在新的个体、环境和未见过的压力类型上的准确性。

Method: 本文提出了一种称为“按人剪枝子网交集”的新颖技术，通过关注个人间的共享特征来提高泛化能力，并利用连续标签防止过拟合。该方法在七个不同的压力数据集上进行测试，其中四个是自己收集的，三个是公开的数据集。

Result: HHISS在跨多种数据的表现中始终优于最先进的基线方法，证明其在实际操作中的有效性和实用性。消融研究、实证理由和运行时间评估验证了HHISS在实际应用中移动压力感知的可行性和可扩展性。

Conclusion: HHISS方法通过消除个体因变量来提高模型的泛化能力，在应对不同人群、环境及压力类型时的表现优于现有方法，具有实际应用价值，尤其是在敏感的真实世界压力感知应用中。

Abstract: Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [182] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/abs/2506.02269)
*YuQing Xie,Tess Smidt*

Main category: cs.LG

TL;DR: 本文研究等变神经网络中等变约束对优化的潜在影响，通过理论分析和实验展示了等变性松弛可能成为解决某些学习障碍的途径。


<details>
  <summary>Details</summary>
Motivation: 等变神经网络在具有已知潜在对称性的任务中表现出色，但优化起来可能较为棘手，训练最佳实践尚不如标准网络成熟。

Method: 通过理论分析损失景观几何来研究等变约束对优化的影响。重点研究使用置换表示构建的网络，可以将其视为非约束MLP的子集。

Result: 展示了非约束模型参数对等变子空间损失景观有非平凡的影响，在某些条件下可以阻止全局最小值的学习。实验表明在这种情况下松弛到非约束MLP有时可以解决问题。

Conclusion: 有效的松弛等变性可能不仅需要添加不等变的自由度，还需要重新考虑隐藏层中的群表示固定选择。

Abstract: Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [183] [Latent Stochastic Interpolants](https://arxiv.org/abs/2506.02276)
*Saurabh Singh,Dmitry Lagun*

Main category: cs.LG

TL;DR: 提出了一种潜在随机插值方法（LSI），能够在潜空间中实现端到端的优化学习，提高生成模型的灵活性和效率，并在ImageNet基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 探索如何在潜空间中联合优化潜变量模型，以增强生成模型的灵活性和计算效率。

Method: LSI通过连续时间导出原理证据下界（ELBO）目标进行联合学习，实现潜变量模型的端到端优化。

Result: LSI能够优化潜空间中的生成过程，将任意先验分布转化为编码器定义的聚合后验，并在ImageNet生成基准测试中表现出色。

Conclusion: LSI能够在潜空间中实现端到端的优化学习，展示了其在生成建模中的有效性，能够灵活处理两种分布之间的转换。

Abstract: Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [184] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)
*Qinsi Wang,Jinghan Ke,Hancheng Ye,Yueqian Lin,Yuzhe Fu,Jianyi Zhang,Kurt Keutzer,Chenfeng Xu,Yiran Chen*

Main category: cs.LG

TL;DR: 本文提出GAIN-RL，通过内在信号提升大型语言模型的训练效率，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型强化微调范式在样本效率方面存在问题，现有方法未充分利用模型自己生成的内在学习信号。

Method: 提出一种名为GAIN-RL的框架，通过利用模型的内在角度集中信号动态选择训练数据以提高训练效率。

Result: 实验证明，GAIN-RL在多种任务中实现了超过2.5倍的训练效率加速，并在数据效率方面显著提高。

Conclusion: GAIN-RL框架在不同任务和模型尺度上显著提升了训练效率，并在使用一半数据的情况下超越了使用全部数据的常规方法。

Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [185] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)
*Aaron Defazio*

Main category: cs.LG

TL;DR: 长时间大型语言模型训练中梯度范数增加是由于权重衰减等导致，提出简单修正解决并降低训练损失。


<details>
  <summary>Details</summary>
Motivation: 在长时间运行大型语言模型训练时，梯度范数在训练结束时迅速增加，这可能是由于权重衰减、归一化层和学习率计划之间的相互作用导致的。因此需要进行修正。

Method: 提出了一种简单的修正方案，解决了权重衰减、归一化层和学习率计划之间的非预期交互。

Result: 通过应用修正方案，整个训练期间的损失值显著降低。

Conclusion: 造成梯度范数增加的问题可以通过简单的修正解决，这不仅能够解决问题，还能在整个训练期间获得更低的损失值。

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [186] [On Universality Classes of Equivariant Networks](https://arxiv.org/abs/2506.02293)
*Marco Pacini,Gabriele Santin,Bruno Lepri,Shubhendu Trivedi*

Main category: cs.LG

TL;DR: 本文阐述了等变神经网络的逼近能力，指出分离能力并不能完全体现表达性，并探讨了浅层网络在某些对称条件下实现普适性的可能性。


<details>
  <summary>Details</summary>
Motivation: 探讨等变神经网络在分离能力之外的逼近能力，以及在哪些情况下浅层模型能够实现普适性。

Method: 研究将等变模型降低到不变模型时的普适性类别，以及对称群的结构性质对模型表达能力的影响。

Result: 尽管相同的分离能力可能导致不同的逼近能力，但在某些情况下，浅层等变网络可以实现分离约束的普适性。

Conclusion: 在对称的结构下，浅层不变网络的表达能力可以通过表示其普适性类别来表征。虽然浅层等变网络不能总是实现普适性，但在某些情况下，它们可以实现基于分离的普适性。

Abstract: Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [187] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)
*Farzaneh Mahdisoltani,Saeed Mahdisoltani,Roger B. Grosse,David J. Fleet*

Main category: cs.LG

TL;DR: 该论文提出了一种利用网络梯度进行类别间隐含路径可视化的新框架，实验表明其提供了语义上有意义的变换，帮助理解神经网络的决策机制。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的内部表示和决策机制仍然是一个未解决的关键挑战。现有的可解释性方法通常能够识别出对输入有影响的区域，但可能无法阐明模型如何区分类别或具体改变如何使输入从一个类别过渡到另一个。

Method: 我们提出了一种新的框架，通过将网络梯度视为一种微小的运动来可视化类别之间的隐含路径。我们借鉴基于相位的运动放大，首先使用可逆变换来分解图像，具体地使用了复数方向金字塔，然后在变换空间中计算类别条件梯度。我们放大一步梯度到输入并进行线性外推，以揭示模型如何从源类别移动到目标类别。通过在可操作的金字塔域工作，这些放大的梯度产生语义上有意义、空间上连贯的变形，突出了分类器最敏感的方向，提供了对其决策边界几何的新见解。

Result: 在合成和现实世界的数据集上进行的实验表明，我们的相位聚焦外推产生了感知上对齐、语义上有意义的变换，提供了一种新颖的、可解释的方法来观察神经分类器的内部表示。

Conclusion: 该研究为理解深度神经网络的内部表示和决策机制提供了一种新颖的视角，通过可视化类别间的隐含路径，并提供对决策边界几何的深刻见解。

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [188] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/abs/2506.02306)
*Aditya Gorla,Ryan Wang,Zhengtong Liu,Ulzee An,Sriram Sankararaman*

Main category: cs.LG

TL;DR: 提出了一种名为 CACTI 的方法，通过结合数据集特定的上下文信息和缺失模式，来有效插补表格数据，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用数据集中特定的上下文信息和缺失模式，通过表格数据的有效插补，提升插补效果。

Method: CACTI 使用一种称为中值截断复制掩饰的训练方法，结合特征间的语义关系，来更好地表示特征依赖性。

Result: CACTI 比最好的方法平均 $R^2$ 提高了 7.8%，在不同的缺失情况下表现更佳。

Conclusion: CACTI 在处理缺失率模式和上下文信息方面表现出色，能有效提高表格数据的插补效果。

Abstract: We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [189] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/abs/2506.02308)
*Xiaojun Shan,Qi Cao,Xing Han,Haofei Yu,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出MINT任务分组策略，在多模态模型指令微调中表现出色。


<details>
  <summary>Details</summary>
Motivation: 虽然增加指令微调任务的数量成为趋势，但发现简单增加任务数量不一定带来更好的性能。因此，有必要探索新的任务分组策略。

Method: 引入了一种任务分组策略MINT，根据多模态交互类型来进行任务分组。

Result: 所提出的方法在多模态指令微调的表现上大大优于现有的任务分组基线，能够在泛化和专业化之间取得平衡。

Conclusion: 本文提出了一种名为MINT的任务分组策略，能够在多模态指令微调中有效提升模型表现。

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [190] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/abs/2506.02315)
*D. Isaiah Harp,Joshua Ott,John Alora,Dylan Asmar*

Main category: cs.LG

TL;DR: 提出了一种创新的飞行测试方法，通过低阶模型和机器学习来提高预测准确性，消除了对特定测试点的需求。


<details>
  <summary>Details</summary>
Motivation: 传统飞行测试点的前提是测试员必须严格遵循预设的模型条件，否则就会破坏模型假设。但存在数据带和容限的问题，这比飞行员技术不足更为基础。本文旨在通过消除测试点来解决此问题。

Method: 本文利用机器学习方法生成低阶模型(ROM)，而不是高保真模型生成点预测。ROM可以在任何条件下进行预测，并通过飞行数据更新，使结果不断优化。

Result: 通过使用T-38C飞行测试数据验证了“无测试点”架构。在更新ROM后，该模型生成了评估纵向动态合规性所需的参数。

Conclusion: 提出了一种创新的“无测试点”方法，通过改进的低阶模型进行预测和更新，来验证高保真模型的准确性。这种方法提高了飞行测试的灵活性和效率。

Abstract: The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [191] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/abs/2506.02318)
*Yuchen Liang,Renxiang Huang,Lifeng Lai,Ness Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文对吸收率矩阵下的离散扩散模型进行了首次的有限时间错误界限和收敛率分析，证实其在 $
abla\leaping$ 和均匀化采样器方面具有优于均匀率矩阵的收敛速率，并提供可不依赖早停的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 尽管实验证实吸收率矩阵常常比均匀率矩阵产生更好的生成质量，但现有理论工作主要集中在均匀率矩阵的情况，并且对于吸收扩散模型的收敛保证和误差分析仍旧缺失。

Method: 本文首先通过推导前向过程的KL散度上限，引入代理初始化分布以解决吸收平稳分布单点性造成的KL散度无法定义的问题。随后，通过建立Jensen型论证以界定前向过程收敛，提出了界定吸收得分函数的新技术，最后实现了初始化附近得分的非发散上限以去除早停的必要。

Result: 提出了用于吸收率矩阵的离散扩散模型在有限时间内的误差界限和收敛分析方法，并首次证明了$
abla\leaping$和均匀化采样器在吸收率矩阵下的收敛保证，相较于均匀率矩阵，其表现出更好的收敛速率。此外，在适当假设下，无需早停的收敛保证也得以提供。

Conclusion: 本文通过引入吸收率矩阵，提供了解决离散扩散模型在有限时间内错误界限和收敛速率分析的问题，为 $
abla\leaping$ 和均匀化采样器在吸收率矩阵下建立了首个收敛保证，显示出其相对均匀率矩阵有更优的收敛速率。

Abstract: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [192] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/abs/2506.02323)
*Aleix Boquet-Pujadas,Pol del Aguila Pla,Michael Unser*

Main category: cs.LG

TL;DR: Optimized estimation of probability densities using splines on grids, enhancing sparsity with nuclear norm regularization; spatially adaptive and stable, successfully tested with applications including PET rebinning.


<details>
  <summary>Details</summary>
Motivation: To efficiently estimate probability densities in multidimensional problems with uneven sampling probability by utilizing detector sensitivity and computational advantages of splines.

Method: We use an optimization approach that involves splines on a grid to estimate probability densities, incorporating detector sensitivity. Regularization of the spline's Hessian is achieved via the nuclear norm to enhance sparsity.

Result: The method was tested successfully on standard densities, and a new approach to PET rebinning was introduced as an application of the framework, along with the provision of software.

Conclusion: The method is spatially adaptive and stable with respect to the regularization parameter, making it effective for estimating probability densities in multidimensional problems.

Abstract: We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [193] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/abs/2506.02337)
*Adrienne M. Propp,Jonas A. Actor,Elise Walker,Houman Owhadi,Nathaniel Trask,Daniel M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种通过高斯过程在图上学习Dirichlet-to-Neumann映射的新方法，能在数据稀缺的情况下准确预测并量化不确定性，适用于需要遵循保守律的偏微分方程相关科学问题。


<details>
  <summary>Details</summary>
Motivation: 为了在具备保守性约束的偏微分方程背景下进行跨计算子域的多物理模拟耦合，以确保状态变量和流的连续性。

Method: 该方法利用高斯过程学习图上的Dirichlet-to-Neumann映射，结合离散外微积分和非线性最优恢复技术，推断顶点值和边值之间的关系，并通过在再生核Hilbert空间范数上进行优化，结合最大似然估计惩罚实现保守性律的严格遵循。

Result: 该方法在应用于地下断裂网络和动脉血流的场景时表现出了即使在数据极端稀缺的情况下也能保持高精度和不确定性估计良好的特点。

Conclusion: 在有限数据下，该方法可以确保高精度和良好的不确定性估计，适用于需要可靠不确定性量化的科学应用中。

Abstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [194] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/abs/2506.02355)
*Andre He,Daniel Fried,Sean Welleck*

Main category: cs.LG

TL;DR: GRPO在多样本任务中存在偏差问题，引入不太可能的奖励改善了性能，通过实验证实其效果。


<details>
  <summary>Details</summary>
Motivation: 识别了GRPO算法在多样本性能任务中的关键缺陷，提出了一种改进方法以在大样本情况下提高性能。

Method: 提出了不太可能的奖励方法，这是一种明确鼓励强化稀有但正确解的简便方法。此外，还增加了PPO周期数以减轻偏差。

Result: 实验显示，引入不太可能的奖励后，Pass@$N$指标在广泛的N范围内均得到显著提高，样本多样性也有所增加。

Conclusion: 引入不太可能的奖励显著提高了大样本下的通过率性能，解决了GRPO的偏差问题。

Abstract: Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [195] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)
*Ram Potham*

Main category: cs.LG

TL;DR: 该论文提出了一种使用简单网格世界评估大型语言模型（LLM）代理在原则冲突条件下对安全原则的优先程度的方法。初步研究展示了该方法的可行性，并为评估AI系统可控性提供了初步见解。


<details>
  <summary>Details</summary>
Motivation: 针对高级AI开发，制定可信的安全计划需要验证代理行为和早期检测潜在的控制缺陷的方法，尤其是在安全关键原则与操作目标冲突时，确保代理优先注意这些原则。

Method: 本文引入了一种轻量级、可解释的基准方法，使用简单的网格世界来评估大型语言模型代理在面对与任务指令冲突时，维持预定义高层次安全原则（例如，“绝不进入危险区域”）的能力。

Result: 论证了评估对层次原则的遵循是了解构建可控AI系统能力的重要早期步骤。

Conclusion: 初步研究表明，该方法可行，并为在原则冲突下的代理行为提供了初步见解，同时讨论了这些基准测试如何为评估可控性提供实证依据。

Abstract: Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [196] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)
*Zhe Li,Bicheng Ying,Zidong Liu,Chaosheng Dong,Haibo Yang*

Main category: cs.LG

TL;DR: 引入Hessian信息的HiSo方法能在通信成本不变的情况下加速联邦学习的收敛速度，且实验结果卓越。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，通过引入Hessian信息加速优化速度，同时解决零阶梯度估计高方差导致的收敛速度慢的问题。

Method: 提出了一种利用Hessian信息的零阶优化和仅标量通信的快速联邦微调方法。

Result: HiSo在保持低通信成本的同时，加速了收敛速度，并且在理论上提供了与全局Lipschitz常数无关的收敛保证。实验结果表明，其在基准数据集和LLM微调任务上显著优于现有基于ZO的FL方法。

Conclusion: HiSo能够在保持低通信成本的同时显著提高收敛速度和通信效率，且实验表明其在基准数据集和大型语言模型微调任务中表现优异。

Abstract: Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [197] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/abs/2506.02371)
*Haoye Lu,Darren Lo,Yaoliang Yu*

Main category: cs.LG

TL;DR: 提出了一种名为SFBD flow的连续生成方法，以解决敏感数据集和隐私问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型依赖大量包含敏感内容的数据集以及模型记忆习惯导致的隐私问题。

Method: 重新解释SFBD为交替投影算法，引入连续变种SFBD flow，避免需要交替步骤。

Result: 展示了Online SFBD在各项基准测试中稳定地超越强基线。

Conclusion: SFBD flow 实现了无需交替步骤的生成过程，且其实际应用Online SFBD在各项基准测试中表现优于强基线。

Abstract: Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [198] [Multi-agent Markov Entanglement](https://arxiv.org/abs/2506.02385)
*Shuze Chen,Tianyi Peng*

Main category: cs.LG

TL;DR: 论文揭示具有非纠缠转移矩阵的马尔可夫决策过程适于值分解，并引入Markov纠缠度来评估多智能体系统的值分解误差。


<details>
  <summary>Details</summary>
Motivation: 论文探讨多智能体系统中的值分解技术，该技术在动态规划和强化学习中已被广泛使用，但如何有效地理论上解释其有效性尚未完全研究。

Method: 引入“Markov纠缠度”这一概念来测量多智能体系统中Markov决策过程的纠缠程度，并通过此测量来界定值分解误差。

Result: 证明了一类广泛使用的索引政策在弱纠缠下的值分解误差具有次线性规模，并展示了如何在实践中有效估计Markov纠缠度。

Conclusion: 识别并界定了多智能体马尔可夫决策过程中能够影响值分解有效性的纠缠结构，为理论上的值分解有效性提供了支持。

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [199] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/abs/2506.02386)
*Jie Bian,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 该文提出一种针对线性赌博问题的新算法，在固定预算下优化臂识别过程，保证误差概率以指数速率衰减，并验证其有效性超过多个基准算法。


<details>
  <summary>Details</summary>
Motivation: 解决文献中存在的一个显著空缺，即在带有高斯噪声的K臂赌博问题中，误差概率趋于零的确切指数速率尚未得到确定。

Method: 引入一种新算法，通过结合博弈论中的策略，基于最小学习者和最大学习者的游戏式采样规则的后验采样框架，实现最优臂识别，保证误差概率的指数衰减。

Result: 综合实证评估表明该算法在不同复杂度的问题实例中优于多个基准算法，不仅具备更高的准确性，还具备更强的效率。

Conclusion: 提出一种能够保证误差概率指数衰减的最优臂识别算法，并且其衰减速率——以指数为特征——符合通过信息论原则推导的理论下界。

Abstract: The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [200] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)
*Chamara Madarasingha,Nasrin Sohrabi,Zahir Tari*

Main category: cs.LG

TL;DR: LLMPred通过时间序列转换为文本并应用两种预处理技术，提升LLM在复杂时间序列数据上的预测能力，实验结果表明其效果优于或媲美当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在时间序列预测方面的广泛泛化能力和较少预训练要求，研究其在处理复杂的、嘈杂的多变量时间序列数据中的有效性具有重要意义。

Method: LLMPred通过将时间序列转换为文本输入到LLM进行零次预测，并应用时间序列分解和轻量级提示处理策略来增强预测准确性。

Result: 实验结果表明，LLMPred在使用较小的LLM如Llama 2 7B、Llama 3.2 3B、GPT-4o-mini和DeepSeek 7B时表现优异，与最先进的基线进行了竞争或取得了更好的性能。详细的消融研究也强调了LLMPred中关键组件的重要性。

Conclusion: LLMPred通过将时间序列数据转换为文本并通过LLM进行预测，结合两种主要数据预处理技术，能够在处理复杂、嘈杂的单变量和多变量时间序列数据中表现出色。

Abstract: Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [201] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/abs/2506.02390)
*Qin Xie,Qinghua Zhang,Shuyin Xia,Xinran Zhou,Guoyin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为GAdaBoost的粒度计算自适应Boosting方法，通过数据粒化和改进的Boosting算法提高在噪声环境下的分类效率和准确性。


<details>
  <summary>Details</summary>
Motivation: AdaBoost在多类分类任务中面对标签噪声问题时存在挑战，而现有方法要么缺乏有效处理标签噪声的机制，要么由于冗余数据使用而导致计算成本高。因此，本文提出了一个新颖的两阶段框架GAdaBoost，以提高在噪声条件下的效率和鲁棒性。

Method: 提出了一种称为GAdaBoost的两阶段框架，包括数据粒化阶段和自适应Boosting阶段。首先，设计了一种粒状球生成方法用于压缩数据，同时保留多样性并减轻标签噪声。其次，基于粒状球的SAMME算法专注于粒状球而非单个样本，从而提高效率并降低对噪声的敏感性。

Result: 实验结果表明，在一些噪声数据集上，所提出的方法在鲁棒性和效率上优于现有方法，有效地扩展了AdaBoost和SAMME。

Conclusion: 实验表明，GAdaBoost方法在处理多类分类中的标签噪声方面具有更好的鲁棒性和效率。

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [202] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)
*Yuanyao Chen,Rongsheng Chen,Fu Luo,Zhenkun Wang*

Main category: cs.LG

TL;DR: 提出了一种利用大型语言模型进行分布投影的方法，提升了NCO模型在大规模问题上的性能，且无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 现有的NCO方法在小规模实例上训练时表现优异，但在大规模场景应用时性能显著下降，因训练和测试数据的分布差异导致。

Method: 引入大型语言模型的学习框架，将其用于学习训练和测试分布之间的投影，并在推理阶段应用。

Result: 通过大规模实验验证，本文方法使得仅在100节点实例上训练的基础模型在多种分布的多达100K节点的大规模TSP和CVRP问题上实现了更好的性能。

Conclusion: 提出了一种在推理阶段利用大型语言模型进行分布投影的方法，可以在无需重新训练模型的情况下显著提升NCO模型在大规模问题上的性能。

Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [203] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/abs/2506.02406)
*Renat Sergazinov,Jing Wu,Shao-An Yin*

Main category: cs.LG

TL;DR: 将随机傅里叶特征作为表格深度学习的预处理步骤，显著提高了训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 弥补表格深度学习管道中的不足，揭示通过神经切线核（NTK）分析显示的缺陷。

Method: 将随机傅里叶特征映射重新用于表格深度学习，通过正弦和余弦投影将每个输入投射到固定的特征空间中。

Result: 深度网络在经过傅里叶变换的输入上更快收敛，通常需要更少的迭代次和超参数调整，表现更稳健。

Conclusion: 随机傅里叶预处理被确立为表格深度学习的理论驱动型、即插即用的增强方法。

Abstract: While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [204] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/abs/2506.02415)
*Karthikeyan Vaiapury*

Main category: cs.LG

TL;DR: AERO, inspired by Judo's redirection principle, enhances stability and adaptability in uncertain environments, showing significant success in solar energy forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing optimization methods struggle with stability and adaptability in dynamic, nonlinear systems, especially under uncertainty. The study aims to address these challenges by introducing a new optimization framework.

Method: AERO uses 15 interrelated axioms including adversarial correction, energy conservation, and disturbance-aware learning. It projects gradients, integrates uncertainty-driven dynamics, and manages learning energy for better stability and robustness.

Result: Applied to probabilistic solar energy forecasting, AERO showed significant improvements in predictive accuracy, reliability, and adaptability, particularly in noisy and uncertain conditions.

Conclusion: AERO is a promising new direction in both theoretical and practical aspects of optimization, particularly for applications requiring stability and adaptability under uncertainty.

Abstract: Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [205] [Weak Supervision for Real World Graphs](https://arxiv.org/abs/2506.02451)
*Pratheeksha Nair,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: 论文提出WSNET，一种利用弱信号进行节点分类的弱监督对比学习框架，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在标签稀缺和噪声严重的高风险领域，现有节点分类方法面临挑战，因此亟需一种能够利用弱信号有效学习的方法。

Method: 提出WSNET，一个弱监督图对比学习框架，通过对比目标来整合图结构、节点特征和多个噪声监控源，适用于弱标签数据。

Result: 在三个真实世界数据集和带有可控噪声的合成基准测试中，WSNET的F1分数比现有最先进的对比和噪声标签学习方法高出最多15%。

Conclusion: 弱监督对比学习框架WSNET可以有效提高节点分类的性能，尽管面临标签稀缺和噪声挑战。

Abstract: Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [206] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
*Jiaxi Hu,Yongqi Pan,Jusen Du,Disen Lan,Xiaqiang Tang,Qingsong Wen,Yuxuan Liang,Weigao Sun*

Main category: cs.LG

TL;DR: 本文提出了一种新的非线性RNN模型Comba，基于闭环控制理论，表现出色的计算效率和性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的序列建模方法如Gated DeltaNet, TTT, 和RWKV-7通过Delta学习规则在递归记忆管理方面取得了性能改善，但存在非线性递归结构的问题。因此，需要引入非线性RNN模型并探索其优势和局限性。

Method: 基于闭环控制理论提出了一种新的非线性RNN变体，称为Comba，采用了标量加低秩状态转换，结合状态反馈和输出反馈修正。此外，还在Triton中实现了硬件高效的分块并行内核，并在大规模语料上训练了具有340M/1.3B参数的模型。

Result: 表明Comba在语言和视觉建模任务中具备更高的性能和计算效率。

Conclusion: Comba展示了其在语言和视觉建模方面的优越性能和计算效率。

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [207] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/abs/2506.02504)
*Xingyu Chen,Bokun Wang,Ming Yang,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 研究提出了用于非光滑FCCO的随机动量方法，将迭代复杂度从O(1/ε^6)提升至O(1/ε^5)，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的FCCO结果在深度学习应用中存在高迭代复杂度和算法适用性差的问题。

Method: 提出了针对非光滑FCCO的随机动量优化方法，并提供收敛性保证。

Result: 在处理约束非凸优化问题时，实现了O(1/ε^5)的复杂度，超过了之前的O(1/ε^6)。

Conclusion: 提出的随机动量方法有效解决了非光滑FCCO问题，并在多个任务上证明了其有效性。

Abstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [208] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/abs/2506.02539)
*Thong Q. Nguyen,Shubhang Desai,Yash Jain,Tanvir Aumi,Vishal Chowdhary*

Main category: cs.LG

TL;DR: VerificAgent框架通过专家知识、训练优化和人类核查，提高计算机代理记忆管理，成功率提升111.1%。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效管理计算机代理的记忆以提升其在生产力软件等领域的任务解决能力。

Method: 提出了一种名为VerificAgent的新框架，通过专家精心编选的领域知识种子、训练阶段的迭代轨迹记忆优化以及人类专家的事后事实核查来管理计算机代理的记忆。

Result: 在OSWorld生产力任务中，VerificAgent实现了比基线计算机代理成功率高出111.1%的相对提高。

Conclusion: VerificAgent框架能有效管理计算机代理的记忆，防止记忆累积导致的性能下降，并显著提高生产力任务的成功率。

Abstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [209] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.LG

TL;DR: 现有机器遗忘方法在选择替代输出时常导致不良后果，PUBG方法通过引导输出改善这些问题，确保提供视觉基础且信息丰富的响应而不会泄露隐私。


<details>
  <summary>Details</summary>
Motivation: 通过机器学习进行大规模网络数据的训练可能导致大型视觉语言模型（LVLMs）存在隐私风险。现有的遗忘方法在选择替代输出目标时，可能导致不良后果，包括退化、幻觉或过度拒绝的响应。因此，强调通过新方法改善这些问题的重要性。

Method: 提出了一种新的遗忘任务，该任务要求模型提供既保护隐私又能提供信息和视觉基础的响应。为了实现这一目标，提出了一种名为PUBG的新型遗忘方法，旨在引导遗忘后的行为向理想的输出分布发展。

Result: 实验结果表明，虽然现有的方法能有效防止隐私泄露，但仍会遭受遗忘后的不良后果，而PUBG能够有效减轻这些问题，生成视觉基础且信息丰富的响应，并且不会造成遗忘目标的隐私泄漏。

Conclusion: PUBG方法能成功地减轻现有机器遗忘方法导致的不良后果，提供质量和信息丰富的响应，同时确保隐私不泄露。

Abstract: Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [210] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann,Masoud Ramuz,Zeeshan Nisar,Nadine S. Schaadt,Hendrik Annuth,Benjamin Doerr,Friedrich Feuerhake,Thomas Lampert,Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: 提出了一种新型异质图神经网络HIEGNet，用于肾小球健康分类，其结合了肾小球及其周围免疫细胞，实验表明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 在肾脏病理学中，肾小球健康分类是一项重要的任务，图神经网络虽然在组织病理学中表现出色，但在此任务中尚未得到广泛探索，构图过程中节点、边缘的识别及信息特征提取存在独特的困难。

Method: 我们提出了一种由传统和基于机器学习的计算机视觉技术组成的管道，用于识别节点、边缘及其相应特征以形成异质图，然后提出了一种用于肾小球分类的新型异质GNN架构，称为HIEGNet。

Result: 实验结果表明，HIEGNet优于几种基线模型，并在所有基线模型中在患者之间具有最佳泛化能力。

Conclusion: 我们的HIEGNet在肾移植患者的全片图像数据集上表现出色，优于几种基线模型，并在所有基线模型中在患者之间具有最佳泛化能力。

Abstract: Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [211] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
*Shenghua He,Tian Xia,Xuan Zhou,Hui Wei*

Main category: cs.LG

TL;DR: 研究解决了大语言模型强化学习中的零奖励假设问题，并提出了一个新的算法TRePO，提供了一种更加实用和有效的模型微调策略。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型的强化学习中，中间动作往往无法获得任务特定的即时奖励，因此需要新的方法来有效利用响应级别的奖励信号。

Method: 引入了轨迹策略梯度定理，分析了在零奖励假设下的REINFORCE和Actor-Critic家族算法，提出了一种新的算法TRePO。

Result: 展示了诸如PPO等广泛使用的方法固有地具有能够建模token级别奖励信号的能力，并提出的TRePO方法在内存效率上与GRPO相当，同时也更简单。

Conclusion: 该研究提出了一种统一的理论视角，证明了在无需精确的token级别奖励的情况下，可以不偏不倚地估计策略梯度，从而为响应级别奖励模型提供了理论支持。

Abstract: We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [212] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/abs/2506.02563)
*Roie Reshef,Kfir Yehuda Levy*

Main category: cs.LG

TL;DR: 提出了一种适用于联邦学习中部分参与场景的差分隐私方法，具有最优性能和高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决在联邦学习中部分参与下实现差分隐私的挑战，因为现有方法在这种情况下表现不佳。

Method: 引入了一种新颖的噪声消除机制，并在随机凸优化框架中进行分析。

Result: 该方法对于同质和异质数据分布均能实现最优性能，扩展了差分隐私在联邦学习中的适用性。

Conclusion: 该研究提出了一种适用于部分参与场景的联邦学习差分隐私新方法，能够在保持隐私保护的同时不影响收敛速度和计算效率。

Abstract: This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [213] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)
*Ping Gong,Jiawei Yi,Shengnan Wang,Juncheng Zhang,Zewen Jin,Ouxiang Zhou,Ruibo Liu,Guanbin Xu,Youhui Bai,Bowen Ye,Kun Yuan,Tong Yang,Gong Zhang,Renhai Chen,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: HATA是一种新型的注意力加速方法，通过使用哈希技术实现顶级注意力，相较于传统方法实现显著加速，并保持模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的顶级注意力机制在LLM推断中难以在效率和准确性之间取得平衡。

Method: 提出了一种新的Hash-Aware Top-k Attention (HATA)，将低开销学习-哈希技术系统地集成到Top-k注意力过程中。

Result: HATA在保持模型准确性的同时，比传统的全注意力达到了最高7.2倍的加速。在多个主流LLM模型和不同任务中，HATA在准确性和效率方面都优于目前的顶级注意力方法。

Conclusion: HATA提供了一种有效的解决方案，通过引入哈希技术，实现了更快速且准确的顶级注意力。

Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [214] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/abs/2506.02577)
*Wenyan Yang,Joni Pajarinen*

Main category: cs.LG

TL;DR: RWS improves offline goal-conditioned RL policy performance by optimizing sampling with a reachability classifier.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency of uniform sampling in offline goal-conditioned reinforcement learning, which results in many unreachable state-goal-action pairs and degrades policy performance.

Method: The paper introduces Reachability Weighted Sampling (RWS), which employs a reachability classifier trained via positive-unlabeled (PU) learning. This classifier assigns reachability scores to state-action values, used to prioritize sampling.

Result: RWS was tested on six simulated robotic manipulation tasks and showed significant performance improvements. For example, performance on the HandBlock-Z task improved by nearly 50% compared to the baseline.

Conclusion: Reachability Weighted Sampling (RWS) can significantly improve policy performance in offline goal-conditioned reinforcement learning by optimizing the sampling process to favor transitions that enable goal achievement.

Abstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [215] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg,Marion Neumeier,Sinan Hasirlioglu,Mohamed Essayed Bouzouraa,Michael Botsch*

Main category: cs.LG

TL;DR: 本文提出了使用CVQ-VAE进行交通场景聚类的方法，显著提高了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶系统在复杂交通情况下的安全性，需要精确理解交通场景。

Method: 使用Clustering Vector Quantized - Variational Autoencoder (CVQ-VAE)对高速公路交通场景进行聚类，并创建具有不同种类类别的目录。

Result: 研究显示，与以往研究相比，所提出的方法在聚类表现上有显著提升，并通过分析讨论了类别数对场景类别完整性的影响。

Conclusion: CVQ-VAE在高速公路交通场景的聚类中表现优于以往研究。

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [216] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/abs/2506.02612)
*Jan Robine,Marc Höftmann,Stefan Harmeling*

Main category: cs.LG

TL;DR: 提出了一种简单、好用且快速的世界模型SGF，展示了其在Atari 100k基准测试中的良好表现。


<details>
  <summary>Details</summary>
Motivation: 探讨构建世界模型的基本组件，并尝试在不使用RNN、transformers、离散表示和图像重构的情况下进行探索。

Method: 使用自监督表示学习，通过帧和动作堆叠捕获短时依赖，并通过数据增强提高对模型错误的鲁棒性。

Result: 通过消融研究评估了SGF的构建模块，并通过与已有世界模型的广泛比较证明了其良好的性能。

Conclusion: SGF模型在Atari 100k基准测试中表现良好，证明了无需复杂组件即可构建有效的世界模型。

Abstract: What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [217] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/abs/2506.02616)
*Qi Liao,Parijat Bhattacharjee*

Main category: cs.LG

TL;DR: 本论文介绍了一种新方法，通过组成性学习在多代理系统中提高性能和安全性，显著减少切换失败并提高网络效率，优于传统多代理强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 自组织网络中复杂参数相互依赖和目标冲突构成挑战，需要新的方法来提高其效率和安全性。

Method: 提出了组成性深度强化学习（CDRL）和组成性预测决策（CPDM）方法，并采用模块化的双层框架进行评估。该框架中包含单元级和单元对级代理，用以管理异构代理的粒度并降低模型复杂度。

Result: 数值模拟表明该方法显著减少了切换失败，并提高了吞吐量和时延表现。此外，该方法在大型自组织网络中展现了更好的可扩展性、更快的收敛速度、更高的样本效率和更安全的训练性能。

Conclusion: 该研究引入了两种组成性学习方法，证明在多代理系统中训练时间和安全性约束下具有显著优势。提出的方法能够有效减少切换失败率，并提高网络吞吐量和时延表现。

Abstract: Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [218] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/abs/2506.02619)
*Yanbei Liu,Chongxu Wang,Zhitao Xiao,Lei Geng,Yanwei Pang,Xiao Wang*

Main category: cs.LG

TL;DR: 提出了一种最优传输机制的自监督异构图神经网络（HGOT），无须图增强，简化样本选择，性能优于现有方法，节点分类准确率提升超6%。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络在处理异构信息网络方面表现优异，但无标签情况下的自监督学习仍面临设计精细的图增强策略和样本选择的挑战

Method: 提出了一种基于最优传输的自监督异构图神经网络方法（HGOT），通过设计聚合视图和引入最优传输计划以简化正负样本选择过程

Result: 在四个真实世界数据集上的实验表明，HGOT在各种下游任务上可实现最先进的性能，尤其是在节点分类任务中提高了平均超过6%的准确率

Conclusion: HGOT方法在无需图增强策略的情况下有效促进了异构图的自监督学习，提高了表示学习质量

Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [219] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou,Ferrante Neri,Yew-Soon Ong,Ruibin Bai*

Main category: cs.LG

TL;DR: 提出了一种利用连体网络的轻量级代理模型SiamNAS，用以优化神经网络架构搜索，在降低计算成本的同时，成功识别出帕累托最优解，并在多任务优化中展示了潜力。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络架构搜索（NAS）需要在精确度、参数数量和计算成本之间进行多目标平衡，这使得NAS在计算上非常昂贵，需要高效的近似方法才能解决。

Method: 提出了一种新颖的代理建模方法，利用一组连体网络模块预测候选架构之间的支配关系。该代理模型轻量、易于训练，替代了在幸存选择策略中使用的拥挤距离计算，采用基于模型大小的启发式规则。

Result: 在NAS-Bench-201上的实验表明，该框架能够以显著降低的计算成本识别出帕累托最优解。SiamNAS识别出包含NAS-Bench-201中CIFAR-10最佳架构和ImageNet第二佳架构的最终非支配集合，其测试错误率仅在0.01 GPU天内完成。

Conclusion: 这项概念验证研究强调了所提出的连体网络代理模型在多任务优化中推广的潜力，并能在任务间进行同时优化。这还为生成多个帕累托集合提供了可能性，从而为异质任务设置提供多样化的帕累托最优解。

Abstract: Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [220] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/abs/2506.02630)
*Tom Jacobs,Advait Gadhikar,Celia Rubio-Madrigal,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 提出了一种新的优化方法HAM，通过双曲镜步骤提升深度学习模型性能，并在多项任务上取得了良好的实验效果。


<details>
  <summary>Details</summary>
Motivation: 优化算法的隐性偏置在深度学习模型泛化行为解释中占据重要位置。尤其是过参数化引起的双曲隐性偏置尽管有效促进稀疏性，但会导致有效学习率降低，使收敛速度变慢。HAM方法的提出旨在解决这一问题，改善算法收敛及学习性能。

Method: 提出HAM（Hyperbolic Aware Minimization），通过交替进行优化步骤和新的双曲镜步骤，结合Riemannian梯度流和梯度下降实现。此方法与自然梯度下降相关，并对其在线性回归中的隐性偏置进行精确刻画。

Result: HAM通过实验验证了其性能提升效果，并能与不同稀疏化方法结合，超越现有技术水平。其双曲步骤所需计算和内存开销较小，即使在小批量情况下也能成功运行。

Conclusion: HAM（Hyperbolic Aware Minimization）方法在处理过参数化模型时能有效提升性能。通过引入双曲镜步骤，改善了算法的收敛性和特征学习效果，并能轻松与现有优化器结合使用。实验证明了HAM在视觉、图形及节点分类、大型语言模型微调等多项任务中具有优异表现。

Abstract: Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [221] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/abs/2506.02654)
*Shiyu Shen,Bin Pan,Guirong Xue*

Main category: cs.LG

TL;DR: TrafficPPT通过结合多种数据源进行城市交通量预测，具备不确定性意识，并在多个城市场景的模拟数据上预训练，通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决交通数据不完整和城市间模型泛化性不足的问题，引入TrafficPPT这一工具。

Method: 引入TrafficPPT，一种预训练的概率Transformer，通过模拟多个城市场景的大规模数据进行预训练，然后在目标城市进行微调，以适应特定领域。

Result: TrafficPPT在极端数据稀疏的条件下，超过了最新的基线效果。

Conclusion: TrafficPPT提供了一种有效的方法来处理不完整和偏倚的交通数据，通过利用多种异构数据源，模型在不同的城市背景下表现出色。

Abstract: City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [222] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/abs/2506.02665)
*Tianci Liu,Tong Yang,Quan Zhang,Qi Lei*

Main category: cs.LG

TL;DR: 提出一种嵌入可见水印的方法，以增强版权保护的长期鲁棒性和有效性，实验结果表现优越。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的进步，版权内容面临着被未经授权使用的风险，为了建立长久的保护机制，提出一种嵌入可见且难以去除水印的通用方法。

Method: 本研究提出了一种基于概率和逆问题的新框架，旨在最大化原始内容与最优重构之间的差异，并开发了一种有效且高效的近似算法来避免复杂的二级优化。

Result: 实验结果证明该方法有效，能够在不同场景下提供更好的鲁棒性和保护。

Conclusion: 实验结果表明，该方法在多个场景中表现出优越性。

Abstract: As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [223] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/abs/2506.02694)
*Daichi Kimura,Tomonori Izumitani,Hisashi Kashima*

Main category: cs.LG

TL;DR: 引入基于Chatterjee秩相关系数的新型注意力机制XicorAttention，显著提升时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力机制可能无法充分捕捉时间序列数据中固有的非线性依赖关系，因此需要改进。

Method: 提出了一种基于Chatterjee秩相关系数的新型注意力机制，使用SoftSort和SoftRank引入可微近似，以取代标准注意力机制中的矩阵乘法来测量查询和键之间的关系。

Result: 实验证明，与现有模型相比，在真实数据集上融入非线性相关性显著提高了预测准确性，最多可提升约9.1%。

Conclusion: 提出的XicorAttention机制能够有效捕捉时间序列中的非线性依赖性，提高了Transformer模型的预测性能。

Abstract: Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [224] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/abs/2506.02703)
*Khizar Hayat,Baptiste Magnier*

Main category: cs.LG

TL;DR: 研究揭示信用卡欺诈检测中的方法评估缺陷，强调评估方法比模型复杂性更重要。


<details>
  <summary>Details</summary>
Motivation: 揭示信用卡欺诈检测研究中的基本评估缺陷，强调方法论严谨性的必要性。

Method: 通过故意试验不当评估协议，分析四个主要问题：数据泄露、方法论报告模糊、时间验证不足及指标优化的缺陷。

Result: 一个简单的神经网络架构由于数据泄露超过许多复杂方法，达到99.9%召回率，揭示了评估方法论的重要性。

Conclusion: 研究表明，在信用卡欺诈检测研究中，恰当的方法论评估比模型复杂性更重要。

Abstract: This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [225] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/abs/2506.02712)
*Jayadev Naram,Fredrik Hellström,Ziming Wang,Rebecka Jörnsten,Giuseppe Durisi*

Main category: cs.LG

TL;DR: 提出了一种基于部分最优传输的PDA新算法WARMPOT，提供了新权重方案并且实验表明其具竞争力。


<details>
  <summary>Details</summary>
Motivation: 在很多实际应用中，目标数据集的标记数据稀缺，而相关源数据集的标记数据丰富。当目标标签空间是源标签空间的一个子集时，就形成了部分域适配（PDA）的问题。现有的方法通常没有理论支持的权重分配，本研究旨在为此提供理论基础。

Method: 该研究通过部分最优传输（partial optimal transport）推导出了用于PDA问题的泛化界，并使用部分Wasserstein距离作为域对齐项，提出了经验源损失权重的理论化表达。随后，受这些泛化界的启发，研究人员开发了一个名为WARMPOT的实用算法。

Result: 该研究的理论结果验证了利用部分Wasserstein距离作为域对齐项的有效性，并为经验源损失权重提供了理论化表达。实际算法WARMPOT通过大量数值实验证明了其优越性，尤其是在提升权重分配方案上的表现。

Conclusion: 该研究提出了一种名为WARMPOT的新算法，可在部分域适配问题中有效地分配权重。实验结果显示，WARMPOT算法具有竞争力，并通过改进权重方案来提升性能。

Abstract: In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [226] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)
*Guanzhong Chen,Shaoxiong Yang,Chao Li,Wei Liu,Jian Luan,Zenglin Xu*

Main category: cs.LG

TL;DR: 提出MHGPO算法，通过消除Critic网络解决多智能体强化学习中的挑战，在LLM搜索系统中表现优于MAPPO。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在实际应用中面临固定知识截止和生成可控、准确输出的困难。多智能体系统（MAS）和强化学习（尤其是多智能体强化学习，MARL）提供了一种解决方案，但现有MARL算法依赖Critic网络，导致训练不稳定和计算负担。

Method: 提出了一种新的无Critic网络的算法——Multi-Agent Heterogeneous Group Policy Optimization (MHGPO)，通过估计异质组的相对奖励优势来指导策略更新。引入了三种组回滚采样策略，以在效率和效果之间进行权衡。

Result: MHGPO通过消除Critic网络来增强稳定性并降低计算负担，在实验中展示了其在多智能体LLM搜索系统中的优越性能和计算效率。

Conclusion: MHGPO在多智能体LLM搜索系统中的性能和计算效率优于MAPPO，不需要预热，展现了其在复杂LLM多智能体系统中稳定和可扩展优化的潜力。

Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [227] [Adversarial control of synchronization in complex oscillator networks](https://arxiv.org/abs/2506.02403)
*Yasutoshi Nagahama,Kosuke Miyazato,Kazuhiro Takemoto*

Main category: nlin.AO

TL;DR: 提出了一种基于梯度的对抗性优化方法，通过小的相位扰动控制Kuramoto振荡器网络的同步，在不同网络上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性攻击（来自深度学习的概念）如何通过精心设计的最小扰动来控制同步动力学。提出了一种优化方法来显著增强或抑制Kuramoto振荡器网络的集体同步。

Method: 我们提出了一种基于梯度的优化方法，通过计算秩参数相对于振荡器相位的梯度来确定最佳扰动方向，以实现同步控制。

Result: 实验证明，极小的相位扰动可以显著控制网络的同步。同步增强在各种网络大小中都可实现，抑制效果在较大网络中特别明显且效果随网络规模有利扩展。经过对标准模型网络和真实世界网络的系统验证，该对抗性框架为通过深度学习概念管理网络动力系统的同步提供了一种新范例。

Conclusion: 通过对网络振荡器施加极小的相位扰动，可以在不同的网络架构上实现显著的同步控制。

Abstract: This study investigates adversarial attacks, a concept from deep learning,
designed to control synchronization dynamics through strategically crafted
minimal perturbations. We propose a gradient-based optimization method that
identifies small phase perturbations to dramatically enhance or suppress
collective synchronization in Kuramoto oscillator networks. Our approach
formulates synchronization control as an adversarial optimization problem,
computing gradients of the order parameter with respect to oscillator phases to
determine optimal perturbation directions. Results demonstrate that extremely
small phase perturbations applied to network oscillators can achieve
significant synchronization control across diverse network architectures. Our
analysis reveals that synchronization enhancement is achievable across various
network sizes, while synchronization suppression becomes particularly effective
in larger networks, with effectiveness scaling favorably with network size. The
method is systematically validated on canonical model networks including
scale-free and small-world topologies, and real-world networks representing
power grids and brain connectivity patterns. This adversarial framework
represents a novel paradigm for synchronization management by introducing deep
learning concepts to networked dynamical systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [228] [FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs](https://arxiv.org/abs/2506.01969)
*Pencuo Zeren,Qiuming Luo,Rui Mao,Chang Kong*

Main category: cs.DC

TL;DR: FlashMLA-ETAP框架通过高效转置注意力管道提高了MLA推理性能，特别是在资源受限条件下。


<details>
  <summary>Details</summary>
Motivation: 解决在单个多GPU服务器上运行DeepSeek-R1 671B模型的挑战，特别是在资源受限的推理中提供可扩展的解决方案。

Method: 提出了高效转置注意力管道（ETAP），通过转置操作重新配置注意力计算，以减少冗余计算。

Result: 在64K序列长度（批处理大小为16）上，FlashMLA-ETAP相较于FlashMLA实现了2.78倍的速度提升，并在数值稳定性上具有显著提高。

Conclusion: 该论文提出了一种新方法FlashMLA-ETAP，通过重新配置注意力计算来提高MLA推理性能，特别是在单实例部署场景下有效。

Abstract: Efficient inference of Multi-Head Latent Attention (MLA) is challenged by
deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper
introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the
single-instance deployment scenario on NVIDIA H20 GPUs. We propose the
Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention
computation through transposition to align the KV context length with the
\(M\)-dimension in WGMMA operations, significantly reducing redundant
computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K
sequence length (batch size 16), with 5.24x and 4.94x improvements over
FlashAttention-3 and FlashInfer, respectively, while maintaining numerical
stability with a 15.2x lower RMSE (\(1.25 \times 10^{-5}\)) than
FlashAttention-3. Furthermore, ETAP's design enables seamless integration into
frameworks like FlashAttention-3 and FlashInfer, supported by a detailed
theoretical analysis. Our work addresses a critical gap in resource-constrained
inference, offering a scalable solution for mid-tier GPUs and paving the way
for broader adoption in hardware-aware optimization. Code is available at
https://github.com/pengcuo/FlashMLA-ETAP.

</details>


### [229] [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)
*Yuhao Shen,Junyi Shen,Quan Kong,Tianyu Liu,Yao Lu,Cong Wang*

Main category: cs.DC

TL;DR: SpecBranch框架通过并行推测分支和改进的草稿策略，显著加速LLM推理并减少回滚，提高实际应用的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的SD方法由于串行执行而受到限制，导致草稿和目标模型间出现互相等待的“气泡”。该论文旨在解决这一问题，提高模型推理速度。

Method: 首先对SD中的分支并行潜力进行深入分析，然后通过引入并行推测分支，以及自适应草稿长度和模型特征再利用的混合组合提高并行性。

Result: SpecBranch实现了1.8到4.5倍的速度提升，并减少了50%的回滚令牌，尤其在对齐较差的模型中表现出色。

Conclusion: SpecBranch框架通过引入并行的推测分支并联合编排自适应草稿长度显著提高LLM推理速度，并减少回滚令牌，提高了实际部署的适用性。

Abstract: Recently, speculative decoding (SD) has emerged as a promising technique to
accelerate LLM inference by employing a small draft model to propose draft
tokens in advance, and validating them in parallel with the large target model.
However, the existing SD methods still remain fundamentally constrained by
their serialized execution, which causes the mutual waiting bubbles between the
draft and target models. To address this challenge, we draw inspiration from
branch prediction in modern processors and propose a novel framework
\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first
take an in-depth analysis of the potential of branch parallelism in SD, and
recognize that the key challenge lies in the trade-offs between parallelization
and token rollback. Based on the analysis, we strategically introduce parallel
speculative branches to preemptively hedge against likely rejections.
Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft
lengths with a hybrid combination of the implicit draft model confidence and
explicit reusing of target model features. Extensive experiments across various
models and benchmarks show that SpecBranch achieves over \textbf{1.8}$\times
\sim$ \textbf{4.5}$\times$ speedups against the auto-regressive decoding and
reduces rollback tokens by $\textbf{50}$\% for poorly aligned models, realizing
its applicability for real-world deployments.

</details>


### [230] [eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems](https://arxiv.org/abs/2506.02007)
*Ruilin Xu,Zongxuan Xie,Pengfei Chen*

Main category: cs.DC

TL;DR: eACGM是基于eBPF的AI/ML系统监控框架，通过实时数据收集和使用GMM分析性能指标，有效识别系统性能异常，支持大规模系统优化。


<details>
  <summary>Details</summary>
Motivation: 为了监控AI/ML系统中的关键硬件组件和软件栈性能，无需对代码进行修改，并能够快速诊断系统瓶颈和异常行为。

Method: 使用eBPF基础构建的eACGM框架，收集来自GPU、网络通信层以及软件栈（如CUDA、Python和PyTorch）的实时性能数据，并通过Gaussian Mixture Model对多维性能指标进行统计建模和聚类分析。

Result: eACGM在多节点分布式训练场景中完成了大规模实证研究和案例分析，证明其在监控模型训练和推理期间的性能异常方面具有稳定的表现。

Conclusion: eACGM提供了一种有效的性能优化和故障诊断工具，在大型AI/ML系统中具有很强的实际应用价值和可扩展性。

Abstract: We present eACGM, a full-stack AI/ML system monitoring framework based on
eBPF. eACGM collects real-time performance data from key hardware components,
including the GPU and network communication layer, as well as from key software
stacks such as CUDA, Python, and PyTorch, all without requiring any code
instrumentation or modifications. Additionally, it leverages libnvml to gather
process-level GPU resource usage information. By applying a Gaussian Mixture
Model (GMM) to the collected multidimensional performance metrics for
statistical modeling and clustering analysis, eACGM effectively identifies
complex failure modes, such as latency anomalies, hardware failures, and
communication inefficiencies, enabling rapid diagnosis of system bottlenecks
and abnormal behaviors.
  To evaluate eACGM's effectiveness and practicality, we conducted extensive
empirical studies and case analyses in multi-node distributed training
scenarios. The results demonstrate that eACGM, while maintaining a
non-intrusive and low-overhead profile, successfully captures critical
performance anomalies during model training and inference. Its stable anomaly
detection performance and comprehensive monitoring capabilities validate its
applicability and scalability in real-world production environments, providing
strong support for performance optimization and fault diagnosis in large-scale
AI/ML systems.

</details>


### [231] [Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling](https://arxiv.org/abs/2506.02025)
*Prachi Jadhav,Hongwei Jin,Ewa Deelman,Prasanna Balaprakash*

Main category: cs.DC

TL;DR: 提出了一种LLM调度器，能有效平衡HPC调度中的多种目标，但存在计算效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的HPC作业调度方法存在缺乏适应性的问题，尤其在面对动态工作负载和异构系统时。

Method: 提出了一种基于大型语言模型(LLM)的调度器，使用ReAct风格框架，具有迭代、可解释的决策能力。

Result: 在七种真实世界的HPC工作负载场景中评估，结果显示LLM调度在多目标平衡上表现优异，同时具备透明推理能力。

Conclusion: LLM调度能够处理多目标优化问题，但在计算效率方面的限制需引起注意。

Abstract: High-Performance Computing (HPC) job scheduling involves balancing
conflicting objectives such as minimizing makespan, reducing wait times,
optimizing resource use, and ensuring fairness. Traditional methods, including
heuristic-based (e.g., First-Come-First-Served) or intensive optimization
techniques, often lack adaptability to dynamic workloads and heterogeneous HPC
systems. To address this, we propose a novel Large Language Model (LLM)-based
scheduler using a ReAct-style framework (Reason + Act), enabling iterative,
interpretable decision-making. The system incorporates a scratchpad memory to
track scheduling history and refine decisions via natural language feedback,
while a constraint enforcement module ensures feasibility and safety. We
evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across
seven real-world HPC workload scenarios, including heterogeneous mixes, bursty
patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,
and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling
effectively balances multiple objectives while offering transparent reasoning
through natural language traces. The method excels in constraint satisfaction
and adapts to diverse workloads without domain-specific training. However, a
trade-off between reasoning quality and computational overhead challenges
real-time deployment. This work presents the first comprehensive study of
reasoning-capable LLMs for HPC scheduling, demonstrating their potential to
handle multiobjective optimization while highlighting limitations in
computational efficiency. The findings provide insights into leveraging
advanced language models for complex scheduling problems in dynamic HPC
environments.

</details>


### [232] [EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration](https://arxiv.org/abs/2506.02049)
*Beichen Huang,Ran Cheng,Kay Chen Tan*

Main category: cs.DC

TL;DR: EvoGit是一个去中心化的多智能体框架，使用自主代码演化进行协作软件开发，并在实验中成功产生功能性软件。


<details>
  <summary>Details</summary>
Motivation: 开发一个去中心化的框架，利用自主代码演化进行协作软件开发。

Method: EvoGit采用分散多智能体框架，通过Git库中的系统跟踪版本历史进行异步协调。

Result: EvoGit成功在两个实际任务中自动生成功能性和模块化的软件构件。

Conclusion: EvoGit展示了在软件开发中采用分散、自动化和持续发展的新范式的潜力。

Abstract: We introduce EvoGit, a decentralized multi-agent framework for collaborative
software development driven by autonomous code evolution. EvoGit deploys a
population of independent coding agents, each proposing edits to a shared
codebase without centralized coordination, explicit message passing, or shared
memory. Instead, all coordination emerges through a Git-based phylogenetic
graph that tracks the full version lineage and enables agents to asynchronously
read from and write to the evolving code repository. This graph-based structure
supports fine-grained branching, implicit concurrency, and scalable agent
interaction while preserving a consistent historical record. Human involvement
is minimal but strategic: users define high-level goals, periodically review
the graph, and provide lightweight feedback to promote promising directions or
prune unproductive ones. Experiments demonstrate EvoGit's ability to
autonomously produce functional and modular software artifacts across two
real-world tasks: (1) building a web application from scratch using modern
frameworks, and (2) constructing a meta-level system that evolves its own
language-model-guided solver for the bin-packing optimization problem. Our
results underscore EvoGit's potential to establish a new paradigm for
decentralized, automated, and continual software development. EvoGit is
open-sourced at https://github.com/BillHuang2001/evogit.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [233] [Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers](https://arxiv.org/abs/2506.01994)
*Wanshan Cui,Yejin Jeong,Inwook Song,Gyuri Kim,Minsang Kwon,Donghun Lee*

Main category: cond-mat.soft

TL;DR: 通过多算法离群点检测和选择性重实验，提高数据集质量，减少聚合物材料性质预测中的误差。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型可能因实验数据中的离群点导致错误预测，影响新材料开发。

Method: 提出了一种通过整合多算法离群点检测与选择性重实验不可靠离群点的方法。

Result: 在多种机器学习模型中，方法减少了预测误差（RMSE），大幅提高了精度，仅需重测约5%的数据集。

Conclusion: 提高数据集质量对机器学习模型在聚合物科学中的可靠应用至关重要。

Abstract: Accurate prediction of polymer material properties through data-driven
approaches greatly accelerates novel material development by reducing redundant
experiments and trial-and-error processes. However, inevitable outliers in
empirical measurements can severely skew machine learning results, leading to
erroneous prediction models and suboptimal material designs. To address this
limitation, we propose a novel approach to enhance dataset quality efficiently
by integrating multi-algorithm outlier detection with selective
re-experimentation of unreliable outlier cases. To validate the empirical
effectiveness of the approach, we systematically construct a new dataset
containing 701 measurements of three key mechanical properties: glass
transition temperature ($T_g$), tan $\delta$ peak, and crosslinking density
($v_{c}$). To demonstrate its general applicability, we report the performance
improvements across multiple machine learning models, including Elastic Net,
SVR, Random Forest, and TPOT, to predict the three key properties. Our method
reliably reduces prediction error (RMSE) and significantly improves accuracy
with minimal additional experimental work, requiring only about 5% of the
dataset to be re-measured.These findings highlight the importance of data
quality enhancement in achieving reliable machine learning applications in
polymer science and present a scalable strategy for improving predictive
reliability in materials science.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [234] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)
*Wang Mengjie,Zhu Huiping,Li Jian,Shi Wenxiu,Zhang Song*

Main category: cs.CV

TL;DR: 本文提出了一种优化驾驶场景中的多模态模型的方法，包括动态提示优化等，实验表明该方法提高了模型准确性和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术的发展对理解复杂驾驶场景的能力提出了更高的要求。多模态通用大模型成为解决这一挑战的方案，但在纵向领域应用时涉及数据采集、模型训练和部署优化等困难。

Method: 提出了一种针对驾驶场景中的多模态模型优化的综合方法，包括动态提示优化、数据集构建、模型训练和部署。利用知识蒸馏、动态微调和量化等先进技术，减少存储和计算成本，同时提高性能。

Result: 实验结果表明，所提出的系统优化方法显著提高了模型在关键任务上的准确性，并实现了高效的资源利用。

Conclusion: 系统优化方法不仅显著提高了模型在关键任务上的准确性，而且实现了高效的资源利用，为驾驶场景感知技术的实际应用提供了有力支持。

Abstract: With the advancement of autonomous and assisted driving technologies, higher
demands are placed on the ability to understand complex driving scenarios.
Multimodal general large models have emerged as a solution for this challenge.
However, applying these models in vertical domains involves difficulties such
as data collection, model training, and deployment optimization. This paper
proposes a comprehensive method for optimizing multimodal models in driving
scenarios, including cone detection, traffic light recognition, speed limit
recommendation, and intersection alerts. The method covers key aspects such as
dynamic prompt optimization, dataset construction, model training, and
deployment. Specifically, the dynamic prompt optimization adjusts the prompts
based on the input image content to focus on objects affecting the ego vehicle,
enhancing the model's task-specific focus and judgment capabilities. The
dataset is constructed by combining real and synthetic data to create a
high-quality and diverse multimodal training dataset, improving the model's
generalization in complex driving environments. In model training, advanced
techniques like knowledge distillation, dynamic fine-tuning, and quantization
are integrated to reduce storage and computational costs while boosting
performance. Experimental results show that this systematic optimization method
not only significantly improves the model's accuracy in key tasks but also
achieves efficient resource utilization, providing strong support for the
practical application of driving scenario perception technologies.

</details>


### [235] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/abs/2506.02021)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 通过动态感知视频蒸馏（DAViD），我们克服了视频数据集的时间信息冗余问题，展示了视频语义自适应分辨率的方法，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集蒸馏方法假定视频语义的时间冗余水平是统一的，这限制了这些方法在视频数据集上的效果。

Method: 我们提出了一种名为动态感知视频蒸馏（DAViD）的强化学习方法，用于预测合成视频的最佳时间分辨率。

Result: 我们的方法显著优于现有的数据集蒸馏方法，并显示出性能的重大改进。

Conclusion: 我们的研究显著优于现有的数据集蒸馏方法，并显示出性能的重大改进。

Abstract: With the rapid development of vision tasks and the scaling on datasets and
models, redundancy reduction in vision datasets has become a key area of
research. To address this issue, dataset distillation (DD) has emerged as a
promising approach to generating highly compact synthetic datasets with
significantly less redundancy while preserving essential information. However,
while DD has been extensively studied for image datasets, DD on video datasets
remains underexplored. Video datasets present unique challenges due to the
presence of temporal information and varying levels of redundancy across
different classes. Existing DD approaches assume a uniform level of temporal
redundancy across all different video semantics, which limits their
effectiveness on video datasets. In this work, we propose Dynamic-Aware Video
Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the
optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop
reward function is proposed to update the RL agent policy. To the best of our
knowledge, this is the first study to introduce adaptive temporal resolution
based on video semantics in video dataset distillation. Our approach
significantly outperforms existing DD methods, demonstrating substantial
improvements in performance. This work paves the way for future research on
more efficient and semantic-adaptive video dataset distillation research.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [236] [No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction](https://arxiv.org/abs/2506.02039)
*Haoshuai Zhou,Changgeng Mo,Boxuan Cao,Linkai Li,Shan Xiang Wang*

Main category: eess.AS

TL;DR: 本研究提出了支持样本智能预测网络（SSIPNet），用于个性化语音理解能力预测，在少量支持样本情况下表现优于听力图预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要依赖听力图来评估听力，这种方法受限于仅能捕捉听众对纯音的听阈。为提高预测准确性，本研究提出了一种不依赖额外特征的预测方法。

Method: 使用支持样本智能预测网络（SSIPNet），通过利用多组支持样本构建听众语音识别能力的高维表示，以实现未见音频的准确预测。

Result: 在Clarity Prediction Challenge数据集上的结果表明，SSIPNet能够基于少量支持数据实现对未见音频的高准确度预测，且优于基于听力图的方法。

Conclusion: 本研究提出了一种新的个性化语音理解能力预测方法，即支持样本智能预测网络（SSIPNet），其能够基于少量的支持音频和得分对，准确预测未见音频的识别能力，并优于传统的基于听力图的预测方法。

Abstract: Personalized speech intelligibility prediction is challenging. Previous
approaches have mainly relied on audiograms, which are inherently limited in
accuracy as they only capture a listener's hearing threshold for pure tones.
Rather than incorporating additional listener features, we propose a novel
approach that leverages an individual's existing intelligibility data to
predict their performance on new audio. We introduce the Support Sample-Based
Intelligibility Prediction Network (SSIPNet), a deep learning model that
leverages speech foundation models to build a high-dimensional representation
of a listener's speech recognition ability from multiple support (audio, score)
pairs, enabling accurate predictions for unseen audio. Results on the Clarity
Prediction Challenge dataset show that, even with a small number of support
(audio, score) pairs, our method outperforms audiogram-based predictions. Our
work presents a new paradigm for personalized speech intelligibility
prediction.

</details>


### [237] [Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data](https://arxiv.org/abs/2506.02078)
*Emmy Postma,Cristian Tejedor-Garcia*

Main category: eess.AS

TL;DR: 本研究评估了三种音频嵌入模型在PD分类中的表现，发现OpenL3在关键声学特征捕获方面表现最佳，但也揭示了性别偏见和异常语音模式处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于个体讲话差异，深层声学特征在PD分类中的效果差异大，激发研究者探索使用音频数据进行PD诊断技术的发展。

Method: 研究采用三种预训练音频嵌入模型（OpenL3, VGGish 和 Wav2Vec2.0）进行PD分类，并使用NeuroVoz数据集进行评估。

Result: OpenL3在DDK和LR任务中表现突出，Wav2Vec2.0显现性别偏见。错误分类揭示了异常语音模式的挑战，强调了改进特征提取和模型鲁棒性的必要性。

Conclusion: OpenL3在PD检测的DDK和LR任务中表现最佳，而Wav2Vec2.0在男声中表现较好，但存在性别偏见。

Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),
motivating the development of diagnostic techniques using speech data for
clinical applications. Although deep acoustic features have shown promise for
PD classification, their effectiveness often varies due to individual speaker
differences, a factor that has not been thoroughly explored in the existing
literature. This study investigates the effectiveness of three pre-trained
audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.
Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)
and listen and repeat (LR) tasks, capturing critical acoustic features for PD
detection. Only Wav2Vec2.0 shows significant gender bias, achieving more
favorable results for male speakers, in DDK tasks. The misclassified cases
reveal challenges with atypical speech patterns, highlighting the need for
improved feature extraction and model robustness in PD detection.

</details>


### [238] [Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge](https://arxiv.org/abs/2506.02080)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

TL;DR: 提出了一种新的基于替换感知的无对齐GOP方法，通过限制音素替换提高了计算效率，并在多个数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的GOP方法依赖于强制对齐，由于声学变异性，容易出现标记和分割错误。无对齐方法虽然解决了这些问题，但计算昂贵且在音素序列长度和库存规模方面扩展性差。为了提高效率，我们提出了新的无对齐GOP方法。

Method: 引入了一种基于替换感知的无对齐GOP方法，通过限制基于音素簇和常见学习者错误的音素替换，来提高计算效率。

Result: 在两个L2英语语音数据集上评估了新方法，结果显示限制音素替换的无对齐方法优于基线，并在某些设置下优于不限制音素替换的方法。

Conclusion: 本文介绍了一种新的基于替换感知的无对齐GOP方法，解决了传统GOP方法中的标签和分割错误问题，并在一定程度上提高了计算效率。实验结果表明，这种新方法能够在某些数据集上优于基线。

Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic
measures of pronunciation quality, such as the goodness of pronunciation (GOP)
metric. GOP relies on forced alignments, which are prone to labeling and
segmentation errors due to acoustic variability. While alignment-free methods
address these challenges, they are computationally expensive and scale poorly
with phoneme sequence length and inventory size. To enhance efficiency, we
introduce a substitution-aware alignment-free GOP that restricts phoneme
substitutions based on phoneme clusters and common learner errors. We evaluated
our GOP on two L2 English speech datasets, one with child speech, My
Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult
speech. We compared RPS (restricted phoneme substitutions) and UPS
(unrestricted phoneme substitutions) setups within alignment-free methods,
which outperformed the baseline. We discuss our results and outline avenues for
future research.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [239] [Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders](https://arxiv.org/abs/2506.02051)
*Hui Liu,Shiye Tian,Xuejun Liu*

Main category: q-bio.BM

TL;DR: SmilesGEN通过结合药物和表达谱VAE模型生成具潜在治疗效果的分子，效果优于当前模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成分子时多依赖表达谱，而忽略了分子对细胞环境的干扰效应。为克服此局限，提出了SmilesGEN模型。

Method: 本文基于变分自编码器（VAE）架构设计了SmilesGEN生成模型，将预训练药物VAE（SmilesNet）与表达谱VAE（ProfileNet）相结合，实现药物扰动和转录响应的共同潜空间建模。

Result: 实验证明SmilesGEN在生成有效性、独特性、新颖性较高的分子方面优于现有模型，并在脚手架优化和治疗剂生成上表现突出，与已批准药物的相似性更高。

Conclusion: SmilesGEN框架利用基因签名生成具有药物潜在的分子，展示了在细胞表型改变中的前景。

Abstract: The de novo generation of drug-like molecules capable of inducing desirable
phenotypic changes is receiving increasing attention. However, previous methods
predominantly rely on expression profiles to guide molecule generation, but
overlook the perturbative effect of the molecules on cellular contexts. To
overcome this limitation, we propose SmilesGEN, a novel generative model based
on variational autoencoder (VAE) architecture to generate molecules with
potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE
(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the
interplay between drug perturbations and transcriptional responses in a common
latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment
expression profiles when eliminating drug-induced perturbations in the latent
space, while SmilesNet is informed by desired expression profiles to generate
drug-like molecules. Our empirical experiments demonstrate that SmilesGEN
outperforms current state-of-the-art models in generating molecules with higher
degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity
to known ligands targeting the relevant proteins. Moreover, we evaluate
SmilesGEN for scaffold-based molecule optimization and generation of
therapeutic agents, and confirmed its superior performance in generating
molecules with higher similarity to approved drugs. SmilesGEN establishes a
robust framework that leverages gene signatures to generate drug-like molecules
that hold promising potential to induce desirable cellular phenotypic changes.

</details>


### [240] [Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications](https://arxiv.org/abs/2506.02052)
*Shuo Yan,Yuliang Yan,Bin Ma,Chenao Li,Haochun Tang,Jiahua Lu,Minhua Lin,Yuyuan Feng,Hui Xiong,Enyan Dai*

Main category: q-bio.BM

TL;DR: 本文引入了Protap基准，比较不同模型和策略在多种蛋白质应用中的表现，发现小规模训练有监督编码器表现优于大规模预训练，而结构信息和领域知识可以提高某些任务的性能。


<details>
  <summary>Details</summary>
Motivation: 为了通过比较不同深度学习架构、预训练策略和领域特定模型，以支持下游蛋白质应用，并填补现有基准中缺失的工业相关专门任务。

Method: 引入了一种名为Protap的综合基准，系统地比较不同骨干架构、预训练策略和领域特定模型在多种真实的下游蛋白应用中的表现。

Result: 在Protap基准测试中，大规模预训练编码器虽然表现出色，但在小规模训练集上不如有监督训练的编码器。此外，在下游微调过程中引入结构信息可以与大规模序列语料库预训练的蛋白语言模型匹敌，甚至超越。领域特定的生物学先验对专业下游任务的表现有增强作用。

Conclusion: 尽管大规模预训练编码器取得了优秀的结果，但在小规模下游训练集上往往不如有监督编码器表现好。将结构信息引入下游微调过程中可以匹配甚至优于预训练在大规模序列语料库上的蛋白语言模型。领域特定的生物学先验可以提高在专业化下游任务上的性能。

Abstract: Recently, extensive deep learning architectures and pretraining strategies
have been explored to support downstream protein applications. Additionally,
domain-specific models incorporating biological knowledge have been developed
to enhance performance in specialized tasks. In this work, we introduce
$\textbf{Protap}$, a comprehensive benchmark that systematically compares
backbone architectures, pretraining strategies, and domain-specific models
across diverse and realistic downstream protein applications. Specifically,
Protap covers five applications: three general tasks and two novel specialized
tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted
protein degradation, which are industrially relevant yet missing from existing
benchmarks. For each application, Protap compares various domain-specific
models and general architectures under multiple pretraining settings. Our
empirical studies imply that: (i) Though large-scale pretraining encoders
achieve great results, they often underperform supervised encoders trained on
small downstream training sets. (ii) Incorporating structural information
during downstream fine-tuning can match or even outperform protein language
models pretrained on large-scale sequence corpora. (iii) Domain-specific
biological priors can enhance performance on specialized downstream tasks. Code
and datasets are publicly available at
https://github.com/Trust-App-AI-Lab/protap.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [241] [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
*David Sasu,Kweku Andoh Yamoah,Benedict Quartey,Natalie Schluter*

Main category: cs.RO

TL;DR: 提出一种利用语音韵律特征推断指令意图的方法，在人机交流中取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 机器人准确解释和执行口头语言指令对于有效的人机协作至关重要。传统方法依赖语音识别将语音转录为文本，通常会丢弃用于解歧义意图的重要韵律线索。

Method: 我们的方法直接利用语音的韵律特征来推断和解决指令意图，并通过情境学习将预测意图集成到大型语言模型中以消除模糊和选择适当的任务计划。

Result: 我们的方法在检测话语中的指称意图方面达到了95.79%的准确率，在判定模糊指令的预期任务计划时达到了71.96%的准确率。

Conclusion: 我们提出的方法不仅在检测话语中的指称意图上取得了95.79%的准确率，而且在判定模糊指令的预期任务计划时达到了71.96%的准确率，展示了其在显著改善人机交流方面的潜力。

Abstract: Enabling robots to accurately interpret and execute spoken language
instructions is essential for effective human-robot collaboration. Traditional
methods rely on speech recognition to transcribe speech into text, often
discarding crucial prosodic cues needed for disambiguating intent. We propose a
novel approach that directly leverages speech prosody to infer and resolve
instruction intent. Predicted intents are integrated into large language models
via in-context learning to disambiguate and select appropriate task plans.
Additionally, we present the first ambiguous speech dataset for robotics,
designed to advance research in speech disambiguation. Our method achieves
95.79% accuracy in detecting referent intents within an utterance and
determines the intended task plan of ambiguous instructions with 71.96%
accuracy, demonstrating its potential to significantly improve human-robot
communication.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [242] [The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era](https://arxiv.org/abs/2506.02027)
*Shriphani Palakodety*

Main category: cs.CR

TL;DR: ULIs are outdated in the AI era due to privacy risks; a new cryptographic framework is proposed to address this issue while maintaining existing workflows.


<details>
  <summary>Details</summary>
Motivation: To address the systemic privacy risks posed by ULIs in the AI era, as traditional safeguards are insufficient.

Method: The paper presents a cryptographic framework designed for identity systems in the AI era, which retains compatibility with existing identifier workflows.

Result: The proposed cryptographic framework supports essential functions such as auditability and delegation, and provides a practical migration path beyond ULIs.

Conclusion: Universal Lifelong Identifiers (ULIs) must be phased out as they pose systemic privacy risks and are incompatible with the AI era. Instead, a new cryptographic framework is needed for identity systems.

Abstract: Many identity systems assign a single, static identifier to an individual for
life, reused across domains like healthcare, finance, and education. These
Universal Lifelong Identifiers (ULIs) underpin critical workflows but now pose
systemic privacy risks. We take the position that ULIs are fundamentally
incompatible with the AI era and must be phased out. We articulate a threat
model grounded in modern AI capabilities and show that traditional safeguards
such as redaction, consent, and access controls are no longer sufficient. We
define core properties for identity systems in the AI era and present a
cryptographic framework that satisfies them while retaining compatibility with
existing identifier workflows. Our design preserves institutional workflows,
supports essential functions such as auditability and delegation, and offers a
practical migration path beyond ULIs.

</details>


### [243] [Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges](https://arxiv.org/abs/2506.02032)
*Raj Patel,Himanshu Tripathi,Jasper Stone,Noorbakhsh Amiri Golilarz,Sudip Mittal,Shahram Rahimi,Vini Chaudhary*

Main category: cs.CR

TL;DR: 本文探讨了机器学习运维（MLOps）生态系统中的安全问题，特别关注其统一性带来的漏洞风险。通过应用MITRE ATLAS框架，系统分析了各个阶段的攻击方式，并提出了相应的缓解策略。研究强调了从一开始就实施强有力的安全协议的重要性，以保护MLOps免受网络攻击。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习技术的迅速普及，各行业组织需要有效的方法加速模型的开发到部署。同时，随着MLOps市场的增长，保护这些系统的安全性变得越来越关键。

Method: 本文系统应用MITRE ATLAS框架，以评估MLOps生态系统不同阶段的攻击，并通过红队演练和现实中的例子支持，提出了攻击技术和缓解策略的分类。

Result: 文章提出了一系列可行的早期防御措施，加强MLOps生态系统的安全性，并强调了迫切需要关注的关键研究空白。

Conclusion: 加强MLOps生态系统的安全性，从一开始实施强有力的协议来预防不断发展的网络攻击是至关重要的。

Abstract: The rapid adoption of machine learning (ML) technologies has driven
organizations across diverse sectors to seek efficient and reliable methods to
accelerate model development-to-deployment. Machine Learning Operations (MLOps)
has emerged as an integrative approach addressing these requirements by
unifying relevant roles and streamlining ML workflows. As the MLOps market
continues to grow, securing these pipelines has become increasingly critical.
However, the unified nature of MLOps ecosystem introduces vulnerabilities,
making them susceptible to adversarial attacks where a single misconfiguration
can lead to compromised credentials, severe financial losses, damaged public
trust, and the poisoning of training data. Our paper presents a systematic
application of the MITRE ATLAS (Adversarial Threat Landscape for
Artificial-Intelligence Systems) framework, a comprehensive and continuously
updated catalog of AI-focused attacks, to systematically assess attacks across
different phases of the MLOps ecosystem. We begin by examining the preparatory
phases during which adversaries acquire the essential intelligence required to
initiate their attacks. We then present a structured taxonomy of attack
techniques explicitly mapped to corresponding phases of the MLOps ecosystem,
supported by examples drawn from red-teaming exercises and real-world
incidents. This is followed by a taxonomy of mitigation strategies aligned with
these attack categories, offering actionable early-stage defenses to strengthen
the security of MLOps ecosystem. Given the rapid evolution and adoption of
MLOps, we further highlight key research gaps that require immediate attention.
Our work emphasizes the importance of implementing robust security protocols
from the outset, empowering practitioners to safeguard MLOps ecosystem against
evolving cyber attacks.

</details>


### [244] [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)
*Lajos Muzsai,David Imolai,András Lukács*

Main category: cs.CR

TL;DR: 本文通过引入“随机加密”挑战生成框架及GRPO优化方法，提高了大语言模型在网络安全任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在网络安全应用中面临的问题解决中结构化推理和工具辅助计算的挑战。

Method: 提出并使用“随机加密”作为密钥捕获挑战生成框架，通过引导强化提示优化（GRPO）对增强工具的Llama-3.1-8B进行微调，使代理可以在隔离的REPL中迭代编写和执行Python。

Result: GRPO使得未见过的“随机加密”任务在Pass@8上提升了53%（从0.35提升到0.88），并将Majority@8提升至0.41。此外，经过微调的代理在picoCTF加密问题子集上将Pass@8提高了13个百分点。

Conclusion: 本文提出的方法显著提高了大语言模型在“随机加密”任务中的Pass@8表现，并能够对外部数据集进行推广。

Abstract: Large Language Models (LLMs) still struggle with the structured reasoning and
tool-assisted computation needed for problem solving in cybersecurity
applications. In this work, we introduce "random-crypto", a cryptographic
Capture-the-Flag (CTF) challenge generator framework that we use to fine-tune a
tool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation
(GRPO), allowing the agent to iteratively write and execute Python inside an
isolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen
"random-crypto" tasks (0.35 -> 0.88) and raises Majority@8 to 0.41. The
fine-tuned agent also generalizes to an external dataset. On a subset of
picoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the
gains stem from more reliable tool invocation and code synthesis, rather than
superficial prompt adaptation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [245] [Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability](https://arxiv.org/abs/2506.02073)
*Mengliang He,Jiayi Zeng,Yankai Jiang,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.SE

TL;DR: 这项工作提出了Flow2Code，一个新的评估基准测试，实验表明当前LLMs无法完美进行流程图代码生成，但监督微调显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试忽略了基于流程图的代码生成，亟需一个适用于评估这种代码生成的基准测试。

Method: 进行多模态LLMs的实验以评估流程图生成代码的能力，并验证监督微调技术的影响。

Result: 实验结果显示现有的LLMs在流程图生成代码方面并不完美，但监督微调技术提升了模型表现。

Conclusion: 当前LLMs在基于流程图的代码生成方面表现有限，但通过监督微调技术可以显著提升性能。

Abstract: While large language models (LLMs) show promise in code generation, existing
benchmarks neglect the flowchart-based code generation. To promote further
research on flowchart-based code generation, this work presents Flow2Code, a
novel benchmark for flowchart-based code generation evaluation. The evaluation
dataset spans 15 programming languages and includes 5,622 code segments paired
with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive
experiments with 13 multimodal LLMs reveal that current LLMs can not generate
code based on flowcharts perfectly. Besides, experiment results show that the
supervised fine-tuning technique contributes greatly to the models'
performance. We publicly release our code and datasets at
https://github.com/hml-github/Flow2Code.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [246] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/abs/2506.01980)
*Lianhao Yin,Ozanan Meireles,Guy Rosman,Daniela Rus*

Main category: eess.IV

TL;DR: 提出了C2E自监督框架，有效提升手术视频理解，能够在无标签数据集上实现强大任务泛化，提升手术AI性能。


<details>
  <summary>Details</summary>
Motivation: 实时视频理解对微创手术中的指导过程至关重要，但监督学习方法需要大量标注数据集，而这些数据集因标注工作量巨大而稀缺，自监督方法能解决此问题，但当前方法常常无法捕获具有任务泛化性的结构和物理信息。

Method: 提出了Compress-to-Explore (C2E)自监督框架，利用Kolmogorov复杂性从手术视频中学习紧凑且信息丰富的表征，通过熵最大化解码器在压缩图像的同时保留临床相关细节，提升无标签数据的编码器性能。

Result: C2E在大规模未标记的手术数据集上训练后，展示出在多种手术机器学习任务中的强大泛化能力，包括工作流分类、工具组织交互分类、分割和诊断任务，且性能更优。

Conclusion: C2E模型展示了强大的泛化能力，能够提升手术视觉基础模型的性能，揭示了自监督学习在提升外科人工智能和微创手术结果中的潜力。

Abstract: Real-time video understanding is critical to guide procedures in minimally
invasive surgery (MIS). However, supervised learning approaches require large,
annotated datasets that are scarce due to annotation efforts that are
prohibitive, e.g., in medical fields. Although self-supervision methods can
address such limitations, current self-supervised methods often fail to capture
structural and physical information in a form that generalizes across tasks. We
propose Compress-to-Explore (C2E), a novel self-supervised framework that
leverages Kolmogorov complexity to learn compact, informative representations
from surgical videos. C2E uses entropy-maximizing decoders to compress images
while preserving clinically relevant details, improving encoder performance
without labeled data. Trained on large-scale unlabeled surgical datasets, C2E
demonstrates strong generalization across a variety of surgical ML tasks, such
as workflow classification, tool-tissue interaction classification,
segmentation, and diagnosis tasks, providing improved performance as a surgical
visual foundation model. As we further show in the paper, the model's internal
compact representation better disentangles features from different structural
parts of images. The resulting performance improvements highlight the yet
untapped potential of self-supervised learning to enhance surgical AI and
improve outcomes in MIS.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [247] [Music interpretation and emotion perception: A computational and neurophysiological investigation](https://arxiv.org/abs/2506.01982)
*Vassilis Lyberatos,Spyridon Kantarelis,Ioanna Zioga,Christina Anagnostopoulou,Giorgos Stamou,Anastasia Georgaki*

Main category: cs.HC

TL;DR: 该研究探讨音乐表演中的情感表达和感知，发现即兴演奏可增强观众情感反应和放松效果。


<details>
  <summary>Details</summary>
Motivation: 探讨在不同演奏环境和表达水平下，音乐表演者的情感交流及观众反应。

Method: 研究使用计算和神经生理学方法，结合音频分析和情感分析，以及神经生理测量。

Result: 表达性和即兴演奏展示了独特的声音特征，并产生了更强烈的情感反应，神经生理学测量表明即兴演奏中放松程度更高。

Conclusion: 表达性在音乐表现中的重要性显著，能够增强情感交流和观众参与。

Abstract: This study investigates emotional expression and perception in music
performance using computational and neurophysiological methods. The influence
of different performance settings, such as repertoire, diatonic modal etudes,
and improvisation, as well as levels of expressiveness, on performers'
emotional communication and listeners' reactions is explored. Professional
musicians performed various tasks, and emotional annotations were provided by
both performers and the audience. Audio analysis revealed that expressive and
improvisational performances exhibited unique acoustic features, while emotion
analysis showed stronger emotional responses. Neurophysiological measurements
indicated greater relaxation in improvisational performances. This multimodal
study highlights the significance of expressivity in enhancing emotional
communication and audience engagement.

</details>


### [248] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
*Takao Fujii,Katie Seaborn,Madeleine Steeds,Jun Kato*

Main category: cs.HC

TL;DR: 本研究探索声音和自称如何影响日语会话代理的性别化及身份感知，发现交叉性自称可规避性别化，且年龄和形式感知与性别化交织。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互中类似人类的会话代理出现，关于这类代理人类身份线索的伦理性已引发讨论，特别是身份中立性的假设受到了质疑。

Method: 研究通过众包研究方法，邀请204名日本参与者评价三种ChatGPT声音及其使用的七个自称用法，从而分析声音性别化等现象。

Result: 研究发现声音存在明显的性别化现象，而某些交叉性自称代词则能够通过中立性和含糊性实现对这种性别化的规避。同时，参与者对代理人的年龄和形式的感知也受到了这种性别化的影响。

Conclusion: 该研究表明，通过交叉性自称代词来传达中立和模糊的身份特征可能在消除声源性别化方面有效。同时，年龄和形式感知与性别化交织，与社会语言学理论相符。

Abstract: Conversational agents that mimic people have raised questions about the
ethics of anthropomorphizing machines with human social identity cues. Critics
have also questioned assumptions of identity neutrality in humanlike agents.
Recent work has revealed that intersectional Japanese pronouns can elicit
complex and sometimes evasive impressions of agent identity. Yet, the role of
other "neutral" non-pronominal self-referents (NPSR) and voice as a socially
expressive medium remains unexplored. In a crowdsourcing study, Japanese
participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and
Ember) using seven self-referents. We found strong evidence of voice gendering
alongside the potential of intersectional self-referents to evade gendering,
i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age
and formality intersected with gendering as per sociolinguistic theories,
especially boku and watakushi. This work provides a nuanced take on agent
identity perceptions and champions intersectional and culturally-sensitive work
on voice agents.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [249] [Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment](https://arxiv.org/abs/2506.02046)
*Mohammad Saleh Torkestani,Taha Mansouri*

Main category: cs.CY

TL;DR: 介绍了一种应对生成式人工智能在高等教育评估挑战的理论框架，通过静态分析和动态测试来评估评估方法的漏洞。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在高等教育评估中的应用带来了挑战，传统评估方法难以应对学生使用这些工具进行学术活动的现象。

Method: 提出了一种静态分析与动态测试相结合的双策略范式，包括设计一个综合理论框架进行评估漏洞评估。

Result: 不仅识别了生成式人工智能在高等教育评估中的弱点，还提出了一种评估漏洞评分的理论框架，以量化评估。

Conclusion: 传统的检测工具和评估方法有显著的局限性，需要创新的理论框架进行评估漏洞分析。

Abstract: This paper presents a theoretical framework for addressing the challenges
posed by generative artificial intelligence (AI) in higher education assessment
through a machine-versus-machine approach. Large language models like GPT-4,
Claude, and Llama increasingly demonstrate the ability to produce sophisticated
academic content, traditional assessment methods face an existential threat,
with surveys indicating 74-92% of students experimenting with these tools for
academic purposes. Current responses, ranging from detection software to manual
assessment redesign, show significant limitations: detection tools demonstrate
bias against non-native English writers and can be easily circumvented, while
manual frameworks rely heavily on subjective judgment and assume static AI
capabilities. This paper introduces a dual strategy paradigm combining static
analysis and dynamic testing to create a comprehensive theoretical framework
for assessment vulnerability evaluation. The static analysis component
comprises eight theoretically justified elements: specificity and
contextualization, temporal relevance, process visibility requirements,
personalization elements, resource accessibility, multimodal integration,
ethical reasoning requirements, and collaborative elements. Each element
addresses specific limitations in generative AI capabilities, creating barriers
that distinguish authentic human learning from AI-generated simulation. The
dynamic testing component provides a complementary approach through
simulation-based vulnerability assessment, addressing limitations in
pattern-based analysis. The paper presents a theoretical framework for
vulnerability scoring, including the conceptual basis for quantitative
assessment, weighting frameworks, and threshold determination theory.

</details>


### [250] [Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI](https://arxiv.org/abs/2506.02055)
*Nikola Balic*

Main category: cs.CY

TL;DR: 研究探讨了专业人士对自主多代理AI系统的看法，发现部署决策复杂且需更大样本，并建议组织解决合规问题和建立治理框架。


<details>
  <summary>Details</summary>
Motivation: 理解专业人士对自主多代理AI系统的看法对于预判其采纳挑战、伦理考量和未来劳动力发展至关重要。

Method: 通过对130位参与者关于AI代理能力、影响和治理的调查结果进行分析，结合初始逻辑回归模型，探索当前AI代理部署的相关因素。

Result: 研究发现了受访者的三种不同集群，初始逻辑回归模型未能产生统计显著的预测因子，表明部署决策复杂且可能受未完全捕捉的因素影响，或者需要更大的样本。

Conclusion: 组织需要解决合规问题（通常被认为是障碍之一）并在将自主代理集成到工作流程中时建立明确的治理框架。

Abstract: Autonomous multi-agent AI systems are poised to transform various industries,
particularly software development and knowledge work. Understanding current
perceptions among professionals is crucial for anticipating adoption
challenges, ethical considerations, and future workforce development. This
study analyzes responses from 130 participants to a survey on the capabilities,
impact, and governance of AI agents. We explore expected timelines for AI
replacing programmers, identify perceived barriers to deployment, and examine
beliefs about responsibility when agents make critical decisions. Key findings
reveal three distinct clusters of respondents. While the study explored factors
associated with current AI agent deployment, the initial logistic regression
model did not yield statistically significant predictors, suggesting that
deployment decisions are complex and may be influenced by factors not fully
captured or that a larger sample is needed. These insights highlight the need
for organizations to address compliance concerns (a commonly cited barrier) and
establish clear governance frameworks as they integrate autonomous agents into
their workflows.

</details>


### [251] [AI Data Development: A Scorecard for the System Card Framework](https://arxiv.org/abs/2506.02071)
*Tadesse K. Bahiru,Haileleol Tibebu,Ioannis A. Kakadiaris*

Main category: cs.CY

TL;DR: 论文引入评分卡评估AI数据集质量，提供针对性建议以提升透明度和完整性。


<details>
  <summary>Details</summary>
Motivation: 由于人工智能系统的可靠性高度依赖于数据集的质量，因而需要开发透明且负责任的数据集评估方法，以增强系统的透明度、问责性和减少潜在偏差。

Method: 该方法采用结构化方式，通过录入表格和评分标准来评估数据集的质量和完整性，并将其应用于四个不同的数据集以揭示优缺点。

Result: 通过评分系统为数据集提供量身定制的建议，以改进其透明性和完整性，从技术和伦理角度进行全面评估，最终改进数据集的质量。

Conclusion: 这一研究通过引入一个评分卡系统来评估AI数据集的开发，关注数据生命周期的多个关键领域，以提升数据集透明度和完整性。

Abstract: Artificial intelligence has transformed numerous industries, from healthcare
to finance, enhancing decision-making through automated systems. However, the
reliability of these systems is mainly dependent on the quality of the
underlying datasets, raising ongoing concerns about transparency,
accountability, and potential biases. This paper introduces a scorecard
designed to evaluate the development of AI datasets, focusing on five key areas
from the system card framework data development life cycle: data dictionary,
collection process, composition, motivation, and pre-processing. The method
follows a structured approach, using an intake form and scoring criteria to
assess the quality and completeness of the data set. Applied to four diverse
datasets, the methodology reveals strengths and improvement areas. The results
are compiled using a scoring system that provides tailored recommendations to
enhance the transparency and integrity of the data set. The scorecard addresses
technical and ethical aspects, offering a holistic evaluation of data
practices. This approach aims to improve the quality of the data set. It offers
practical guidance to curators and researchers in developing responsible AI
systems, ensuring fairness and accountability in decision support systems.

</details>
