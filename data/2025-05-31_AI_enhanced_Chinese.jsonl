{"id": "2505.22704", "pdf": "https://arxiv.org/pdf/2505.22704", "abs": "https://arxiv.org/abs/2505.22704", "authors": ["Feng Yao", "Zilong Wang", "Liyuan Liu", "Junxia Cui", "Li Zhong", "Xiaohan Fu", "Haohui Mai", "Vish Krishnan", "Jianfeng Gao", "Jingbo Shang"], "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.", "AI": {"tldr": "REAL \u6846\u67b6\u5229\u7528\u7a0b\u5e8f\u5206\u6790\u548c\u5355\u5143\u6d4b\u8bd5\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u5b9e\u73b0\u751f\u4ea7\u8d28\u91cf\u7684\u5feb\u901f\u9ad8\u6548\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u4eba\u5de5\u6ce8\u91ca\u6216\u8106\u5f31\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u96be\u4ee5\u6269\u5c55\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u81ea\u52a8\u6709\u6548\u6fc0\u52b1\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86 REAL \u6846\u67b6\uff0c\u7ed3\u5408\u7a0b\u5e8f\u5206\u6790\u68c0\u6d4b\u5b89\u5168\u6027\u6216\u53ef\u7ef4\u62a4\u6027\u7f3a\u9677\uff0c\u4ee5\u53ca\u5355\u5143\u6d4b\u8bd5\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u6fc0\u52b1\u4fe1\u53f7\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0cREAL \u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u8d28\u91cf\u3002", "conclusion": "REAL \u6846\u67b6\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u5f15\u5bfc\u53cd\u9988\uff0c\u6210\u529f\u6fc0\u52b1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u751f\u4ea7\u8d28\u91cf\u7684\u4ee3\u7801\u3002\u5b9e\u9a8c\u8868\u660e\uff0cREAL \u5728\u529f\u80fd\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u7684\u540c\u6b65\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.22752", "pdf": "https://arxiv.org/pdf/2505.22752", "abs": "https://arxiv.org/abs/2505.22752", "authors": ["Rafik Mankour", "Yassine Chafai", "Hamada Saleh", "Ghassen Ben Hassine", "Thibaud Barreau", "Peter Tankov"], "title": "Climate Finance Bench", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://github.com/Pladifes/climate_finance_bench", "summary": "Climate Finance Bench introduces an open benchmark that targets\nquestion-answering over corporate climate disclosures using Large Language\nModels. We curate 33 recent sustainability reports in English drawn from\ncompanies across all 11 GICS sectors and annotate 330 expert-validated\nquestion-answer pairs that span pure extraction, numerical reasoning, and\nlogical reasoning. Building on this dataset, we propose a comparison of RAG\n(retrieval-augmented generation) approaches. We show that the retriever's\nability to locate passages that actually contain the answer is the chief\nperformance bottleneck. We further argue for transparent carbon reporting in\nAI-for-climate applications, highlighting advantages of techniques such as\nWeight Quantization.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86RAG\u65b9\u6cd5\u5728\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u68c0\u7d22\u5668\u7684\u6709\u6548\u6027\u662f\u5173\u952e\u74f6\u9888\uff0c\u5e76\u5efa\u8bae\u900f\u660e\u7684\u78b3\u62a5\u544a\u6280\u672f\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u5173\u4e8e\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u7684\u95ee\u7b54\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b33\u4efd\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u548c330\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86RAG\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002", "result": "\u8bc1\u660e\u4e86RAG\u65b9\u6cd5\u5728\u54cd\u5e94\u95ee\u7b54\u4efb\u52a1\u65f6\u53d7\u9650\u4e8e\u68c0\u7d22\u5668\u5b9a\u4f4d\u6b63\u786e\u7b54\u6848\u6bb5\u843d\u7684\u80fd\u529b\uff0c\u5e76\u5021\u5bfcAI\u6c14\u5019\u5e94\u7528\u4e2d\u900f\u660e\u7684\u78b3\u62a5\u544a\u3002", "conclusion": "\u63ed\u793a\u4e86RAG\u65b9\u6cd5\u4e2d\u7684\u68c0\u7d22\u5668\u5728\u5b9a\u4f4d\u5305\u542b\u7b54\u6848\u7684\u6bb5\u843d\u7684\u80fd\u529b\u662f\u5176\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2505.22757", "pdf": "https://arxiv.org/pdf/2505.22757", "abs": "https://arxiv.org/abs/2505.22757", "authors": ["Ansar Aynetdinov", "Alan Akbik"], "title": "Pre-Training Curriculum for Multi-Token Prediction in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Multi-token prediction (MTP) is a recently proposed pre-training objective\nfor language models. Rather than predicting only the next token (NTP), MTP\npredicts the next $k$ tokens at each prediction step, using multiple prediction\nheads. MTP has shown promise in improving downstream performance, inference\nspeed, and training efficiency, particularly for large models. However, prior\nwork has shown that smaller language models (SLMs) struggle with the MTP\nobjective. To address this, we propose a curriculum learning strategy for MTP\ntraining, exploring two variants: a forward curriculum, which gradually\nincreases the complexity of the pre-training objective from NTP to MTP, and a\nreverse curriculum, which does the opposite. Our experiments show that the\nforward curriculum enables SLMs to better leverage the MTP objective during\npre-training, improving downstream NTP performance and generative output\nquality, while retaining the benefits of self-speculative decoding. The reverse\ncurriculum achieves stronger NTP performance and output quality, but fails to\nprovide any self-speculative decoding benefits.", "AI": {"tldr": "The paper proposes curriculum learning strategies for MTP in SLMs, showing forward curriculum benefits in enhancing NTP and output quality while retaining decoding advantages, unlike the reverse approach.", "motivation": "The motivation is to enhance the capability of smaller language models (SLMs) to effectively utilize the multi-token prediction (MTP) objective, which has been challenging for them.", "method": "The paper introduces a curriculum learning strategy for MTP training, exploring two variants: forward curriculum and reverse curriculum.", "result": "The forward curriculum allows SLMs to leverage MTP better during pre-training, enhancing downstream NTP performance and generative output quality. The reverse curriculum improves NTP performance and output quality but doesn't aid self-speculative decoding.", "conclusion": "SLMs can better leverage the MTP objective through a forward curriculum, improving NTP performance while retaining self-speculative decoding benefits. The reverse curriculum improves NTP performance and output quality but lacks self-speculative decoding enhancements."}}
{"id": "2505.22814", "pdf": "https://arxiv.org/pdf/2505.22814", "abs": "https://arxiv.org/abs/2505.22814", "authors": ["Jonghan Lim", "Ilya Kovalenko"], "title": "A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Manufacturing environments are becoming more complex and unpredictable due to\nfactors such as demand variations and shorter product lifespans. This\ncomplexity requires real-time decision-making and adaptation to disruptions.\nTraditional control approaches highlight the need for advanced control\nstrategies capable of overcoming unforeseen challenges, as they demonstrate\nlimitations in responsiveness within dynamic industrial settings. Multi-agent\nsystems address these challenges through decentralization of decision-making,\nenabling systems to respond dynamically to operational changes. However,\ncurrent multi-agent systems encounter challenges related to real-time\nadaptation, context-aware decision-making, and the dynamic exploration of\nresource capabilities. Large language models provide the possibility to\novercome these limitations through context-aware decision-making capabilities.\nThis paper introduces a large language model-enabled control architecture for\nmulti-agent manufacturing systems to dynamically explore resource capabilities\nin response to real-time disruptions. A simulation-based case study\ndemonstrates that the proposed architecture improves system resilience and\nflexibility. The case study findings show improved throughput and efficient\nresource utilization compared to existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u542f\u7528\u7684\u63a7\u5236\u67b6\u6784\uff0c\u4ee5\u6539\u5584\u591a\u4ee3\u7406\u5236\u9020\u7cfb\u7edf\u5728\u5b9e\u65f6\u4e2d\u65ad\u65f6\u7684\u8d44\u6e90\u80fd\u529b\u63a2\u7d22\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5236\u9020\u73af\u5883\u590d\u6742\u4e14\u4e0d\u53ef\u9884\u6d4b\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u52a8\u6001\u5de5\u4e1a\u73af\u5883\u4e2d\u53cd\u5e94\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5148\u8fdb\u7684\u63a7\u5236\u7b56\u7565\u6765\u5904\u7406\u4e0d\u53ef\u9884\u89c1\u7684\u6311\u6218\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u51b3\u7b56\u7684\u53bb\u4e2d\u5fc3\u5316\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u5f53\u524d\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u5b9e\u65f6\u9002\u5e94\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u63d0\u51fa\u672c\u6587\u7684\u67b6\u6784\u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u542f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\u67b6\u6784\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u5236\u9020\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5f39\u6027\u3001\u7075\u6d3b\u6027\u3001\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u542f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u63a7\u5236\u67b6\u6784\uff0c\u53ef\u5e94\u5bf9\u5236\u9020\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u4e2d\u65ad\uff0c\u5e76\u52a8\u6001\u63a2\u7d22\u8d44\u6e90\u80fd\u529b\u3002\u901a\u8fc7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u67b6\u6784\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u7075\u6d3b\u6027\uff0c\u6539\u8fdb\u4e86\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2505.22759", "pdf": "https://arxiv.org/pdf/2505.22759", "abs": "https://arxiv.org/abs/2505.22759", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.", "AI": {"tldr": "FAMA introduces open science speech models for English and Italian, achieving efficient performance and promoting transparency in research by releasing all resources as open source.", "motivation": "The motivation is to address reproducibility and fair evaluation challenges posed by the closed nature of current speech foundation models.", "method": "The paper develops a family of open science SFMs for English and Italian using 150k+ hours of open-source speech data, and introduces a new dataset with 16k hours of cleaned and pseudo-labeled speech.", "result": "FAMA achieves competitive performance and is up to 8 times faster than existing SFMs.", "conclusion": "FAMA achieves competitive performance compared to existing SFMs, and promotes openness in speech technology research by releasing all artifacts under open-source licenses."}}
{"id": "2505.23352", "pdf": "https://arxiv.org/pdf/2505.23352", "abs": "https://arxiv.org/abs/2505.23352", "authors": ["Xu Shen", "Yixin Liu", "Yiwei Dai", "Yili Wang", "Rui Miao", "Yue Tan", "Shirui Pan", "Xin Wang"], "title": "Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The communication topology in large language model-based multi-agent systems\nfundamentally governs inter-agent collaboration patterns, critically shaping\nboth the efficiency and effectiveness of collective decision-making. While\nrecent studies for communication topology automated design tend to construct\nsparse structures for efficiency, they often overlook why and when sparse and\ndense topologies help or hinder collaboration. In this paper, we present a\ncausal framework to analyze how agent outputs, whether correct or erroneous,\npropagate under topologies with varying sparsity. Our empirical studies reveal\nthat moderately sparse topologies, which effectively suppress error propagation\nwhile preserving beneficial information diffusion, typically achieve optimal\ntask performance. Guided by this insight, we propose a novel topology design\napproach, EIB-leanrner, that balances error suppression and beneficial\ninformation propagation by fusing connectivity patterns from both dense and\nsparse graphs. Extensive experiments show the superior effectiveness,\ncommunication cost, and robustness of EIB-leanrner.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEIB-leanrner\u7684\u65b0\u578b\u901a\u4fe1\u62d3\u6251\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u8fde\u63a5\u5bc6\u5ea6\u6765\u4f18\u5316\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u5e76\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u5728\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\u51b3\u5b9a\u4e86\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\u6a21\u5f0f\uff0c\u5f71\u54cd\u96c6\u4f53\u51b3\u7b56\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u8fc7\u4e8e\u4e13\u6ce8\u4e8e\u7a00\u758f\u7ed3\u6784\u7684\u6784\u5efa\uff0c\u800c\u5ffd\u89c6\u4e86\u7a00\u758f\u548c\u5bc6\u96c6\u62d3\u6251\u5728\u534f\u4f5c\u4e2d\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u4e0d\u540c\u7a00\u758f\u5ea6\u7684\u62d3\u6251\u5bf9\u667a\u80fd\u4f53\u8f93\u51fa\u4f20\u64ad\u65b9\u5f0f\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aEIB-leanrner\u7684\u65b0\u578b\u62d3\u6251\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u7a20\u5bc6\u548c\u7a00\u758f\u56fe\u7684\u8fde\u63a5\u6a21\u5f0f\uff0c\u5e73\u8861\u9519\u8bef\u6291\u5236\u548c\u6709\u76ca\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEIB-leanrner\u5728\u6709\u6548\u6027\u3001\u901a\u4fe1\u6210\u672c\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u9002\u5ea6\u7a00\u758f\u7684\u62d3\u6251\u7ed3\u6784\u53ef\u4ee5\u5728\u6291\u5236\u9519\u8bef\u4f20\u64ad\u7684\u540c\u65f6\u4fdd\u6301\u6709\u76ca\u7684\u4fe1\u606f\u6269\u6563\uff0c\u901a\u5e38\u8fbe\u5230\u6700\u4f73\u4efb\u52a1\u6027\u80fd\u3002EIB-leanrner\u901a\u8fc7\u878d\u5408\u7a20\u5bc6\u548c\u7a00\u758f\u56fe\u7684\u8fde\u63a5\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9519\u8bef\u6291\u5236\u548c\u6709\u76ca\u4fe1\u606f\u4f20\u64ad\u7684\u5e73\u8861\u3002"}}
{"id": "2505.22698", "pdf": "https://arxiv.org/pdf/2505.22698", "abs": "https://arxiv.org/abs/2505.22698", "authors": ["Luca Fantin", "Marco Antonelli", "Margherita Cesetti", "Daniele Irto", "Bruno Zamengo", "Francesco Silvestri"], "title": "Design and testing of an agent chatbot supporting decision making with public transport data", "categories": ["cs.AI"], "comment": null, "summary": "Assessing the quality of public transportation services requires the analysis\nof large quantities of data on the scheduled and actual trips and documents\nlisting the quality constraints each service needs to meet. Interrogating such\ndatasets with SQL queries, organizing and visualizing the data can be quite\ncomplex for most users. This paper presents a chatbot offering a user-friendly\ntool to interact with these datasets and support decision making. It is based\non an agent architecture, which expands the capabilities of the core Large\nLanguage Model (LLM) by allowing it to interact with a series of tools that can\nexecute several tasks, like performing SQL queries, plotting data and creating\nmaps from the coordinates of a trip and its stops. This paper also tackles one\nof the main open problems of such Generative AI projects: collecting data to\nmeasure the system's performance. Our chatbot has been extensively tested with\na workflow that asks several questions and stores the generated query, the\nretrieved data and the natural language response for each of them. Such\nquestions are drawn from a set of base examples which are then completed with\nactual data from the database. This procedure yields a dataset for the\nevaluation of the chatbot's performance, especially the consistency of its\nanswers and the correctness of the generated queries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u5b83\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u5de5\u5177\u7b80\u5316\u4e86\u516c\u5171\u4ea4\u901a\u670d\u52a1\u6570\u636e\u7684\u67e5\u8be2\u4e0e\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u6807\u51c6\u5316\u95ee\u9898\u6d4b\u8bd5\u5176\u6027\u80fd\uff0c\u9a8c\u8bc1\u751f\u6210\u67e5\u8be2\u7684\u6b63\u786e\u6027\u548c\u56de\u7b54\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u7531\u4e8e\u5206\u6790\u516c\u5171\u4ea4\u901a\u670d\u52a1\u8d28\u91cf\u9700\u8981\u5904\u7406\u5927\u91cf\u590d\u6742\u7684\u6570\u636e\uff0c\u8bb8\u591a\u7528\u6237\u96be\u4ee5\u4f7f\u7528SQL\u67e5\u8be2\u7b49\u5de5\u5177\u3002\u56e0\u6b64\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u53cb\u597d\u7684\u65b9\u5f0f\u6765\u4e0e\u8fd9\u4e9b\u6570\u636e\u4ea4\u4e92\u662f\u975e\u5e38\u5fc5\u8981\u7684\u3002", "method": "\u8be5\u804a\u5929\u673a\u5668\u4eba\u7efc\u5408\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u4e00\u7cfb\u5217\u5de5\u5177\uff0c\u901a\u8fc7\u4ee3\u7406\u67b6\u6784\u5b9e\u73b0\uff0c\u5982\u6267\u884cSQL\u67e5\u8be2\u3001\u6570\u636e\u53ef\u89c6\u5316\u548c\u521b\u5efa\u5730\u56fe\u3002\u5b83\u8fd8\u901a\u8fc7\u8bbe\u5b9a\u95ee\u9898\u7684\u5de5\u4f5c\u6d41\u7a0b\u6765\u6d4b\u8bd5\u5176\u6027\u80fd\uff0c\u5e76\u5b58\u50a8\u751f\u6210\u7684\u67e5\u8be2\u548c\u54cd\u5e94\u6570\u636e\u3002", "result": "\u804a\u5929\u673a\u5668\u4eba\u7ecf\u8fc7\u5168\u9762\u6d4b\u8bd5\uff0c\u5176\u6570\u636e\u96c6\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u80fd\u591f\u751f\u6210\u4e00\u81f4\u3001\u6b63\u786e\u7684\u81ea\u7136\u8bed\u8a00\u56de\u7b54\u548cSQL\u67e5\u8be2\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u5f00\u53d1\u7684\u804a\u5929\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u5730\u4e0e\u7528\u6237\u4ea4\u4e92\u5e76\u652f\u6301\u51b3\u7b56\u5236\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u9a8c\u8bc1\u56de\u7b54\u7684\u4e00\u81f4\u6027\u548c\u751f\u6210\u67e5\u8be2\u7684\u6b63\u786e\u6027\u65b9\u9762\u3002"}}
{"id": "2505.22686", "pdf": "https://arxiv.org/pdf/2505.22686", "abs": "https://arxiv.org/abs/2505.22686", "authors": ["Ange-Clement Akazan", "Verlon Roel Mbingui", "Gnankan Landry Regis N'guessan", "Issa Karambal"], "title": "Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Weather forecasting is crucial for managing risks and economic planning,\nparticularly in tropical Africa, where extreme events severely impact\nlivelihoods. Yet, existing forecasting methods often struggle with the region's\ncomplex, non-linear weather patterns. This study benchmarks deep recurrent\nneural networks such as $\\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and\nKolmogorov-Arnold-based models $(\\texttt{KAN} and \\texttt{TKAN})$ for daily\nforecasting of temperature, precipitation, and pressure in two tropical cities:\nAbidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce\ntwo customized variants of $ \\texttt{TKAN}$ that replace its original\n$\\texttt{SiLU}$ activation function with $ \\texttt{GeLU}$ and \\texttt{MiSH},\nrespectively. Using station-level meteorological data spanning from 2010 to\n2024, we evaluate all the models on standard regression metrics. $\\texttt{KAN}$\nachieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,\n$\\texttt{MSE} < 0.0014~^\\circ C ^2$), while $\\texttt{TKAN}$ variants minimize\nabsolute errors for precipitation forecasting in low-rainfall regimes. The\ncustomized $\\texttt{TKAN}$ models demonstrate improvements over the standard\n$\\texttt{TKAN}$ across both datasets. Classical \\texttt{RNNs} remain highly\ncompetitive for atmospheric pressure ($R^2 \\approx 0.83{-}0.86$), outperforming\n$\\texttt{KAN}$-based models in this task. These results highlight the potential\nof spline-based neural architectures for efficient and data-efficient\nforecasting.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u4e9b\u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8eKolmogorov-Arnold\u7684\u6a21\u578b\u7528\u4e8e\u70ed\u5e26\u57ce\u5e02\u7684\u5929\u6c14\u9884\u62a5\uff0c\u53d1\u73b0KAN\u6a21\u578b\u5728\u6e29\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800cTKAN\u53d8\u4f53\u5728\u964d\u6c34\u9884\u62a5\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u4e14\u5728\u6c14\u538b\u9884\u6d4b\u65b9\u9762\u4f20\u7edfRNN\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "motivation": "\u5929\u6c14\u9884\u62a5\u5bf9\u4e8e\u98ce\u9669\u7ba1\u7406\u548c\u7ecf\u6d4e\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u5929\u6c14\u4e25\u91cd\u5f71\u54cd\u751f\u8ba1\u7684\u70ed\u5e26\u975e\u6d32\u5730\u533a\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u96be\u4ee5\u5e94\u5bf9\u8be5\u5730\u533a\u590d\u6742\u7684\u975e\u7ebf\u6027\u5929\u6c14\u6a21\u5f0f\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bfb\u627e\u66f4\u6709\u6548\u7684\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9\u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08\u5982LSTM, GRU, BiLSTM, BiGRU\uff09\u4ee5\u53ca\u57fa\u4e8eKolmogorov-Arnold\u6a21\u578b\uff08KAN\u548cTKAN\uff09\u7684\u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u4e86\u81ea\u5b9a\u4e49\u7684TKAN\u53d8\u4f53\uff0c\u66ff\u6362\u4e86TKAN\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u6e29\u5ea6\u3001\u964d\u6c34\u548c\u6c14\u538b\u9884\u6d4b\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "KAN\u6a21\u578b\u5728\u6e29\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\uff0cR2\u5206\u522b\u8fbe\u5230\u4e860.9986\uff08\u963f\u6bd4\u8ba9\uff09\u548c0.9998\uff08\u57fa\u52a0\u5229\uff09\uff0cMSE\u5c0f\u4e8e0.0014\u6444\u6c0f\u5ea6\u5e73\u65b9\u3002\u5728\u964d\u6c34\u9884\u6d4b\u4e2d\uff0cTKAN\u53d8\u4f53\u5728\u4f4e\u964d\u96e8\u72b6\u6001\u4e0b\u7684\u7edd\u5bf9\u8bef\u5dee\u6700\u5c0f\u3002\u7ecf\u5178RNN\u5728\u6c14\u538b\u9884\u6d4b\u4e2d\u4ecd\u8868\u73b0\u5f3a\u52b2\uff0cR2\u7ea6\u4e3a0.83\u81f30.86\uff0c\u8d85\u8fc7\u4e86KAN\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u6837\u6761\u7684\u795e\u7ecf\u67b6\u6784\u5728\u6709\u6548\u548c\u6570\u636e\u9ad8\u6548\u7684\u5929\u6c14\u9884\u62a5\u4e2d\u5177\u6709\u6f5c\u529b\u3002KAN\u548cTKAN\u6a21\u578b\u5728\u6e29\u5ea6\u548c\u964d\u6c34\u9884\u6d4b\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u4f20\u7edfRNN\u5728\u6c14\u538b\u9884\u6d4b\u4e2d\u4ecd\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2505.22765", "pdf": "https://arxiv.org/pdf/2505.22765", "abs": "https://arxiv.org/abs/2505.22765", "authors": ["Iddo Yosha", "Gallil Maimon", "Yossi Adi"], "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faStressTest\u57fa\u51c6\u548cStress17k\u6570\u636e\u96c6\uff0c\u7528\u4ee5\u63d0\u5347\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u53e5\u5b50\u91cd\u97f3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u6210\u529f\u4f18\u5316\u51fa\u6027\u80fd\u4f18\u5f02\u7684StresSLM\u6a21\u578b\u3002", "motivation": "\u53e5\u5b50\u91cd\u97f3\u5728\u8bed\u97f3\u7684\u610f\u4e49\u548c\u8bf4\u8bdd\u8005\u610f\u56fe\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5728\u73b0\u6709\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u548c\u5f00\u53d1\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u91cd\u89c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5e76\u521b\u5efaStress17k\u6570\u636e\u96c6\uff0c\u4ee5\u6a21\u62df\u91cd\u97f3\u53d8\u5316\u5e26\u6765\u7684\u610f\u4e49\u53d8\u5316\uff0c\u5e76\u7528\u5176\u4f18\u5316\u6a21\u578b\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578bStresSLM\u5728\u53e5\u5b50\u91cd\u97f3\u63a8\u7406\u548c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u6a21\u578bStresSLM\u5728\u53e5\u5b50\u91cd\u97f3\u63a8\u7406\u548c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2505.23584", "pdf": "https://arxiv.org/pdf/2505.23584", "abs": "https://arxiv.org/abs/2505.23584", "authors": ["Sumbal Malik", "Majid Khonji", "Khaled Elbassioni", "Jorge Dias"], "title": "Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "The rapid growth of e-commerce and the increasing demand for timely,\ncost-effective last-mile delivery have increased interest in collaborative\nlogistics. This research introduces a novel collaborative synchronized\nmulti-platform vehicle routing problem with drones and robots (VRP-DR), where a\nfleet of $\\mathcal{M}$ trucks, $\\mathcal{N}$ drones and $\\mathcal{K}$ robots,\ncooperatively delivers parcels. Trucks serve as mobile platforms, enabling the\nlaunching, retrieving, and en-route charging of drones and robots, thereby\naddressing critical limitations such as restricted payload capacities, limited\nrange, and battery constraints. The VRP-DR incorporates five realistic\nfeatures: (1) multi-visit service per trip, (2) multi-trip operations, (3)\nflexible docking, allowing returns to the same or different trucks (4) cyclic\nand acyclic operations, enabling return to the same or different nodes; and (5)\nen-route charging, enabling drones and robots to recharge while being\ntransported on the truck, maximizing operational efficiency by utilizing idle\ntransit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)\nto minimize both operational costs and makespan. To overcome the computational\nchallenges of solving large-scale instances, a scalable heuristic algorithm,\nFINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to\nprovide efficient, near-optimal solutions. Numerical experiments across various\ninstance sizes evaluate the performance of the MILP and heuristic approaches in\nterms of solution quality and computation time. The results demonstrate\nsignificant time savings of the combined delivery mode over the truck-only mode\nand substantial cost reductions from enabling multi-visits. The study also\nprovides insights into the effects of en-route charging, docking flexibility,\ndrone count, speed, and payload capacity on system performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u540c\u540c\u6b65\u591a\u5e73\u53f0\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP-DR\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u4ee5\u63d0\u4f9b\u9ad8\u6548\u7684\u8fd1\u4f3c\u6700\u4f18\u89e3\u3002\u7ed3\u679c\u663e\u793a\uff0c\u591a\u5e73\u53f0\u9012\u9001\u6a21\u5f0f\u5927\u5e45\u8282\u7701\u4e86\u65f6\u95f4\u5e76\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u591a\u9879\u56e0\u7d20\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u7684\u5feb\u901f\u589e\u957f\u548c\u5bf9\u53ca\u65f6\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u6700\u540e\u4e00\u516c\u91cc\u4ea4\u4ed8\u7684\u9700\u6c42\u589e\u52a0\u4e86\u5bf9\u534f\u540c\u7269\u6d41\u7684\u5174\u8da3\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u9012\u9001\u65b9\u5f0f\u4e2d\u5b58\u5728\u7684\u6709\u6548\u8f7d\u8377\u9650\u5236\u3001\u8303\u56f4\u9650\u5236\u548c\u7535\u6c60\u7ea6\u675f\u7b49\u5173\u952e\u7f3a\u9677\u3002", "method": "\u672c\u7814\u7a76\u5c06\u591a\u5e73\u53f0\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP-DR\uff09\u6784\u5efa\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\uff0c\u4ee5\u6700\u5c0f\u5316\u8fd0\u8425\u6210\u672c\u548c\u5b8c\u5de5\u65f6\u95f4\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5FINDER\uff08\u7075\u6d3b\u96c6\u6210\u4ea4\u4ed8\u4e0e\u80fd\u6e90\u5145\u7535\uff09\uff0c\u7528\u4e8e\u63d0\u4f9b\u6709\u6548\u7684\u8fd1\u4f3c\u6700\u4f18\u89e3\u3002", "result": "\u901a\u8fc7\u591a\u5b9e\u4f8b\u89c4\u6a21\u7684\u6570\u503c\u5b9e\u9a8c\u8bc4\u4f30MILP\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u5361\u8f66\u6a21\u5f0f\u76f8\u6bd4\uff0c\u8054\u5408\u9012\u9001\u6a21\u5f0f\u663e\u8457\u8282\u7701\u4e86\u65f6\u95f4\uff0c\u5e76\u4f7f\u6210\u672c\u5927\u5e45\u5ea6\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528\u591a\u5e73\u53f0\u534f\u540c\u9012\u9001\u6a21\u5f0f\u76f8\u6bd4\u4e8e\u4ec5\u4f7f\u7528\u5361\u8f66\u6a21\u5f0f\u53ef\u4ee5\u663e\u8457\u8282\u7701\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u542f\u7528\u591a\u6b21\u8bbf\u95ee\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u5728\u9014\u5145\u7535\u3001\u505c\u9760\u7075\u6d3b\u6027\u3001\u65e0\u4eba\u673a\u6570\u91cf\u3001\u901f\u5ea6\u548c\u6709\u6548\u8f7d\u8377\u80fd\u529b\u5bf9\u7cfb\u7edf\u6027\u80fd\u5f71\u54cd\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.22753", "pdf": "https://arxiv.org/pdf/2505.22753", "abs": "https://arxiv.org/abs/2505.22753", "authors": ["Arseniy Pertzovsky", "Roni Stern", "Ariel Felner", "Roie Zivan"], "title": "Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": null, "summary": "We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent\nPath Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of\nagents must move to their goal locations without collisions, whereas in LMAPF,\nnew goals are generated upon arrival. We propose methods for incorporating APFs\nin a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and\nPriority Inheritance with Backtracking (PIBT). Experimental results show that\nusing APF is not beneficial for MAPF but yields up to a 7-fold increase in\noverall system throughput for LMAPF.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5de5\u52bf\u573a(APFs)\u5728MAPF\u548cLMAPF\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5bf9MAPF\u65e0\u76ca\uff0c\u4f46\u5728LMAPF\u4e2d\u5927\u5e45\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5c06\u4eba\u5de5\u52bf\u573a(APFs)\u5e94\u7528\u4e8e\u89e3\u51b3\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212(MAPF)\u548c\u957f\u671f\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212(LMAPF)\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u7814\u7a76\u5728\u4e0d\u540c\u7684MAPF\u7b97\u6cd5\u4e2d\u5f15\u5165\u4eba\u5de5\u52bf\u573a(APFs)\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4f18\u5148\u7ea7\u89c4\u5212\u3001MAPF-LNS2\u548c\u5177\u6709\u56de\u6eaf\u7684\u4f18\u5148\u7ea7\u7ee7\u627f(PIBT)\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAPFs\u867d\u7136\u5bf9MAPF\u6ca1\u6709\u5b9e\u8d28\u6027\u597d\u5904\uff0c\u4f46\u5728LMAPF\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "\u4f7f\u7528\u4eba\u5de5\u52bf\u573a(APFs)\u5bf9\u4e8e\u4f20\u7edf\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212(MAPF)\u6ca1\u6709\u4f18\u52bf\uff0c\u4f46\u5bf9\u4e8e\u957f\u671f\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212(LMAPF)\uff0cAPFs\u53ef\u4ee5\u63d0\u9ad8\u591a\u8fbe7\u500d\u7684\u7cfb\u7edf\u541e\u5410\u91cf\u3002"}}
{"id": "2505.22689", "pdf": "https://arxiv.org/pdf/2505.22689", "abs": "https://arxiv.org/abs/2505.22689", "authors": ["Jialong Guo", "Xinghao Chen", "Yehui Tang", "Yunhe Wang"], "title": "SlimLLM: Accurate Structured Pruning for Large Language Models", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Large language models(LLMs) have garnered significant attention and\ndemonstrated impressive capabilities in a wide range of applications. However,\ndue to their enormous computational costs, the deployment and application of\nLLMs are often severely limited. To address this issue, structured pruning is\nan effective solution to compress the parameters of LLMs. Determining the\nimportance of each sub-module in LLMs and minimizing performance loss are\ncritical issues that need to be carefully addressed in structured pruning. In\nthis paper, we propose an effective and fast structured pruning method named\nSlimLLM for large language models. For channel and attention head pruning, we\nevaluate the importance based on the entire channel or head, rather than merely\naggregating the importance of individual elements within a sub-module. This\napproach enables a more holistic consideration of the interdependence among\nelements within the sub-module. In addition, we design a simple linear\nregression strategy for the output matrix to quickly recover performance. We\nalso propose layer-based importance ratio to determine the pruning ratio for\neach layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other\nmethods and achieves state-of-the-art performance.", "AI": {"tldr": "SlimLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u6709\u6548\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u4f53\u8bc4\u4f30\u901a\u9053\u548c\u6ce8\u610f\u529b\u5934\u7684\u91cd\u8981\u6027\u5e76\u7ed3\u5408\u7ebf\u6027\u56de\u5f52\u7b56\u7565\uff0c\u80fd\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\uff0c\u5728LLaMA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5176\u90e8\u7f72\u548c\u5e94\u7528\u53d7\u5230\u4e25\u91cd\u9650\u5236\u3002\u7ed3\u6784\u5316\u526a\u679d\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u5e76\u6700\u5c0f\u5316\u6027\u80fd\u635f\u5931\u3002", "method": "\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u5bf9\u901a\u9053\u548c\u6ce8\u610f\u529b\u5934\u7684\u6574\u4f53\u91cd\u8981\u6027\u8bc4\u4f30\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u6c47\u603b\u5b50\u6a21\u5757\u4e2d\u5404\u4e2a\u5143\u7d20\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u7b56\u7565\u5feb\u901f\u6062\u590d\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u5c42\u7684\u91cd\u8981\u6027\u6bd4\u7387\u6765\u51b3\u5b9a\u6bcf\u5c42\u7684\u526a\u679d\u6bd4\u7387\u3002", "result": "\u6211\u4eec\u7684SlimLLM\u65b9\u6cd5\u5728LLaMA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSlimLLM\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u5e76\u4fdd\u6301\u6027\u80fd\u3002"}}
{"id": "2505.22771", "pdf": "https://arxiv.org/pdf/2505.22771", "abs": "https://arxiv.org/abs/2505.22771", "authors": ["Christopher Ormerod"], "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, AIME-Con Conference Submission", "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell-correction and an encoder-based token classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u96c6\u6210\u53cd\u9988\u5bfc\u5411\u6807\u6ce8\u63d0\u9ad8\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u5347\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u6574\u5408\u53cd\u9988\u5bfc\u5411\u7684\u6807\u6ce8\u6765\u6539\u5584\u8bc4\u5206\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6807\u6ce8\uff0c\u5176\u4e2d\u751f\u6210\u578b\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u62fc\u5199\u7ea0\u6b63\uff0c\u800c\u7f16\u7801\u5668\u578b\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u8bc6\u522b\u548c\u6807\u8bb0\u8bba\u8bc1\u5143\u7d20\u3002\u5c06\u6807\u6ce8\u6574\u5408\u5230\u8bc4\u5206\u8fc7\u7a0b\u4e2d\uff0c\u91c7\u7528\u5fae\u8c03\u7684\u7f16\u7801\u5668\u4f5c\u4e3a\u5206\u7c7b\u5668\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u5fae\u8c03\u7684\u7f16\u7801\u5668\u578b\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u5206\uff0c\u6027\u80fd\u5f97\u5230\u4e86\u63d0\u9ad8\u3002", "conclusion": "\u7ed3\u5408\u53cd\u9988\u5bfc\u5411\u7684\u6807\u6ce8\u53ef\u4ee5\u63d0\u9ad8\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u62fc\u5199\u548c\u8bed\u6cd5\u9519\u8bef\u4ee5\u53ca\u8bba\u8bc1\u7ec4\u4ef6\u65b9\u9762\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u7f16\u7801\u5668\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5206\u7c7b\u5668\u65f6\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.22756", "pdf": "https://arxiv.org/pdf/2505.22756", "abs": "https://arxiv.org/abs/2505.22756", "authors": ["Tian Qin", "Core Francisco Park", "Mujin Kwun", "Aaron Walsman", "Eran Malach", "Nikhil Anand", "Hidenori Tanaka", "David Alvarez-Melis"], "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning tasks have become prominent benchmarks for assessing\nthe reasoning capabilities of LLMs, especially with reinforcement learning (RL)\nmethods such as GRPO showing significant performance gains. However, accuracy\nmetrics alone do not support fine-grained assessment of capabilities and fail\nto reveal which problem-solving skills have been internalized. To better\nunderstand these capabilities, we propose to decompose problem solving into\nfundamental capabilities: Plan (mapping questions to sequences of steps),\nExecute (correctly performing solution steps), and Verify (identifying the\ncorrectness of a solution). Empirically, we find that GRPO mainly enhances the\nexecution skill-improving execution robustness on problems the model already\nknows how to solve-a phenomenon we call temperature distillation. More\nimportantly, we show that RL-trained models struggle with fundamentally new\nproblems, hitting a 'coverage wall' due to insufficient planning skills. To\nexplore RL's impact more deeply, we construct a minimal, synthetic\nsolution-tree navigation task as an analogy for mathematical problem-solving.\nThis controlled setup replicates our empirical findings, confirming RL\nprimarily boosts execution robustness. Importantly, in this setting, we\nidentify conditions under which RL can potentially overcome the coverage wall\nthrough improved exploration and generalization to new solution paths. Our\nfindings provide insights into the role of RL in enhancing LLM reasoning,\nexpose key limitations, and suggest a path toward overcoming these barriers.\nCode is available at https://github.com/cfpark00/RL-Wall.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u589e\u5f3a\u4e86\u6267\u884c\u7a33\u5065\u6027\uff0c\u4f46\u5728\u5168\u65b0\u95ee\u9898\u4e0a\u53d7\u5236\u4e8e\u8ba1\u5212\u6280\u80fd\u7684\u4e0d\u8db3\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u62df\u4efb\u52a1\u6765\u9a8c\u8bc1\u5e76\u63d0\u51fa\u65b9\u6cd5\u4ee5\u6f5c\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u5df2\u6210\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u57fa\u51c6\uff0c\u4f46\u5355\u9760\u7cbe\u5ea6\u6307\u6807\u96be\u4ee5\u7ec6\u81f4\u5730\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\uff0c\u65e0\u6cd5\u63ed\u793a\u6a21\u578b\u5185\u5316\u4e86\u54ea\u4e9b\u95ee\u9898\u89e3\u51b3\u6280\u80fd\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u5c06\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u5206\u89e3\u4e3a\u57fa\u672c\u80fd\u529b\uff1a\u89c4\u5212\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u3002", "method": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u3001\u5408\u6210\u7684\u89e3\u9898\u6811\u5bfc\u822a\u4efb\u52a1\uff0c\u4ee5\u6a21\u62df\u6570\u5b66\u95ee\u9898\u89e3\u51b3\uff0c\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u63d0\u5347\u4e86\u89e3\u9898\u8fc7\u7a0b\u4e2d\u7684\u6267\u884c\u7a33\u5065\u6027\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0cGRPO\u4e3b\u8981\u589e\u5f3a\u4e86\u89e3\u9898\u4e2d\u7684\u6267\u884c\u6280\u80fd\u2014\u2014\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5df2\u77e5\u5982\u4f55\u89e3\u51b3\u7684\u95ee\u9898\u4e0a\u7684\u6267\u884c\u7a33\u5065\u6027\u3002\u7136\u800c\uff0cRL\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u9762\u5bf9\u5168\u65b0\u95ee\u9898\u65f6\u9762\u4e34'\u8986\u76d6\u5899'\uff0c\u56e0\u4e3a\u5176\u8ba1\u5212\u6280\u80fd\u4e0d\u8db3\u3002\u5728\u6211\u4eec\u7684\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u4e00\u4e9b\u6761\u4ef6\uff0c\u4f7fRL\u80fd\u591f\u901a\u8fc7\u6539\u5584\u63a2\u7d22\u548c\u5bf9\u65b0\u89e3\u8def\u5f84\u7684\u6cdb\u5316\u6765\u6f5c\u5728\u5730\u514b\u670d\u8986\u76d6\u5899\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u66b4\u9732\u4e86\u5173\u952e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5efa\u8bae\u4e86\u4e00\u6761\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u7684\u8def\u5f84\u3002"}}
{"id": "2505.22694", "pdf": "https://arxiv.org/pdf/2505.22694", "abs": "https://arxiv.org/abs/2505.22694", "authors": ["Dacao Zhang", "Kun Zhang", "Shimao Chu", "Le Wu", "Xin Li", "Si Wei"], "title": "MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning", "categories": ["cs.LG"], "comment": "This paper has been accepted to ACL 2025 Findings", "summary": "With the rapid development of Large Language Models (LLMs),\nParameter-Efficient Fine-Tuning (PEFT) methods have gained significant\nattention, which aims to achieve efficient fine-tuning of LLMs with fewer\nparameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)\nintroduces low-rank matrices to approximate the incremental tuning parameters\nand achieves impressive performance over multiple scenarios. After that, plenty\nof improvements have been proposed for further improvement. However, these\nmethods either focus on single-task scenarios or separately train multiple LoRA\nmodules for multi-task scenarios, limiting the efficiency and effectiveness of\nLoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in\nthis paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for\nmulti-task PEFT. Specifically, instead of using an individual LoRA for each\ntask, we align different ranks of LoRA module with different tasks, which we\nnamed low-rank experts. Moreover, we design a novel adaptive rank selector to\nselect the appropriate expert for each task. By jointly training low-rank\nexperts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task\nscenarios. Finally, we conduct extensive experiments over multiple multi-task\nbenchmarks along with different LLMs to verify model performance. Experimental\nresults demonstrate that compared to traditional LoRA and its variants, MoRE\nsignificantly improves the performance of LLMs in multi-task scenarios and\nincurs no additional inference cost. We also release the model and code to\nfacilitate the community.", "AI": {"tldr": "MoRE improves multi-task LLM performance by aligning LoRA ranks with tasks, outperforming other methods without extra inference costs.", "motivation": "Overcome the limitations of existing LoRA methods in multi-task scenarios by aligning different ranks of LoRA modules with specific tasks to enhance efficiency and effectiveness.", "method": "Proposing MoRE, a Mixture of Low-Rank Experts, which includes an adaptive rank selector to align different LoRA ranks with specific tasks.", "result": "MoRE enhances the adaptability and efficiency of LoRA in multi-task scenarios, outperforming traditional LoRA and its variants over multiple benchmarks with different LLMs.", "conclusion": "MoRE significantly improves the performance of LLMs in multi-task scenarios without adding extra inference costs."}}
{"id": "2505.22774", "pdf": "https://arxiv.org/pdf/2505.22774", "abs": "https://arxiv.org/abs/2505.22774", "authors": ["Kaja Dobrovoljc"], "title": "Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel treebank-driven approach to comparing syntactic\nstructures in speech and writing using dependency-parsed corpora. Adopting a\nfully inductive, bottom-up method, we define syntactic structures as\ndelexicalized dependency (sub)trees and extract them from spoken and written\nUniversal Dependencies (UD) treebanks in two syntactically distinct languages,\nEnglish and Slovenian. For each corpus, we analyze the size, diversity, and\ndistribution of syntactic inventories, their overlap across modalities, and the\nstructures most characteristic of speech. Results show that, across both\nlanguages, spoken corpora contain fewer and less diverse syntactic structures\nthan their written counterparts, with consistent cross-linguistic preferences\nfor certain structural types across modalities. Strikingly, the overlap between\nspoken and written syntactic inventories is very limited: most structures\nattested in speech do not occur in writing, pointing to modality-specific\npreferences in syntactic organization that reflect the distinct demands of\nreal-time interaction and elaborated writing. This contrast is further\nsupported by a keyness analysis of the most frequent speech-specific\nstructures, which highlights patterns associated with interactivity,\ncontext-grounding, and economy of expression. We argue that this scalable,\nlanguage-independent framework offers a useful general method for\nsystematically studying syntactic variation across corpora, laying the\ngroundwork for more comprehensive data-driven theories of grammar in use.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u97f3\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\u6709\u663e\u8457\u5dee\u5f02\uff0c\u8bed\u97f3\u4e2d\u5b58\u5728\u7684\u8bb8\u591a\u7ed3\u6784\u5728\u4e66\u9762\u8bed\u4e2d\u5e76\u672a\u51fa\u73b0\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u6846\u67b6\uff0c\u4ee5\u4fbf\u7cfb\u7edf\u5730\u7814\u7a76\u4e0d\u540c\u8bed\u6599\u5e93\u4e2d\u7684\u53e5\u6cd5\u53d8\u5316\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u5f52\u7eb3\u3001\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u5c06\u53e5\u6cd5\u7ed3\u6784\u5b9a\u4e49\u4e3a\u53bb\u8bcd\u5316\u7684\u4f9d\u5b58\uff08\u5b50\uff09\u6811\uff0c\u5e76\u4ece\u82f1\u8bed\u548c\u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed\u7684UD\u6811\u5e93\u4e2d\u63d0\u53d6\u8fd9\u4e9b\u7ed3\u6784\u3002", "result": "\u901a\u8fc7\u5206\u6790\uff0c\u53d1\u73b0\u8bed\u97f3\u8bed\u6599\u5e93\u5305\u542b\u7684\u53e5\u6cd5\u7ed3\u6784\u6570\u91cf\u8f83\u5c11\uff0c\u4e14\u591a\u6837\u6027\u4f4e\u4e8e\u4e66\u9762\u8bed\u6599\u5e93\uff0c\u5e76\u4e14\u8bed\u97f3\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\u91cd\u53e0\u975e\u5e38\u6709\u9650\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u5e93\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u8bed\u97f3\u548c\u4e66\u9762\u8bed\u4e2d\u7684\u53e5\u6cd5\u7ed3\u6784\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8bed\u97f3\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\u5b58\u5728\u5f88\u5927\u5dee\u5f02\uff0c\u8bed\u97f3\u4e2d\u7684\u53e5\u6cd5\u7ed3\u6784\u8f83\u5c11\u4e14\u591a\u6837\u6027\u8f83\u4f4e\uff0c\u5927\u591a\u6570\u5728\u8bed\u97f3\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u672a\u5728\u4e66\u9762\u8bed\u4e2d\u51fa\u73b0\u3002"}}
{"id": "2505.22804", "pdf": "https://arxiv.org/pdf/2505.22804", "abs": "https://arxiv.org/abs/2505.22804", "authors": ["Jonghan Lim", "Ilya Kovalenko"], "title": "Dynamic Task Adaptation for Multi-Robot Manufacturing Systems with Large Language Models", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Recent manufacturing systems are increasingly adopting multi-robot\ncollaboration to handle complex and dynamic environments. While multi-agent\narchitectures support decentralized coordination among robot agents, they often\nface challenges in enabling real-time adaptability for unexpected disruptions\nwithout predefined rules. Recent advances in large language models offer new\nopportunities for context-aware decision-making to enable adaptive responses to\nunexpected changes. This paper presents an initial exploratory implementation\nof a large language model-enabled control framework for dynamic task\nreassignment in multi-robot manufacturing systems. A central controller agent\nleverages the large language model's ability to interpret structured robot\nconfiguration data and generate valid reassignments in response to robot\nfailures. Experiments in a real-world setup demonstrate high task success rates\nin recovering from failures, highlighting the potential of this approach to\nimprove adaptability in multi-robot manufacturing systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u5236\u9020\u7cfb\u7edf\u7684\u4efb\u52a1\u91cd\u65b0\u5206\u914d\u80fd\u529b\uff0c\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u5728\u6545\u969c\u6062\u590d\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u534f\u4f5c\u5728\u5904\u7406\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u65b9\u9762\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u4f46\u5176\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u7cfb\u7edf\u5728\u5b9e\u65f6\u9002\u5e94\u610f\u5916\u4e2d\u65ad\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u4efb\u52a1\u91cd\u65b0\u5206\u914d\uff0c\u4e2d\u592e\u63a7\u5236\u5668\u4ee3\u7406\u5229\u7528\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u673a\u5668\u4eba\u914d\u7f6e\u6570\u636e\uff0c\u5e76\u5728\u6545\u969c\u53d1\u751f\u65f6\u751f\u6210\u6709\u6548\u7684\u91cd\u65b0\u5206\u914d\u65b9\u6848\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4ece\u6545\u969c\u4e2d\u6062\u590d\u7684\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u673a\u5668\u4eba\u7684\u6545\u969c\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u5236\u9020\u7cfb\u7edf\u9002\u5e94\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.22779", "pdf": "https://arxiv.org/pdf/2505.22779", "abs": "https://arxiv.org/abs/2505.22779", "authors": ["Mohammad Helal Uddin", "Sabur Baidya"], "title": "Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds", "categories": ["cs.AI"], "comment": null, "summary": "Mental disorders including depression, anxiety, and other neurological\ndisorders pose a significant global challenge, particularly among individuals\nexhibiting social avoidance tendencies. This study proposes a hybrid approach\nby leveraging smartphone sensor data measuring daily physical activities and\nanalyzing their social media (Twitter) interactions for evaluating an\nindividual's depression level. Using CNN-based deep learning models and Naive\nBayes classification, we identify human physical activities accurately and also\nclassify the user sentiments. A total of 33 participants were recruited for\ndata acquisition, and nine relevant features were extracted from the physical\nactivities and analyzed with their weekly depression scores, evaluated using\nthe Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six\nare derived from physical activities, achieving an activity recognition\naccuracy of 95%, while three features stem from sentiment analysis of Twitter\nactivities, yielding a sentiment analysis accuracy of 95.6%. Notably, several\nphysical activity features exhibited significant correlations with the severity\nof depression symptoms. For classifying the depression severity, a support\nvector machine (SVM)-based algorithm is employed that demonstrated a very high\naccuracy of 94%, outperforming alternative models, e.g., the multilayer\nperceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly\neffective in the long run for monitoring depression without breaching personal\nprivacy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\u548c\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u8bc4\u4f30\u6291\u90c1\u6c34\u5e73\uff0c\u4f7f\u7528SVM\u7b97\u6cd5\u5206\u7c7b\uff0c\u7ed3\u679c\u663e\u793a\u9ad8\u51c6\u786e\u7387\u548c\u663e\u8457\u76f8\u5173\u6027\u3002", "motivation": "\u7cbe\u795e\u969c\u788d\uff0c\u5305\u62ec\u6291\u90c1\u3001\u7126\u8651\u53ca\u5176\u4ed6\u795e\u7ecf\u7cfb\u7edf\u5931\u8c03\uff0c\u7279\u522b\u5728\u6709\u793e\u4ea4\u56de\u907f\u503e\u5411\u7684\u4e2a\u4f53\u4e2d\uff0c\u6784\u6210\u4e86\u5168\u7403\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528CNN\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u8bc6\u522b\u4eba\u7c7b\u7684\u65e5\u5e38\u8eab\u4f53\u6d3b\u52a8\uff0c\u5e76\u5bf9\u7528\u6237\u60c5\u611f\u8fdb\u884c\u5206\u7c7b\u3002\u901a\u8fc7\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u7b97\u6cd5\u5bf9\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u8fdb\u884c\u5206\u7c7b\uff0c\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u3002", "result": "\u901a\u8fc7\u8eab\u4f53\u6d3b\u52a8\u8bc6\u522b\u548c\u60c5\u611f\u5206\u6790\u53d6\u5f9795%\u548c95.6%\u7684\u51c6\u786e\u7387\uff0c\u6700\u7ec8\u4f7f\u7528SVM\u7b97\u6cd5\u5bf9\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u8fdb\u884c\u5206\u7c7b\uff0c\u8fbe\u523094%\u7684\u9ad8\u51c6\u786e\u7387\u3002\u591a\u4e2a\u8eab\u4f53\u6d3b\u52a8\u7279\u5f81\u4e0e\u6291\u90c1\u75c7\u72b6\u7684\u4e25\u91cd\u7a0b\u5ea6\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\u548c\u793e\u4ea4\u5a92\u4f53\u4e92\u52a8\u5206\u6790\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e2a\u4f53\u7684\u6291\u90c1\u6c34\u5e73\u3002"}}
{"id": "2505.22695", "pdf": "https://arxiv.org/pdf/2505.22695", "abs": "https://arxiv.org/abs/2505.22695", "authors": ["Tengfei Lyu", "Siyuan Feng", "Hao Liu", "Hai Yang"], "title": "LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning", "categories": ["cs.LG"], "comment": null, "summary": "Ride-hailing platforms face significant challenges in optimizing order\ndispatching and driver repositioning operations in dynamic urban environments.\nTraditional approaches based on combinatorial optimization, rule-based\nheuristics, and reinforcement learning often overlook driver income fairness,\ninterpretability, and adaptability to real-world dynamics. To address these\ngaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models\n(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in\nride-hailing services. LLM-ODDR framework comprises three key components: (1)\nMulti-objective-guided Order Value Refinement, which evaluates orders by\nconsidering multiple objectives to determine their overall value; (2)\nFairness-aware Order Dispatching, which balances platform revenue with driver\nincome fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,\nwhich optimizes idle vehicle placement based on historical patterns and\nprojected supply. We also develop JointDR-GPT, a fine-tuned model optimized for\nODDR tasks with domain knowledge. Extensive experiments on real-world datasets\nfrom Manhattan taxi operations demonstrate that our framework significantly\noutperforms traditional methods in terms of effectiveness, adaptability to\nanomalous conditions, and decision interpretability. To our knowledge, this is\nthe first exploration of LLMs as decision-making agents in ride-hailing ODDR\ntasks, establishing foundational insights for integrating advanced language\nmodels within intelligent transportation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f7f\u7528LLMs\u7684\u65b0\u6846\u67b6LLM-ODDR\uff0c\u4ee5\u4f18\u5316\u53eb\u8f66\u670d\u52a1\u4e2d\u7684\u8ba2\u5355\u6d3e\u9001\u548c\u53f8\u673a\u91cd\u65b0\u5b9a\u4f4d\uff0c\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u4e86\u53f8\u673a\u6536\u5165\u516c\u5e73\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u771f\u5b9e\u52a8\u6001\u7684\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b0\u6846\u67b6LLM-ODDR\u6765\u8fdb\u884c\u8ba2\u5355\u6d3e\u9001\u548c\u53f8\u673a\u91cd\u65b0\u5b9a\u4f4d\u3002\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u76ee\u6807\u6307\u5bfc\u7684\u8ba2\u5355\u4ef7\u503c\u4f18\u5316\u3001\u516c\u5e73\u611f\u77e5\u7684\u8ba2\u5355\u6d3e\u9001\u548c\u65f6\u7a7a\u9700\u6c42\u611f\u77e5\u7684\u53f8\u673a\u91cd\u65b0\u5b9a\u4f4d\u3002", "result": "\u901a\u8fc7\u66fc\u54c8\u987f\u51fa\u79df\u8f66\u8fd0\u8425\u7684\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLLM-ODDR\u6846\u67b6\u5728\u6709\u6548\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u6709\u6548\u6027\u3001\u9002\u5e94\u5f02\u5e38\u6761\u4ef6\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
