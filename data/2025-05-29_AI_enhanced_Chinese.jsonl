{"id": "2505.21534", "pdf": "https://arxiv.org/pdf/2505.21534", "abs": "https://arxiv.org/abs/2505.21534", "authors": ["Yao Fehlis"], "title": "Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Scientific laboratories, particularly those in pharmaceutical and\nbiotechnology companies, encounter significant challenges in optimizing\nworkflows due to the complexity and volume of tasks such as compound screening\nand assay execution. We introduce Cycle Time Reduction Agents (CTRA), a\nLangGraph-based agentic workflow designed to automate the analysis of lab\noperational metrics. CTRA comprises three main components: the Question\nCreation Agent for initiating analysis, Operational Metrics Agents for data\nextraction and validation, and Insights Agents for reporting and visualization,\nidentifying bottlenecks in lab processes. This paper details CTRA's\narchitecture, evaluates its performance on a lab dataset, and discusses its\npotential to accelerate pharmaceutical and biotechnological development. CTRA\noffers a scalable framework for reducing cycle times in scientific labs.", "AI": {"tldr": "CTRA\u662f\u4e00\u4e2a\u57fa\u4e8eLangGraph\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5b9e\u9a8c\u5ba4\u64cd\u4f5c\u6307\u6807\u5206\u6790\u6765\u52a0\u901f\u5236\u836f\u548c\u751f\u7269\u6280\u672f\u53d1\u5c55\u3002", "motivation": "\u5236\u836f\u548c\u751f\u7269\u6280\u672f\u516c\u53f8\u7684\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u5728\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u9047\u5230\u663e\u8457\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5316\u5408\u7269\u7b5b\u9009\u548c\u6d4b\u5b9a\u6267\u884c\u7684\u4efb\u52a1\u590d\u6742\u6027\u548c\u4efb\u52a1\u91cf\u4e0a\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLangGraph\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5177\u4f53\u7531\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u7ec4\u6210\uff1a\u95ee\u9898\u521b\u5efa\u4ee3\u7406\u8d1f\u8d23\u542f\u52a8\u5206\u6790\uff0c\u8fd0\u8425\u6307\u6807\u4ee3\u7406\u8d1f\u8d23\u6570\u636e\u63d0\u53d6\u548c\u9a8c\u8bc1\uff0c\u6d1e\u5bdf\u4ee3\u7406\u8d1f\u8d23\u62a5\u544a\u548c\u53ef\u89c6\u5316\u3002", "result": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86CTRA\u7684\u67b6\u6784\uff0c\u5e76\u5bf9\u5176\u5728\u5b9e\u9a8c\u5ba4\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8868\u660e\u5176\u80fd\u591f\u52a0\u901f\u5236\u836f\u548c\u751f\u7269\u6280\u672f\u7684\u53d1\u5c55\u3002", "conclusion": "CTRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u7684\u5468\u671f\u65f6\u95f4\uff0c\u4ece\u800c\u4fc3\u8fdb\u5236\u836f\u548c\u751f\u7269\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.21559", "pdf": "https://arxiv.org/pdf/2505.21559", "abs": "https://arxiv.org/abs/2505.21559", "authors": ["Julien Soul\u00e9", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Th\u00e9ron"], "title": "Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "In cloud-native systems, Kubernetes clusters with interdependent services\noften face challenges to their operational resilience due to poor workload\nmanagement issues such as resource blocking, bottlenecks, or continuous pod\ncrashes. These vulnerabilities are further amplified in adversarial scenarios,\nsuch as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal\nPod Autoscaling (HPA) approaches struggle to address such dynamic conditions,\nwhile reinforcement learning-based methods, though more adaptable, typically\noptimize single goals like latency or resource usage, neglecting broader\nfailure scenarios. We propose decomposing the overarching goal of maintaining\noperational resilience into failure-specific sub-goals delegated to\ncollaborative agents, collectively forming an HPA Multi-Agent System (MAS). We\nintroduce an automated, four-phase online framework for HPA MAS design: 1)\nmodeling a digital twin built from cluster traces; 2) training agents in\nsimulation using roles and missions tailored to failure contexts; 3) analyzing\nagent behaviors for explainability; and 4) transferring learned policies to the\nreal cluster. Experimental results demonstrate that the generated HPA MASs\noutperform three state-of-the-art HPA systems in sustaining operational\nresilience under various adversarial conditions in a proposed complex cluster.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u65b9\u6cd5\u6765\u63d0\u9ad8Kubernetes\u96c6\u7fa4\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u5f39\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u56db\u9636\u6bb5\u5728\u7ebf\u6846\u67b6\u6765\u8bbe\u8ba1HPA MAS\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5728\u4e91\u539f\u751f\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u8d44\u6e90\u963b\u585e\u3001\u74f6\u9888\u6216\u6301\u7eed\u7684Pod\u5d29\u6e83\uff0cKubernetes\u96c6\u7fa4\u7684\u76f8\u4e92\u4f9d\u8d56\u670d\u52a1\u7ecf\u5e38\u9762\u4e34\u5176\u64cd\u4f5c\u5f39\u6027\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u6f0f\u6d1e\u5728\u5206\u5e03\u5f0f\u62d2\u7edd\u670d\u52a1\u653b\u51fb\uff08DDoS\uff09\u7b49\u5bf9\u6297\u6027\u573a\u666f\u4e2d\u8fdb\u4e00\u6b65\u653e\u5927\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u5c06\u7ef4\u6301\u64cd\u4f5c\u5f39\u6027\u7684\u6574\u4f53\u76ee\u6807\u5206\u89e3\u4e3a\u59d4\u6d3e\u7ed9\u534f\u4f5c\u4ee3\u7406\u7684\u7279\u5b9a\u4e8e\u6545\u969c\u7684\u5b50\u76ee\u6807\uff0c\u5f62\u6210HPA\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u3002\u6211\u4eec\u5f15\u5165\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u56db\u9636\u6bb5\u5728\u7ebf\u6846\u67b6\u7528\u4e8eHPA MAS\u8bbe\u8ba1\uff1a1\uff09\u4f7f\u7528\u96c6\u7fa4\u75d5\u8ff9\u5efa\u6a21\u4e00\u4e2a\u6570\u5b57\u5b6a\u751f\uff1b 2\uff09\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u4ee3\u7406\uff0c\u4f7f\u7528\u9488\u5bf9\u6545\u969c\u4e0a\u4e0b\u6587\u7684\u89d2\u8272\u548c\u4efb\u52a1\uff1b 3\uff09\u5206\u6790\u4ee3\u7406\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\uff1b 4\uff09\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8fc1\u79fb\u5230\u771f\u5b9e\u96c6\u7fa4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u63d0\u51fa\u7684\u590d\u6742\u96c6\u7fa4\u4e2d\u7684\u5404\u79cd\u5bf9\u6297\u6761\u4ef6\u4e0b\uff0c\u751f\u6210\u7684HPA MAS\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u4e09\u79cd\u6700\u5148\u8fdb\u7684HPA\u7cfb\u7edf\uff0c\u8868\u73b0\u5176\u5728\u7ef4\u6301\u64cd\u4f5c\u5f39\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684HPA\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u7ef4\u6301\u590d\u6742\u96c6\u7fa4\u4e0b\u7684\u64cd\u4f5c\u5f39\u6027\u65b9\u9762\u4f18\u4e8e\u4e09\u79cd\u6700\u5148\u8fdb\u7684HPA\u7cfb\u7edf\u3002"}}
{"id": "2505.21588", "pdf": "https://arxiv.org/pdf/2505.21588", "abs": "https://arxiv.org/abs/2505.21588", "authors": ["Young-Min Cho", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have enabled the\nemergence of multi-agent systems where LLMs interact, collaborate, and make\ndecisions in shared environments. While individual model behavior has been\nextensively studied, the dynamics of peer influence in such systems remain\nunderexplored. In this paper, we investigate herd behavior, the tendency of\nagents to align their outputs with those of their peers, within LLM-based\nmulti-agent interactions. We present a series of controlled experiments that\nreveal how herd behaviors are shaped by multiple factors. First, we show that\nthe gap between self-confidence and perceived confidence in peers significantly\nimpacts an agent's likelihood to conform. Second, we find that the format in\nwhich peer information is presented plays a critical role in modulating the\nstrength of herd behavior. Finally, we demonstrate that the degree of herd\nbehavior can be systematically controlled, and that appropriately calibrated\nherd tendencies can enhance collaborative outcomes. These findings offer new\ninsights into the social dynamics of LLM-based systems and open pathways for\ndesigning more effective and adaptive multi-agent collaboration frameworks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728LLM\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u7f8a\u7fa4\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5e76\u80fd\u901a\u8fc7\u64cd\u63a7\u8fd9\u4e9b\u56e0\u7d20\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002", "motivation": "\u63a2\u7a76\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0cLLMs\u95f4\u4e92\u52a8\u5982\u4f55\u53d7\u540c\u4f34\u5f71\u54cd\uff0c\u5f15\u53d1\u96c6\u4f53\u884c\u4e3a\u5f62\u5f0f\u7684\u52a8\u6001\u53d8\u5316\uff0c\u7279\u522b\u662f\"\u7f8a\u7fa4\u884c\u4e3a\"\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u53d7\u63a7\u5b9e\u9a8c\u6765\u7814\u7a76\u7f8a\u7fa4\u884c\u4e3a\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u56e0\u7d20\u5305\u62ec\u81ea\u4fe1\u4e0e\u5bf9\u540c\u4f34\u4fe1\u4efb\u5ea6\u7684\u5dee\u5f02\u53ca\u4fe1\u606f\u5448\u73b0\u683c\u5f0f\u7b49\u3002", "result": "\u63ed\u793a\u4e86\u5728LLM\u57fa\u7840\u7684\u591a\u4ee3\u7406\u4ea4\u4e92\u4e2d\uff0c\u7f8a\u7fa4\u884c\u4e3a\u5982\u4f55\u53d7\u81ea\u4fe1\u4e0e\u540c\u4f34\u4fe1\u4efb\u5dee\u5f02\u3001\u4fe1\u606f\u5448\u73b0\u5f62\u5f0f\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u5408\u7406\u8c03\u8282\u7f8a\u7fa4\u503e\u5411\u6765\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u53d1\u73b0\u7f8a\u7fa4\u884c\u4e3a\u53d7\u591a\u4e2a\u56e0\u7d20\u5f71\u54cd\uff0c\u5e76\u53ef\u4ee5\u7cfb\u7edf\u5730\u52a0\u4ee5\u63a7\u5236\uff0c\u4ece\u800c\u5728\u591a\u4ee3\u7406\u534f\u4f5c\u4e2d\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2505.21741", "pdf": "https://arxiv.org/pdf/2505.21741", "abs": "https://arxiv.org/abs/2505.21741", "authors": ["Dongjune Chang", "Sola Kim", "Young Soo Park"], "title": "AI-Supported Platform for System Monitoring and Decision-Making in Nuclear Waste Management with Large Language Models", "categories": ["cs.MA", "cs.CY", "cs.IR"], "comment": null, "summary": "Nuclear waste management requires rigorous regulatory compliance assessment,\ndemanding advanced decision-support systems capable of addressing complex\nlegal, environmental, and safety considerations. This paper presents a\nmulti-agent Retrieval-Augmented Generation (RAG) system that integrates large\nlanguage models (LLMs) with document retrieval mechanisms to enhance decision\naccuracy through structured agent collaboration. Through a structured 10-round\ndiscussion model, agents collaborate to assess regulatory compliance and safety\nrequirements while maintaining document-grounded responses. Implemented on\nconsumer-grade hardware, the system leverages Llama 3.2 and\nmxbai-embed-large-v1 embeddings for efficient retrieval and semantic\nrepresentation. A case study of a proposed temporary nuclear waste storage site\nnear Winslow, Arizona, demonstrates the framework's effectiveness. Results show\nthe Regulatory Agent achieves consistently higher relevance scores in\nmaintaining alignment with legal frameworks, while the Safety Agent effectively\nmanages complex risk assessments requiring multifaceted analysis. The system\ndemonstrates progressive improvement in agreement rates between agents across\ndiscussion rounds while semantic drift decreases, indicating enhanced\ndecision-making consistency and response coherence. The system ensures\nregulatory decisions remain factually grounded, dynamically adapting to\nevolving regulatory frameworks through real-time document retrieval. By\nbalancing automated assessment with human oversight, this framework offers a\nscalable and transparent approach to regulatory governance. These findings\nunderscore the potential of AI-driven, multi-agent systems in advancing\nevidence-based, accountable, and adaptive decision-making for high-stakes\nenvironmental management scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u6587\u6863\u68c0\u7d22\uff0c\u4ee5\u63d0\u9ad8\u6838\u5e9f\u6599\u7ba1\u7406\u4e2d\u7684\u5408\u89c4\u6027\u548c\u5b89\u5168\u6027\u51b3\u7b56\u3002\u4f7f\u7528\u666e\u901a\u786c\u4ef6\u548c\u5d4c\u5165\u6280\u672f\uff0c\u8be5\u7cfb\u7edf\u5728\u6a21\u62df\u6848\u4f8b\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u5e76\u80fd\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6cd5\u89c4\u73af\u5883\u3002", "motivation": "\u6838\u5e9f\u6599\u7ba1\u7406\u9700\u8981\u7b26\u5408\u4e25\u683c\u7684\u6cd5\u89c4\uff0c\u8fd9\u9700\u8981\u80fd\u591f\u5904\u7406\u590d\u6742\u6cd5\u5f8b\u3001\u73af\u5883\u548c\u5b89\u5168\u8003\u8651\u7684\u9ad8\u7ea7\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u6587\u6863\u68c0\u7d22\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4ee3\u7406\u534f\u4f5c\u6765\u63d0\u9ad8\u51b3\u7b56\u51c6\u786e\u6027\u3002\u7cfb\u7edf\u5728\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u65bd\uff0c\u5229\u7528Llama 3.2\u548cmxbai-embed-large-v1\u5d4c\u5165\u8fdb\u884c\u9ad8\u6548\u7684\u68c0\u7d22\u548c\u8bed\u4e49\u8868\u793a\u3002\u901a\u8fc7\u4e00\u4e2a\u7ed3\u6784\u5316\u768410\u8f6e\u8ba8\u8bba\u6a21\u578b\uff0c\u4ee3\u7406\u4eec\u5408\u4f5c\u8bc4\u4f30\u6cd5\u89c4\u5408\u89c4\u6027\u548c\u5b89\u5168\u8981\u6c42\uff0c\u5e76\u4fdd\u6301\u6587\u6863\u4e3a\u57fa\u7840\u7684\u54cd\u5e94\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6cd5\u5f8b\u6846\u67b6\u7684\u4fdd\u6301\u4e0a\uff0c\u6cd5\u89c4\u4ee3\u7406\u83b7\u5f97\u4e86\u4e00\u81f4\u66f4\u9ad8\u7684\u76f8\u5173\u6027\u8bc4\u5206\uff0c\u800c\u5b89\u5168\u4ee3\u7406\u5728\u7ba1\u7406\u9700\u8981\u591a\u65b9\u9762\u5206\u6790\u7684\u590d\u6742\u98ce\u9669\u8bc4\u4f30\u4e2d\u8868\u73b0\u6709\u6548\u3002\u7cfb\u7edf\u663e\u793a\u51fa\u5728\u5404\u8f6e\u8ba8\u8bba\u4e2d\u4ee3\u7406\u4e4b\u95f4\u4e00\u81f4\u6027\u548c\u54cd\u5e94\u8fde\u8d2f\u6027\u7684\u8fdb\u6b65\uff0c\u8bed\u4e49\u6f02\u79fb\u964d\u4f4e\uff0c\u8fd9\u8868\u660e\u51b3\u7b56\u4e00\u81f4\u6027\u589e\u5f3a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u5b9e\u65f6\u6587\u6863\u68c0\u7d22\u786e\u4fdd\u6cd5\u89c4\u51b3\u7b56\u4ecd\u7136\u57fa\u4e8e\u4e8b\u5b9e\uff0c\u5e76\u968f\u7740\u4e0d\u65ad\u53d1\u5c55\u7684\u6cd5\u89c4\u6846\u67b6\u8fdb\u884c\u52a8\u6001\u8c03\u6574\uff0c\u5728\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u5de5\u76d1\u7763\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u6cd5\u89c4\u6cbb\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u548c\u900f\u660e\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86AI\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u63a8\u8fdb\u9ad8\u98ce\u9669\u73af\u5883\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u5faa\u8bc1\u3001\u8d1f\u8d23\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.", "AI": {"tldr": "\u6a21\u578b\u5728\u751f\u6210\u63a8\u7406\u94fe\u65f6\u53ef\u80fd\u4f1a\u51fa\u73b0\u5e7b\u89c9\uff0c\u5f15\u5165RH-AUC\u548cRH-Bench\u4ee5\u8bc4\u4f30\u63a8\u7406\u4e0e\u5e7b\u89c9\u7684\u6743\u8861\uff0c\u53d1\u73b0\u66f4\u5927\u578b\u7684\u6a21\u578b\u901a\u5e38\u5b9e\u73b0\u66f4\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u63a2\u8ba8\u63a8\u7406\u94fe\u957f\u5ea6\u5bf9\u89c6\u89c9\u8f93\u5165\u5173\u6ce8\u5ea6\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f71\u54cd\u5982\u4f55\u5bfc\u81f4\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u5f15\u5165RH-AUC\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u968f\u63a8\u7406\u957f\u5ea6\u53d8\u5316\u7684\u611f\u77e5\u51c6\u786e\u6027\uff0c\u5e76\u53d1\u5e03RH-Bench\u8bca\u65ad\u57fa\u51c6\u4ee5\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u5728\u63a8\u7406\u548c\u611f\u77e5\u4e4b\u95f4\u7684\u5e73\u8861\u53d7\u8bad\u7ec3\u6570\u636e\u7684\u7c7b\u578b\u548c\u9886\u57df\u5f71\u54cd\u66f4\u5927\uff0c\u5927\u578b\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u66f4\u5927\u7684\u6a21\u578b\u901a\u5e38\u5728\u63a8\u7406\u548c\u611f\u77e5\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4e14\u8fd9\u79cd\u5e73\u8861\u53d7\u8bad\u7ec3\u6570\u636e\u7684\u7c7b\u578b\u548c\u9886\u57df\u5f71\u54cd\u66f4\u5927\uff0c\u800c\u4e0d\u662f\u5b83\u7684\u603b\u4f53\u6570\u91cf\u3002"}}
{"id": "2505.21515", "pdf": "https://arxiv.org/pdf/2505.21515", "abs": "https://arxiv.org/abs/2505.21515", "authors": ["Hongqian Wu", "Hongzhong Deng", "Jichao Li", "Chengxing Wu", "Zhuoting Yu", "Haidong Zhang", "Gaoxin Qi"], "title": "Edge Games: Cooperative Partner Selection in Network Cooperation Evolution", "categories": ["physics.soc-ph", "nlin.AO"], "comment": "53 pages, 20 figures,6 tables", "summary": "The phenomenon of group cooperation constitutes a fundamental mechanism\nunderlying various social and biological systems. Complex networks provide a\nstructural framework for group interactions, where individuals can not only\nobtain information from their neighbors but also choose neighbors as\ncooperative partners. However, traditional evolutionary game theory models,\nwhere nodes are the game players, are not convenient for directly choosing\ncooperative partners. Here, we exchange the roles of nodes and edges and\ninnovatively propose the \"edge game\" model, using edges in complex networks as\nvirtual game players for group games. Theoretical analysis and simulation\nexperiments show that by configuring a synergy factor (r) that satisfies the\n\"moderate cooperation\" condition, a stable cooperative structure can be\nachieved for any network at the evolutionary equilibrium. Specifically, when\nthere is no constraint on the number of cooperators per node, the condition for\nthe evolution of cooperation in the network is r > kmax, where kmax is the\nmaximum degree of the nodes. When there is a threshold constraint, in\nnearest-neighbor coupled networks (with degree k), the condition for \"moderate\ncooperation\" is k < r < 2k. In heterogeneous networks, a variable synergy\nfactor scheme is adopted, where the synergy factor for each game group (rx) is\ndefined to be proportional to the degree of the central node (kx) in the group\n(rx = n-fold*kx), \"moderate cooperation\" can be achieved when 1 < n-fold < 2.\nIf the value of r exceeds the range, it may lead to \"excessive cooperation\"\nwith node overload. Comparing algorithm performance and time complexity, edge\ngames demonstrate advantages over other optimization algorithms. Simple and\nuniversal, the edge game provides a new approach to addressing multi-agent\ncooperation problems in the era of machine intelligence.", "AI": {"tldr": "\u8fb9\u7f18\u535a\u5f08\u6a21\u578b\u901a\u8fc7\u89d2\u8272\u4ea4\u6362\u53ca\u5f15\u5165\u534f\u540c\u56e0\u5b50r\uff0c\u6539\u5584\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5408\u4f5c\u6f14\u5316\uff0c\u8f83\u4f20\u7edf\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e0a\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u6f14\u5316\u535a\u5f08\u8bba\u6a21\u578b\u4e0d\u4fbf\u4e8e\u76f4\u63a5\u9009\u62e9\u5408\u4f5c\u4f19\u4f34\uff0c\u56e0\u6b64\u5f15\u5165\u8fb9\u7f18\u535a\u5f08\u6a21\u578b\uff0c\u5c06\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u8fb9\u7f18\u4f5c\u4e3a\u7fa4\u4f53\u6e38\u620f\u7684\u865a\u62df\u73a9\u5bb6\u3002", "method": "\u901a\u8fc7\u914d\u7f6e\u4e00\u4e2a\u534f\u540c\u56e0\u5b50r\u6ee1\u8db3\u201c\u9002\u5ea6\u5408\u4f5c\u201d\u6761\u4ef6\uff0c\u5b9e\u73b0\u4efb\u610f\u7f51\u7edc\u5728\u6f14\u5316\u5747\u8861\u72b6\u6001\u4e0b\u7684\u7a33\u5b9a\u5408\u4f5c\u7ed3\u6784\u3002", "result": "\u5728\u65e0\u7ea6\u675f\u8282\u70b9\u5408\u4f5c\u8005\u6570\u91cf\u60c5\u51b5\u4e0b\uff0c\u5408\u4f5c\u6f14\u5316\u6761\u4ef6\u4e3ar > kmax\uff1b\u6709\u9608\u503c\u7ea6\u675f\u65f6\uff0c\u5728\u6700\u8fd1\u90bb\u8026\u5408\u7f51\u7edc\u4e2d\u6761\u4ef6\u4e3ak < r < 2k\uff1b\u5f02\u8d28\u7f51\u7edc\u4e2d\u91c7\u7528\u53ef\u53d8\u534f\u540c\u56e0\u5b50\uff0c\u9002\u5ea6\u5408\u4f5c\u53ef\u57281 < n-fold < 2\u65f6\u5b9e\u73b0\u3002", "conclusion": "\u8fb9\u7f18\u535a\u5f08\u6a21\u578b\u4e3a\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u65b9\u6cd5\uff0c\u5728\u7b97\u6cd5\u6027\u80fd\u4e0e\u65f6\u95f4\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u4f18\u5316\u7b97\u6cd5\u3002"}}
{"id": "2505.21552", "pdf": "https://arxiv.org/pdf/2505.21552", "abs": "https://arxiv.org/abs/2505.21552", "authors": ["Diogo Cruz"], "title": "Understanding the learned look-ahead behavior of chess neural networks", "categories": ["cs.AI", "cs.LG"], "comment": "40 pages, 47 figures", "summary": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86Leela Chess Zero \u7684\u524d\u77bb\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5904\u7406\u7684\u4fe1\u606f\u6df1\u5ea6\u53ef\u8fbe\u4e03\u6b65\uff0c\u80fd\u591f\u8003\u8651\u591a\u79cd\u53ef\u80fd\u7684\u8d70\u68cb\u5e8f\u5217\uff0c\u4e30\u5bcc\u4e86\u5bf9AI\u5728\u590d\u6742\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u7406\u89e3\u3002", "motivation": "\u6269\u5c55\u73b0\u6709\u7814\u7a76\uff0c\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u5728\u56fd\u9645\u8c61\u68cb\u5bf9\u5f08\u4e2d\u7684\u524d\u77bb\u80fd\u529b\uff0c\u5e76\u9610\u660e\u795e\u7ecf\u7f51\u7edc\u5728\u6218\u7565\u4efb\u52a1\u4e2d\u590d\u6742\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u7684\u80fd\u529b\u6765\u8003\u8651\u672a\u6765\u79fb\u52a8\u548c\u66ff\u4ee3\u5e8f\u5217\uff0c\u5c24\u5176\u662f\u8d85\u8fc7\u4e0b\u4e00\u4e2a\u5373\u65f6\u79fb\u52a8\u7684\u5206\u6790\u80fd\u529b\u3002", "result": "\u7f51\u7edc\u53ef\u4ee5\u5904\u7406\u68cb\u5c40\u72b6\u6001\u7684\u4fe1\u606f\uff0c\u80fd\u591f\u63d0\u524d\u8003\u8651\u591a\u8fbe\u4e03\u6b65\uff0c\u5e76\u663e\u793a\u51fa\u5728\u4e0d\u540c\u7684\u672a\u6765\u65f6\u95f4\u6b65\u4e0a\u4f7f\u7528\u76f8\u4f3c\u7684\u5185\u90e8\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLeela Chess Zero \u7b56\u7565\u7f51\u7edc\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u9ad8\u5ea6\u76f8\u5173\u7684\u80cc\u666f\u4e0b\u5904\u7406\u4e03\u6b65\u4ee5\u5185\u68cb\u5c40\u7684\u4fe1\u606f\uff0c\u4e14\u80fd\u591f\u8003\u8651\u591a\u79cd\u53ef\u80fd\u7684\u8d70\u68cb\u5e8f\u5217\u3002"}}
{"id": "2505.21512", "pdf": "https://arxiv.org/pdf/2505.21512", "abs": "https://arxiv.org/abs/2505.21512", "authors": ["Harry Li", "Gabriel Appleby", "Kenneth Alperin", "Steven R Gomez", "Ashley Suh"], "title": "The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful data structures, but exploring them\neffectively remains difficult for even expert users. Large language models\n(LLMs) are increasingly used to address this gap, yet little is known\nempirically about how their usage with KGs shapes user trust, exploration\nstrategies, or downstream decision-making - raising key design challenges for\nLLM-based KG visual analysis systems. To study these effects, we developed\nLinkQ, a KG exploration system that converts natural language questions into\nstructured queries with an LLM. We collaborated with KG experts to design five\nvisual mechanisms that help users assess the accuracy of both KG queries and\nLLM responses: an LLM-KG state diagram that illustrates which stage of the\nexploration pipeline LinkQ is in, a query editor displaying the generated query\npaired with an LLM explanation, an entity-relation ID table showing extracted\nKG entities and relations with semantic descriptions, a query structure graph\nthat depicts the path traversed in the KG, and an interactive graph\nvisualization of query results. From a qualitative evaluation with 14\npractitioners, we found that users - even KG experts - tended to overtrust\nLinkQ's outputs due to its \"helpful\" visualizations, even when the LLM was\nincorrect. Users exhibited distinct workflows depending on their prior\nfamiliarity with KGs and LLMs, challenging the assumption that these systems\nare one-size-fits-all - despite often being designed as if they are. Our\nfindings highlight the risks of false trust in LLM-assisted data analysis tools\nand the need for further investigation into the role of visualization as a\nmitigation technique.", "AI": {"tldr": "\u5f00\u53d1LinkQ\uff0c\u901a\u8fc7\u89c6\u89c9\u673a\u5236\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u67e5\u8be2\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4f46\u53d1\u73b0\u7528\u6237\u5bb9\u6613\u8fc7\u5ea6\u4fe1\u4efb\u5de5\u5177\uff0c\u5373\u4f7f\u5176\u51fa\u9519\u3002", "motivation": "\u7814\u7a76\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u65f6\u5bf9\u7528\u6237\u4fe1\u4efb\u3001\u63a2\u7d22\u7b56\u7565\u548c\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1LinkQ\u7cfb\u7edf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u5e76\u7ed3\u5408\u4e94\u79cd\u89c6\u89c9\u673a\u5236\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u67e5\u8be2\u548cLLM\u7684\u51c6\u786e\u6027\u3002", "result": "\u7528\u6237\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u540c\uff0c\u53d6\u51b3\u4e8e\u4ed6\u4eec\u5bf9\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u719f\u6089\u7a0b\u5ea6\uff0c\u8fd9\u6311\u6218\u4e86\u8bbe\u8ba1\u6b64\u7c7b\u7cfb\u7edf\u7684\u201c\u4e00\u5200\u5207\u201d\u5047\u8bbe\u3002", "conclusion": "\u7528\u6237\u503e\u5411\u4e8e\u8fc7\u5ea6\u4fe1\u4efbLinkQ\u7684\u8f93\u51fa\uff0c\u5c3d\u7ba1LLM\u53ef\u80fd\u51fa\u9519\u3002"}}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications.", "AI": {"tldr": "\u7814\u7a76\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4ee3\u7406\u5efa\u6a21\uff0c\u4ee5\u63d0\u5347\u57ce\u5e02\u79fb\u52a8\u4eff\u771f\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4fe1\u606f\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165LLM\u589e\u5f3a\u4ee3\u7406\u591a\u6837\u6027\u548c\u73b0\u5b9e\u6027\uff0c\u4ee5\u6539\u5584\u57ce\u5e02\u79fb\u52a8\u4eff\u771f\uff0c\u65e8\u5728\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4fe1\u606f\u3002", "method": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u57fa\u4e8e\u4ee3\u7406\u7684\u5efa\u6a21\uff08ABM\uff09\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6765\u6a21\u62df\u53f0\u5317\u5e02\u7684\u4e2a\u4f53\u884c\u4e3a\u548c\u5927\u89c4\u6a21\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "\u5173\u952e\u89c1\u89e3\u5982\u8def\u7ebf\u70ed\u56fe\u548c\u7279\u5b9a\u6a21\u5f0f\u6307\u793a\u5668\uff0c\u53ef\u4ee5\u4e3a\u57ce\u5e02\u89c4\u5212\u4eba\u5458\u63d0\u4f9b\u653f\u7b56\u5236\u5b9a\u7684\u53ef\u64cd\u4f5c\u4fe1\u606f\u3002", "conclusion": "\u672a\u6765\u7684\u5de5\u4f5c\u96c6\u4e2d\u4e8e\u5efa\u7acb\u5f3a\u5927\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u5728\u57ce\u5e02\u89c4\u5212\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578", "abs": "https://arxiv.org/abs/2505.21578", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b25,000\u5c0f\u65f6\u591a\u6837\u6027\u82f1\u6587\u8bed\u97f3\u7684\u65b0\u6570\u636e\u96c6Loquacious Set\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u76ee\u524d\u7684\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u5982LibriSpeech\u3001MOSEL\u3001YODAS\u7b49\u5b58\u5728\u5982\u8bb8\u53ef\u8bc1\u4e0d\u901a\u7528\u3001\u8f6c\u5f55\u4e0d\u53ef\u9760\u3001\u97f3\u9891\u6570\u636e\u9519\u8bef\u6216\u7f3a\u4e4f\u8bc4\u4f30\u96c6\u7b49\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u5de5\u4e1a\u548c\u5b66\u672f\u754c\u7684\u7814\u7a76\u5408\u4f5c\u548c\u6bd4\u8f83\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5b8c\u5584\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86Loquacious Set\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b25,000\u5c0f\u65f6\u7cbe\u9009\u8bed\u97f3\u7684\u96c6\u6210\uff0c\u6db5\u76d6\u4e86\u6570\u5341\u4e07\u540d\u62e5\u6709\u4e0d\u540c\u53e3\u97f3\u7684\u8bf4\u8bdd\u8005\u548c\u591a\u79cd\u8bed\u97f3\u7c7b\u578b\uff08\u6717\u8bfb\u3001\u81ea\u53d1\u3001\u6f14\u8bb2\u3001\u6e05\u6670\u3001\u5608\u6742\uff09\u3002", "result": "Loquacious Set\u80fd\u591f\u5728\u5546\u4e1a\u4e0a\u4f7f\u7528\uff0c\u62e5\u6709\u4e30\u5bcc\u7684\u53e3\u97f3\u548c\u8bed\u97f3\u79cd\u7c7b\uff0c\u4e3a\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u4e86\u6784\u5efa\u73b0\u5b9e\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u57fa\u7840\u3002", "conclusion": "Loquacious Set\u4f5c\u4e3a\u4e00\u4e2a\u5546\u4e1a\u53ef\u7528\u7684\u82f1\u8bed\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u88ab\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u7814\u7a76\u4eba\u5458\u7528\u4e8e\u6784\u5efa\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3002"}}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98.", "AI": {"tldr": "R1-Code-Interpreter\u662f\u901a\u8fc7SFT\u548cRL\u8bad\u7ec3\u7684\u6269\u5c55\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86GPT-4o\uff0c\u4ec5\u6b21\u4e8eGPT-4o\u5e26\u4ee3\u7801\u89e3\u91ca\u5668\u7248\u672c\u3002", "motivation": "\u5c3d\u7ba1R1\u7c7b\u6a21\u578b\u5728\u63a8\u7406\u548c\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u3001\u7b26\u53f7\u64cd\u4f5c\u3001\u4f18\u5316\u548c\u7b97\u6cd5\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u4ecd\u7136\u56f0\u96be\uff0c\u56e0\u4e3a\u6587\u672c\u63a8\u7406\u7f3a\u4e4f\u4ee3\u7801\u6267\u884c\u7684\u4e25\u683c\u6027\u3002\u5173\u952e\u6311\u6218\u662f\u4f7fLLMs\u80fd\u591f\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528\u6587\u672c\u63a8\u7406\u4e0e\u4ee3\u7801\u751f\u6210\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR1-Code-Interpreter\u7684\u6269\u5c55\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u8f6e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\uff0c\u4f7f\u5176\u5728\u9010\u6b65\u63a8\u7406\u4e2d\u81ea\u4e3b\u751f\u6210\u591a\u4e2a\u4ee3\u7801\u67e5\u8be2\u3002", "result": "\u6a21\u578bR1-CI-14B\u7684\u6700\u7ec8\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5176\u572837\u4e2a\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u5230\u4e8664.1%\uff0c\u8d85\u8fc7\u4e86\u672a\u5e26\u4ee3\u7801\u89e3\u91ca\u5668\u7684GPT-4o\uff0858.6%\uff09\uff0c\u5e76\u63a5\u8fd1\u5e26\u4ee3\u7801\u89e3\u91ca\u5668\u7684GPT-4o\uff0870.9%\uff09\u3002", "conclusion": "\u6700\u7ec8\u6a21\u578bR1-CI-14B\u5728\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u5e73\u5747\u51c6\u786e\u7387\u4ece44.0%\u63d0\u9ad8\u523064.1%\uff0c\u8d85\u8d8a\u4e86GPT-4o\uff08\u4ec5\u6587\u672c\uff1a58.6%\uff09\uff0c\u5e76\u63a5\u8fd1\u4f7f\u7528\u4ee3\u7801\u89e3\u91ca\u5668\u7684GPT-4o\uff0870.9%\uff09\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u5b9e\u73b0\u4e86\u81ea\u6211\u68c0\u67e5\u884c\u4e3a\u7684\u4ea7\u751f\u3002"}}
{"id": "2505.21514", "pdf": "https://arxiv.org/pdf/2505.21514", "abs": "https://arxiv.org/abs/2505.21514", "authors": ["Mingchao Jiang", "Abhinav Jain", "Sophia Zorek", "Chris Jermaine"], "title": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation", "categories": ["cs.LG", "cs.PL", "cs.SE"], "comment": "Keywords: Benchmark Dataset, LLM Evaluation, Gen-AI, Program\n  Synthesis; TLDR: SimCoPilot is a benchmark for evaluating LLMs as\n  \"copilot\"-style interactive coding assistants, testing their ability to\n  integrate and complete code within complex real-world software environments", "summary": "We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners.", "AI": {"tldr": "SIMCOPILOT\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9488\u5bf9Java\u548cPython\u4ee3\u7801\uff0c\u63d0\u4f9b\u7ec6\u81f4\u5206\u6790\u4ee5\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u7f16\u7a0b\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u7f16\u7a0b\u73af\u5883\u4e2d\u7684\u6548\u7528\uff0c\u63d0\u4f9b\u4e00\u4e2a\u771f\u5b9e\u8be6\u7ec6\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u5e76\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7ecf\u5e38\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\u6765\u8bc4\u4f30LLM\u5728\u7f16\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u9488\u5bf9Java\u548cPython\u7684\u5b50\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u6a21\u62dfLLM\u4f5c\u4e3a\u7f16\u7801\u52a9\u624b\u7684\u4e92\u52a8\u89d2\u8272\u3002", "result": "\u5bf9\u7b97\u6cd5\u3001\u6570\u636e\u5e93\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u9886\u57df\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u4f18\u52bf\u7684\u6d1e\u5bdf\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u4e2d\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524d\u7684LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u4ecd\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u4f46\u6b63\u9010\u6b65\u4ece\u4ec5\u4ec5\u662f\u8bed\u6cd5\u8bc6\u522b\u7684\u751f\u6210\u5668\u8fc7\u6e21\u5230\u53ef\u9760\u7684\u667a\u80fd\u8f6f\u4ef6\u5f00\u53d1\u52a9\u624b\u3002"}}
{"id": "2505.21985", "pdf": "https://arxiv.org/pdf/2505.21985", "abs": "https://arxiv.org/abs/2505.21985", "authors": ["Naoto Yoshida", "Tadahiro Taniguchi"], "title": "Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "In multi-agent reinforcement learning (MARL), effective communication\nimproves agent performance, particularly under partial observability. We\npropose MARL-CPC, a framework that enables communication among fully\ndecentralized, independent agents without parameter sharing. MARL-CPC\nincorporates a message learning model based on collective predictive coding\n(CPC) from emergent communication research. Unlike conventional methods that\ntreat messages as part of the action space and assume cooperation, MARL-CPC\nlinks messages to state inference, supporting communication in non-cooperative,\nreward-independent settings. We introduce two algorithms -Bandit-CPC and\nIPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that\nboth outperform standard message-as-action approaches, establishing effective\ncommunication even when messages offer no direct benefit to the sender. These\nresults highlight MARL-CPC's potential for enabling coordination in complex,\ndecentralized environments.", "AI": {"tldr": "MARL-CPC uses predictive coding-based message learning to improve communication in decentralized, non-cooperative MARL, surpassing traditional approaches.", "motivation": "In non-cooperative MARL tasks, traditional message-as-action approaches are inadequate. The proposed MARL-CPC framework seeks to improve communication without assuming cooperation or parameter sharing.", "method": "MARL-CPC employs a message learning model based on collective predictive coding (CPC), avoiding the conventional assumption of cooperation by relating messages to state inference.", "result": "Benchmarks demonstrate that MARL-CPC, particularly its algorithms Bandit-CPC and IPPO-CPC, outperform standard methods in non-cooperative MARL tasks by enabling coordination through effective message communication.", "conclusion": "MARL-CPC establishes effective communication in non-cooperative, decentralized multi-agent environments, outperforming traditional message-as-action approaches."}}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598", "abs": "https://arxiv.org/abs/2505.21598", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture.", "AI": {"tldr": "\u5bf9\u73b0\u6709\u6570\u636e\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u7ec6\u81f4\u5206\u7c7b\uff0c\u5e76\u603b\u7ed3\u5176\u95ee\u9898\u53ca\u7b97\u6cd5\uff0c\u8ba8\u8bba\u4f18\u7f3a\u70b9\u548c\u5173\u952e\u6311\u6218\u3002", "motivation": "\u786e\u5b9a\u8de8\u4e0d\u540c\u6570\u636e\u57df\u7684\u6743\u91cd\uff0c\u4ee5\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8bad\u7ec3\u6700\u4f73\u8868\u73b0\u7684\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u5e76\u5bf9\u6bcf\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u95ee\u9898\u7684\u8868\u8ff0\uff0c\u603b\u7ed3\u4e86\u4ee3\u8868\u7b97\u6cd5\uff0c\u660e\u786e\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u533a\u522b\u3002", "result": "\u4e3a\u7ebf\u4e0b\u65b9\u6cd5\u63d0\u51fa\u4e86\u57fa\u4e8e\u542f\u53d1\u5f0f\u3001\u7b97\u6cd5\u548c\u51fd\u6570\u62df\u5408\u7684\u5206\u7c7b\uff1b\u5bf9\u4e8e\u7ebf\u4e0a\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u5728\u7ebf\u6781\u5c0f\u5316-\u6781\u5927\u5316\u4f18\u5316\u3001\u5728\u7ebf\u6df7\u5408\u6cd5\u5219\u53ca\u5176\u4ed6\u65b9\u6cd5\u7684\u5206\u7c7b\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u8054\u7cfb\u548c\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u6570\u636e\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u6307\u51fa\u4e86\u6570\u636e\u6df7\u5408\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8bad\u7ec3\u6700\u4f73\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.21671", "pdf": "https://arxiv.org/pdf/2505.21671", "abs": "https://arxiv.org/abs/2505.21671", "authors": ["Davin Choo", "Yuqi Pan", "Tonghan Wang", "Milind Tambe", "Alastair van Heerden", "Cheryl Johnson"], "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing", "categories": ["cs.AI", "cs.DS", "cs.LG", "math.OC"], "comment": null, "summary": "We study a sequential decision-making problem on a $n$-node graph $G$ where\neach node has an unknown label from a finite set $\\mathbf{\\Sigma}$, drawn from\na joint distribution $P$ that is Markov with respect to $G$. At each step,\nselecting a node reveals its label and yields a label-dependent reward. The\ngoal is to adaptively choose nodes to maximize expected accumulated discounted\nrewards. We impose a frontier exploration constraint, where actions are limited\nto neighbors of previously selected nodes, reflecting practical constraints in\nsettings such as contact tracing and robotic exploration. We design a Gittins\nindex-based policy that applies to general graphs and is provably optimal when\n$G$ is a forest. Our implementation runs in $O(n^2 \\cdot |\\mathbf{\\Sigma}|^2)$\ntime while using $O(n \\cdot |\\mathbf{\\Sigma}|^2)$ oracle calls to $P$ and\n$O(n^2 \\cdot |\\mathbf{\\Sigma}|)$ space. Experiments on synthetic and real-world\ngraphs show that our method consistently outperforms natural baselines,\nincluding in non-tree, budget-limited, and undiscounted settings. For example,\nin HIV testing simulations on real-world sexual interaction networks, our\npolicy detects nearly all positive cases with only half the population tested,\nsubstantially outperforming other baselines.", "AI": {"tldr": "\u7814\u7a76\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u9a6c\u5c14\u79d1\u592b\u6027\u8d28\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\u6700\u5927\u5316\u7d2f\u8ba1\u6298\u6263\u5956\u52b1\uff0c\u8be5\u7b56\u7565\u5728\u56fe\u4e3a\u68ee\u6797\u65f6\u6700\u4f18\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5176\u4e2d\u8282\u70b9\u6807\u7b7e\u7684\u5206\u5e03\u6ee1\u8db3\u56fe\u7684\u9a6c\u5c14\u79d1\u592b\u6027\u8d28\u3002\u901a\u8fc7\u9009\u62e9\u8282\u70b9\u63ed\u793a\u6807\u7b7e\u5e76\u83b7\u5f97\u6807\u7b7e\u76f8\u5173\u7684\u5956\u52b1\uff0c\u76ee\u7684\u662f\u5728\u672a\u66fe\u9009\u62e9\u7684\u8282\u70b9\u7684\u90bb\u5c45\u4e2d\u9009\u62e9\u4ee5\u6700\u5927\u5316\u671f\u671b\u7d2f\u8ba1\u6298\u6263\u5956\u52b1\uff0c\u9002\u7528\u4e8e\u63a5\u89e6\u8ffd\u8e2a\u548c\u673a\u5668\u4eba\u63a2\u7d22\u7b49\u5b9e\u9645\u7ea6\u675f\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eGittins\u6307\u6570\u7684\u653f\u7b56\uff0c\u8be5\u653f\u7b56\u9002\u7528\u4e8e\u4e00\u822c\u56fe\u3002\u5728\u5b9e\u73b0\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd0\u884c\u65f6\u95f4\u4e3aO(n^2 \\cdot |\\mathbf{\\Sigma}|^2)\uff0c\u4f7f\u7528O(n \\cdot |\\mathbf{\\Sigma}|^2)\u4e2a\u8c03\u7528oracle P\uff0c\u5e76\u5360\u7528O(n^2 \\cdot |\\mathbf{\\Sigma}|)\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u975e\u6811\u7ed3\u6784\u3001\u9884\u7b97\u53d7\u9650\u548c\u65e0\u6298\u6263\u60c5\u51b5\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u81ea\u7136\u57fa\u7ebf\uff0c\u5305\u62ecHIV\u68c0\u6d4b\u6a21\u62df\u4e2d\u6211\u4eec\u7684\u7b56\u7565\u663e\u8457\u8d85\u8d8a\u5176\u4ed6\u57fa\u7ebf\u3002", "conclusion": "\u6211\u4eec\u8bbe\u8ba1\u7684\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\u5728\u4e00\u822c\u56fe\u4e0a\u9002\u7528\uff0c\u5e76\u5728\u56feG\u4e3a\u68ee\u6797\u65f6\u8bc1\u660e\u662f\u6700\u4f18\u7684\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u4e2d\u4e00\u81f4\u5730\u4f18\u4e8e\u81ea\u7136\u57fa\u7ebf\u3002\u5728HIV\u68c0\u6d4b\u6a21\u62df\u4e2d\uff0c\u6211\u4eec\u7684\u7b56\u7565\u80fd\u591f\u5728\u4ec5\u6d4b\u8bd5\u4e00\u534a\u4eba\u53e3\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u51e0\u4e4e\u6240\u6709\u9633\u6027\u75c5\u4f8b\uff0c\u663e\u8457\u8d85\u8d8a\u5176\u4ed6\u57fa\u7ebf\u3002"}}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86TERSE\uff0c\u4e00\u79cd\u65b0\u578b\u7684SFDA\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u65f6\u95f4\u6062\u590d\u548c\u7a7a\u95f4\u91cd\u7ec4\u5b9e\u73b0\u8de8\u57df\u7a7a\u95f4-\u65f6\u95f4\u4f9d\u8d56\u7684\u9002\u5e94\uff0c\u66f4\u6709\u6548\u4e14\u53ef\u4f5c\u4e3a\u6a21\u5757\u96c6\u6210\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6e90\u65e0\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u8003\u8651\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff08MTS\uff09\u6570\u636e\u4e2d\u56fa\u6709\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u9002\u5e94\u6548\u679c\u4e0d\u4f73\u3002\u51c6\u786e\u8868\u793aMTS\u6570\u636e\u548c\u4fdd\u6301\u57df\u95f4\u7684\u4e0d\u53d8\u4fe1\u606f\u4f9d\u8d56\u4e8e\u8fd9\u4e9b\u7a7a\u95f4\u76f8\u5173\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5TERSE\uff0c\u4e13\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff08MTS\uff09\u6570\u636e\u8bbe\u8ba1\u3002TERSE\u5305\u62ec\u5b9a\u5236\u7684\u65f6\u7a7a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u6355\u6349\u57fa\u7840\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u6062\u590d\u548c\u7a7a\u95f4\u91cd\u7ec4\u4efb\u52a1\uff0c\u91cd\u65b0\u5efa\u7acb\u65f6\u95f4\u63a9\u7801\u7684\u65f6\u95f4\u5e8f\u5217\u548c\u7a7a\u95f4\u63a9\u7801\u7684\u76f8\u5173\u7ed3\u6784\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86TERSE\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "TERSE\u80fd\u591f\u6709\u6548\u5730\u5bf9\u7a7a\u95f4-\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u548c\u8fc1\u79fb\uff0c\u5b9e\u73b0\u9690\u542b\u7684\u7279\u5f81\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u6e90\u65e0\u9886\u57df\u9002\u5e94\u6027\u3002\u4f5c\u4e3a\u9996\u4e2a\u540c\u65f6\u8003\u8651\u7a7a\u95f4-\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0cTERSE\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u591a\u7528\u9014\u7684\u6a21\u5757\u96c6\u6210\u5230\u5176\u4ed6\u73b0\u6709\u7684\u6e90\u65e0\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u4e2d\u3002"}}
{"id": "2505.22125", "pdf": "https://arxiv.org/pdf/2505.22125", "abs": "https://arxiv.org/abs/2505.22125", "authors": ["Melrose Tia", "Jezreel Sophia Lanuzo", "Lei Rigi Baltazar", "Marie Joy Lopez-Relente", "Diwa Malaya Qui\u00f1ones", "Jason Albia"], "title": "Sentiment Simulation using Generative AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CY", "I.2; I.6; J.4"], "comment": "18 pages, 10 figures", "summary": "Traditional sentiment analysis relies on surface-level linguistic patterns\nand retrospective data, limiting its ability to capture the psychological and\ncontextual drivers of human sentiment. These limitations constrain its\neffectiveness in applications that require predictive insight, such as policy\ntesting, narrative framing, and behavioral forecasting. We present a robust\nframework for sentiment simulation using generative AI agents embedded with\npsychologically rich profiles. Agents are instantiated from a nationally\nrepresentative survey of 2,485 Filipino respondents, combining sociodemographic\ninformation with validated constructs of personality traits, values, beliefs,\nand socio-political attitudes. The framework includes three stages: (1) agent\nembodiment via categorical or contextualized encodings, (2) exposure to\nreal-world political and economic scenarios, and (3) generation of sentiment\nratings accompanied by explanatory rationales. Using Quadratic Weighted\nAccuracy (QWA), we evaluated alignment between agent-generated and human\nresponses. Contextualized encoding achieved 92% alignment in replicating\noriginal survey responses. In sentiment simulation tasks, agents reached\n81%--86% accuracy against ground truth sentiment, with contextualized profile\nencodings significantly outperforming categorical (p < 0.0001, Cohen's d =\n0.70). Simulation results remained consistent across repeated trials\n(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,\nCohen's d = 0.02). Our findings establish a scalable framework for sentiment\nmodeling through psychographically grounded AI agents. This work signals a\nparadigm shift in sentiment analysis from retrospective classification to\nprospective and dynamic simulation grounded in psychology of sentiment\nformation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u751f\u6210\u6027AI\u4ee3\u7406\u8fdb\u884c\u60c5\u611f\u6a21\u62df\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u8c03\u67e5\u83b7\u5f97\u7684\u793e\u4f1a\u4eba\u53e3\u4fe1\u606f\u548c\u5fc3\u7406\u7279\u5f81\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u60c5\u5883\u7f16\u7801\u7684\u51c6\u786e\u6027\u8f83\u9ad8\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u5206\u6790\u4f9d\u8d56\u8868\u9762\u7ea7\u7684\u8bed\u8a00\u6a21\u5f0f\u548c\u56de\u987e\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u4eba\u7c7b\u60c5\u611f\u5fc3\u7406\u548c\u60c5\u5883\u9a71\u52a8\u56e0\u7d20\u7684\u80fd\u529b\uff0c\u5f71\u54cd\u4e86\u9700\u8981\u9884\u6d4b\u6027\u6d1e\u5bdf\u7684\u5e94\u7528\u3002", "method": "\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a1.\u901a\u8fc7\u7c7b\u522b\u6216\u60c5\u666f\u5316\u7f16\u7801\u8fdb\u884c\u4ee3\u7406\u4f53\u73b0\uff1b2.\u66b4\u9732\u4e8e\u771f\u5b9e\u7684\u653f\u6cbb\u548c\u7ecf\u6d4e\u60c5\u666f\uff1b3.\u751f\u6210\u60c5\u611f\u8bc4\u5206\u5e76\u9644\u5e26\u89e3\u91ca\u6027\u7406\u7531\u3002\u4f7f\u7528QWA\u8bc4\u4f30\u4ee3\u7406\u751f\u6210\u4e0e\u4eba\u7c7b\u54cd\u5e94\u7684\u5339\u914d\u5ea6\u3002", "result": "\u60c5\u5883\u7f16\u7801\u5728\u590d\u5236\u539f\u59cb\u8c03\u67e5\u54cd\u5e94\u4e2d\u5b9e\u73b0\u4e8692%\u7684\u5339\u914d\u5ea6\u3002\u5728\u60c5\u611f\u6a21\u62df\u4efb\u52a1\u4e2d\uff0c\u4e0e\u5b9e\u9645\u60c5\u611f\u76f8\u6bd4\uff0c\u4ee3\u7406\u8fbe\u523081%\u81f386%\u7684\u51c6\u786e\u5ea6\uff0c\u5176\u4e2d\u60c5\u5883\u5316\u7684\u914d\u7f6e\u660e\u663e\u4f18\u4e8e\u5206\u7c7b\u914d\u7f6e\uff08p < 0.0001\uff0cCohen's d = 0.70\uff09\u3002\u6a21\u62df\u7ed3\u679c\u5728\u91cd\u590d\u8bd5\u9a8c\u4e2d\u4fdd\u6301\u4e00\u81f4\uff08\u6807\u51c6\u5dee\u4e3a+/-0.2\u81f30.5%\uff09\uff0c\u5e76\u5bf9\u60c5\u666f\u8bbe\u7f6e\u7684\u53d8\u5316\u8868\u73b0\u51fa\u97e7\u6027\uff08p = 0.9676\uff0cCohen's d = 0.02\uff09\u3002", "conclusion": "\u901a\u8fc7\u5fc3\u7406\u4e0a\u624e\u5b9e\u7684AI\u4ee3\u7406\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u60c5\u611f\u6a21\u62df\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u60c5\u611f\u5206\u6790\u4ece\u56de\u987e\u6027\u5206\u7c7b\u5230\u5fc3\u7406\u57fa\u7840\u4e0a\u7684\u60c5\u611f\u5f62\u6210\u7684\u524d\u77bb\u6027\u548c\u52a8\u6001\u6a21\u62df\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.", "AI": {"tldr": "R2R\u901a\u8fc7\u667a\u80fd\u8def\u7531\u4ee4\u724c\u6539\u5584\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u5347\u6548\u7387\u4e14\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u5728\u91cd\u8981\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u8f83\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u90e8\u7f72\u56f0\u96be\u3002\u5c0f\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5176\u6027\u80fd\u56e0\u65e0\u6cd5\u6709\u6548\u8ddf\u968f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8def\u5f84\u800c\u53d7\u635f\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e00\u79cd\u79f0\u4e3aR2R\u7684\u795e\u7ecf\u5143\u4ee4\u724c\u8def\u7531\u65b9\u6cd5\uff0c\u81ea\u52a8\u8bc6\u522b\u548c\u5904\u7406\u5173\u952e\u7684\u8def\u5f84\u5206\u6b67\u4ee4\u724c\uff0c\u5e76\u8fd0\u884c\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5bf9\u8fd9\u4e9b\u4ee4\u724c\u8fdb\u884c\u7279\u5b9a\u5904\u7406\u3002", "result": "R2R\u5728\u5e73\u5747\u6fc0\u6d3b\u53c2\u6570\u89c4\u6a21\u4e3a5.6B\u7684\u60c5\u51b5\u4e0b\uff0c\u7cbe\u5ea6\u8d85\u8fc7\u4e86R1-7B\u76841.6\u500d\uff0c\u8868\u73b0\u4f18\u4e8eR1-14B\u6a21\u578b\u3002\u540c\u65f6\uff0c\u76f8\u8f83\u4e8eR1-32B\uff0cR2R\u5b9e\u73b0\u4e862.8\u500d\u7684\u65f6\u949f\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684R2R\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u7b97\u672f\u3001\u7f16\u7a0b\u548c\u95ee\u7b54\u7b49\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4e00\u4e9b\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2505.21674", "pdf": "https://arxiv.org/pdf/2505.21674", "abs": "https://arxiv.org/abs/2505.21674", "authors": ["Michael Katz", "Harsha Kokel", "Christian Muise", "Shirin Sohrabi", "Sarath Sreedharan"], "title": "Make Planning Research Rigorous Again!", "categories": ["cs.AI"], "comment": null, "summary": "In over sixty years since its inception, the field of planning has made\nsignificant contributions to both the theory and practice of building planning\nsoftware that can solve a never-before-seen planning problem. This was done\nthrough established practices of rigorous design and evaluation of planning\nsystems. It is our position that this rigor should be applied to the current\ntrend of work on planning with large language models. One way to do so is by\ncorrectly incorporating the insights, tools, and data from the automated\nplanning community into the design and evaluation of LLM-based planners. The\nexperience and expertise of the planning community are not just important from\na historical perspective; the lessons learned could play a crucial role in\naccelerating the development of LLM-based planners. This position is\nparticularly important in light of the abundance of recent works that replicate\nand propagate the same pitfalls that the planning community has encountered and\nlearned from. We believe that avoiding such known pitfalls will contribute\ngreatly to the progress in building LLM-based planners and to planning in\ngeneral.", "AI": {"tldr": "\u5efa\u8bae\u5c06\u4f20\u7edf\u81ea\u52a8\u5316\u89c4\u5212\u7684\u7ecf\u9a8c\u6574\u5408\u5230\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u5668\u4e2d\uff0c\u4ee5\u907f\u514d\u91cd\u590d\u9519\u8bef\u5e76\u52a0\u5feb\u53d1\u5c55\u3002", "motivation": "\u867d\u7136\u8ba1\u5212\u9886\u57df\u5728\u89e3\u51b3\u65b0\u578b\u7684\u8ba1\u5212\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4f46\u6700\u8fd1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u5212\u5de5\u4f5c\u91cd\u590d\u4e86\u5df2\u77e5\u7684\u9677\u9631\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u501f\u9274\u4f20\u7edf\u89c4\u5212\u9886\u57df\u7684\u7ecf\u9a8c\uff0c\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u7684\u53d1\u751f\uff0c\u52a0\u901fLLM\u89c4\u5212\u5668\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u5efa\u8bae\u5c06\u81ea\u52a8\u89c4\u5212\u793e\u533a\u7684\u7ecf\u9a8c\u3001\u5de5\u5177\u548c\u6570\u636e\u878d\u5165\u5230\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u5668\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4e2d\uff0c\u4ee5\u907f\u514d\u91cd\u590d\u5df2\u77e5\u7684\u9519\u8bef\u548c\u9677\u9631\uff0c\u4ece\u800c\u52a0\u901f\u8fd9\u4e9b\u89c4\u5212\u5668\u7684\u53d1\u5c55\u3002", "result": "\u901a\u8fc7\u5c06\u81ea\u52a8\u89c4\u5212\u9886\u57df\u7684\u7ecf\u9a8c\u5e94\u7528\u4e8eLLM\u89c4\u5212\u5668\uff0c\u80fd\u591f\u907f\u514d\u91cd\u590d\u8fc7\u53bb\u7684\u9519\u8bef\uff0c\u5e76\u63a8\u8fdb\u65b0\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u6574\u5408\u81ea\u52a8\u5316\u89c4\u5212\u793e\u533a\u7684\u6d1e\u5bdf\u3001\u5de5\u5177\u548c\u6570\u636e\uff0c\u53ef\u4ee5\u52a0\u901f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u5668\u7684\u53d1\u5c55\uff0c\u540c\u65f6\u907f\u514d\u5df2\u77e5\u7684\u9677\u9631\uff0c\u8fd9\u5c06\u6781\u5927\u5730\u4fc3\u8fdbLLM\u89c4\u5212\u5668\u548c\u6574\u4f53\u89c4\u5212\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u4ee3\u7406\u5806\u53e0\u7ed3\u6784\u7684\u65b9\u6cd5\u2014\u2014ChemHAS\uff0c\u4ee5\u51cf\u5c11\u5316\u5b66\u5de5\u5177\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u6765\u51cf\u5c11\u5316\u5b66\u5de5\u5177\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u5316\u5b66\u5206\u5c42\u4ee3\u7406\u5806\u53e0\uff08ChemHAS\uff09\u65b9\u6cd5\uff0c\u4f18\u5316\u4ee3\u7406\u5806\u53e0\u7ed3\u6784\u4ee5\u6539\u5584\u5316\u5b66\u5de5\u5177\u7684\u6027\u80fd\u3002", "result": "ChemHAS\u5728\u56db\u4e2a\u57fa\u7840\u5316\u5b66\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8bc6\u522b\u51fa\u56db\u79cd\u4e0d\u540c\u7684\u4ee3\u7406\u5806\u53e0\u884c\u4e3a\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u6027\u5e76\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u65b0\u53ef\u80fd\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u8865\u507f\u5316\u5b66\u5de5\u5177\u7684\u9884\u6d4b\u8bef\u5dee\u3002"}}
{"id": "2505.22192", "pdf": "https://arxiv.org/pdf/2505.22192", "abs": "https://arxiv.org/abs/2505.22192", "authors": ["Yue Cui", "Liuyi Yao", "Zitao Li", "Yaliang Li", "Bolin Ding", "Xiaofang Zhou"], "title": "Efficient Leave-one-out Approximation in LLM Multi-agent Debate Based on Introspection", "categories": ["cs.MA"], "comment": null, "summary": "Multi-agent systems based on large language models (LLMs) advance automatic\ntask completion in various fields, where debate is a common cooperation form\nfor agents to solve complicated problems with reasoning and cross-review to\nsolidify answers. Assessing the individual contributions of agents within these\ndebates is crucial for system refinement and outcome reliability. Traditional\nleave-one-out (LOO) method offers a clear framework for evaluating each agent's\nrole but face challenges in LLM-based systems due to high computational costs\nand associated financial implications. This paper presents\nintrospective-leave-one-out (IntrospecLOO), a simple yet effective prompting\nfor approximation of LOO in LLM-powered multi-agent debates. IntrospecLOO\nintroduces an additional querying round after standard debates, prompting\nagents to update their answers while ignoring responses from a designated\nagent. This strategy effectively isolates and gauges each participant's\ninfluence at a reduced query complexity compared to the original LOO\napproaches. Validation through experiments on three benchmark datasets confirms\nthe effectiveness of IntrospecLOO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86IntrospecLOO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728LLM\u591a\u4ee3\u7406\u4e89\u8bba\u540e\u6dfb\u52a0\u4e00\u4e2a\u67e5\u8be2\u56de\u5408\uff0c\u6709\u6548\u8bc4\u4f30\u6bcf\u4e2a\u4ee3\u7406\u7684\u8d21\u732e\uff0c\u964d\u4f4e\u4e86\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "motivation": "\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u8bc4\u4f30\u4e2a\u4f53\u8d21\u732e\u5bf9\u4e8e\u7cfb\u7edf\u6539\u8fdb\u548c\u7ed3\u679c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfLOO\u65b9\u6cd5\u5728LLM\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faIntrospecLOO\u65b9\u6cd5\uff0c\u901a\u8fc7\u9644\u52a0\u67e5\u8be2\u56de\u5408\uff0c\u5728\u4e0d\u8003\u8651\u6307\u5b9a\u4ee3\u7406\u7684\u60c5\u51b5\u4e0b\u4fc3\u4f7f\u4ee3\u7406\u66f4\u65b0\u7b54\u6848\uff0c\u4ee5\u4f4e\u67e5\u8be2\u590d\u6742\u5ea6\u8fd1\u4f3c\u4f20\u7edfLOO\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86IntrospecLOO\u7684\u6709\u6548\u6027\u3002", "conclusion": "IntrospecLOO\u80fd\u5728\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u6709\u6548\u8bc4\u4f30\u6bcf\u4e2a\u53c2\u4e0e\u8005\u7684\u5f71\u54cd\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u89d2\u8272\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench.", "AI": {"tldr": "\u63d0\u51faMisBench\u57fa\u51c6\u4ee5\u8bc4\u4f30\u548c\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u8bef\u4fe1\u606f\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u65b0\u65b9\u6cd5\u4ee5\u589e\u5f3a\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u7ec6\u7c92\u5ea6\u5206\u6790\u7f3a\u4e4f\u7684\u7a7a\u767d\uff0c\u7814\u7a76LLMs\u5728\u7279\u5b9a\u65b9\u9762\u53d7\u8bef\u4fe1\u606f\u5f71\u54cd\u7684\u7a0b\u5ea6\u3002", "method": "\u63d0\u51faMisBench\uff0c\u8fd9\u662f\u6700\u5927\u7684\u8bc4\u4f30LLMs\u5bf9\u8bef\u4fe1\u606f\u884c\u4e3a\u548c\u77e5\u8bc6\u504f\u597d\u57fa\u51c6\uff0c\u5305\u542b10,346,712\u6761\u8bef\u4fe1\u606f\uff0c\u540c\u65f6\u8003\u8651\u77e5\u8bc6\u51b2\u7a81\u548c\u98ce\u683c\u53d8\u5316\u3002\u63d0\u51faReconstruct to Discriminate (RtD) \u65b9\u6cd5\u6765\u589e\u5f3aLLMs\u8bc6\u522b\u8bef\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u8868\u660eLLMs\u5728\u8bc6\u522b\u8bef\u4fe1\u606f\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u6613\u53d7\u77e5\u8bc6\u51b2\u7a81\u548c\u98ce\u683c\u53d8\u5316\u5f71\u54cd\u3002", "conclusion": "MisBench\u80fd\u591f\u4f5c\u4e3a\u6709\u6548\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u68c0\u6d4b\u5668\u5e76\u63d0\u9ad8\u5176\u53ef\u9760\u6027\uff0c\u4e3a\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4fdd\u969c\u3002"}}
{"id": "2505.21765", "pdf": "https://arxiv.org/pdf/2505.21765", "abs": "https://arxiv.org/abs/2505.21765", "authors": ["Sohyun An", "Ruochen Wang", "Tianyi Zhou", "Cho-Jui Hsieh"], "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "categories": ["cs.AI"], "comment": "Work In Progress", "summary": "While recent success of large reasoning models (LRMs) significantly advanced\nLLMs' reasoning capability by optimizing the final answer accuracy using\nreinforcement learning, they may also drastically increase the output length\ndue to overthinking, characterized by unnecessarily complex reasoning paths\nthat waste computation and potentially degrade the performance. We hypothesize\nthat such inefficiencies stem from LRMs' limited capability to dynamically\nselect the proper modular reasoning strategies, termed thinking patterns at the\nright position. To investigate this hypothesis, we propose a dynamic\noptimization framework that segments model-generated reasoning paths into\ndistinct thinking patterns, systematically identifying and promoting beneficial\npatterns that improve the answer while removing detrimental ones. Empirical\nanalysis confirms that our optimized thinking paths yield more concise yet\nsufficiently informative trajectories, enhancing reasoning efficiency by\nreducing attention FLOPs by up to 47% while maintaining accuracy for originally\ncorrect responses. Moreover, a non-trivial portion of originally incorrect\nresponses are transformed into correct ones, achieving a 15.6% accuracy\nimprovement with reduced length. Motivated by the improvement brought by the\noptimized thinking paths, we apply a preference optimization technique\nsupported by a pairwise dataset contrasting suboptimal and optimal reasoning\npaths. Experimental evaluations across multiple mathematical reasoning\nbenchmarks reveal that our method notably reduces computational overhead while\nsimultaneously improving reasoning accuracy, achieving up to a 12% accuracy\nimprovement and reducing token usage from approximately 5,000 to 3,000 tokens.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u601d\u7ef4\u8def\u5f84\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u8f93\u51fa\u957f\u5ea6\u3002", "motivation": "\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u53ef\u80fd\u7531\u4e8e\u601d\u8003\u8fc7\u5ea6\u800c\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u8fc7\u4e8e\u590d\u6742\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u5e76\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\uff0c\u9700\u8981\u6539\u8fdb\u601d\u7ef4\u6a21\u5f0f\u9009\u62e9\u7684\u52a8\u6001\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8def\u5f84\u5206\u5272\u6210\u4e0d\u540c\u7684\u601d\u7ef4\u6a21\u5f0f\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u4fc3\u8fdb\u6709\u76ca\u7684\u6a21\u5f0f\uff0c\u540c\u65f6\u53bb\u9664\u4e0d\u5229\u7684\u6a21\u5f0f\u3002", "result": "\u4f18\u5316\u540e\u7684\u601d\u7ef4\u8def\u5f84\u4f7f\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe47%\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6b63\u786e\u54cd\u5e94\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c06\u90e8\u5206\u539f\u59cb\u9519\u8bef\u54cd\u5e94\u8f6c\u6362\u4e3a\u6b63\u786e\u54cd\u5e94\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8615.6%\u5e76\u51cf\u5c11\u4e86\u8f93\u51fa\u957f\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u65b9\u6cd5\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8612%\uff0c\u4f7f\u7528\u7684token\u4ece\u7ea65,000\u51cf\u5c11\u52303,000\u3002", "conclusion": "\u901a\u8fc7\u5e94\u7528\u52a8\u6001\u4f18\u5316\u6846\u67b6\u548c\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6570\u5b66\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u8f93\u51fa\u957f\u5ea6\u3002"}}
{"id": "2505.21571", "pdf": "https://arxiv.org/pdf/2505.21571", "abs": "https://arxiv.org/abs/2505.21571", "authors": ["Yao Lu", "Tengfei Ma", "Zeyu Wang", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid development of wireless communications and the growing\ncomplexity of digital modulation schemes, traditional manual modulation\nrecognition methods struggle to extract reliable signal features and meet\nreal-time requirements in modern scenarios. Recently, deep learning based\nAutomatic Modulation Recognition (AMR) approaches have greatly improved\nclassification accuracy. However, their large model sizes and high\ncomputational demands hinder deployment on resource-constrained devices. Model\npruning provides a general approach to reduce model complexity, but existing\nweight, channel, and layer pruning techniques each present a trade-off between\ncompression rate, hardware acceleration, and accuracy preservation. To this\nend, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning\nframework that combines channel-level pruning with layer-level collapse\ndiagnosis to achieve extreme compression, high performance and efficient\ninference. In the first stage of FCOS, hierarchical clustering and parameter\nfusion are applied to channel weights to achieve channel-level pruning. Then a\nLayer Collapse Diagnosis (LaCD) module uses linear probing to identify layer\ncollapse and removes the collapsed layers due to high channel compression\nratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms\nexisting channel and layer pruning methods. Specifically, FCOS achieves 95.51%\nFLOPs reduction and 95.31% parameter reduction while still maintaining\nperformance close to the original ResNet56, with only a 0.46% drop in accuracy\non Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.", "AI": {"tldr": "FCOS\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u526a\u679d\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u6781\u5927\u538b\u7f29\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\uff0c\u7cbe\u5ea6\u4ec5\u5c0f\u5e45\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u8c03\u5236\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u53ef\u9760\u7684\u4fe1\u53f7\u7279\u5f81\u5e76\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c3d\u7ba1\u63d0\u9ad8\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f46\u53d7\u9650\u4e8e\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u8981\u6c42\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u73b0\u6709\u7684\u526a\u679d\u6280\u672f\u5728\u538b\u7f29\u7387\u3001\u786c\u4ef6\u52a0\u901f\u548c\u7cbe\u5ea6\u4fdd\u7559\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u4e3a\u4e86\u5728\u6781\u5927\u538b\u7f29\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u8bba\u6587\u5f15\u5165FCOS\u6846\u67b6\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u526a\u679d\u6846\u67b6FCOS\uff0c\u7b2c\u4e00\u9636\u6bb5\u4e3b\u8981\u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u548c\u53c2\u6570\u878d\u5408\u5b9e\u73b0\u901a\u9053\u526a\u679d\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u8bc6\u522b\u5c42\u5d29\u6e83\u5e76\u79fb\u9664\u56e0\u9ad8\u901a\u9053\u538b\u7f29\u6bd4\u800c\u5bfc\u81f4\u5d29\u6e83\u7684\u5c42\u3002", "result": "\u901a\u8fc7\u591a\u4e2aAMR\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cFCOS\u572895.51% FLOPs\u51cf\u5c11\u548c95.31%\u53c2\u6570\u51cf\u5c11\u7684\u540c\u65f6\uff0c\u4ec5\u5728Sig2019-12\u4e0a\u7684\u7cbe\u5ea6\u635f\u5931\u4e860.46%\uff0c\u6027\u80fd\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "FCOS\u5728\u6781\u5927\u538b\u7f29\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u663e\u793a\u51fa\u5728\u5145\u5206\u4fdd\u6301\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u548c\u6a21\u578b\u5c3a\u5bf8\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.22467", "pdf": "https://arxiv.org/pdf/2505.22467", "abs": "https://arxiv.org/abs/2505.22467", "authors": ["Jiaxi Yang", "Mengqi Zhang", "Yiqiao Jin", "Hao Chen", "Qingsong Wen", "Lu Lin", "Yi He", "Weijie Xu", "James Evans", "Jindong Wang"], "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\u4ee5\u5f00\u53d1\u62d3\u6251\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5f3a\u8c03\u62d3\u6251\u7ed3\u6784\u5728\u63d0\u5347\u534f\u4f5c\u6027\u80fd\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u53ef\u80fd\u7684\u6311\u6218\u548c\u673a\u9047\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5982\u4f55\u4f18\u5316\u4ee3\u7406\u7684\u7ed3\u6784\u5316\u7ec4\u7ec7\u4ee5\u5b9e\u73b0\u6700\u4f73\u5408\u4f5c\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u8be5\u7814\u7a76\u65e8\u5728\u5f15\u5bfc\u7814\u7a76\u793e\u533a\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u62d3\u6251\u7ec4\u7ec7\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u4ee3\u7406\u9009\u62e9\u3001\u7ed3\u6784\u5206\u6790\u548c\u62d3\u6251\u5408\u6210\uff0c\u4ee5\u5f00\u53d1\u9762\u5411\u7279\u5b9a\u4efb\u52a1\u7684\u62d3\u6251\u611f\u77e5\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5c06\u6fc0\u53d1\u8bed\u8a00\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u5b66\u4e60\u548c\u751f\u6210\u5efa\u6a21\u7b49\u9886\u57df\u7684\u65b0\u7814\u7a76\u673a\u4f1a\uff0c\u5e76\u53ef\u80fd\u6fc0\u53d1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5f15\u5165\u62d3\u6251\u611f\u77e5\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u6548\u7387\u548c\u534f\u8c03\u6027\u80fd\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646", "abs": "https://arxiv.org/abs/2505.21646", "authors": ["Lei Zhang", "Markus Stricker"], "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u6863\u9009\u62e9\u548cWord2Vec\u6a21\u578b\u8bad\u7ec3\u6765\u52a0\u901f\u6750\u6599\u53d1\u73b0\uff0c\u6210\u529f\u9884\u6d4b\u5e76\u9a8c\u8bc1\u4e86\u6c27\u8fd8\u539f\u3001\u6c22\u8fdb\u5316\u548c\u6c27\u8fdb\u5316\u53cd\u5e94\u4e2d\u7684\u9ad8\u6027\u80fd\u6750\u6599\u3002", "motivation": "\u6750\u6599\u7684\u53d1\u73b0\u548c\u4f18\u5316\u53d7\u5230\u53ef\u80fd\u7684\u5143\u7d20\u7ec4\u5408\u548c\u76f8\u5173\u5c5e\u6027\u51e0\u4e4e\u65e0\u9650\u6570\u91cf\u7684\u9650\u5236\uff0c\u88ab\u79f0\u4e3a\u201c\u7ec4\u5408\u7206\u70b8\u201d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7\u6218\u7565\u6027\u9009\u62e9\u591a\u6837\u6027\u6700\u5927\u7684\u6587\u6863\u6765\u7ec6\u5316\u79d1\u5b66\u8bed\u6599\u5e93\uff0c\u8bad\u7ec3Word2Vec\u6a21\u578b\uff0c\u5e76\u76d1\u63a7\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6210\u5206-\u5c5e\u6027\u76f8\u5173\u6027\u7684\u6536\u655b\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u9884\u6d4b\u4e86\u9ad8\u6027\u80fd\u6c27\u8fd8\u539f\uff08ORR\uff09\u3001\u6c22\u8fdb\u5316\uff08HER\uff09\u548c\u6c27\u8fdb\u5316\uff08OER\uff09\u53cd\u5e94\u6750\u6599\u7684\u7ec4\u6210\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5ba4\u4e2d\u7535\u50ac\u5316\u6027\u80fd\u7684\u5b9e\u9a8c\u6d4b\u91cf\u9a8c\u8bc1\u4e86\u6700\u9ad8\u6027\u80fd\u7684\u7ec4\u6210\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u8fed\u4ee3\u8bed\u6599\u5e93\u7ec6\u5316\u65b9\u6cd5\u52a0\u901f\u6750\u6599\u53d1\u73b0\u548c\u4f18\u5316\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6570\u636e\u7a00\u7f3a\u6216\u4e0d\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u5bf9\u5927\u89c4\u6a21\u6210\u5206\u7a7a\u95f4\u8fdb\u884c\u7b5b\u9009\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE", "AI": {"tldr": "AIDSAFE uses multi-agent deliberation to generate high-quality CoT datasets, improving LLM safety training and robustness.", "motivation": "To overcome challenges in creating accurate and policy-compliant CoT datasets for LLM safety reasoning.", "method": "AIDSAFE, a data generation recipe using multi-agent deliberation and data refiners for high-quality CoTs.", "result": "AIDSAFE provides datasets that improve safety training, leading to better safety compliance and reduced jailbreak vulnerabilities in LLMs.", "conclusion": "AIDSAFE-generated CoTs enhance policy adherence and reasoning quality, improving safety generalization and jailbreak robustness of LLMs."}}
{"id": "2505.21573", "pdf": "https://arxiv.org/pdf/2505.21573", "abs": "https://arxiv.org/abs/2505.21573", "authors": ["Han Wan", "Rui Zhang", "Hao Sun"], "title": "Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial differential equations (PDEs) govern the spatiotemporal evolution of\nvarious physical systems. Classical numerical solvers, while accurate, require\nfine discretization and full knowledge of the governing PDEs, limiting their\napplicability when the physics is unknown or fast inference is required.\nData-driven neural PDE solvers alleviate these constraints by learning from\ndata but demand large training datasets and perform poorly in data-scarce\nregimes. Physics-aware methods mitigate data requirements by incorporating\nphysical knowledge yet rely on known PDE terms or local numerical schemes,\nrestricting their ability to handle unknown or globally coupled systems. In\nthis work, we propose the Spectral-inspired Neural Operator (SINO), a novel\nframework that learns PDE operators from limited trajectories (as few as 2-5),\nwithout any known PDE terms. SINO operates in the frequency domain and\nintroduces a Frequency-to-Vector module to learn spectral representations\nanalogous to derivative multipliers. To model nonlinear physical interactions,\nwe design a nonlinear operator block that includes a $\\Pi$-Block with low-pass\nfiltering to prevent aliasing. Finally, we introduce an operator distillation\ntechnique to distill the trained model for efficient inference. SINO achieves\nstate-of-the-art results across multiple PDE benchmarks, demonstrating strong\ndiscretization invariance and robust generalization to out-of-distribution\ninitial conditions. To our knowledge, SINO is the first physics-aware method\ncapable of accurately simulating globally coupled systems (e.g., the\nNavier-Stokes equations) from limited data without any explicit PDE terms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u8c31\u7075\u611f\u7684\u795e\u7ecf\u7b97\u5b50SINO\uff0c\u5b83\u53ef\u4ee5\u4ece\u6709\u9650\u7684\u6570\u636e\u5b66\u4e60PDE\u7b97\u5b50\uff0c\u5e76\u4e14\u6ca1\u6709\u5df2\u77e5\u7684PDE\u9879\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5168\u5c40\u8026\u5408\u7cfb\u7edf\u7684\u51c6\u786e\u6a21\u62df\u3002", "motivation": "\u7ecf\u5178\u6570\u503c\u6c42\u89e3\u5668\u9700\u8981\u7ec6\u81f4\u7684\u79bb\u6563\u5316\u548c\u5b8c\u6574\u7684PDE\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5728\u7269\u7406\u672a\u77e5\u6216\u9700\u8981\u5feb\u901f\u63a8\u7406\u65f6\u7684\u9002\u7528\u6027\u3002\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecfPDE\u6c42\u89e3\u5668\u867d\u7136\u7f13\u89e3\u4e86\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u7269\u7406\u611f\u77e5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u6765\u7f13\u89e3\u6570\u636e\u9700\u6c42\uff0c\u4f46\u4f9d\u8d56\u4e8e\u5df2\u77e5\u7684PDE\u672f\u8bed\u6216\u672c\u5730\u6570\u503c\u65b9\u6848\uff0c\u9650\u5236\u4e86\u5176\u5904\u7406\u672a\u77e5\u6216\u5168\u5c40\u8026\u5408\u7cfb\u7edf\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u79f0\u4e3aSpectral-inspired Neural Operator (SINO)\uff0c\u6b64\u6846\u67b6\u5728\u9891\u57df\u4e2d\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165\u4e86Frequency-to-Vector\u6a21\u5757\uff0c\u4ee5\u5b66\u4e60\u7c7b\u4f3c\u4e8e\u5bfc\u6570\u4e58\u5b50\u4f53\u7684\u8c31\u8868\u793a\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u975e\u7ebf\u6027\u7b97\u5b50\u5757\uff0c\u5305\u62ec\u4e00\u4e2a\u5177\u6709\u4f4e\u901a\u6ee4\u6ce2\u529f\u80fd\u7684\\u03a0-Block\uff0c\u4ee5\u9632\u6b62\u6df7\u53e0\u3002\u6b64\u5916\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7b97\u5b50\u84b8\u998f\u6280\u672f\uff0c\u4ee5\u63d0\u70bc\u5df2\u8bad\u7ec3\u7684\u6a21\u578b\u4ee5\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002", "result": "SINO\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u79bb\u6563\u5316\u4e0d\u53d8\u6027\u548c\u5bf9\u5206\u5e03\u5916\u521d\u59cb\u6761\u4ef6\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SINO\u80fd\u591f\u5728\u6709\u9650\u7684\u6570\u636e\u6761\u4ef6\u4e0b\u7cbe\u786e\u6a21\u62df\u5168\u5c40\u8026\u5408\u7cfb\u7edf\uff0c\u662f\u9996\u4e2a\u5728\u6ca1\u6709\u4efb\u4f55\u663e\u5f0fPDE\u9879\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8fd9\u4e00\u529f\u80fd\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.21550", "pdf": "https://arxiv.org/pdf/2505.21550", "abs": "https://arxiv.org/abs/2505.21550", "authors": ["Rishi Sharma", "Martijn de Vos", "Pradyumna Chari", "Ramesh Raskar", "Anne-Marie Kermarrec"], "title": "Collaborative Agentic AI Needs Interoperability Across Ecosystems", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "Collaborative agentic AI is projected to transform entire industries by\nenabling AI-powered agents to autonomously perceive, plan, and act within\ndigital environments. Yet, current solutions in this field are all built in\nisolation, and we are rapidly heading toward a landscape of fragmented,\nincompatible ecosystems. In this position paper, we argue that\ninteroperability, achieved by the adoption of minimal standards, is essential\nto ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To\nthis end, we devise a minimal architectural foundation for collaborative\nagentic AI, named Web of Agents, which is composed of four components:\nagent-to-agent messaging, interaction interoperability, state management, and\nagent discovery. Web of Agents adopts existing standards and reuses existing\ninfrastructure where possible. With Web of Agents, we take the first but\ncritical step toward interoperable agentic systems and offer a pragmatic path\nforward before ecosystem fragmentation becomes the norm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Web of Agents\u67b6\u6784\uff0c\u901a\u8fc7\u91c7\u7528\u6700\u4f4e\u6807\u51c6\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u5b64\u7acb\u53d1\u5c55\uff0c\u5bfc\u81f4\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\u548c\u4e0d\u517c\u5bb9\u6027\uff0c\u9700\u8981\u91c7\u7528\u6700\u4f4e\u6807\u51c6\u4ee5\u5b9e\u73b0\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86Web of Agents\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff0c\u65e8\u5728\u589e\u5f3a\u667a\u80fd\u4ee3\u7406\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "result": "\u63d0\u51fa\u4e86Web of Agents\u67b6\u6784\uff0c\u4ee5\u7edf\u4e00\u6807\u51c6\u5b9e\u73b0\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u57fa\u7840\u3002", "conclusion": "\u63a8\u52a8\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u662f\u9632\u6b62\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\u7684\u5173\u952e\u3002"}}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.", "AI": {"tldr": "SMILE\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u900f\u660e\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6539\u53d8\u8f93\u5165\u6765\u663e\u793a\u5bf9\u6a21\u578b\u54cd\u5e94\u5f71\u54cd\u6700\u5927\u7684\u8bcd\uff0c\u5e76\u5df2\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u505a\u51fa\u56de\u5e94\u7684\u51b3\u7b56\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u63d0\u9ad8\u5728\u4fe1\u4efb\u548c\u8d23\u4efb\u9886\u57df\u7684\u53ef\u63a7\u6027\u3002", "method": "SMILE\u901a\u8fc7\u5fae\u8c03\u8f93\u5165\u6765\u6d4b\u91cf\u8f93\u51fa\u53d8\u5316\uff0c\u8fdb\u800c\u7a81\u51fa\u5bf9\u6a21\u578b\u53cd\u5e94\u5f71\u54cd\u6700\u5927\u7684\u8bcd\uff0c\u5e76\u521b\u5efa\u7b80\u6613\u7684\u89c6\u89c9\u70ed\u56fe\u3002", "result": "SMILE\u5728\u591a\u4e2a\u9886\u5148\u7684LLMs\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5c55\u793a\u51fa\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u3001\u7a33\u5b9a\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u53ef\u4ee5\u63d0\u4f9b\u6e05\u6670\u548c\u53ef\u9760\u7684\u89e3\u91ca\u3002", "conclusion": "SMILE\u4f7f\u5f97\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u900f\u660e\u3001\u66f4\u53ef\u4fe1\u3002"}}
{"id": "2505.21828", "pdf": "https://arxiv.org/pdf/2505.21828", "abs": "https://arxiv.org/abs/2505.21828", "authors": ["Chen Yueh-Han", "Guy Davidson", "Brenden M. Lake"], "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "categories": ["cs.AI"], "comment": null, "summary": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main.", "AI": {"tldr": "SAGE-Eval tests LLMs' ability to apply safety facts in naive queries; results show models like Claude-3.7-sonnet fail to generalize well, prompting the need for better pre-deployment evaluations.", "motivation": "The motivation is to test if LLMs can generalize critical safety facts to novel situations, as failing to do so, especially for naive user questions, may result in dangerous consequences.", "method": "SAGE-Eval, a benchmark consisting of 104 safety facts from reputable sources, systematically augmented into 10,428 test scenarios across 7 domains, is introduced to evaluate LLMs.", "result": "The top model, Claude-3.7-sonnet, only passed 58% of the safety facts, indicating that model capabilities and training compute weakly correlate with performance on safety evaluations.", "conclusion": "Frontier LLMs still lack robust generalization ability in applying safety facts to naive user queries, and scaling up models is not the ultimate solution."}}
{"id": "2505.21576", "pdf": "https://arxiv.org/pdf/2505.21576", "abs": "https://arxiv.org/abs/2505.21576", "authors": ["Jiawei Tang", "Yuheng Jia"], "title": "Concentration Distribution Learning from Label Distributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Label distribution learning (LDL) is an effective method to predict the\nrelative label description degree (a.k.a. label distribution) of a sample.\nHowever, the label distribution is not a complete representation of an instance\nbecause it overlooks the absolute intensity of each label. Specifically, it's\nimpossible to obtain the total description degree of hidden labels that not in\nthe label space, which leads to the loss of information and confusion in\ninstances. To solve the above problem, we come up with a new concept named\nbackground concentration to serve as the absolute description degree term of\nthe label distribution and introduce it into the LDL process, forming the\nimproved paradigm of concentration distribution learning. Moreover, we propose\na novel model by probabilistic methods and neural networks to learn label\ndistributions and background concentrations from existing LDL datasets.\nExtensive experiments prove that the proposed approach is able to extract\nbackground concentrations from label distributions while producing more\naccurate prediction results than the state-of-the-art LDL methods. The code is\navailable in https://github.com/seutjw/CDL-LD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80cc\u666f\u6d53\u5ea6\u7684\u6982\u5ff5\u6765\u6539\u8fdb\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u672a\u80fd\u5b8c\u6574\u8868\u793a\u5b9e\u4f8b\uff0c\u5c24\u5176\u662f\u5ffd\u7565\u4e86\u6bcf\u4e2a\u6807\u7b7e\u7684\u7edd\u5bf9\u5f3a\u5ea6\uff0c\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u548c\u5b9e\u4f8b\u6df7\u6dc6\u3002", "method": "\u91c7\u7528\u6982\u7387\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u6807\u7b7e\u5206\u5e03\u548c\u80cc\u666f\u6d53\u5ea6\uff0c\u5e76\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u6807\u7b7e\u5206\u5e03\u4e2d\u63d0\u53d6\u80cc\u666f\u6d53\u5ea6\uff0c\u5e76\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u6700\u65b0\u7684LDL\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u80cc\u666f\u6d53\u5ea6\u548c\u6539\u8fdb\u7684\u6d53\u5ea6\u5206\u5e03\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u5206\u5e03\u4e0d\u80fd\u5b8c\u5168\u8868\u793a\u5b9e\u4f8b\u7684\u95ee\u9898\uff0c\u4e14\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u9884\u6d4b\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684LDL\u65b9\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2505.21585", "pdf": "https://arxiv.org/pdf/2505.21585", "abs": "https://arxiv.org/abs/2505.21585", "authors": ["Guillaume Moinard", "Matthieu Latapy"], "title": "Improving flocking behaviors in street networks with vision", "categories": ["physics.soc-ph", "cs.MA", "cs.SI"], "comment": null, "summary": "We improve a flocking model on street networks introduced in a previous\npaper. We expand the field of vision of walkers, making the model more\nrealistic. Under such conditions, we obtain groups of walkers whose gathering\ntimes and robustness to break ups are better than previous results. We explain\nsuch improvements because the alignment rule with vision guaranties walkers do\nnot split into divergent directions at intersections anymore, and because the\nattraction rule with vision gathers distant groups. This paves the way to a\nbetter understanding of events where walkers have collective decentralized\ngoals, like protests.", "AI": {"tldr": "\u901a\u8fc7\u6269\u5927\u884c\u8d70\u8005\u7684\u89c6\u91ce\u6539\u5584\u6a21\u578b\uff0c\u4f7f\u5f97\u884c\u8d70\u8005\u5728\u4ea4\u53c9\u8def\u53e3\u4e0d\u5206\u6563\u5e76\u805a\u96c6\u5206\u6563\u7fa4\u4f53\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u805a\u96c6\u65f6\u95f4\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6211\u4eec\u5e0c\u671b\u6539\u5584\u8857\u9053\u7f51\u7edc\u4e2d\u7684\u805a\u96c6\u6a21\u578b\uff0c\u5e76\u4f7f\u6a21\u578b\u66f4\u52a0\u903c\u771f\u3002", "method": "\u6211\u4eec\u91c7\u7528\u4e86\u6269\u5927\u7684\u89c6\u91ce\u5b57\u6bb5\u6765\u6539\u5584\u6a21\u578b\u7684\u73b0\u5b9e\u6027\u3002", "result": "\u901a\u8fc7\u6b64\u6539\u8fdb\uff0c\u6211\u4eec\u83b7\u5f97\u7684\u884c\u8d70\u8005\u7fa4\u4f53\u5728\u805a\u96c6\u65f6\u95f4\u548c\u62b5\u6297\u89e3\u4f53\u65b9\u9762\u4f18\u4e8e\u8fc7\u53bb\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u89c6\u91ce\u4e2d\u7684\u5bf9\u9f50\u89c4\u5219\u548c\u5438\u5f15\u89c4\u5219\uff0c\u884c\u8d70\u8005\u5728\u4ea4\u53c9\u8def\u53e3\u4e0d\u518d\u5206\u6563\uff0c\u5e76\u80fd\u591f\u805a\u96c6\u5206\u6563\u7684\u7fa4\u4f53\uff0c\u6539\u8fdb\u4e86\u7fa4\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u805a\u96c6\u6548\u7387\u3002"}}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5f02\u5e38\u503c\u5e76\u63d0\u51fa\u65b9\u6cd5\u4ee5\u51cf\u5c11\u5f02\u5e38\u503c\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u5f02\u5e38\u503c\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u91cf\u5316\u548c\u538b\u7f29\u65b9\u9762\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u56e0\u6b64\u8bc6\u522b\u548c\u89e3\u51b3\u8fd9\u4e9b\u5f02\u5e38\u503c\u6709\u52a9\u4e8e\u63d0\u9ad8\u91cf\u5316\u8fc7\u7a0b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e2d\u7684\u5f02\u5e38\u503c\u8fdb\u884c\u6df1\u5165\u8c03\u67e5\uff0c\u5206\u6790\u5f02\u5e38\u503c\u5f62\u6210\u673a\u5236\u3002\u63d0\u51fa\u53ef\u80fd\u7684\u7b56\u7565\u4ee5\u51cf\u5c11\u5f02\u5e38\u503c\u7684\u53d1\u751f\u3002\u6700\u7ec8\u63d0\u51fa\u4e86\u4e00\u4e9b\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u6d88\u9664\u5927\u591a\u6570\u5927\u89c4\u6a21\u6fc0\u6d3b\u548c\u901a\u9053\u5f02\u5e38\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e9b\u51cf\u5c11\u5f02\u5e38\u53d1\u751f\u7684\u7b56\u7565\u5e76\u5f15\u5165\u4e86\u9ad8\u6548\u7684\u65b9\u6cd5\u6d88\u9664\u5f02\u5e38\u503c\uff0c\u5bf9\u7cbe\u5ea6\u7684\u5f71\u54cd\u964d\u5230\u4e86\u6700\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e9b\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6d88\u9664\u5927\u591a\u6570\u5927\u89c4\u6a21\u6fc0\u6d3b\u548c\u901a\u9053\u5f02\u5e38\uff0c\u5e76\u4e14\u4ec5\u5bf9\u7cbe\u5ea6\u9020\u6210\u6700\u5c0f\u5f71\u54cd\u3002"}}
{"id": "2505.21887", "pdf": "https://arxiv.org/pdf/2505.21887", "abs": "https://arxiv.org/abs/2505.21887", "authors": ["Ahmed Heakl", "Yahia Salaheldin Shaaban", "Martin Takac", "Salem Lahlou", "Zangir Iklassov"], "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "18 pages, 14 figures, 11 tables", "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.", "AI": {"tldr": "The paper presents SVRPBench, a benchmark for vehicle routing under uncertainty, revealing weaknesses in current RL solvers compared to classical methods.", "motivation": "The motivation is to address the gap in current benchmarks that assume static and idealized settings, and instead create a benchmark that captures the complexity and uncertainty of real-world logistics.", "method": "The paper introduces SVRPBench, a benchmark that simulates high-fidelity stochastic dynamics in vehicle routing at urban scale, incorporating realistic delivery conditions and generating diverse, constraint-rich scenarios.", "result": "Benchmarking results show that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust.", "conclusion": "SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty."}}
{"id": "2505.21584", "pdf": "https://arxiv.org/pdf/2505.21584", "abs": "https://arxiv.org/abs/2505.21584", "authors": ["Afaf Taik", "Khaoula Chehbouni", "Golnoosh Farnadi"], "title": "Fairness in Federated Learning: Fairness for Whom?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in federated learning has emerged as a rapidly growing area of\nresearch, with numerous works proposing formal definitions and algorithmic\ninterventions. Yet, despite this technical progress, fairness in FL is often\ndefined and evaluated in ways that abstract away from the sociotechnical\ncontexts in which these systems are deployed. In this paper, we argue that\nexisting approaches tend to optimize narrow system level metrics, such as\nperformance parity or contribution-based rewards, while overlooking how harms\narise throughout the FL lifecycle and how they impact diverse stakeholders. We\nsupport this claim through a critical analysis of the literature, based on a\nsystematic annotation of papers for their fairness definitions, design\ndecisions, evaluation practices, and motivating use cases. Our analysis reveals\nfive recurring pitfalls: 1) fairness framed solely through the lens of server\nclient architecture, 2) a mismatch between simulations and motivating use-cases\nand contexts, 3) definitions that conflate protecting the system with\nprotecting its users, 4) interventions that target isolated stages of the\nlifecycle while neglecting upstream and downstream effects, 5) and a lack of\nmulti-stakeholder alignment where multiple fairness definitions can be relevant\nat once. Building on these insights, we propose a harm centered framework that\nlinks fairness definitions to concrete risks and stakeholder vulnerabilities.\nWe conclude with recommendations for more holistic, context-aware, and\naccountable fairness research in FL.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u53d1\u73b0\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u591a\u6837\u5316\u5229\u76ca\u76f8\u5173\u8005\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u5371\u5bb3\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\u6765\u6539\u5584\u516c\u5e73\u6027\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7684\u516c\u5e73\u6027\u7814\u7a76\u901a\u5e38\u4f18\u5316\u72ed\u9698\u7684\u7cfb\u7edf\u7ea7\u6307\u6807\u800c\u5ffd\u7565\u4e86\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u4ea7\u751f\u7684\u5371\u5bb3\u4ee5\u53ca\u5b83\u4eec\u5bf9\u591a\u6837\u5316\u5229\u76ca\u76f8\u5173\u8005\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5730\u6ce8\u89e3\u8bba\u6587\u4e2d\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u3001\u8bbe\u8ba1\u51b3\u7b56\u3001\u8bc4\u4f30\u5b9e\u8df5\u548c\u52a8\u673a\u7528\u4f8b\uff0c\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4e94\u4e2a\u5e38\u89c1\u95ee\u9898\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e94\u4e2a\u95ee\u9898\uff1a1\uff09\u4ec5\u901a\u8fc7\u670d\u52a1\u5668\u5ba2\u6237\u7aef\u67b6\u6784\u6846\u67b6\u516c\u5e73\u6027\uff0c2\uff09\u6a21\u62df\u4e0e\u52a8\u673a\u7528\u4f8b\u548c\u60c5\u5883\u4e0d\u5339\u914d\uff0c3\uff09\u5b9a\u4e49\u6df7\u6dc6\u4e86\u7cfb\u7edf\u4fdd\u62a4\u4e0e\u7528\u6237\u4fdd\u62a4\uff0c4\uff09\u5e72\u9884\u4ec5\u9488\u5bf9\u751f\u547d\u5468\u671f\u4e2d\u7684\u5b64\u7acb\u9636\u6bb5\u800c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6e38\u6548\u5e94\uff0c5\uff09\u7f3a\u4e4f\u591a\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u9f50\uff0c\u5bfc\u81f4\u591a\u79cd\u516c\u5e73\u6027\u5b9a\u4e49\u540c\u65f6\u76f8\u5173\u3002", "conclusion": "\u6211\u4eec\u5efa\u8bae\u5efa\u7acb\u4e00\u4e2a\u4ee5\u5371\u5bb3\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5c06\u516c\u5e73\u6027\u5b9a\u4e49\u4e0e\u5177\u4f53\u98ce\u9669\u548c\u5229\u76ca\u76f8\u5173\u8005\u7684\u8106\u5f31\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u63d0\u4f9b\u5efa\u8bae\u4ee5\u63a8\u52a8\u66f4\u5168\u9762\u3001\u5177\u6709\u60c5\u5883\u610f\u8bc6\u548c\u8d23\u4efb\u5fc3\u7684\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7814\u7a76\u3002"}}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u610f\u8bc6\u7684\u591a\u4ee3\u7406\u7cfb\u7edfCo-Saving\uff0c\u5b83\u5229\u7528\u7ecf\u9a8c\u77e5\u8bc6\u548c\"\u5feb\u6377\u8def\u5f84\"\u6765\u63d0\u9ad8\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u4e0e\u6700\u5148\u8fdb\u7684MAS\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5176token\u4f7f\u7528\u91cf\u51cf\u5c1150.85%\uff0c\u4ee3\u7801\u8d28\u91cf\u63d0\u9ad810.06%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u81ea\u4e3b\u4ee3\u7406\u5728\u8bb8\u591a\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u72ec\u7acb\u4ee3\u7406\u5728\u5904\u7406\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684\u590d\u6742\u4efb\u52a1\u65f6\u5e38\u5e38\u9047\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u610f\u8bc6\u7684\u591a\u4ee3\u7406\u7cfb\u7edf--Co-Saving\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u7ecf\u9a8c\u77e5\u8bc6\u6765\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u5e76\u5f15\u5165\u5386\u53f2\u6210\u529f\u8f68\u8ff9\u4e2d\u5b66\u5230\u7684\"\u5feb\u6377\u8def\u5f84\"\u6765\u52a0\u901f\u96c6\u4f53\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u3002", "result": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0cCo-Saving\u6a21\u578b\u76f8\u6bd4\u4e8e\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u4e0e\u6700\u5148\u8fdb\u7684MAS ChatDev\u76f8\u6bd4\uff0c\u5e73\u5747\u51cf\u5c11\u4e8650.85%\u7684token\u4f7f\u7528\u91cf\uff0c\u5e76\u63d0\u9ad8\u4e8610.06%\u7684\u6574\u4f53\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "\u5f15\u5165\u8d44\u6e90\u610f\u8bc6\u7684Co-Saving\u591a\u4ee3\u7406\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
