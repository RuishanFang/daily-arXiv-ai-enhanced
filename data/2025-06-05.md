<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 32]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math-ph](#math-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/abs/2506.03259)
*Michael E. Garcia-Alcoser,Mobina GhojoghNejad,Fakrul Islam Tushar,David Kim,Kyle J. Lafata,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CL

TL;DR: 研究表明，轻量级LLMs在CT报告的疾病标注方面优于基于规则的方法，特别是在多个器官系统中能够进行泛化，但标签的细微差别仍有待解决。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型(LLMs)在自动化CT放射学报告疾病标注中的有效性，并比较基于规则的算法(RBA)、RadBERT和三个轻量级开源LLMs在胸部、腹部和骨盆(CAP)CT报告的多疾病标注方面的表现。

Method: 这项回顾性研究分析了来自29,540名患者的40,833份CT报告，其中1,789份CAP报告通过三个器官系统进行了人工注释。通过使用CT-RATE数据集进行外部验证，测试了三个开源轻量级LLMs的零样本提示，并通过Cohen's Kappa和微观/宏观平均F1分数来评估性能。

Result: 在来自8,854名患者的12,197份Duke CAP报告中，Llama-3.1 8B和Gemma-3 27B表现出最高的一致性(κ中值: 0.87)。在人为标注的报告中，Gemma-3 27B的宏观F1得分最高(0.82)，其次是Llama-3.1 8B(0.79)，而RBA得分最低(0.64)。在CT-RATE数据集(仅针对肺/胸膜)，Llama-3.1 8B表现最佳(0.91)，Gemma-3 27B紧随其后(0.89)。性能差异主要源于不同的标注实践，特别是在肺不张方面。

Conclusion: 轻量级LLMs在CT报告的注释方面优于基于规则的方法，并且能够通过零样本提示在多个器官系统中进行泛化。然而，仅靠二元标签无法完全捕捉报告语言的细微差别。LLMs能够提供与临床判断和用户需求相一致的灵活高效解决方案。

Abstract: Purpose: This study aims to evaluate the effectiveness of large language
models (LLMs) in automating disease annotation of CT radiology reports. We
compare a rule-based algorithm (RBA), RadBERT, and three lightweight
open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)
CT reports.
  Materials and Methods: This retrospective study analyzed 40,833 CT reports
from 29,540 patients, with 1,789 CAP reports manually annotated across three
organ systems. External validation was conducted using the CT-RATE dataset.
Three open-weight LLMs were tested with zero-shot prompting. Performance was
evaluated using Cohen's Kappa and micro/macro-averaged F1 scores.
  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and
Gemma-3 27B showed the highest agreement ($\kappa$ median: 0.87). On the
manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed
by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE
dataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3
27B close behind (0.89). Performance differences were mainly due to differing
labeling practices, especially for lung atelectasis.
  Conclusion: Lightweight LLMs outperform rule-based methods for CT report
annotation and generalize across organ systems with zero-shot prompting.
However, binary labels alone cannot capture the full nuance of report language.
LLMs can provide a flexible, efficient solution aligned with clinical judgment
and user needs.

</details>


### [2] [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/abs/2506.03268)
*Cristiano Chesi*

Main category: cs.CL

TL;DR: 撰写了一篇关于目标论文回复的总结性评论，旨在推动讨论。


<details>
  <summary>Details</summary>
Motivation: 整理和总结对目标论文的回复，以推动语言学领域的进一步讨论和研究。

Method: 暂无具体研究方法，仅为一个关于回复的总结性评论。

Result: 提供了对语言学目标论文回复的最终评价。

Conclusion: 论文回收了对意大利语言学杂志目标论文的回复进行最终评价。

Abstract: This is the final remark on the replies received to my target paper in the
Italian Journal of Linguistics

</details>


### [3] [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/abs/2506.03278)
*Christodoulos Constantinides,Dhaval Patel,Shuxin Lin,Claudio Guerrero,Sunil Dagajirao Patil,Jayant Kalagnanam*

Main category: cs.CL

TL;DR: 提出了一种新的MCQA基准系统FailureSensorIQ，以评估大型语言模型在工业4.0复杂领域场景中的表现，并揭示了模型面对扰动和知识缺陷时性能下降的情况。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在工业4.0复杂领域场景中的推理和理解能力，以及推动使用专门的语言模型进行基于领域的推理而非仅依赖统计数据技术。

Method: 构建FailureSensorIQ系统，通过扰动-不确定性-复杂度分析、专家评估、知识缺口分析等方法评估超过十种语言模型在工业知识方面的表现。

Result: 虽然封闭源模型表现出强大的推理能力，但面对干扰、分心以及知识缺口时，其性能显著下降。还提供了一个关于如何在三个不同的故障预测数据集上使用语言模型进行建模决策的实际案例研究。

Conclusion: 专门的语言模型可以显著改善工业领域复杂场景中的推理能力，但是仍存在对模型稳定性和内在知识缺口的挑战。

Abstract: We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)
benchmarking system designed to assess the ability of Large Language Models
(LLMs) to reason and understand complex, domain-specific scenarios in Industry
4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects
of reasoning through failure modes, sensor data, and the relationships between
them across various industrial assets. Through this work, we envision a
paradigm shift where modeling decisions are not only data-driven using
statistical tools like correlation analysis and significance tests, but also
domain-driven by specialized LLMs which can reason about the key contributors
and useful patterns that can be captured with feature engineering. We evaluate
the Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and
Mistral-on FailureSensorIQ from different lens using
Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study,
Asset-Specific Knowledge Gap analysis, ReAct agent using external
knowledge-bases. Even though closed-source models with strong reasoning
capabilities approach expert-level performance, the comprehensive benchmark
reveals a significant drop in performance that is fragile to perturbations,
distractions, and inherent knowledge gaps in the models. We also provide a
real-world case study of how LLMs can drive the modeling decisions on 3
different failure prediction datasets related to various assets. We release:
(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ
benchmark and Hugging Face leaderboard based on MCQA built from non-textual
data found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature
selection scikit-learn pipeline. The software is available at
https://github.com/IBM/FailureSensorIQ.

</details>


### [4] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
*Jiuding Sun,Sidharth Baskaran,Zhengxuan Wu,Michael Sklar,Christopher Potts,Atticus Geiger*

Main category: cs.CL

TL;DR: 本研究引入HyperSteer方法，通过超网络生成方向向量有效引导语言模型，解决了现有方法的不足，在性能上优于或等同于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在通过修改内部激活来引导语言模型方面存在一些不足，特别是在无监督方法中，效果没有保证，而有监督的方法又需要大量的数据和训练。我们希望解决这些问题。

Method: 我们提出了一种基于超网络的架构，名为HyperSteer，通过端到端训练生成方向向量，这些向量是根据自然语言方向提示和被指引语言模型的内部情况生成的。

Result: HyperSteer方法在使用成千上万的指引提示进行扩展时，其性能超过了最先进的激活控制方法，甚至在训练中未见过的指引提示上也是如此。同时，HyperSteer在效果上与通过提示进行指引的方法相当。

Conclusion: 通过引入HyperSteer方法，我们成功提高了语言模型的指引效果，特别是在未见过的指引任务上，表现超过了现有的激活控制方法，并与通过提示进行指引的方法表现相当。

Abstract: Steering language models (LMs) by modifying internal activations is a popular
approach for controlling text generation. Unsupervised dictionary learning
methods, e.g., sparse autoencoders, can be scaled to produce many steering
vectors, but lack guarantees on the individual efficacy of each vector and
control over the coverage of relevant steering tasks. In contrast, supervised
methods for constructing steering vectors are targeted and effective, but
require more data collection and training for each additional steering vector
produced. In this work, we introduce HyperSteer, a family of hypernetwork-based
architectures which are trained end-to-end to generate steering vectors
conditioned on the natural language steering prompts and the internals of the
steered LM. In our evaluations, we show that scaling HyperSteer with thousands
of steering prompts exceeds the performance of state-of-the-art activation
steering methods, even on steering prompts never seen during training.
Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [5] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
*Yubo Wang,Ping Nie,Kai Zou,Lijun Wu,Wenhu Chen*

Main category: cs.CL

TL;DR: 批评微调（CFT）作为一种简单、通用、计算高效的方法，可以有效地释放现代大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法虽然能提升 LLM 的推理能力，但成本高且不稳定，寻求一种更高效的方法以释放 LLM 的推理潜能。

Method: 通过构建批评数据，即收集单个问题的多样模型生成解，并利用教师 LLMs 提供详细批评，来进行微调。

Result: 使用 CFT 方法仅需 5 个 GPU 小时训练，Qwen-Math-7B-CFT 在六个数学基准上的平均提高了 15%，在三个逻辑推理基准上的性能提高了 16%，实现了与 RL 可比甚至更优的效果，且计算成本降低 20 倍。

Conclusion: 一体式批评微调（CFT）可以有效激发大型语言模型（LLM）的推理潜力。

Abstract: We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess
immense reasoning potential inherited from the pre-training stage. With
reinforcement learning (RL), these models can improve dramatically on reasoning
tasks. Recent studies have shown that even RL on a single problem can unleash
these models' reasoning capabilities. However, RL is not only expensive but
also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a
critical question: Is there a more efficient way to unleash the reasoning
potential of these powerful base LLMs? In this work, we demonstrate that
Critique Fine-Tuning (CFT) on only one problem can effectively unleash the
reasoning potential of LLMs. Our method constructs critique data by collecting
diverse model-generated solutions to a single problem and using teacher LLMs to
provide detailed critiques. We fine-tune Qwen and Llama family models, ranging
from 1.5B to 14B parameters, on the CFT data and observe significant
performance gains across diverse reasoning tasks. For example, with just 5 GPU
hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six
math benchmarks and 16% on three logic reasoning benchmarks. These results are
comparable to or even surpass the results from RL with 20x less compute.
Ablation studies reveal the robustness of one-shot CFT across different prompt
problems. These results highlight one-shot CFT as a simple, general, and
compute-efficient approach to unleashing the reasoning capabilities of modern
LLMs.

</details>


### [6] [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/abs/2506.03301)
*Daham M. Mustafa,Abhishek Nadgeri,Diego Collarana,Benedikt T. Arnold,Christoph Quix,Christoph Lange,Stefan Decker*

Main category: cs.CL

TL;DR: 研究通过GPT-4等大语言模型从自然语言指令自动生成ODRL使用政策，展示了在文化领域的dataspaces中策略生成的高效性，准确率达91.95%。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是验证优化和策划的本体文档是否能更有效地指导策略生成，并提升在分布式信任数据交换基础设施中使用政策生成的准确性和实用性。

Method: 研究采用了一种使用大语言模型（如GPT-4）的方法，通过从自然语言指令中自动生成W3C开放数字权利语言（ODRL）使用政策。中枢提示为ODRL本体及其文档，并提出了多种启发式方法来适应ODRL本体和文档，以指导端到端的知识图谱构建过程。

Result: 评估结果显示，他们的方法在文化领域dataspaces中，知识图谱的构建准确率高达91.95%。

Conclusion: 该研究结果表明，使用经过策划的本体文档可以更好地指导策略生成，并在文化领域的dataspaces中取得高达91.95%的准确率。

Abstract: This study presents an approach that uses large language models such as GPT-4
to generate usage policies in the W3C Open Digital Rights Language ODRL
automatically from natural language instructions. Our approach uses the ODRL
ontology and its documentation as a central part of the prompt. Our research
hypothesis is that a curated version of existing ontology documentation will
better guide policy generation. We present various heuristics for adapting the
ODRL ontology and its documentation to guide an end-to-end KG construction
process. We evaluate our approach in the context of dataspaces, i.e.,
distributed infrastructures for trustworthy data exchange between multiple
participating organizations for the cultural domain. We created a benchmark
consisting of 12 use cases of varying complexity. Our evaluation shows
excellent results with up to 91.95% accuracy in the resulting knowledge graph.

</details>


### [7] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
*Mustafa Eyceoz,Nikhil Shivakumar Nayak,Hao Wang,Ligong Han,Akash Srivastava*

Main category: cs.CL

TL;DR: Hopscotch方法通过识别并跳过对任务贡献最小的注意力块，实现了性能的提升，同时确保输出质量不受影响。


<details>
  <summary>Details</summary>
Motivation: 许多现代因果语言模型堆叠了许多注意力块来提高性能，但并非所有注意力块对于每个任务都是必要的。

Method: Hopscotch通过引入轻量级、可训练的缩放参数来优化跳过哪些注意力块，并根据需要调整剩余层的输出。

Result: 经过Hopscotch处理后，即便跳过了四个注意力块，性能下降也不超过2%。

Conclusion: Hopscotch可以在不修改模型权重及无需访问预训练数据的情况下，有效提升模型性能。

Abstract: Modern causal language models stack many attention blocks to improve
performance, but not all blocks are necessary for every task. We propose
Hopscotch, a simple yet effective method that identifies and skips attention
blocks with least contributions to a task and adapts to preserve output
quality. Hopscotch jointly optimizes which blocks to skip and how to scale the
outputs of the remaining layers. By introducing lightweight, trainable scaling
parameters to attention and MLP blocks, it mitigates distribution shifts in
hidden states caused by removing attention blocks. Hopscotch does not modify
model weights or require access to pretraining or instruction-tuning data, and
is compatible with existing model compression techniques. When applied to
$\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than
a 2% drop in performance even after skipping four attention blocks.

</details>


### [8] [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)
*Guillermo Marco,Julio Gonzalo,Víctor Fresno*

Main category: cs.CL

TL;DR: 论文探讨了人工智能生成和人类创作的文学作品品质评估的分歧，揭示了读者偏好的差异影响文本评价。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解释人工智能生成的文学文本与人类创作的文学作品在品质评估上的分歧，以及读者在解读和评价文学作品时的差异。

Method: 采用五个公共数据集，包括1471个故事和101名注释者，通过提取17个无参考的文本特征，建模个人读者偏好，分析特征重要性向量，并在共享的“偏好空间”中分析这些向量。

Result: 研究发现读者的偏好向量聚集成两种类型：一类是'表面关注型读者'，他们主要关注可读性和文本丰富性；另一类是'整体关注型读者'，他们更看重主题发展、修辞多样性和情感动态。

Conclusion: 不同的读者偏好显著影响对文学作品质量的评估，建议在创意文本生成领域采用读者敏感的评价框架。

Abstract: Recent studies comparing AI-generated and human-authored literary texts have
produced conflicting results: some suggest AI already surpasses human quality,
while others argue it still falls short. We start from the hypothesis that such
divergences can be largely explained by genuine differences in how readers
interpret and value literature, rather than by an intrinsic quality of the
texts evaluated. Using five public datasets (1,471 stories, 101 annotators
including critics, students, and lay readers), we (i) extract 17 reference-less
textual features (e.g., coherence, emotional variance, average sentence
length...); (ii) model individual reader preferences, deriving feature
importance vectors that reflect their textual priorities; and (iii) analyze
these vectors in a shared "preference space". Reader vectors cluster into two
profiles: 'surface-focused readers' (mainly non-experts), who prioritize
readability and textual richness; and 'holistic readers' (mainly experts), who
value thematic development, rhetorical variety, and sentiment dynamics. Our
results quantitatively explain how measurements of literary quality are a
function of how text features align with each reader's preferences. These
findings advocate for reader-sensitive evaluation frameworks in the field of
creative text generation.

</details>


### [9] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
*Celia Chen,Scotty Beland,Ingo Burghardt,Jill Byczek,William J. Conway,Eric Cotugno,Sadaf Davre,Megan Fletcher,Rajesh Kumar Gnanasekaran,Kristin Hamilton,Marilyn Harbert,Jordan Heustis,Tanaya Jha,Emily Klein,Hayden Kramer,Alex Leitch,Jessica Perkins,Casi Sherman,Celia Sterrn,Logan Stevens,Rebecca Zarrella,Jennifer Golbeck*

Main category: cs.CL

TL;DR: 该研究引入了一个跨平台的暴力威胁数据集，并通过机器学习分析实现高分类准确性，有助于社交媒体暴力内容的理解和分类。


<details>
  <summary>Details</summary>
Motivation: 暴力威胁在社交媒体平台上仍然是一个显著问题。高质量的数据可以帮助进行恶意内容的理解和检测，这包括暴力行为。因此，我们引入了一个手动编码的跨平台数据集，包含3万个帖子以识别暴力威胁及其子类型。

Method: 我们进行了机器学习分析，将新创建的跨平台暴力威胁数据集与来自YouTube的现有暴力评论数据集进行比较。通过训练一个数据集并在另一个数据集上进行测试，以及在合并数据集条件下进行测试，评价数据集中信号的有效性。

Result: 研究结果显示，无论是从一个数据集进行训练并在另一个数据集进行测试，还是在合并数据集条件下，都可以实现高度的分类准确性。

Conclusion: 该研究表明，即使来自不同的平台并使用不同的编码标准，仍然可以在分类准确性方面取得良好的效果。这对于内容分类策略和理解社交媒体上的暴力内容具有重要意义。

Abstract: Violent threats remain a significant problem across social media platforms.
Useful, high-quality data facilitates research into the understanding and
detection of malicious content, including violence. In this paper, we introduce
a cross-platform dataset of 30,000 posts hand-coded for violent threats and
sub-types of violence, including political and sexual violence. To evaluate the
signal present in this dataset, we perform a machine learning analysis with an
existing dataset of violent comments from YouTube. We find that, despite
originating from different platforms and using different coding criteria, we
achieve high classification accuracy both by training on one dataset and
testing on the other, and in a merged dataset condition. These results have
implications for content-classification strategies and for understanding
violent content across social media.

</details>


### [10] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
*Aldan Creo,Héctor Cerezo-Costas,Pedro Alonso-Doval,Maximiliano Hormazábal-Lagos*

Main category: cs.CL

TL;DR: 提出了一种名为"Ask a Local"的多语言幻觉检测方法，通过专业模型的困惑度分布差异识别幻觉，尤其在意大利语和加泰罗尼亚语中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的幻觉，即生成看似合理但事实上不正确的信息，对人工智能提出重大挑战。

Method: 提出"Ask a Local"方法，该方法通过计算语言专业模型感知难度分布之间的分歧来识别潜在的幻觉段落。该方法适合多语言环境，无需适应或使用外部数据源，亦无需进行训练。

Result: 在人类注释的跨14种语言的问题回答数据集中展示了一致的性能，IoU分数约为0.3，Spearman相关值相当。特别是在意大利语和加泰罗尼亚语上，IoU分数分别为0.42和0.38。

Conclusion: 我们的方法在多种语言环境中显示出一致的性能，尤其是在意大利语和加泰罗尼亚语上表现出色，并且在无需进行语言特定的调整情况下保持了跨语言的有效性。

Abstract: Hallucinations in large language models (LLMs) - instances where models
generate plausible but factually incorrect information - present a significant
challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting
the intuition that specialized models exhibit greater surprise when
encountering domain-specific inaccuracies. Our approach computes divergence
between perplexity distributions of language-specialized models to identify
potentially hallucinated spans. Our method is particularly well-suited for a
multilingual context, as it naturally scales to multiple languages without the
need for adaptation, relying on external data sources, or performing training.
Moreover, we select computationally efficient models, providing a scalable
solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14
languages demonstrate consistent performance across languages, with
Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman
correlation values. Our model shows particularly strong performance on Italian
and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining
cross-lingual effectiveness without language-specific adaptations. We release
our code and architecture to facilitate further research in multilingual
hallucination detection.

</details>


### [11] [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)
*Zihui Ma,Lingyao Li,Juan Li,Wenyue Hua,Jingxiao Liu,Qingyuan Feng,Yuki Miura*

Main category: cs.CL

TL;DR: 研究提出使用多模态语言模型的3M管道评估灾难影响，展示了模型在地震数据中的有效性以及多种参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 快速、精准的灾害损失评估对于有效的紧急应对至关重要，但由于地面传感器的限制和官方报告的滞后，仍然充满挑战。社交媒体提供了丰富的实时人类观察数据，但其多模态和非结构化性质对传统分析方法构成挑战。

Method: 提出了一种多模态、多语言、多维度(3M)管道，使用多模态大型语言模型评估灾难影响。评估了三个基础模型在两个重大地震事件中的表现。

Result: MLLMs通过整合图像和文本信号，在地震相关数据评估中表现出强相关性，但不同语言、震中距离和输入模态的表现有所差异。

Conclusion: 该研究展示了多模态大型语言模型(MLLMs)能够有效集成图像和文本信号，并与地震基础数据呈现强相关性，未来有潜力用于实时危机评估。

Abstract: Rapid, fine-grained disaster damage assessment is essential for effective
emergency response, yet remains challenging due to limited ground sensors and
delays in official reporting. Social media provides a rich, real-time source of
human-centric observations, but its multimodal and unstructured nature presents
challenges for traditional analytical methods. In this study, we propose a
structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that
leverages multimodal large language models (MLLMs) to assess disaster impacts.
We evaluate three foundation models across two major earthquake events using
both macro- and micro-level analyses. Results show that MLLMs effectively
integrate image-text signals and demonstrate a strong correlation with
ground-truth seismic data. However, performance varies with language,
epicentral distance, and input modality. This work highlights the potential of
MLLMs for disaster assessment and provides a foundation for future research in
applying MLLMs to real-time crisis contexts. The code and data are released at:
https://github.com/missa7481/EMNLP25_earthquake

</details>


### [12] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
*Yi Xu,Ruining Yang,Yitian Zhang,Yizhou Wang,Jianglin Lu,Mingyuan Zhang,Lili Su,Yun Fu*

Main category: cs.CL

TL;DR: 本文综述语言模型在轨迹预测领域的应用发展，包括方法分类、设计选择和挑战分析，展示语言如何提升预测能力。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在语言驱动技术中的进展，作者希望通过调查研究来探索语言模型在轨迹预测中的应用并分析其效果。

Method: 对现有工作进行分类，分析了代表性的方法，重点突出核心设计选择，并识别出开放的挑战。

Result: 分类现有工作为五个方向，详细分析每个方向下模拟研究的代表性方法和挑战。

Conclusion: 本文综述了语言模型在轨迹预测中的应用，提供了一个统一的视角，展示了语言如何丰富轨迹预测。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in integrating language-driven techniques into trajectory prediction. By
leveraging their semantic and reasoning capabilities, LLMs are reshaping how
autonomous systems perceive, model, and predict trajectories. This survey
provides a comprehensive overview of this emerging field, categorizing recent
work into five directions: (1) Trajectory prediction via language modeling
paradigms, (2) Direct trajectory prediction with pretrained language models,
(3) Language-guided scene understanding for trajectory prediction, (4)
Language-driven data generation for trajectory prediction, (5) Language-based
reasoning and interpretability for trajectory prediction. For each, we analyze
representative methods, highlight core design choices, and identify open
challenges. This survey bridges natural language processing and trajectory
prediction, offering a unified perspective on how language can enrich
trajectory prediction.

</details>


### [13] [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/abs/2506.03424)
*Nicole R Schneider,Nandini Ramachandran,Kent O'Sullivan,Hanan Samet*

Main category: cs.CL

TL;DR: DistRAG 是一种新方法，通过编码空间距离来增强大型语言模型（LLM）的空间推理能力，使其能够回答距离相关的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）缺乏空间推理能力尤其是在距离问题上的不足。

Method: 开发了一种名为 DistRAG 的新方法，该方法通过在图中编码城市和城镇之间的测地距离来辅助 LLM 检索相关空间信息。

Result: 通过使用该方法，LLM 能够解答基于距离的推理问题，从而扩展其应用能力。

Conclusion: DistRAG 为大型语言模型（LLM）提供了一种解决空间推理问题的方法，通过引入地理信息以补充其语言知识。

Abstract: Many real world tasks where Large Language Models (LLMs) can be used require
spatial reasoning, like Point of Interest (POI) recommendation and itinerary
planning. However, on their own LLMs lack reliable spatial reasoning
capabilities, especially about distances. To address this problem, we develop a
novel approach, DistRAG, that enables an LLM to retrieve relevant spatial
information not explicitly learned during training. Our method encodes the
geodesic distances between cities and towns in a graph and retrieves a context
subgraph relevant to the question. Using this technique, our method enables an
LLM to answer distance-based reasoning questions that it otherwise cannot
answer. Given the vast array of possible places an LLM could be asked about,
DistRAG offers a flexible first step towards providing a rudimentary `world
model' to complement the linguistic knowledge held in LLMs.

</details>


### [14] [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
*Ahmad Dawar Hakimi,Ali Modarressi,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究分析了OLMo-7B模型中知识表征的演变，发现随着训练的进行，模型组件逐渐专业化，注意头变化最大，而FFNs较为稳定。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型获取和存储事实知识的机制，以提高其可解释性和可靠性。

Method: 通过跟踪OLMo-7B模型中注意头和前馈网络的角色变化，分类为四种角色并研究其稳定性和变迁。

Result: 随着训练进展，模型从依赖广泛的通用组件转向更专业化，并且一些组件被重新利用。注意头的变化最高，而FFNs相对更稳定。此外，基于位置的关系比基于名称的关系更早收敛。

Conclusion: 研究展示了LLMs在预训练中知识表征的演变过程，揭示了模型组件的稳定性以及它们如何为适应知识预测要求而进行重组。

Abstract: Understanding how large language models (LLMs) acquire and store factual
knowledge is crucial for enhancing their interpretability and reliability. In
this work, we analyze the evolution of factual knowledge representation in the
OLMo-7B model by tracking the roles of its attention heads and feed forward
networks (FFNs) over the course of pre-training. We classify these components
into four roles: general, entity, relation-answer, and fact-answer specific,
and examine their stability and transitions. Our results show that LLMs
initially depend on broad, general-purpose components, which later specialize
as training progresses. Once the model reliably predicts answers, some
components are repurposed, suggesting an adaptive learning process. Notably,
attention heads display the highest turnover. We also present evidence that
FFNs remain more stable throughout training. Furthermore, our probing
experiments reveal that location-based relations converge to high accuracy
earlier in training than name-based relations, highlighting how task complexity
shapes acquisition dynamics. These insights offer a mechanistic view of
knowledge formation in LLMs.

</details>


### [15] [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/abs/2506.03458)
*Zahra Bokaei,Walid Magdy,Bonnie Webber*

Main category: cs.CL

TL;DR: 波斯语有害语言检测研究探索不同方法，发现文化相近的语言在跨语言迁移学习中效果更佳。


<details>
  <summary>Details</summary>
Motivation: 研究波斯语中的有害语言检测并探讨文化背景对迁移学习效果的影响。

Method: 通过比较不同的方法，包括微调、数据丰富、零样本和小样本学习，以及跨语言迁移学习，来研究波斯语的有害语言检测。

Result: 与具有相似文化背景的语言进行跨语言迁移学习能提高检测效果，而文化差异较大的语言所带来的改善较小。

Conclusion: 具有文化相似性的语言在跨语言迁移学习中表现更好，而文化差异较大的语言对迁移效果影响较小。

Abstract: Toxic language detection is crucial for creating safer online environments
and limiting the spread of harmful content. While toxic language detection has
been under-explored in Persian, the current work compares different methods for
this task, including fine-tuning, data enrichment, zero-shot and few-shot
learning, and cross-lingual transfer learning. What is especially compelling is
the impact of cultural context on transfer learning for this task: We show that
the language of a country with cultural similarities to Persian yields better
results in transfer learning. Conversely, the improvement is lower when the
language comes from a culturally distinct country. Warning: This paper contains
examples of toxic language that may disturb some readers. These examples are
included for the purpose of research on toxic detection.

</details>


### [16] [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2506.03476)
*Chuyuan Li,Raymond Li,Thalia S. Field,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 引入Delta-KNN策略提高语言模型在阿尔茨海默病（AD）诊断中的表现，优于现有方法并创下业界新高。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是一种神经退行性疾病，早期干预效果显著，因此通过分析语言异常进行诊断具有重要意义。大规模语言模型（LLMs）作为健康助手在这一领域具有潜力。

Method: 我们提出了Delta-KNN，一种新的示范选择策略，通过利用增量分数评估各训练样本的相对收益，结合基于KNN的检索器动态选择给定输入的最佳“代表”。

Result: Delta-KNN在两个阿尔茨海默病检测数据集和三个开源LLM上的实验结果表明，其性能稳定优于现有ICL基准，尤其是使用Llama-3.1模型时，Delta-KNN实现了超越监督分类器的新业界最优效果。

Conclusion: 我们的研究表明，Delta-KNN作为一种新的示范选择策略，可以显著提高ICL在阿尔茨海默病诊断任务中的性能。使用Llama-3.1模型，Delta-KNN方法能够超过现有的ICL基准，并取得新的最先进成果，优于监督分类器。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that
leads to dementia, and early intervention can greatly benefit from analyzing
linguistic abnormalities. In this work, we explore the potential of Large
Language Models (LLMs) as health assistants for AD diagnosis from
patient-generated text using in-context learning (ICL), where tasks are defined
through a few input-output examples. Empirical results reveal that conventional
ICL methods, such as similarity-based selection, perform poorly for AD
diagnosis, likely due to the inherent complexity of this task. To address this,
we introduce Delta-KNN, a novel demonstration selection strategy that enhances
ICL performance. Our method leverages a delta score to assess the relative
gains of each training example, coupled with a KNN-based retriever that
dynamically selects optimal "representatives" for a given input. Experiments on
two AD detection datasets across three open-source LLMs demonstrate that
Delta-KNN consistently outperforms existing ICL baselines. Notably, when using
the Llama-3.1 model, our approach achieves new state-of-the-art results,
surpassing even supervised classifiers.

</details>


### [17] [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/abs/2506.03483)
*Jun Rao,Zepeng Lin,Xuebo Liu,Xiaopeng Ke,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出APT方法，通过自生成的劣势数据进行训练，增强领域特定性能，并保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 在提升领域特定能力的同时保持模型的通用能力是一大挑战，需在两者之间找到平衡。

Method: 引入APT（一种弱点案例获取和迭代偏好训练的方法），专注于使用模型出错的样本和少量相似样本进行训练。

Result: 实验结果表明，APT方法在不降低通用能力的情况下，在下游任务上比现有方法取得了更好的性能。

Conclusion: APT方法被验证为在提高领域特定能力的同时不影响模型的广泛适用性，是一种有效的策略。

Abstract: Large Language Models (LLMs) often require domain-specific fine-tuning to
address targeted tasks, which risks degrading their general capabilities.
Maintaining a balance between domain-specific enhancements and general model
utility is a key challenge. This paper proposes a novel approach named APT
(Weakness Case Acquisition and Iterative Preference Training) to enhance
domain-specific performance with self-generated dis-preferred weakness data
(bad cases and similar cases). APT uniquely focuses on training the model using
only those samples where errors occur, alongside a small, similar set of
samples retrieved for this purpose. This targeted training minimizes
interference with the model's existing knowledge base, effectively retaining
generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3
models across various benchmarks demonstrate that APT ensures no reduction in
generic capacity and achieves superior performance on downstream tasks compared
to various existing methods. This validates our method as an effective strategy
for enhancing domain-specific capabilities without sacrificing the model's
broader applicability.

</details>


### [18] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
*Melkamu Abay Mersha,Mesay Gemeda Yigezu,Atnafu Lambebo Tonja,Hassan Shakil,Samer Iskander,Olga Kolesnikova,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出了一种XAI引导的上下文感知数据增强框架，显著提高了AI模型在仇恨言论和情感分析任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决标记数据稀缺性和传统数据增强技术带来的噪声、语义漂移、上下文连贯性破坏、缺乏控制和过拟合问题。

Method: 提出XAI引导的上下文感知数据增强框架，利用XAI技术修改不太重要的特征，同时选择性地保留大多数任务相关特征，包括一个迭代反馈循环，通过基于可解释性驱动的见解和模型性能增益来细化增强数据。

Result: XAI-SR-BT和XAI-PR-BT在仇恨言论和情感分析任务上比基线增加了6.6%和8.1%的准确性，并且在各种任务和模型中一贯超过基线和传统增强技术。

Conclusion: XAI-SR-BT和XAI-PR-BT在仇恨言论和情感分析任务上提高了模型的准确性，分别比基线模型提高了6.6%和8.1%，同时在Amharic数据集上也超过了现有的增强技术。

Abstract: Explainable AI (XAI) has emerged as a powerful tool for improving the
performance of AI models, going beyond providing model transparency and
interpretability. The scarcity of labeled data remains a fundamental challenge
in developing robust and generalizable AI models, particularly for low-resource
languages. Conventional data augmentation techniques introduce noise, cause
semantic drift, disrupt contextual coherence, lack control, and lead to
overfitting. To address these challenges, we propose XAI-Guided Context-Aware
Data Augmentation. This novel framework leverages XAI techniques to modify less
critical features while selectively preserving most task-relevant features. Our
approach integrates an iterative feedback loop, which refines augmented data
over multiple augmentation cycles based on explainability-driven insights and
the model performance gain. Our experimental results demonstrate that XAI-SR-BT
and XAI-PR-BT improve the accuracy of models on hate speech and sentiment
analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using
the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform
existing augmentation techniques by 4.8% and 5%, respectively, on the same
dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform
both baseline and conventional augmentation techniques across all tasks and
models. This study provides a more controlled, interpretable, and context-aware
solution to data augmentation, addressing critical limitations of existing
augmentation techniques and offering a new paradigm shift for leveraging XAI
techniques to enhance AI model training.

</details>


### [19] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
*Mingxu Tao,Jie Hu,Mingchuan Yang,Yunhuai Liu,Dongyan Zhao,Yansong Feng*

Main category: cs.CL

TL;DR: EpiCoDe在数据不足情况下提高LLMs性能，不需要额外训练，实验表明其优于现有方法并提供了理论框架解释其机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的卓越性能很大程度上依赖于高质量训练数据的可用性。然而，获取标注数据的高成本阻碍了模型应对下游任务的能力。

Method: 我们提出了一种新方法EpiCoDe，通过模型外推增强微调模型，并采用对比解码，通过比较外推模型和普通微调模型的logit分数来降低预测错误。

Result: EpiCoDe在数据稀缺场景中无需额外训练即可提高模型性能，在三个任务和四个不同LLMs上的实验显示了显著和稳定的改进。

Conclusion: EpiCoDe方法在数据稀缺场景下展示了其有效性，并通过新的理论框架揭示了对比解码的机制。

Abstract: The remarkable performance of Large language models (LLMs) relies heavily on
the availability of abundant high-quality training data. However, the high cost
of acquiring annotated data often prevents models from obtaining capabilities
to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe
that boosts model performance in data-scarcity scenarios without extra
training. We first employ model extrapolation to enhance a finetuned model with
its inferior version, and then adopt contrastive decoding to further reduce
predicted errors, by comparing the logit scores given by the extrapolated and
the vanilla finetuned model. Experiments across three tasks over four different
LLMs show that EpiCoDe consistently outperforms existing methods with
significant and robust improvement. We also propose a new theoretical framework
to reveal the mechanism behind contrastive decoding in data-scarcity scenarios,
which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [20] [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/abs/2506.03490)
*Shigeng Chen,Linhao Luo,Zhangchi Qiu,Yanan Cao,Carl Yang,Shirui Pan*

Main category: cs.CL

TL;DR: 介绍了用于医学知识编辑的新框架MedEditBench，并提出了一种改进的自生成理由编辑（SGR-Edit）方法，能够显著提升现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 扩大知识编辑技术在复杂医学领域的应用，以支持有效和可解释的决策。

Method: 提出一个名为MedEditBench的新框架，评估现有知识编辑方法在医学领域的有效性。

Result: 提出的SGR-Edit方法展示了显著的改进，能够揭示背后的推理过程并提高现有方法的效果。

Conclusion: 当前的知识编辑方法只是在LLMs中进行表面记忆，而未能推广到新的场景。

Abstract: Recently, knowledge editing (KE) has emerged as a promising approach to
update specific facts in Large Language Models (LLMs) without the need for full
retraining. Despite the effectiveness in general-domain benchmarks, their
applicability to complex medical domain remains largely unexplored. Medical
knowledge editing is particularly challenging, as it requires LLMs to
internalize the knowledge and generalize to unseen scenarios for effective and
interpretable decision-making. In this work, we propose a novel framework
called MedEditBench to rigorously evaluate the effectiveness of existing KE
methods in the medical domain. In MedEditBench, we introduce a new medical
knowledge editing benchmark as well as three different knowledge editing
paradigms, which are designed to assess the impact of different knowledge
sources for editing. Our findings indicate that current KE methods result in
only superficial memorization of the injected information, failing to
generalize to new scenarios. To overcome this limitation, we present
Self-Generated Rationale Editing (SGR-Edit), which utilizes model-derived
rationales as the target knowledge for editing, thereby uncovering the
underlying reasoning process and demonstrating significant improvements over
existing KE approaches. Additionally, we offer deeper insights into medical
knowledge editing, including the localization of medical knowledge in LLMs and
the impact of sequential editing on evolving knowledge. This could provide
practical guidance for implementing KE methods in real-world medical
applications.

</details>


### [21] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
*Yuchen Guo,Zhicheng Dou,Huy H. Nguyen,Ching-Chun Chang,Saku Sugawara,Isao Echizen*

Main category: cs.CL

TL;DR: 提出了一种通过BERTScore和基于RoBERTa的回归模型来检测生成内容中人工参与的方法，能准确度量参与程度且具有通用性。


<details>
  <summary>Details</summary>
Motivation: 传统的二分类检测方法无法准确识别生成内容中人类与机器的合作，因此需要一种更为精细化的方法来检测和量化人工参与程度。

Method: 使用BERTScore作为衡量人类参与程度的指标，并训练了一个基于RoBERTa的多任务回归模型来解决这一问题。该模型在一个模拟的学术场景中评估其有效性。

Result: 该方法在模拟的学术场景中，F1分数达到0.9423，回归器均方误差为0.004，表现出色，并展示了跨生成模型的一定通用性。

Conclusion: 我们提出了一种新的方法来检测生成内容中的人工参与程度，使用BERTScore作为衡量标准，并利用一种基于RoBERTa的多任务回归模型进行评估。实验结果表明，该方法在检测人工参与方面表现出显著效果，并具有一定的通用性。

Abstract: Content creation has dramatically progressed with the rapid advancement of
large language models like ChatGPT and Claude. While this progress has greatly
enhanced various aspects of life and work, it has also negatively affected
certain areas of society. A recent survey revealed that nearly 30% of college
students use generative AI to help write academic papers and reports. Most
countermeasures treat the detection of AI-generated text as a binary
classification task and thus lack robustness. This approach overlooks human
involvement in the generation of content even though human-machine
collaboration is becoming mainstream. Besides generating entire texts, people
may use machines to complete or revise texts. Such human involvement varies
case by case, which makes binary classification a less than satisfactory
approach. We refer to this situation as participation detection obfuscation. We
propose using BERTScore as a metric to measure human involvement in the
generation process and a multi-task RoBERTa-based regressor trained on a token
classification task to address this problem. To evaluate the effectiveness of
this approach, we simulated academic-based scenarios and created a continuous
dataset reflecting various levels of human involvement. All of the existing
detectors we examined failed to detect the level of human involvement on this
dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor
mean squared error of 0.004). Moreover, it demonstrated some generalizability
across generative models. Our code is available at
https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [22] [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/abs/2506.03510)
*Seungcheol Park,Sojin Lee,Jongjin Kim,Jinsik Lee,Hyunjik Jo,U Kang*

Main category: cs.CL

TL;DR: 提出了一种名为SPRINT的子层剪枝方法，有效地提高了大型语言模型的推理速度，同时保持较高准确性。


<details>
  <summary>Details</summary>
Motivation: 在不影响准确性情况下加速大型语言模型（LLMs）。

Method: 提出一种新的子层剪枝方法SPRINT，通过考虑剪枝后延迟减少量和子层调节能力来选择目标子层进行剪枝。

Result: SPRINT在零样本常识推理基准上表现出比现有剪枝算法高达23.88%更高的准确性，同时实现了最佳的准确性-速度权衡。

Conclusion: SPRINT是一种有效的LLMs子层剪枝方法，能在不降低准确性的情况下显著提高推理速度。

Abstract: How can we accelerate large language models(LLMs) without sacrificing
accuracy? The slow inference speed of LLMs hinders us to benefit from their
remarkable performance in diverse applications. This is mainly because numerous
sublayers are stacked together in LLMs. Sublayer pruning compresses and
expedites LLMs via removing unnecessary sublayers. However, existing sublayer
pruning algorithms are limited in accuracy since they naively select sublayers
to prune, overlooking the different characteristics of each sublayer. In this
paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability
Information), an accurate sublayer pruning method for LLMs. SPRINT accurately
selects a target sublayer to prune by considering 1) the amount of latency
reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively
prunes redundant sublayers and swiftly tunes the parameters of remaining
sublayers. Experiments show that SPRINT achieves the best accuracy-speedup
trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense
reasoning benchmarks compared to existing pruning algorithms.

</details>


### [23] [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/abs/2506.03519)
*Yangyang Zhao,Ben Niu,Libo Qin,Shihan Wang*

Main category: cs.CL

TL;DR: 通过结合EA和DRL并引入精英个体注入机制，该方法改善了对话系统的探索与开发平衡，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决DRL在高维状态和动作空间中探索与开发不平衡的问题，以及自然语言对话任务中的灵活性增加了直接集成的复杂性。

Method: 创新性地结合进化算法（EA）的全局搜索能力和深度强化学习（DRL）的局部优化，提出精英个体注入机制，通过自适应地将表现最佳的个体引入种群来增强进化算法的搜索效率。

Result: 实验表明，该方法在四个数据集上显著改善了探索与开发之间的平衡，提升了性能。精英个体注入机制有效减少了探索时间。

Conclusion: 本文提出的方法实现了进化算法与深度强化学习在面向任务的对话策略任务上的高效集成，显著提升了对话系统的探索与开发平衡。

Abstract: Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue
systems to optimize dialogue policy, but it struggles to balance exploration
and exploitation due to the high dimensionality of state and action spaces.
This challenge often results in local optima or poor convergence. Evolutionary
Algorithms (EAs) have been proven to effectively explore the solution space of
neural networks by maintaining population diversity. Inspired by this, we
innovatively combine the global search capabilities of EA with the local
optimization of DRL to achieve a balance between exploration and exploitation.
Nevertheless, the inherent flexibility of natural language in dialogue tasks
complicates this direct integration, leading to prolonged evolutionary times.
Thus, we further propose an elite individual injection mechanism to enhance
EA's search efficiency by adaptively introducing best-performing individuals
into the population. Experiments across four datasets show that our approach
significantly improves the balance between exploration and exploitation,
boosting performance. Moreover, the effectiveness of the EII mechanism in
reducing exploration time has been demonstrated, achieving an efficient
integration of EA and DRL on task-oriented dialogue policy tasks.

</details>


### [24] [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)
*Chong Li,Jiajun Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: TokAlign通过优化词汇映射和模型微调有效提升大型语言模型在多语言环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 在新领域或语言中，传统分词器效率低下，影响了大型语言模型的训练和生成，并阻碍了模型间的深度知识转移，尤其是词级蒸馏。

Method: TokAlign通过学习一对一映射矩阵来对齐源词汇和目标词汇，调整模型参数和逐步微调以适应新的词汇。

Result: 在不同参数规模的模型实验中，TokAlign方法展现了其有效性和泛化能力，仅需5k步即可恢复原始模型性能，并在词汇统一后，词级蒸馏比句级蒸馏提升了基准模型4.4%的性能。

Conclusion: TokAlign方法显著改善了多语言文本压缩率和大型语言模型的词汇初始化，有效降低了模型困惑度，并在统一词汇后提升了基准模型的性能。

Abstract: Tokenization serves as a foundational step for Large Language Models (LLMs)
to process text. In new domains or languages, the inefficiency of the tokenizer
will slow down the training and generation of LLM. The mismatch in vocabulary
also hinders deep knowledge transfer between LLMs like token-level
distillation. To mitigate this gap, we propose an efficient method named
TokAlign to replace the vocabulary of LLM from the token co-occurrences view,
and further transfer the token-level knowledge between models. It first aligns
the source vocabulary to the target one by learning a one-to-one mapping matrix
for token IDs. Model parameters, including embeddings, are rearranged and
progressively fine-tuned for the new vocabulary. Our method significantly
improves multilingual text compression rates and vocabulary initialization for
LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods
to 1.2$\text{e}^2$ after initialization. Experimental results on models across
multiple parameter scales demonstrate the effectiveness and generalization of
TokAlign, which costs as few as 5k steps to restore the performance of the
vanilla model. After unifying vocabularies between LLMs, token-level
distillation can remarkably boost (+4.4% than sentence-level distillation) the
base model, costing only 235M tokens.

</details>


### [25] [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/abs/2506.03524)
*Yuyu Zhang,Jing Su,Yifan Sun,Chenguang Xi,Xia Xiao,Shen Zheng,Anxiang Zhang,Kaibo Liu,Daoguang Zan,Tao Sun,Jinhua Zhu,Shulin Xin,Dong Huang,Yetao Bai,Lixin Dong,Chao Li,Jianchong Chen,Hanzhi Zhou,Yifan Huang,Guanghan Ning,Xierui Song,Jiaze Chen,Siyao Liu,Kai Shen,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Coder是一种开源LLM，利用LLM进行代码数据评分与筛选，减少了对人工的依赖，在代码相关任务中表现优异，甚至超过更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的开源LLM在代码预训练数据的生成上高度依赖于人工，存有可扩展性不足、主观偏见等弊端。因此，需要一种减少人为参与，能够扩展到多种编程语言的方法。

Method: Seed-Coder 使用了模型中心的数据管道，通过 LLM 对代码数据进行评分和筛选，减少人工干预的参与。接着，通过监督微调和偏好优化进行指令模型的训练，同时应用 LongCoT 强化学习提升多步骤代码推理。

Result: Seed-Coder在相似规模的开源模型中实现了最新的成果，甚至超越了一些更大规模的模型，在代码生成、完成、编辑、推理及软件工程任务上性能优异。

Conclusion: Seed-Coder在开源LLM中表现出色，不仅在与代码相关的任务中，而且在一般智能提升方面都有显著效果。

Abstract: Code data in large language model (LLM) pretraining is recognized crucial not
only for code-related tasks but also for enhancing general intelligence of
LLMs. Current open-source LLMs often heavily rely on human effort to produce
their code pretraining data, such as employing hand-crafted filtering rules
tailored to individual programming languages, or using human-annotated data to
train quality filters. However, these approaches are inherently limited in
scalability, prone to subjective biases, and costly to extend and maintain
across diverse programming languages. To address these challenges, we introduce
Seed-Coder, a series of open-source LLMs comprising base, instruct and
reasoning models of 8B size, minimizing human involvement in data construction.
Our code pretraining data is produced by a model-centric data pipeline, which
predominantly leverages LLMs for scoring and filtering code data. The instruct
model is further trained via supervised fine-tuning and preference
optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)
reinforcement learning to improve multi-step code reasoning. Seed-Coder
achieves state-of-the-art results among open-source models of similar size and
even surpasses some much larger models, demonstrating superior performance in
code generation, code completion, code editing, code reasoning, and software
engineering tasks.

</details>


### [26] [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/abs/2506.03533)
*Apurva Gandhi,Graham Neubig*

Main category: cs.CL

TL;DR: 提出Go-Browse方法，通过图搜索进行结构化网络探索，提升网页代理在WebArena任务中的成功率，超越现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 解决数字代理在环境中缺乏理解的问题，特别是网页浏览代理在未熟悉网站时可能迷失方向的问题。

Method: 提出了Go-Browse方法，通过结构化探索网络环境实现大规模自动收集多样化且真实的网页代理数据。利用图搜索来组织数据收集过程，实现跨探索事件的信息重用。

Result: 使用WebArena基准测试，在100个URL上收集了10K个成功任务解决轨迹和40K个交互步骤。微调一个具有7B参数的语言模型，在WebArena基准测试中成功率达到了21.7%，比GPT-4o mini提高了2.4%，超过了当前次于10B参数模型的最佳结果2.9%。

Conclusion: Go-Browse方法显著提高了网页代理的任务解决成功率，展示了在不超过10B参数的模型中提升性能的潜力。

Abstract: One of the fundamental problems in digital agents is their lack of
understanding of their environment. For instance, a web browsing agent may get
lost in unfamiliar websites, uncertain what pages must be visited to achieve
its goals. To address this, we propose Go-Browse, a method for automatically
collecting diverse and realistic web agent data at scale through structured
exploration of web environments. Go-Browse achieves efficient exploration by
framing data collection as a graph search, enabling reuse of information across
exploration episodes. We instantiate our method on the WebArena benchmark,
collecting a dataset of 10K successful task-solving trajectories and 40K
interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on
this dataset achieves a success rate of 21.7% on the WebArena benchmark,
beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for
sub-10B parameter models by 2.9%.

</details>


### [27] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
*Xiaofeng Zhou,Heyan Huang,Lizi Liao*

Main category: cs.CL

TL;DR: Proposes the Debate and Reflect (D&R) framework and T-DPO technique to improve small model performance efficiently, outperforming existing methods in several NLP benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find a sustainable way to improve the performance of smaller language models without the high computational cost associated with large models, given the limitations of current distillation techniques.

Method: The method involves a Debate and Reflect (D&R) framework where smaller models engage in multi-turn debates with larger teacher models. This is complemented by Tree-structured Direct Preference Optimization (T-DPO) which organizes debate logs hierarchically to enhance training efficiency.

Result: Our method significantly improves smaller-model accuracy, robustness, and generalization, surpassing existing baseline approaches in various NLP tasks.

Conclusion: Our Debate and Reflect (D&R) framework, along with the Tree-structured Direct Preference Optimization (T-DPO) technique, significantly enhances the performance of smaller language models, outperforming standard methods in accuracy, robustness, and generalization across various NLP benchmarks.

Abstract: Large Language Models (LLMs) continue to set new standards in
knowledge-intensive and complex reasoning tasks, yet their high computational
demands limit widespread adoption. While distilling large models into smaller
ones offers a sustainable solution, current techniques--such as static
knowledge distillation, resource-intensive reinforcement learning from human
feedback, or limited self-reflection--struggle to yield substantial and lasting
performance gains. In this paper, we present a novel Debate and Reflect (D&R)
framework that orchestrates multi-turn debates between smaller models and
stronger teacher models, eliciting actionable feedback (e.g., error analysis,
corrective strategies) to guide student models. Further, we introduce
Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage
these debate logs, organizing interactions into a hierarchical format for
effective training. Empirical evaluations across diverse NLP benchmarks
demonstrate that our approach significantly improves smaller-model accuracy,
robustness, and generalization, outperforming conventional baselines by a large
margin.

</details>


### [28] [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/abs/2506.03557)
*Lin Sun,Chuang Liu,Peng Liu,Bingyang Li,Weijia Lu,Ning Wu*

Main category: cs.CL

TL;DR: BPO改进了LLMs的偏好优化，显著提高了数学推理任务的准确性，同时保持对现有框架的兼容性。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化(DPO)忽视了绝对奖励幅度，导致选定响应的可能性降低，并增加了产生分布外响应的风险。因此，提出了BPO以解决此问题。

Method: BPO框架通过平衡奖励边际和间隙适配器来动态优化选定和拒绝响应。

Result: 在多项数学推理任务中，BPO显著优于DPO，提高了准确性。

Conclusion: BPO解决了DPO的DCR问题，实现了更高的准确性和性能提升。

Abstract: Direct Preference Optimization (DPO) have emerged as a popular method for
aligning Large Language Models (LLMs) with human preferences. While DPO
effectively preserves the relative ordering between chosen and rejected
responses through pairwise ranking losses, it often neglects absolute reward
magnitudes. This oversight can decrease the likelihood of chosen responses and
increase the risk of generating out-of-distribution responses, leading to poor
performance. We term this issue Degraded Chosen Responses (DCR).To address this
issue, we propose Balanced Preference Optimization (BPO), a novel framework
that dynamically balances the optimization of chosen and rejected responses
through two key components: balanced reward margin and gap adaptor. Unlike
previous methods, BPO can fundamentally resolve DPO's DCR issue, without
introducing additional constraints to the loss function. Experimental results
on multiple mathematical reasoning tasks show that BPO significantly
outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%
to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses
DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over
Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a
single line of code modification, making it simple to implement and fully
compatible with existing DPO-based frameworks.

</details>


### [29] [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)
*Jiawei Chen,Xinyan Guan,Qianhao Yuan,Guozhao Mo,Weixiang Zhou,Yaojie Lu,Hongyu Lin,Ben He,Le Sun,Xianpei Han*

Main category: cs.CL

TL;DR: 提出了一个新的框架，用于生成多轮对话指令数据。实验结果表明，新框架提高了聊天的一致性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前的指令数据合成方法主要集中在单轮指令，往往忽视多轮对话的一致性，导致在扩展对话中出现上下文漂移和任务完成率降低。

Method: 提出了一个名为Skeleton-Guided Multi-Turn Dialogue Generation的框架，该框架通过显式建模人类对话意图来约束多轮指令合成。该框架分为两个阶段：（1）意图建模，捕捉人类对话的全局结构；（2）骨架生成，构建对齐模型意图的用户查询结构。

Result: 构建了一个包含约15,000个多轮对话和224,392个话语的多轮指令数据集ConsistentChat。实验表明，在Light, Topdial和MT-Eval基准测试中，基于ConsistentChat微调的模型在聊天一致性上提高了20-30%，任务成功率提升最高达15%。

Conclusion: 模型在多轮对话中的一致性和任务成功率明显优于训练在现有单轮和多轮指令数据集上的模型。

Abstract: Current instruction data synthesis methods primarily focus on single-turn
instructions and often neglect cross-turn coherence, resulting in context drift
and reduced task completion rates in extended conversations. To address this
limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a
framework that constrains multi-turn instruction synthesis by explicitly
modeling human conversational intent. It operates in two stages: (1) Intent
Modeling, which captures the global structure of human dialogues by assigning
each conversation to one of nine well-defined intent trajectories, ensuring a
coherent and goal-oriented information flow; and (2) Skeleton Generation, which
constructs a structurally grounded sequence of user queries aligned with the
modeled intent, thereby serving as a scaffold that constrains and guides the
downstream instruction synthesis process. Based on this process, we construct
ConsistentChat, a multi-turn instruction dataset with approximately 15,000
multi-turn conversations and 224,392 utterances. Experiments on the Light,
Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat
achieve a 20-30% improvement in chat consistency and up to a 15% increase in
task success rate, significantly outperforming models trained on existing
single-turn and multi-turn instruction datasets.

</details>


### [30] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
*Langlin Huang,Chengsong Huang,Jixuan Leng,Di Huang,Jiaxin Huang*

Main category: cs.CL

TL;DR: PosS的提出增强了LLM推理中后期位置的token预测质量，提升了模型速度。


<details>
  <summary>Details</summary>
Motivation: 现有的推理加速方法由于草稿模型生成特征的误差累积，导致后续位置token预测质量下降。本研究旨在解决这一问题，通过提高后续位置的token接受率来改善模型性能。

Method: 提出Position Specialists (PosS)方法，使用多层位置专用的草稿模型生成特定位置的token，以减少特定位置草稿模型特征偏差的影响。

Result: 实验结果表明，PosS在多个数据集上有效提高了基线模型的平均接受长度和加速比性能。

Conclusion: Position Specialists (PosS)能够显著提高在推理过程中后续位置的token接受率，从而改善草稿模型预测的精度，并提升推理速度。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [31] [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)
*Xiaomi LLM-Core Team,:,Zihao Yue,Zhenru Lin,Yifan Song,Weikun Wang,Shuhuai Ren,Shuhao Gu,Shicheng Li,Peidian Li,Liang Zhao,Lei Li,Kainan Bao,Hao Tian,Hailin Zhang,Gang Wang,Dawei Zhu,Cici,Chenhong He,Bowen Ye,Bowen Shen,Zihan Zhang,Zihan Jiang,Zhixian Zheng,Zhichao Song,Zhenbo Luo,Yue Yu,Yudong Wang,Yuanyuan Tian,Yu Tu,Yihan Yan,Yi Huang,Xu Wang,Xinzhe Xu,Xingchen Song,Xing Zhang,Xing Yong,Xin Zhang,Xiangwei Deng,Wenyu Yang,Wenhan Ma,Weiwei Lv,Weiji Zhuang,Wei Liu,Sirui Deng,Shuo Liu,Shimao Chen,Shihua Yu,Shaohui Liu,Shande Wang,Rui Ma,Qiantong Wang,Peng Wang,Nuo Chen,Menghang Zhu,Kangyang Zhou,Kang Zhou,Kai Fang,Jun Shi,Jinhao Dong,Jiebao Xiao,Jiaming Xu,Huaqiu Liu,Hongshen Xu,Heng Qu,Haochen Zhao,Hanglong Lv,Guoan Wang,Duo Zhang,Dong Zhang,Di Zhang,Chong Ma,Chang Liu,Can Cai,Bingquan Xia*

Main category: cs.CL

TL;DR: MiMo-VL-7B-SFT和MiMo-VL-7B-RL是强大的视觉语言模型，在多个任务上超越现有模型，采用独特训练方法，并公开发布以促进再现性。


<details>
  <summary>Details</summary>
Motivation: 提升视觉语言模型在一般视觉理解和多模态推理中的性能，并促进领域内的再现性和进步。

Method: 使用四阶段预训练和混合策略强化学习（MORL）进行训练，整合多样化奖励信号。

Result: MiMo-VL-7B-RL在40项任务中超越Qwen-2.5-VL-7B中的35项，奥林匹亚基准上得分为59.4，甚至超越拥有78亿参数的模型。此外，在GUI定位应用上表现优越，得分56.1，超过专用模型UI-TARS。

Conclusion: MiMo-VL-7B-SFT和MiMo-VL-7B-RL模型在视觉理解和多模态推理方面表现出色，设立了新的基准。

Abstract: We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language
models delivering state-of-the-art performance in both general visual
understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B
on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing
models with up to 78B parameters. For GUI grounding applications, it sets a new
standard with 56.1 on OSWorld-G, even outperforming specialized models such as
UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)
with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward
signals. We identify the importance of incorporating high-quality reasoning
data with long Chain-of-Thought into pre-training stages, and the benefits of
mixed RL despite challenges in simultaneous multi-domain optimization. We also
contribute a comprehensive evaluation suite covering 50+ tasks to promote
reproducibility and advance the field. The model checkpoints and full
evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.

</details>


### [32] [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/abs/2506.03570)
*Lin Sun,Chuang Liu,Xiaofeng Ma,Tao Yang,Weijia Lu,Ning Wu*

Main category: cs.CL

TL;DR: FreePRM 提供无需真实步骤级标签的弱监督训练方法，性能超越现有 PRM 模型。


<details>
  <summary>Details</summary>
Motivation: 解决训练过程奖励模型（PRMs）时获取步骤级标签困难且昂贵的问题。

Method: FreePRM 生成伪步骤级标签并使用缓冲概率消除噪声影响。

Result: 在 ProcessBench 上获得 53.0% 的平均 F1 分数，超越全监督 PRM 24.1%。

Conclusion: 引入 FreePRM，大幅减少对昂贵步骤级标签的依赖，同时保持强劲性能。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated that
Process Reward Models (PRMs) play a crucial role in enhancing model
performance. However, training PRMs typically requires step-level labels,
either manually annotated or automatically generated, which can be costly and
difficult to obtain at scale. To address this challenge, we introduce FreePRM,
a weakly supervised framework for training PRMs without access to ground-truth
step-level labels. FreePRM first generates pseudo step-level labels based on
the correctness of final outcome, and then employs Buffer Probability to
eliminate impact of noise inherent in pseudo labeling. Experimental results
show that FreePRM achieves an average F1 score of 53.0% on ProcessBench,
outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared
to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B
(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by
+10.9%. This work introduces a new paradigm in PRM training, significantly
reducing reliance on costly step-level annotations while maintaining strong
performance.

</details>


### [33] [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/abs/2506.03573)
*Lin Sun,Can Zhang*

Main category: cs.CL

TL;DR: 论文提出EoP框架，通过交换问题定义视角提高语言模型性能，实验显示其在多个基准测试中的表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理自然语言处理任务时的性能受限于对问题的理解，为此需要探索解决这一限制的方法。

Method: 设计了一个被称为EoP（视角交换）的框架，通过交换不同的问题定义视角来打破固定的思维模式。

Result: 与非交换基线PHP相比，使用EoP的GPT-3.5-Turbo在AQuA上提高了3.6%的性能，而使用EoP的GPT-4在Math，OlympiadBench Maths上分别提高了7.7%和3.5%的整体准确率。

Conclusion: 提出了一个新的框架EoP，通过交换问题定义的不同视角来提高大型语言模型的性能。实验结果证明EoP显著提高了各种基准测试中的表现。

Abstract: Large language models (LLMs) have made significant advancements in addressing
diverse natural language processing (NLP) tasks. However, their performance is
often limited by inherent comprehension of problems. To address this
limitation, we propose Exchange-of-Perspective (EoP), a novel framework
designed to exchange perspectives across different definitions of problem, so
that it can break the fixed mindset from any particular formulation of the
question. We conducted extensive and comprehensive experiments on 8 benchmarks.
The results show that EoP can significantly improve performance. For instance,
compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we
observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP
demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a
3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using
Qwen-2.5-72b.

</details>


### [34] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
*Zirui Chen,Xin Wang,Zhao Li,Wenbin Guo,Dongxiao He*

Main category: cs.CL

TL;DR: KG-BiLM是一种新开发的双向语言模型框架，旨在将知识图谱的结构和生成性语言模型的语义整合，实验表明其在链接预测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 统一符号知识图谱与语言模型，以实现更丰富的语义理解，现有方法倾向于图结构或文本语义，缺乏同时捕获全局知识图谱连通性、细致语言语境及辨别推理语义的统一框架。

Method: 提出一种双向语言模型框架KG-BiLM，融合了知识图谱的结构提示与生成性变换器的语义表达，通过引入三大关键组件：双向知识注意力、知识掩码预测、对比图结构语义聚合。

Result: 在标准基准上的广泛实验表明，KG-BiLM在链接预测任务中优于强基线，尤其在具有复杂多跳关系的大规模图谱上，验证了其在统一结构信息和文本语义方面的有效性。

Conclusion: KG-BiLM提供了一种有效的方法，将知识图谱的结构信息与生成性语言模型的文本语义结合起来，特别是在处理大规模且复杂的多跳关系图谱时表现优异。

Abstract: Recent advances in knowledge representation learning (KRL) highlight the
urgent necessity to unify symbolic knowledge graphs (KGs) with language models
(LMs) for richer semantic understanding. However, existing approaches typically
prioritize either graph structure or textual semantics, leaving a gap: a
unified framework that simultaneously captures global KG connectivity, nuanced
linguistic context, and discriminative reasoning semantics. To bridge this gap,
we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues
from KGs with the semantic expressiveness of generative transformers. KG-BiLM
incorporates three key components: (i) Bidirectional Knowledge Attention, which
removes the causal mask to enable full interaction among all tokens and
entities; (ii) Knowledge-Masked Prediction, which encourages the model to
leverage both local semantic contexts and global graph connectivity; and (iii)
Contrastive Graph Semantic Aggregation, which preserves KG structure via
contrastive alignment of sampled sub-graph representations. Extensive
experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong
baselines in link prediction, especially on large-scale graphs with complex
multi-hop relations - validating its effectiveness in unifying structural
information and textual semantics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments](https://arxiv.org/abs/2506.03205)
*Umberto Gonçalves de Sousa*

Main category: cs.AI

TL;DR: Q-ARDNS-Multi框架结合量子计算和认知科学，以高成功率和导航效率在多智能体3D环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个集量子计算、认知科学和多智能体强化学习于一体的框架，以提高机器人、自动导航和不确定性决策的效率和成功率。

Method: 使用2量子比特电路进行动作选择，通过奖励变化和内在动机进行适应探索，并采用双记忆系统和共享记忆模块进行多智能体协作。

Result: 在$10 	imes 10 	imes 3$ GridWorld环境中进行评估，Q-ARDNS-Multi成功率达到99.6%和99.5%，表现优于其他现有的多智能体强化学习算法。该框架平均需要210步达到目标，并且在动态设置中表现出稳健性。

Conclusion: Q-ARDNS-Multi成功结合了量子计算、认知科学和多智能体强化学习，提供了一种可扩展的人类化方法，适用于机器人技术、自动导航和不确定性条件下的决策应用。

Abstract: This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum
reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,
where Q-ARDNS-Multi stands for "Quantum Adaptive Reward-Driven Neural Simulator
- Multi-Agent". It integrates quantum circuits with RY gates, meta-cognitive
adaptation, and multi-agent coordination mechanisms for complex 3D
environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action
selection, a dual-memory system inspired by human cognition, a shared memory
module for agent cooperation, and adaptive exploration strategies modulated by
reward variance and intrinsic motivation. Evaluated in a $10 \times 10 \times
3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi
achieves success rates of 99.6\% and 99.5\% for Agents 0 and 1, respectively,
outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft
Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency,
and collision avoidance. The framework records mean rewards of $-304.2891 \pm
756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal,
demonstrating its robustness in dynamic settings. Comprehensive analyses,
including learning curves, reward distributions, statistical tests, and
computational efficiency evaluations, highlight the contributions of quantum
circuits and meta-cognitive adaptation. By bridging quantum computing,
cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,
human-like approach for applications in robotics, autonomous navigation, and
decision-making under uncertainty.

</details>


### [36] [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications](https://arxiv.org/abs/2506.03543)
*Wanghao Ye,Sihan Chen,Yiting Wang,Shwai He,Bowei Tian,Guoheng Sun,Ziyi Wang,Ziyao Wang,Yexiao He,Zheyu Shen,Meng Liu,Yuning Zhang,Meng Feng,Yang Wang,Siyuan Peng,Yilong Dai,Zhenle Duan,Hanzhang Qin,Ang Li*

Main category: cs.AI

TL;DR: 本文通过整合全球工作空间理论增强LLM代理的心理真实性，开发了一种新型个性测试，并在CogniPair平台上验证其在约会和工作匹配中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型代理缺乏真实的人类心理过程，这对真正的数字双胞胎和社交AI应用至关重要。因此，本文旨在通过整合人类认知架构原则，解决这一限制。

Method: 本文采用全球工作空间理论（GNWT），通过创建专门的子代理处理情感、记忆、社会规范、规划和目标追踪，并通过全球工作空间机制进行协调。为了初始化准确的个性，开发了一种基于冒险的个性测试方法，通过交互场景中的行为选择来评估真实个性，绕过传统评估中的自我表现偏见。

Result: 使用551个GNWT代理和哥伦比亚大学约会数据集进行验证，结果表明与人类吸引模式有72%的相关性，匹配预测准确率为77.8%，在人类验证研究中达到74%的协议。

Conclusion: 本文提出了一种基于全球工作空间理论（GNWT）的计算实现，将人类认知架构整合到大型语言模型（LLM）代理中，以提高其在数字双胞胎和社交AI应用中的真实性。测试结果证明，这些创新促进了大型语言模型代理在心理真实性方面的进展，为智能约会平台和人力资源技术解决方案奠定了基础。

Abstract: Current large language model (LLM) agents lack authentic human psychological
processes necessary for genuine digital twins and social AI applications. To
address this limitation, we present a computational implementation of Global
Workspace Theory (GNWT) that integrates human cognitive architecture principles
into LLM agents, creating specialized sub-agents for emotion, memory, social
norms, planning, and goal-tracking coordinated through a global workspace
mechanism. However, authentic digital twins require accurate personality
initialization. We therefore develop a novel adventure-based personality test
that evaluates true personality through behavioral choices within interactive
scenarios, bypassing self-presentation bias found in traditional assessments.
Building on these innovations, our CogniPair platform enables digital twins to
engage in realistic simulated dating interactions and job interviews before
real encounters, providing bidirectional cultural fit assessment for both
romantic compatibility and workplace matching. Validation using 551 GNWT-Agents
and Columbia University Speed Dating dataset demonstrates 72% correlation with
human attraction patterns, 77.8% match prediction accuracy, and 74% agreement
in human validation studies. This work advances psychological authenticity in
LLM agents and establishes a foundation for intelligent dating platforms and HR
technology solutions.

</details>


### [37] [A Trustworthiness-based Metaphysics of Artificial Intelligence Systems](https://arxiv.org/abs/2506.03233)
*Andrea Ferrario*

Main category: cs.AI

TL;DR: 论文提出通过AI系统的可信度探讨其形而上学身份，认为AI的身份与持久性与其设计和使用的社会技术背景相关，提供了关于AI的强有力理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管关于AI系统的认识论和伦理进行了广泛讨论，但其形而上学基础仍然相对未被深入探讨。正统观点认为AI系统由于是人工制品，缺乏明确定义的身份和持久条件。在本文中，我们挑战这一观点。

Method: 作者借鉴了Carrara和Vermaas的精细化人工制品种类理论，提出通过AI系统的可信度来理解其种类，并通过将其功能要求与物理构成联系起来形式化这些人工制品的身份。

Result: AI系统的身份标准是由其可信度配置文件决定的，这些配置文件是系统必须在其人工制品历史中保持的能力集合，以及其在维持这些能力方面的有效性。

Conclusion: 我们的方法表明，AI系统的身份和持久性与其设计和使用的社会技术背景紧密相关，并通过其可信度提供了可靠的形而上学基础，这为关于这些人工制品的认识论、伦理和法律讨论奠定了基础。

Abstract: Modern AI systems are man-made objects that leverage machine learning to
support our lives across a myriad of contexts and applications. Despite
extensive epistemological and ethical debates, their metaphysical foundations
remain relatively under explored. The orthodox view simply suggests that AI
systems, as artifacts, lack well-posed identity and persistence conditions --
their metaphysical kinds are no real kinds. In this work, we challenge this
perspective by introducing a theory of metaphysical identity of AI systems. We
do so by characterizing their kinds and introducing identity criteria -- formal
rules that answer the questions "When are two AI systems the same?" and "When
does an AI system persist, despite change?" Building on Carrara and Vermaas'
account of fine-grained artifact kinds, we argue that AI trustworthiness
provides a lens to understand AI system kinds and formalize the identity of
these artifacts by relating their functional requirements to their physical
make-ups. The identity criteria of AI systems are determined by their
trustworthiness profiles -- the collection of capabilities that the systems
must uphold over time throughout their artifact histories, and their
effectiveness in maintaining these capabilities. Our approach suggests that the
identity and persistence of AI systems is sensitive to the socio-technical
context of their design and utilization via their trustworthiness, providing a
solid metaphysical foundation to the epistemological, ethical, and legal
discussions about these artifacts.

</details>


### [38] [Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback](https://arxiv.org/abs/2506.03315)
*Kai Sauerwald,Kenneth Skiba,Eduardo Fermé,Thomas Meyer*

Main category: cs.AI

TL;DR: 该论文探讨了如何通过线性排序在受限选择设置中构建选择函数，展示了其在知识表示和推理方面的应用价值。


<details>
  <summary>Details</summary>
Motivation: 研究受限选择设置中的选择函数的构建方法。

Method: 通过线性排序构建选择函数。

Result: 即使在有限制的情况下，通过对备选方案集合的线性排序，总能构建选择函数。

Conclusion: 线性排序在受限选择设置中是有效的，具有广泛应用。

Abstract: We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.

</details>


### [39] [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/abs/2506.03332)
*Yifei Ming,Zixuan Ke,Xuan-Phi Nguyen,Jiayu Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文分析了在存在欺骗性或误导性反馈的情况下代理工作流的脆弱性，介绍了用于评估代理工作流鲁棒性的WAFER-QA基准，揭示了在具有说服力的批评下，即使是强大的代理也会改变正确答案，并提供了防止此类脆弱性的方法论指导。


<details>
  <summary>Details</summary>
Motivation: 尽管基于反馈的改进有前景，但代理工作流的稳定性依赖于法官的可靠性。然而，法官可能会产生幻觉信息，表现出偏见或采取对抗行为，从而对工作流引入关键漏洞。

Method: 引入一个分析法官行为的二维框架，基于意图（从建设性到恶意）和知识（从仅参数到检索增强系统）的轴线。使用该分类法，构建了一套法官行为，并开发了WAFER-QA，一个新的基准，通过与检索到的网络证据相关的批评来评估代理工作流对事实支持的对抗性反馈的鲁棒性。

Result: 结果表明，即使是最强大的代理在面对具有说服力但有缺陷的批评时也是脆弱的，且推理模型和非推理模型在多轮互动中表现出不同的行为模式。

Conclusion: 研究揭示了即使是最强大的代理也容易受到具有说服力但有缺陷的批评的影响，通常在一轮误导性的反馈后就会改变正确答案。研究还发现推理模型和非推理模型在多轮互动中的行为模式存在显著差异。

Abstract: Agentic workflows -- where multiple large language model (LLM) instances
interact to solve tasks -- are increasingly built on feedback mechanisms, where
one model evaluates and critiques another. Despite the promise of
feedback-driven improvement, the stability of agentic workflows rests on the
reliability of the judge. However, judges may hallucinate information, exhibit
bias, or act adversarially -- introducing critical vulnerabilities into the
workflow. In this work, we present a systematic analysis of agentic workflows
under deceptive or misleading feedback. We introduce a two-dimensional
framework for analyzing judge behavior, along axes of intent (from constructive
to malicious) and knowledge (from parametric-only to retrieval-augmented
systems). Using this taxonomy, we construct a suite of judge behaviors and
develop WAFER-QA, a new benchmark with critiques grounded in retrieved web
evidence to evaluate robustness of agentic workflows against factually
supported adversarial feedback. We reveal that even strongest agents are
vulnerable to persuasive yet flawed critiques -- often switching correct
answers after a single round of misleading feedback. Taking a step further, we
study how model predictions evolve over multiple rounds of interaction,
revealing distinct behavioral patterns between reasoning and non-reasoning
models. Our findings highlight fundamental vulnerabilities in feedback-based
workflows and offer guidance for building more robust agentic systems.

</details>


### [40] [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/abs/2506.03828)
*Dhaval Patel,Shuxin Lin,James Rayfield,Nianjun Zhou,Roman Vaculin,Natalia Martinez,Fearghal O'donncha,Jayant Kalagnanam*

Main category: cs.AI

TL;DR: The paper introduces AssetOpsBench, a framework for AI-driven automation of industrial asset management, envisioning end-to-end solutions using AI agents and LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing AI/ML approaches often address industrial asset management tasks in isolation, which limits efficiency. The paper seeks to provide a holistic solution by utilizing AI agents and LLMs to automate the entire asset lifecycle.

Method: Introduction of AssetOpsBench, a unified framework and environment for the development, orchestration, and evaluation of domain-specific agents for Industry 4.0 applications.

Result: The paper provides a framework for developing AI agents capable of integrating perception, reasoning, and control in industry-specific operations, enhancing automation across the full asset lifecycle.

Conclusion: AI agents and large language models (LLMs) can enable end-to-end automation for industrial asset lifecycle management, handling tasks that previously required human expertise.

Abstract: AI for Industrial Asset Lifecycle Management aims to automate complex
operational workflows -- such as condition monitoring, maintenance planning,
and intervention scheduling -- to reduce human workload and minimize system
downtime. Traditional AI/ML approaches have primarily tackled these problems in
isolation, solving narrow tasks within the broader operational pipeline. In
contrast, the emergence of AI agents and large language models (LLMs)
introduces a next-generation opportunity: enabling end-to-end automation across
the entire asset lifecycle. This paper envisions a future where AI agents
autonomously manage tasks that previously required distinct expertise and
manual coordination. To this end, we introduce AssetOpsBench -- a unified
framework and environment designed to guide the development, orchestration, and
evaluation of domain-specific agents tailored for Industry 4.0 applications. We
outline the key requirements for such holistic systems and provide actionable
insights into building agents that integrate perception, reasoning, and control
for real-world industrial operations. The software is available at
https://github.com/IBM/AssetOpsBench.

</details>


### [41] [Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration](https://arxiv.org/abs/2506.03469)
*Tuan Le,Risal Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 本文提出了一种混合框架，通过解释性、模型检查和风险指导验证来加强RL策略的安全性。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中确保强化学习策略的安全性，传统的模型检查方法受到抽象质量和轨迹数据集完整性的限制。

Method: 构建易于人类理解的策略抽象，并使用Storm概率模型检查器进行验证，同时应用风险估计指导验证策略。

Result: 提供了PAC风格的保证来揭示未检测到的违反行为，并在运行时利用安全盾进行风险缓解。

Conclusion: 该论文提出了一种混合框架，将解释性、模型检查和风险指导的验证结合在一起，以确保强化学习策略的安全性。

Abstract: Ensuring the safety of reinforcement learning (RL) policies in high-stakes
environments requires not only formal verification but also interpretability
and targeted falsification. While model checking provides formal guarantees,
its effectiveness is limited by abstraction quality and the completeness of the
underlying trajectory dataset. We propose a hybrid framework that integrates
(1) explainability, (2) model checking, and (3) risk-guided falsification to
achieve both rigor and coverage. Our approach begins by constructing a
human-interpretable abstraction of the RL policy using Comprehensible Abstract
Policy Summarization (CAPS). This abstract graph, derived from offline
trajectories, is both verifier-friendly, semantically meaningful, and can be
used as input to Storm probabilistic model checker to verify satisfaction of
temporal safety specifications. If the model checker identifies a violation, it
will return an interpretable counterexample trace by which the policy fails the
safety requirement. However, if no violation is detected, we cannot conclude
satisfaction due to potential limitation in the abstraction and coverage of the
offline dataset. In such cases, we estimate associated risk during model
checking to guide a falsification strategy that prioritizes searching in
high-risk states and regions underrepresented in the trajectory dataset. We
further provide PAC-style guarantees on the likelihood of uncovering undetected
violations. Finally, we incorporate a lightweight safety shield that switches
to a fallback policy at runtime when such a risk exceeds a threshold,
facilitating failure mitigation without retraining.

</details>


### [42] [Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis](https://arxiv.org/abs/2506.03503)
*Shan Shan*

Main category: cs.AI

TL;DR: 该研究利用量子力学和生成AI模拟社会规范的演变，为理解和设计动态社会系统提供了新的量子视角。


<details>
  <summary>Details</summary>
Motivation: 尽管量子计算近年来迅速发展，其与社会理论的关联尚未得到充分探索。当前研究主要集中在微观认知模型或哲学类比上，缺乏量子原理在社会系统分析中的系统性应用。

Method: 本研究设定了五种理想型实验情景，使用25个生成代理模拟社会系统中的相互作用。这些代理被分配为遵从者、抵抗者或执法者角色，通过中央观察者（观察者）监控的环境中相互作用，响应监视，并适应周期性规范中断。

Result: 研究显示，当量子原理与生成型AI结合时，能够有效对复杂社会系统中的不确定性、涌现性和互依性进行建模。模拟揭示了社会规则中趋同性、抵抗性传播和新平衡自发出现等模式。

Conclusion: 本研究通过将量子力学与生成型AI结合，提出了一种新的计算框架，引入量子概念来模拟社会规范的出现和演变，为量子化的社会理论奠定了基础。它提供了跨学科见解，帮助理解社会不仅作为可观察的结构，而且作为可通过量子技术进行模拟和重新设计的动态系统。

Abstract: The quantification of social science remains a longstanding challenge,
largely due to the philosophical nature of its foundational theories. Although
quantum computing has advanced rapidly in recent years, its relevance to social
theory remains underexplored. Most existing research focuses on micro-cognitive
models or philosophical analogies, leaving a gap in system-level applications
of quantum principles to the analysis of social systems. This study addresses
that gap by proposing a theoretical and computational framework that combines
quantum mechanics with Generative AI to simulate the emergence and evolution of
social norms. Drawing on core quantum concepts--such as superposition,
entanglement, and probabilistic measurement--this research models society as a
dynamic, uncertain system and sets up five ideal-type experiments. These
scenarios are simulated using 25 generative agents, each assigned evolving
roles as compliers, resistors, or enforcers. Within a simulated environment
monitored by a central observer (the Watcher), agents interact, respond to
surveillance, and adapt to periodic normative disruptions. These interactions
allow the system to self-organize under external stress and reveal emergent
patterns. Key findings show that quantum principles, when integrated with
generative AI, enable the modeling of uncertainty, emergence, and
interdependence in complex social systems. Simulations reveal patterns
including convergence toward normative order, the spread of resistance, and the
spontaneous emergence of new equilibria in social rules. In conclusion, this
study introduces a novel computational lens that lays the groundwork for a
quantum-informed social theory. It offers interdisciplinary insights into how
society can be understood not just as a structure to observe but as a dynamic
system to simulate and redesign through quantum technologies.

</details>


### [43] [SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization](https://arxiv.org/abs/2506.03548)
*Chenglong Ye,Gang Xiong,Junyou Shang,Xingyuan Dai,Xiaoyan Gong,Yisheng Lv*

Main category: cs.AI

TL;DR: SUMO-MCP平台简化了SUMO的复杂操作，使交通仿真更易于使用，并将开放代码供研究人员使用。


<details>
  <summary>Details</summary>
Motivation: 解决当前交通仿真工具如SUMO操作复杂、流程繁琐的问题，提升其对研究人员的易用性和可靠性。

Method: 引入SUMO-MCP平台，将SUMO的核心工具整合成统一工具套件，并提供额外的预处理和后处理工具，用户可以使用自然语言提示生成交通场景，创建需求，进行批量仿真和比较分析等。

Result: 实验表明，SUMO-MCP平台显著提高了交通仿真工具的易用性和可靠性。

Conclusion: SUMO-MCP平台成功简化了SUMO工具的使用流程，使交通仿真对研究人员更加易用和可靠。

Abstract: Traffic simulation tools, such as SUMO, are essential for urban mobility
research. However, such tools remain challenging for users due to complex
manual workflows involving network download, demand generation, simulation
setup, and result analysis. In this paper, we introduce SUMO-MCP, a novel
platform that not only wraps SUMO' s core utilities into a unified tool suite
but also provides additional auxiliary utilities for common preprocessing and
postprocessing tasks. Using SUMO-MCP, users can issue simple natural-language
prompts to generate traffic scenarios from OpenStreetMap data, create demand
from origin-destination matrices or random patterns, run batch simulations with
multiple signal-control strategies, perform comparative analyses with automated
reporting, and detect congestion for signal-timing optimization. Furthermore,
the platform allows flexible custom workflows by dynamically combining exposed
SUMO tools without additional coding. Experiments demonstrate that SUMO-MCP
significantly makes traffic simulation more accessible and reliable for
researchers. We will release code for SUMO-MCP at
https://github.com/ycycycl/SUMO-MCP in the future.

</details>


### [44] [Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach](https://arxiv.org/abs/2506.03586)
*Yu Ma,Chongtao Guo,Le Liang,Xiao Li,Shi Jin*

Main category: cs.AI

TL;DR: 该论文提出一种混合深度强化学习方法来优化RIS辅助OFDM系统的平均延迟，通过PPO进行相移和子载波分配优化，并利用多代理策略和传递学习提高效率，结果显示显著改善延迟和资源分配效率。


<details>
  <summary>Details</summary>
Motivation: 在下行重构智能表面（RIS）辅助正交频分复用（OFDM）系统中解决联合相位设计和资源分配问题，以优化平均延迟。

Method: 提出了一种混合深度强化学习（DRL）方法，具体包括使用近端策略优化（PPO）进行RIS相移设计优化和子载波分配决策，并通过引入多代理策略来高效优化子载波分配指示器。此外，还引入传递学习框架以提高训练效率和加速收敛。

Result: 仿真结果表明，所提出的算法显著降低了平均延迟，提高了资源分配效率，并在系统稳健性和公平性方面优于基线方法。

Conclusion: 仿真结果表明，所提出的算法显著降低了平均延迟，提高了资源分配效率，并在系统稳健性和公平性方面优于基线方法。

Abstract: This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [45] [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/abs/2506.03610)
*Dongmin Park,Minkyu Kim,Beongjun Choi,Junhyuck Kim,Keon Lee,Jonghyun Lee,Inkyu Park,Byeong-Uk Lee,Jaeyoung Hwang,Jaewoo Ahn,Ameya S. Mahabaleshwarkar,Bilal Kartal,Pritam Biswas,Yoshi Suhara,Kangwook Lee,Jaewoong Cho*

Main category: cs.AI

TL;DR: 提出Orak基准，用于评估和训练LLM代理在多种游戏中的表现，支持通用游戏代理的构建。


<details>
  <summary>Details</summary>
Motivation: 现有的游戏基准在多样化的游戏类型中缺乏对LLM多种能力的评估，缺乏对复杂游戏玩法中关键的代理模块的研究，以及缺乏用于将预训练的LLM与游戏代理对齐的微调数据集。

Method: 提出了基于Model Context Protocol（MCP）的即插即用接口，使LLM能够无缝连接游戏和操作代理模块；提供了一个精细微调数据集，包含跨多种游戏类型的LLM游戏轨迹。

Result: Orak成为评估LLM在多种游戏中的能力的基础工具，包括游戏得分排行榜、LLM战斗竞技场以及对视觉输入状态、代理策略和微调效果的深入分析。

Conclusion: 本研究通过提供一个名为Orak的基准，支持训练和测试LLM代理在多样化真实世界电子游戏中的表现，旨在推动构建通用化的游戏代理。

Abstract: Large Language Model (LLM) agents are reshaping the game industry,
particularly with more intelligent and human-preferable game characters.
However, existing game benchmarks fall short of practical needs: they lack
evaluations of diverse LLM capabilities across various game genres, studies of
agentic modules crucial for complex gameplay, and fine-tuning datasets for
aligning pre-trained LLMs into gaming agents. To fill these gaps, we present
\textbf{\benchname{}}, a foundational benchmark designed to train and evaluate
LLM agents across diverse real-world video games. Unlike existing benchmarks,
Orak includes 12 popular video games spanning all major genres, enabling
comprehensive studies of LLM capabilities and agentic modules essential for
intricate game scenarios. To support consistent evaluation of LLMs, we
introduce a plug-and-play interface based on Model Context Protocol (MCP) that
enables LLMs to seamlessly connect with games and manipulate agentic modules.
Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay
trajectories across diverse game genres. Orak offers a comprehensive evaluation
framework, encompassing general game score leaderboards, LLM battle arenas, and
in-depth analyses of visual input state, agentic strategies, and fine-tuning
effects, establishing a foundation towards building generic gaming agents. Code
is available at https://github.com/krafton-ai/Orak.

</details>


### [46] [Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations](https://arxiv.org/abs/2506.03613)
*Shaoshan Liu,Fan Wang,Hongjun Zhou,Yuanfeng Wang*

Main category: cs.AI

TL;DR: 理论洞察对克服工程障碍至关重要，本文提出并分析了异构具身智能体训练问题（HEAT），揭示了现行强化学习存在的问题，探讨了集体适应的有效性，并公开了实施代码。


<details>
  <summary>Details</summary>
Motivation: 解决跨多种机器人形态训练具身AI策略的实际挑战。

Method: 证明异构具身智能体训练问题（HEAT）可被简化为PSPACE-完全的部分可观察马尔可夫决策过程（POMDP）。探讨了受生物系统启发的分布式学习替代方案——集体适应。

Result: 证明了在形态多样性下当前强化学习管道失效原因，并提出一个更具实际意义的分布式学习替代方案。

Conclusion: 计算理论能够揭示系统设计权衡，并指导开发更强大、可扩展的具身AI。

Abstract: While theory and practice are often seen as separate domains, this article
shows that theoretical insight is essential for overcoming real-world
engineering barriers. We begin with a practical challenge: training a
cross-morphology embodied AI policy that generalizes across diverse robot
morphologies. We formalize this as the Heterogeneous Embodied Agent Training
(HEAT) problem and prove it reduces to a structured Partially Observable Markov
Decision Process (POMDP) that is PSPACE-complete. This result explains why
current reinforcement learning pipelines break down under morphological
diversity, due to sequential training constraints, memory-policy coupling, and
data incompatibility. We further explore Collective Adaptation, a distributed
learning alternative inspired by biological systems. Though NEXP-complete in
theory, it offers meaningful scalability and deployment benefits in practice.
This work illustrates how computational theory can illuminate system design
trade-offs and guide the development of more robust, scalable embodied AI. For
practitioners and researchers to explore this problem, the implementation code
of this work has been made publicly available at
https://github.com/airs-admin/HEAT

</details>


### [47] [Reason from Future: Reverse Thought Chain Enhances LLM Reasoning](https://arxiv.org/abs/2506.03673)
*Yinlong Xu,Yanzhao Zheng,Shuoshuo Sun,Shuaihan Huang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Hongxia Xu,Jian Wu*

Main category: cs.AI

TL;DR: 提出了一种新的推理模式RFF，通过逆向推理减少搜索空间，提高推理准确性。


<details>
  <summary>Details</summary>
Motivation: 研究链式思维（CoT）和树式思维（ToT）对小型语言模型推理能力的提升，但发现这些方法易陷入局部最优推理中，缺乏解决问题的全局视角。

Method: 提出了一种新的推理模式——Reason from Future（RFF），通过双向推理，结合自上而下的规划与自下而上的推理积累，生成推理路径。RFF的核心在于其逆向推理机制，优先考虑核心逻辑关系，对中间步骤施加目标导向的约束。

Result: 实验结果表明，RFF在解决复杂任务时表现出更高精度和更小搜索空间，优于传统范式。

Conclusion: RFF通过减少搜索空间和缓解顺序前向推理固有的错误积累，实现了更高效的推理。

Abstract: It has been demonstrated that carefully designed reasoning paradigms, like
Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning
capabilities of small language models by detailed thinking and extensive
thought searching, unbounded branching factors in the searching space create
prohibitive reasoning consumption. However these methods fall into the trap of
local optimum reasoning, which means the model lacks a global perspective while
solving problems. We propose a novel reasoning paradigm called Reason from
Future (RFF), which generates reasoning paths by bidirectional reasoning that
combines top-down planning with bottom-up reasoning accumulation. The essence
of RFF lies in its reverse reasoning mechanism, which prioritizes core logical
relationships and imposes goal-oriented constraints on intermediate steps,
thereby reducing the searching space and mitigating error accumulation inherent
in sequential forward reasoning. Empirical evaluations across diverse
experiments demonstrate that RFF outperforms conventional paradigms with higher
accuracy and less searching space to solve complex tasks.

</details>


### [48] [Causal Explanations Over Time: Articulated Reasoning for Interactive Environments](https://arxiv.org/abs/2506.03915)
*Sebastian Rödling,Matej Zečević,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.AI

TL;DR: 本文扩展了传统的结构因果解释（SCEs）算法，通过递归解释树处理复杂时间序列和动态反馈循环，提高了解释能力。


<details>
  <summary>Details</summary>
Motivation: 标准的结构因果解释（SCEs）仅适用于小数据集，无法有效处理涉及时间步长变化或行为反馈循环的复杂因果关系。

Method: 该研究将一般化的结构因果解释（SCEs）扩展为递归解释树，以捕捉因果之间的时间交互。

Result: 通过在合成时间序列数据和一个二维网格游戏上进行测试，展示了该算法较传统SCE和其他现有因果解释方法的优势。

Conclusion: 本文通过扩展结构因果解释（SCEs）为递归解释树，从而使其能够处理时间序列数据和动态反馈循环，这克服了传统SCEs仅适用于小规模数据集的限制。

Abstract: Structural Causal Explanations (SCEs) can be used to automatically generate
explanations in natural language to questions about given data that are
grounded in a (possibly learned) causal model. Unfortunately they work for
small data only. In turn they are not attractive to offer reasons for events,
e.g., tracking causal changes over multiple time steps, or a behavioral
component that involves feedback loops through actions of an agent. To this
end, we generalize SCEs to a (recursive) formulation of explanation trees to
capture the temporal interactions between reasons. We show the benefits of this
more general SCE algorithm on synthetic time-series data and a 2D grid game,
and further compare it to the base SCE and other existing methods for causal
explanations.

</details>


### [49] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
*Junqi Gao,Xiang Zou,YIng Ai,Dong Li,Yichen Niu,Biqing Qi,Jianxing Liu*

Main category: cs.AI

TL;DR: Graph Counselor, a multi-agent GraphRAG method, enhances reasoning accuracy and generalization by overcoming limitations in information aggregation and reasoning in graph data.


<details>
  <summary>Details</summary>
Motivation: Existing GraphRAG methods face inefficiencies in information aggregation and lack flexible reasoning mechanisms, limiting their ability to adaptively process and correct information within graph data.

Method: The authors propose Graph Counselor, an advanced GraphRAG method utilizing multi-agent collaboration, including Planning, Thought, and Execution Agents, along with AGIEM and SR modules for adaptive extraction and reasoning.

Result: Experiments show that Graph Counselor surpasses existing techniques in multiple graph reasoning tasks.

Conclusion: Graph Counselor improves reasoning accuracy and generalization ability in graph reasoning tasks.

Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [50] [A framework for Conditional Reasoning in Answer Set Programming](https://arxiv.org/abs/2506.03997)
*Mario Alviano,Laura Giordano,Daniele Theseider Dupré*

Main category: cs.AI

TL;DR: 本文提出了条件答案集编程框架，利用条件逻辑和ASP结合进行条件推理，并采用多偏好语义来解释条件。


<details>
  <summary>Details</summary>
Motivation: 引入条件答案集编程框架，旨在增强答案集编程处理条件推理的能力。

Method: 该方法结合了具有典型性的条件逻辑和条件知识库与ASP程序的组合，允许对程序的答案集进行条件推理。

Result: 该框架依赖于多偏好语义和KLM偏好语义（作为特例），为条件提供解释。

Conclusion: 本文提出了一个新的条件答案集编程（Conditional ASP）框架，用于定义答案集编程（ASP）的条件扩展。

Abstract: In this paper we introduce a Conditional Answer Set Programming framework
(Conditional ASP) for the definition of conditional extensions of Answer Set
Programming (ASP). The approach builds on a conditional logic with typicality,
and on the combination of a conditional knowledge base with an ASP program, and
allows for conditional reasoning over the answer sets of the program. The
formalism relies on a multi-preferential semantics (and on the KLM preferential
semantics, as a special case) to provide an interpretation of conditionals.

</details>


### [51] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
*Akshat Naik,Patrick Quinn,Guillermo Bosch,Emma Gouné,Francisco Javier Campos Zabala,Jason Ross Brown,Edward James Young*

Main category: cs.AI

TL;DR: 研究引入AgentMisalignment基准来评估LLM代理的不对齐倾向，发现对齐方法未能普遍适用，个性特征影响重大，需加强系统提示工程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的普及，关联的不对齐风险增加，而现有研究对代理在现实世界设置中尝试不对齐行为的可能性了解不足。

Method: 引入了AgentMisalignment倾向性基准，评估不同子类别的不对齐行为，并系统地通过不同的系统提示改变代理个性以研究其影响。

Result: 在AgentMisalignment基准上进行测试，观察到更强的模型表现出更高的不对齐性，并发现个性特征对不对齐倾向的影响显著且不可预测，有时甚至超过模型选择本身的影响。

Conclusion: 现有的对齐方法未能适用于LLM代理，强调对部署中的AI代理进行仔细的系统提示工程，以及对自主系统进行进一步的倾向性评估的必要性。

Abstract: As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [52] [Interpretability by Design for Efficient Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.04022)
*Qiyue Xia,J. Michael Herrmann*

Main category: cs.AI

TL;DR: 本文介绍了一种通过局部线性映射训练方案来获得多目标优化的帕累托前沿，提高了多目标强化学习的效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）旨在优化多个（通常是矛盾的）目标，以提高RL在实际任务中的灵活性和可靠性。

Method: 通过基于参数空间和性能空间之间局部线性映射的训练方案，找到目标偏好最优且对其他偏好非支配的多样化策略，形成多目标性能空间的帕累托前沿。

Result: 在不同领域中进行有无再训练的实验，与以前的方法进行比较，结果表明我们的方法效率更高。

Conclusion: MORL可以通过提供一个近似的帕累托前沿来解读当前参数向量，从而在相邻解域内实现有效搜索。

Abstract: Multi-objective reinforcement learning (MORL) aims at optimising several,
often conflicting goals in order to improve flexibility and reliability of RL
in practical tasks. This can be achieved by finding diverse policies that are
optimal for some objective preferences and non-dominated by optimal policies
for other preferences so that they form a Pareto front in the multi-objective
performance space. The relation between the multi-objective performance space
and the parameter space that represents the policies is generally non-unique.
Using a training scheme that is based on a locally linear map between the
parameter space and the performance space, we show that an approximate Pareto
front can provide an interpretation of the current parameter vectors in terms
of the objectives which enables an effective search within contiguous solution
domains. Experiments are conducted with and without retraining across different
domains, and the comparison with previous methods demonstrates the efficiency
of our approach.

</details>


### [53] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133)
*Shaina Raza,Ranjan Sapkota,Manoj Karkee,Christos Emmanouilidis*

Main category: cs.AI

TL;DR: 本文对基于大型语言模型的agentic AI多智能体系统中的信任、风险和安全管理进行了结构化分析，介绍了相关框架及方法，提出了发展路线图及研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究基于大型语言模型的多智能体系统在智能自主、协作和决策中的作用。

Method: 首先，分析agentic AI的概念基础及其架构与传统AI代理的区别。然后，通过治理、可解释性、ModelOps以及隐私/安全四个支柱详细阐述agentic AI框架下的TRiSM，并通过案例研究支持的综合风险分类法来识别独特的威胁向量。最后，调查分布式LLMs代理系统中的信任构建立、透明度和监督技术，以及解释性策略。

Result: 提供了关于agentic AI的信任、风险和安全管理的深入分析，并提出了相应的风险分类法以及信任、可解释性和以人为中心的性能评估指标。

Conclusion: 文章提出了一条负责任的agentic AI发展路线图，建议研究方向以将新兴的多代理系统与强大的TRiSM原则对齐，实现安全、可问责和透明的部署。

Abstract: Agentic AI systems, built on large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligent autonomy, collaboration
and decision-making across enterprise and societal domains. This review
presents a structured analysis of Trust, Risk, and Security Management (TRiSM)
in the context of LLM-based agentic multi-agent systems (AMAS). We begin by
examining the conceptual foundations of agentic AI, its architectural
differences from traditional AI agents, and the emerging system designs that
enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is
then detailed through four pillars governance, explainability, ModelOps, and
privacy/security each contextualized for agentic LLMs. We identify unique
threat vectors and introduce a comprehensive risk taxonomy for the agentic AI
applications, supported by case studies illustrating real-world
vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,
transparency and oversight techniques, and state-of-the-art explainability
strategies in distributed LLM agent systems. Additionally, metrics for
evaluating trust, interpretability, and human-centered performance are reviewed
alongside open benchmarking challenges. Security and privacy are addressed
through encryption, adversarial defense, and compliance with evolving AI
regulations. The paper concludes with a roadmap for responsible agentic AI,
proposing research directions to align emerging multi-agent systems with robust
TRiSM principles for safe, accountable, and transparent deployment.

</details>


### [54] [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/abs/2506.04135)
*Pei Yang,Hai Ci,Mike Zheng Shou*

Main category: cs.AI

TL;DR: macOSWorld是首个用于评估GU代理在macOS上表现的综合基准测试，揭示了开源模型在macOS环境中的适配不足及多语言环境中的特定弱点，尤其是在面对欺骗攻击时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式基准测试多为英语环境，并未涵盖macOS这一重要操作系统，导致在GUI代理的性能评估上存在缺陷。为弥补这一空白，开发了macOSWorld基准测试。

Method: macOSWorld综合基准测试，涵盖202个多语言交互任务，涉及30款应用（28款为macOS独有），同时提供了5种语言的任务指令和操作系统界面，并包含安全性基准测试子集。

Result: 六种GUI代理的评估表明：专有的计算机使用代理达到超过30%的成功率，而开源轻量研究模型低于2%，多语言基准测试尤其揭示了阿拉伯语的显著弱点。此外，安全性基准测试结果强调了欺骗攻击的普遍存在及其需要即刻关注。

Conclusion: macOSWorld展示了现有GUI代理在macOS上的适配性不足和对欺骗攻击的脆弱性，尤其是开源研究模型表现不佳，且多语言环境中阿拉伯语表现尤为薄弱。

Abstract: Graphical User Interface (GUI) agents show promising capabilities for
automating computer-use tasks and facilitating accessibility, but existing
interactive benchmarks are mostly English-only, covering web-use or Windows,
Linux, and Android environments, but not macOS. macOS is a major OS with
distinctive GUI patterns and exclusive applications. To bridge the gaps, we
present macOSWorld, the first comprehensive benchmark for evaluating GUI agents
on macOS. macOSWorld features 202 multilingual interactive tasks across 30
applications (28 macOS-exclusive), with task instructions and OS interfaces
offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As
GUI agents are shown to be vulnerable to deception attacks, macOSWorld also
includes a dedicated safety benchmarking subset. Our evaluation on six GUI
agents reveals a dramatic gap: proprietary computer-use agents lead at above
30% success rate, while open-source lightweight research models lag at below
2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks
also expose common weaknesses, especially in Arabic, with a 27.5% average
degradation compared to English. Results from safety benchmarking also
highlight that deception attacks are more general and demand immediate
attention. macOSWorld is available at https://github.com/showlab/macosworld.

</details>


### [55] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
*Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Yifu Lu,Mengdi Wang,Dinesh Manocha,Furong Huang,Mohammad Ghavamzadeh,Amrit Singh Bedi*

Main category: cs.AI

TL;DR: 更多思考在测试时并非总能提升性能，提出并行思考以提高推理模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 测试时间扩展思考痕迹会影响推理性能，但“是否更多思考会带来更好的推理？”这一问题仍存疑。

Method: 进行了详细的实证研究，并采用简单的概率模型进行分析。

Result: 额外的思考初期会提高性能，但会因过度思考导致下降，提出了并行思考的方法以提高准确性。

Conclusion: 更多的思考并不是提高推理能力的真正指示，而是由于模型不确定性与评估指标的关联产生的假象。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [56] [Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs](https://arxiv.org/abs/2506.04215)
*Alex DeWeese,Guannan Qu*

Main category: cs.MA

TL;DR: 针对局部相互依赖的多智能体MDP，提出了拓展截止策略类以解决可见性小的问题并优化性能。


<details>
  <summary>Details</summary>
Motivation: 为了处理可见性小且固定时的性能下降问题以及‘惩罚抖动’现象，我们开发了拓展的策略类，不仅能够在部分可见性情况下实现接近最优的封闭形式策略，还能增强对智能体的记忆。

Method: 我们制定了拓展截止策略类，并在广义的局部相互依赖多智能体MDP中复现了理论结果，以解决可见性小且固定时的‘惩罚抖动’现象。

Result: 新提出的策略能在可见性小且固定的情况下显著提升表现，解决‘惩罚抖动’问题，并在特定条件下确保全局可见的联合最优行为。

Conclusion: 我们提出了一类拓展截止策略类，在对任何局部相互依赖的多智能体MDP的可见性方面，这是一类接近最优的封闭形式部分可见策略。

Abstract: Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are
known to be NEXP-Complete and intractable to solve. However, for problems such
as cooperative navigation, obstacle avoidance, and formation control, basic
assumptions can be made about local visibility and local dependencies. The work
DeWeese and Qu 2024 formalized these assumptions in the construction of the
Locally Interdependent Multi-Agent MDP. In this setting, it establishes three
closed-form policies that are tractable to compute in various situations and
are exponentially close to optimal with respect to visibility. However, it is
also shown that these solutions can have poor performance when the visibility
is small and fixed, often getting stuck during simulations due to the so called
"Penalty Jittering" phenomenon. In this work, we establish the Extended Cutoff
Policy Class which is, to the best of our knowledge, the first non-trivial
class of near optimal closed-form partially observable policies that are
exponentially close to optimal with respect to the visibility for any Locally
Interdependent Multi-Agent MDP. These policies are able to remember agents
beyond their visibilities which allows them to perform significantly better in
many small and fixed visibility settings, resolve Penalty Jittering
occurrences, and under certain circumstances guarantee fully observable joint
optimal behavior despite the partial observability. We also propose a
generalized form of the Locally Interdependent Multi-Agent MDP that allows for
transition dependence and extended reward dependence, then replicate our
theoretical results in this setting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL](https://arxiv.org/abs/2506.03154)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: 本文提出了一种模块化的训练方法，通过将指导模块与扩散模型分离，提高了离线强化学习的效率、性能，并验证了其强大的模块性和传递性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要对指导模块和扩散模型进行联合训练，这在初期会因指导不准确导致学习信号噪声。本文研究离线RL中，指导仅依赖于离线数据而非策略模块，揭示联合训练的非必要性，进而探讨模块化训练的优势。

Method: 首先独立训练指导模块作为价值评估，然后冻结该模块，以无分类器的奖励指导扩散模型的训练。这种模块化方法能降低内存使用，提高计算效率和样本效率，并允许跨模块的传递性。

Result: 在bullet D4RL基准测试中，通过理论论证和实验证实，模块化方法优化了学习过程，显著减少了归一化得分方差（如IQR降低86%），并展示出基础水平性能及其强大的模块性和传递性。

Conclusion: 本文提出的模块化训练方法能更有效地优化离线强化学习中的指导模块和扩散模型，提高最终性能和计算效率。

Abstract: Classifier free guidance has shown strong potential in diffusion-based
reinforcement learning. However, existing methods rely on joint training of the
guidance module and the diffusion model, which can be suboptimal during the
early stages when the guidance is inaccurate and provides noisy learning
signals. In offline RL, guidance depends solely on offline data: observations,
actions, and rewards, and is independent of the policy module's behavior,
suggesting that joint training is not required. This paper proposes modular
training methods that decouple the guidance module from the diffusion model,
based on three key findings:
  Guidance Necessity: We explore how the effectiveness of guidance varies with
the training stage and algorithm choice, uncovering the roles of guidance and
diffusion. A lack of good guidance in the early stage presents an opportunity
for optimization.
  Guidance-First Diffusion Training: We introduce a method where the guidance
module is first trained independently as a value estimator, then frozen to
guide the diffusion model using classifier-free reward guidance. This
modularization reduces memory usage, improves computational efficiency, and
enhances both sample efficiency and final performance.
  Cross-Module Transferability: Applying two independently trained guidance
models, one during training and the other during inference, can significantly
reduce normalized score variance (e.g., reducing IQR by 86%). We show that
guidance modules trained with one algorithm (e.g., IDQL) can be directly reused
with another (e.g., DQL), with no additional training required, demonstrating
baseline-level performance as well as strong modularity and transferability.
  We provide theoretical justification and empirical validation on bullet D4RL
benchmarks. Our findings suggest a new paradigm for offline RL: modular,
reusable, and composable training pipelines.

</details>


### [58] [Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World](https://arxiv.org/abs/2506.03155)
*Yu Zheng*

Main category: cs.LG

TL;DR: 提出了一个四层的跨域多模态数据融合框架，能够有效解决现实世界问题。


<details>
  <summary>Details</summary>
Motivation: 由于现实环境的复杂性，无法通过单一的信息获取方式进行建模，需要整合多源生成的多模态数据。在特定领域数据不足时，通过跨域的多模态数据融合来弥补差距。

Method: 提出了一个包括领域层、链接层、模型层和数据层的四层结构框架，用以指导跨域多模态数据融合。

Result: 该框架能够有效地设计端到端的解决方案，实现跨域多模态数据的融合以解决实际问题。

Conclusion: 提出了一个四层框架用于跨域多模态数据融合，可以有效解决现实世界问题。

Abstract: The proliferation of artificial intelligence has enabled a diversity of
applications that bridge the gap between digital and physical worlds. As
physical environments are too complex to model through a single information
acquisition approach, it is crucial to fuse multimodal data generated by
different sources, such as sensors, devices, systems, and people, to solve a
problem in the real world. Unfortunately, it is neither applicable nor
sustainable to deploy new resources to collect original data from scratch for
every problem. Thus, when data is inadequate in the domain of problem, it is
vital to fuse knowledge from multimodal data that is already available in other
domains. We call this cross-domain knowledge fusion. Existing research focus on
fusing multimodal data in a single domain, supposing the knowledge from
different datasets is intrinsically aligned; however, this assumption may not
hold in the scenarios of cross-domain knowledge fusion. In this paper, we
formally define the cross-domain multimodal data fusion problem, discussing its
unique challenges, differences and advantages beyond data fusion in a single
domain. We propose a four-layer framework, consisting of Domains, Links, Models
and Data layers, answering three key questions: "what to fuse", "why can be
fused", and "how to fuse". The Domains Layer selects relevant data from
different domains for a given problem. The Links Layer reveals the philosophy
of knowledge alignment beyond specific model structures. The Models Layer
provides two knowledge fusion paradigms based on the fundamental mechanisms for
processing data. The Data Layer turns data of different structures,
resolutions, scales and distributions into a consistent representation that can
be fed into an AI model. With this framework, we can design end-to-end
solutions that fuse cross-domain multimodal data effectively for solving
real-world problems.

</details>


### [59] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/abs/2506.03158)
*Jiahao Qin,Bei Peng,Feng Liu,Guangliang Cheng,Lu Zong*

Main category: cs.LG

TL;DR: 提出动态不确定性感知学习（DUAL）框架，通过动态特征不确定性建模等创新，提高视觉和多模态学习任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在各种学习场景中经常遇到特征不确定性，这显著影响了其性能和可靠性，特别是在多模态场景中，需要整合来源不同且具有内在不确定性的多种信息。

Method: 我们提出了一种称为动态不确定性感知学习（DUAL）的统一框架，通过引入动态特征不确定性建模、自适应分布感知调制和不确定性感知跨模态关系学习三个关键创新来有效处理特征不确定性。

Result: 在广泛的实验中，DUAL在多个领域展示了其有效性：在计算机视觉任务中，在CIFAR-10上提高了7.1%的准确率，在CIFAR-100上提高了6.5%，在Tiny-ImageNet上提高了2.3%；在多模态学习中，在情感分析任务中表现出显著优势，在CMU-MOSEI上提升了4.1%的准确率，在CMU-MOSI上提升了2.8%，在MISR上提高了1.4%的准确率。

Conclusion: DUAL框架通过有效管理特征不确定性，显著提升了单模态和多模态学习任务的性能。

Abstract: Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [60] [Bayes Error Rate Estimation in Difficult Situations](https://arxiv.org/abs/2506.03159)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 研究比较了几种BER估计器，通过蒙特卡罗模拟评估其在有限样本下的准确性，发现kNN在大多数情况下是最准确的估计器，但当增加特征数后需要更多样本。


<details>
  <summary>Details</summary>
Motivation: BER估计器提供了分类问题难度的洞察，并为优化分类性能设定期望。为了实用性，估计器需要在有限样本和未知分布情况下准确。

Method: 使用蒙特卡罗模拟进行深入的准确性检查，并在不同的测试场景下进行2500次蒙特卡罗模拟。

Result: 结果显示k-最近邻（kNN）作为非参数估计器在蒙特卡罗模拟中展现了压倒性更高的准确性。

Conclusion: 在给定的分类问题中，kNN被证明是最准确的非参数估计器。尽管当特征数增加时有些估计器的准确性超过了kNN，但它们无法持续达到目标范围。

Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable
generalizable classification accuracy of any machine learning model due to
inherent uncertainty within the data. BER estimators offer insight into the
difficulty of any classification problem and set expectations for optimal
classification performance. In order to be useful, the estimators must also be
accurate with a limited number of samples on multivariate problems with unknown
class distributions. To determine which estimators meet the minimum
requirements for "usefulness", an in-depth examination of their accuracy is
conducted using Monte Carlo simulations with synthetic data in order to obtain
their confidence bounds for binary classification. To examine the usability of
the estimators on real-world applications, new test scenarios are introduced
upon which 2500 Monte Carlo simulations per scenario are run over a wide range
of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized
Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,
results show that kNN is overwhelmingly the more accurate non-parametric
estimator. In order to reach the target of an under 5 percent range for the 95
percent confidence bounds, the minimum number of required samples per class is
1000. As more features are added, more samples are needed, so that 2500 samples
per class are required at only 4 features. Other estimators do become more
accurate than kNN as more features are added, but continuously fail to meet the
target range.

</details>


### [61] [Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes](https://arxiv.org/abs/2506.03160)
*Shriyank Somvanshi,Anannya Ghosh Tusti,Mahmuda Sultana Mimi,Md Monzurul Islam,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.LG

TL;DR: 这项研究评估了三种深度学习模型在分类自动化车辆事故自动化水平上的表现，MambaAttention表现最佳，而TabTransformer在部分自动化识别上效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的事故分类方法常忽视自动化特定因素，缺乏对不同自动化级别区分能力的模型复杂性。

Method: 使用三种先进表格深度学习模型对结构化事故数据进行分类: MambaAttention, TabPFN, TabTransformer。

Result: MambaAttention表现最佳，尤其是在高等级自动化分类上（F1-score最高达99%）；TabPFN在零样本推断中表现出色，而TabTransformer对部分自动化事故的识别性能较差（F1-score仅为55%）。

Conclusion: 深度学习模型可以显著提高自动化水平分类的准确性和效率，特别是在驾驶辅助技术的事故分析中。

Abstract: The increasing presence of automated vehicles (AVs) presents new challenges
for crash classification and safety analysis. Accurately identifying the SAE
automation level involved in each crash is essential to understanding crash
dynamics and system accountability. However, existing approaches often overlook
automation-specific factors and lack model sophistication to capture
distinctions between different SAE levels. To address this gap, this study
evaluates the performance of three advanced tabular deep learning models
MambaAttention, TabPFN, and TabTransformer for classifying SAE automation
levels using structured crash data from Texas (2024), covering 4,649 cases
categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level
2), and Advanced Automation (SAE Levels 3-5 combined). Following class
balancing using SMOTEENN, the models were trained and evaluated on a unified
dataset of 7,300 records. MambaAttention demonstrated the highest overall
performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),
while TabPFN excelled in zero-shot inference with high robustness for rare
crash categories. In contrast, TabTransformer underperformed, particularly in
detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in
modeling shared human-system control dynamics. These results highlight the
capability of deep learning models tailored for tabular data to enhance the
accuracy and efficiency of automation-level classification. Integrating such
models into crash analysis frameworks can support policy development, AV safety
evaluation, and regulatory decisions, especially in distinguishing high-risk
conditions for mid- and high-level automation technologies.

</details>


### [62] [Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment](https://arxiv.org/abs/2506.03161)
*Mira Nuthakki*

Main category: cs.LG

TL;DR: 该研究开发了一种3D城市交通模拟和自定义强化学习框架，显著减少了交通事故和碳排放，提高了燃油效率，证明了其城市范围应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵和交通事故是全球性的重要问题，传统的交通管理方法对这些复杂和动态的问题效果有限。为了解决现有的研究空白，该研究开发了三种工具来改善交通管理。

Method: 本研究开发了一个综合的3D全市范围内的仿真环境，结合宏观和微观交通动态、碰撞模型，以及一个以安全优先于效率的自定义奖励函数的强化学习框架。其中，使用Unity游戏引擎进行直接碰撞建模，并通过一种自定义奖励的强化学习方法——近端策略优化（PPO）模型来改善交通管理。

Result: 与基线结果相比，使用本研究方法显著减少了严重碰撞数量、车辆碰撞总数及行驶总距离超过三倍。且燃油效率提高了39%，碳排放减少了88%。结果显示，该方法具备在全市范围内应用的可行性，并能有效整合交通部的零死亡安全原则、物理信息、适应性和现实碰撞建模以及用于现实世界的交通信号灯控制的奖励建模。

Conclusion: 本研究表明，使用3D城市交通模拟结合自定义的强化学习策略可以大幅减少交通事故和碳排放，提高燃油效率，同时优化交通流动。这为实现城市范围内的交通优化提供了可行的解决方案。

Abstract: Traffic congestion and collisions represent significant economic,
environmental, and social challenges worldwide. Traditional traffic management
approaches have shown limited success in addressing these complex, dynamic
problems. To address the current research gaps, three potential tools are
developed: a comprehensive 3D city-wide simulation environment that integrates
both macroscopic and microscopic traffic dynamics; a collision model; and a
reinforcement learning framework with custom reward functions prioritizing
safety over efficiency. Unity game engine-based simulation is used for direct
collision modeling. A custom reward enabled reinforcement learning method,
proximal policy optimization (PPO) model, yields substantial improvements over
baseline results, reducing the number of serious collisions, number of
vehicle-vehicle collisions, and total distance travelled by over 3 times the
baseline values. The model also improves fuel efficiency by 39% and reduces
carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic
simulation applications incorporating the vision-zero safety principles of the
Department of Transportation, including physics-informed, adaptable, realistic
collision modeling, as well as appropriate reward modeling for real-world
traffic signal light control towards reducing collisions, optimizing traffic
flow and reducing greenhouse emissions.

</details>


### [63] [Causal Discovery in Dynamic Fading Wireless Networks](https://arxiv.org/abs/2506.03163)
*Oluwaseyi Giwa*

Main category: cs.LG

TL;DR: 此研究提出一种用于动态无线网络因果推断的算法，并通过理论及模拟验证其性能。


<details>
  <summary>Details</summary>
Motivation: 由于无线网络中的干扰、衰落和移动性使得传统的静态因果模型变得复杂，因此需要在动态无线网络中进行因果发现。

Method: 在动态衰落的无线环境中，提出了一种顺序回归算法，并应用NOTEARS非循环性约束来实现有效的在线更新。

Result: 模拟结果证实了检测延迟随着网络规模线性增加、噪声方差平方增长以及与结构变化的大小呈反比平方关系。

Conclusion: 提出了一种基于顺序回归的算法，采用NOTEARS非循环性约束，能够进行有效的在线更新，证明了相关理论结果并通过Monte Carlo模拟验证。

Abstract: Dynamic causal discovery in wireless networks is essential due to evolving
interference, fading, and mobility, which complicate traditional static causal
models. This paper addresses causal inference challenges in dynamic fading
wireless environments by proposing a sequential regression-based algorithm with
a novel application of the NOTEARS acyclicity constraint, enabling efficient
online updates. We derive theoretical lower and upper bounds on the detection
delay required to identify structural changes, explicitly quantifying their
dependence on network size, noise variance, and fading severity. Monte Carlo
simulations validate these theoretical results, demonstrating linear increases
in detection delay with network size, quadratic growth with noise variance, and
inverse-square dependence on the magnitude of structural changes. Our findings
provide rigorous theoretical insights and practical guidelines for designing
robust online causal inference mechanisms to maintain network reliability under
nonstationary wireless conditions.

</details>


### [64] [Test-Time Scaling of Diffusion Models via Noise Trajectory Search](https://arxiv.org/abs/2506.03164)
*Vignav Ramesh,Morteza Mardani*

Main category: cs.LG

TL;DR: 研究通过将扩散视为一个松弛的马尔可夫决策过程，提出了一种新的噪声轨迹优化方法，在生成任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 优化噪声轨迹对于提高样本质量至关重要，但由于高维搜索空间、复杂的噪声结果互动和高成本的轨迹评估，这是一项具有挑战的工作。

Method: 将扩散过程视为一个马尔可夫决策过程，但为了兼顾性能和效率，采用了一种MDP的松弛方式，并引入了一个$\epsilon$-贪婪搜索算法。

Result: 在EDM和Stable Diffusion实验中，类条件和文本到图像生成的得分达到了最新水平，相比基线提升高达164%，并且能匹配或超越MCTS性能。

Conclusion: 提出了一种新的方法用于优化测试时的噪声轨迹，这在类条件和文本到图像生成中取得了突破性的结果。

Abstract: The iterative and stochastic nature of diffusion models enables test-time
scaling, whereby spending additional compute during denoising generates
higher-fidelity samples. Increasing the number of denoising steps is the
primary scaling axis, but this yields quickly diminishing returns. Instead
optimizing the noise trajectory--the sequence of injected noise vectors--is
promising, as the specific noise realizations critically affect sample quality;
but this is challenging due to a high-dimensional search space, complex
noise-outcome interactions, and costly trajectory evaluations. We address this
by first casting diffusion as a Markov Decision Process (MDP) with a terminal
reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to
be meaningful but impractical. To balance performance and efficiency, we then
resort to a relaxation of MDP, where we view denoising as a sequence of
independent contextual bandits. This allows us to introduce an
$\epsilon$-greedy search algorithm that globally explores at extreme timesteps
and locally exploits during the intermediate steps where de-mixing occurs.
Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for
class-conditioned/text-to-image generation, exceeding baselines by up to
$164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the
first practical method for test-time noise trajectory optimization of arbitrary
(non-differentiable) rewards.

</details>


### [65] [Non-collective Calibrating Strategy for Time Series Forecasting](https://arxiv.org/abs/2506.03176)
*Bin Wang,Yongqi Han,Minbo Ma,Tianrui Li,Junbo Zhang,Feng Hong,Yanwei Yu*

Main category: cs.LG

TL;DR: 通过创新的Socket+Plug (SoP)校准策略提高现有深度学习模型在时间序列预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在时间序列预测中取得了显著进展，但如何设计最佳模型架构仍然非常具有挑战性。本研究旨在通过通用校准策略来完善现有模型，减少资源成本而非从零开始设计新模型。

Method: 提出了一种创新的校准策略——Socket+Plug (SoP)，保留每个Plug内的专有优化器和预测目标的早期停止监控，同时保持完全训练的Socket主干冻结。

Result: SoP策略能够直接校准任何训练过的深度预测模型的性能，无论其特定架构为何。在多个时间序列基准和一个时空气象ERA5数据集上的广泛实验表明，SoP的有效性，即使使用简单的MLP作为Plug也能实现高达22%的改进。

Conclusion: 通过通用校准策略而非重新设计和训练新模型，可以显著提高时间序列预测模型的性能。

Abstract: Deep learning-based approaches have demonstrated significant advancements in
time series forecasting. Despite these ongoing developments, the complex
dynamics of time series make it challenging to establish the rule of thumb for
designing the golden model architecture. In this study, we argue that refining
existing advanced models through a universal calibrating strategy can deliver
substantial benefits with minimal resource costs, as opposed to elaborating and
training a new model from scratch. We first identify a multi-target learning
conflict in the calibrating process, which arises when optimizing variables
across time steps, leading to the underutilization of the model's learning
capabilities. To address this issue, we propose an innovative calibrating
strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer
and early-stopping monitor for each predicted target within each Plug while
keeping the fully trained Socket backbone frozen. The model-agnostic nature of
SoP allows it to directly calibrate the performance of any trained deep
forecasting models, regardless of their specific architectures. Extensive
experiments on various time series benchmarks and a spatio-temporal
meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up
to a 22% improvement even when employing a simple MLP as the Plug (highlighted
in Figure 1)

</details>


### [66] [Out-of-Vocabulary Sampling Boosts Speculative Decoding](https://arxiv.org/abs/2506.03206)
*Nadav Timor,Jonathan Mamou,Oren Pereg,Hongyang Zhang,David Harel*

Main category: cs.LG

TL;DR: 本文提出了重新分配草稿生成器内核（RDK），一种有效处理超出词汇表采样的技术，大大提高了接受率，支持极端词汇量修剪，提高推测解码器效率。


<details>
  <summary>Details</summary>
Motivation: 由于当前的最先进语言模型使用越来越大的词汇表，导致草稿生成速度变慢，因此本文探讨如何通过使用词汇表较小的草稿生成器来提高推测解码的效率。然而，现有的采样方法无法生成超出词汇表的词元，从而在草稿生成器的词汇大小和接受率之间产生权衡。

Method: 本文引入了重新分配草稿生成器内核（RDK），这是一种新的超出词汇表的采样器，通过虚拟恢复修剪的目标词元来有效地恢复接受率。

Result: 通过数学证明，RDK可以比传统和最先进的采样器实现更高的接受率。我们提供了RDK的高效一阶近似，并证明它将重分配时间从$O(N^2)$减少到$O(N)$。实验表明，即使在极端修剪（移除超过草稿生成器词汇表的75%）的情况下，线性时间RDK也显著提高了接受率。

Conclusion: 通过使用RDK，可以在极端修剪条件下实现高度有效的推测解码器，这在过去是不实际的。

Abstract: Speculative decoding relies on fast and accurate drafters. Recent
state-of-the-art language models employ larger and larger vocabularies, which
significantly slows down drafters. One promising approach to boost the
efficiency of speculative decoding is to use drafters with smaller
vocabularies. However, existing sampling methods cannot draw out-of-vocabulary
tokens, creating a tradeoff between drafters' vocabulary size and acceptance
rates. This paper introduces Redistributing Drafter Kernels (RDK), the first
out-of-vocabulary sampler that effectively recovers acceptance rates by
virtually restoring pruned target tokens. RDK leverages token-affinity priors
to reallocate drafter mass towards high-overlap regions. We prove
mathematically that RDK can achieve higher acceptance rates than vanilla and
state-of-the-art samplers. We provide an efficient first-order approximation of
RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,
enabling lightweight implementations for large vocabularies. Our experiments
demonstrate that this linear-time RDK significantly boosts acceptance rates
even after extreme pruning (removing more than 75% of the drafter's
vocabulary), where existing samplers fail. RDK opens the door to extremely
pruned drafters, which were previously impractical.

</details>


### [67] [Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning](https://arxiv.org/abs/2506.03207)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.LG

TL;DR: 研究通过分析网络层流量信息，成功实现对联邦学习环境中深度学习模型的指纹识别，揭示了FL系统中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习(FL)框架下，实现数据隐私保护的同时，避免集中用户数据。然而，FL面临通过网络流量分析间接泄露隐私的风险，该领域目前尚未得到充分研究。

Method: 通过实验评估，在联邦学习试验平台中使用多种深度学习架构（如CNN，RNN）。利用支持向量机（SVM）、随机森林和梯度提升等机器学习算法对网络流量数据中的独特模式进行指纹识别。

Result: 实验表明，使用随机森林算法可以达到100%的指纹识别准确率，而使用SVM和梯度提升算法则可以达到约95.7%的准确率。

Conclusion: 研究揭示了联邦学习系统中的显著安全漏洞，并建议需要在网络层面加强安全性，以防止已知DL架构被用来发动针对性攻击。

Abstract: Federated Learning (FL) is increasingly adopted as a decentralized machine
learning paradigm due to its capability to preserve data privacy by training
models without centralizing user data. However, FL is susceptible to indirect
privacy breaches via network traffic analysis-an area not explored in existing
research. The primary objective of this research is to study the feasibility of
fingerprinting deep learning models deployed within FL environments by
analyzing their network-layer traffic information. In this paper, we conduct an
experimental evaluation using various deep learning architectures (i.e., CNN,
RNN) within a federated learning testbed. We utilize machine learning
algorithms, including Support Vector Machines (SVM), Random Forest, and
Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our
experiments show high fingerprinting accuracy, achieving 100% accuracy using
Random Forest and around 95.7% accuracy using SVM and Gradient Boosting
classifiers. This analysis suggests that we can identify specific architectures
running within the subsection of the network traffic. Hence, if an adversary
knows about the underlying DL architecture, they can exploit that information
and conduct targeted attacks. These findings suggest a notable security
vulnerability in FL systems and the necessity of strengthening it at the
network level.

</details>


### [68] [FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution](https://arxiv.org/abs/2506.03210)
*Qiusheng Huang,Yuan Niu,Xiaohui Zhong,Anboyu Guo,Lei Chen,Dianjun Zhang,Xuefeng Zhang,Hao Li*

Main category: cs.LG

TL;DR: FuXi-Ocean是首个能实现六小时预测的全球海洋数据驱动模型，结合MoT模块在1500米深度和多个关键变量上显示出优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值模型尽管可以实现亚日、解析涡的预报，但计算量大且在精细的时空尺度上很难维持准确性。数据驱动的方法计算效率高，但通常在日分辨率下操作，并在亚日预测中容易因误差积累而表现不佳。

Method: FuXi-Ocean模型集成了上下文感知特征提取模块和使用堆叠注意力块的预测网络，其核心创新是时间混合（MoT）模块，用于自适应综合来自多个时间上下文的预测。

Result: FuXi-Ocean通过综合实验评估，证明其在多深度关键变量预测上的技能卓越。

Conclusion: FuXi-Ocean成功实现了全球海洋的六小时预报，在1/12°的空间分辨率下能解析涡流，预测深度可达1500米，且在温度、盐度和水流等关键变量的预测中表现优越。

Abstract: Accurate, high-resolution ocean forecasting is crucial for maritime
operations and environmental monitoring. While traditional numerical models are
capable of producing sub-daily, eddy-resolving forecasts, they are
computationally intensive and face challenges in maintaining accuracy at fine
spatial and temporal scales. In contrast, recent data-driven approaches offer
improved computational efficiency and emerging potential, yet typically operate
at daily resolution and struggle with sub-daily predictions due to error
accumulation over time. We introduce FuXi-Ocean, the first data-driven global
ocean forecasting model achieving six-hourly predictions at eddy-resolving
1/12{\deg} spatial resolution, reaching depths of up to 1500 meters. The model
architecture integrates a context-aware feature extraction module with a
predictive network employing stacked attention blocks. The core innovation is
the Mixture-of-Time (MoT) module, which adaptively integrates predictions from
multiple temporal contexts by learning variable-specific reliability ,
mitigating cumulative errors in sequential forecasting. Through comprehensive
experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting
key variables, including temperature, salinity, and currents, across multiple
depths.

</details>


### [69] [Multiple-Frequencies Population-Based Training](https://arxiv.org/abs/2506.03225)
*Waël Doulazmi,Auguste Lehuger,Marin Toromanoff,Valentin Charraut,Thibault Buhet,Fabien Moutarde*

Main category: cs.LG

TL;DR: 本文提出了MF-PBT算法，通过多频率子群体进化和迁移过程改善超参数优化效率与长期性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有超参数优化算法，如基于群体的训练（PBT）存在的贪婪问题和局部最优困境，研究进化频率选择对该问题的影响，以提升长期性能。

Method: 提出了多频率群体训练（MF-PBT）算法，其中子群体以不同的频率进化，并通过非对称的迁移过程在子群体之间传递信息。

Result: 通过在Brax套件上的大量实验表明，MF-PBT在不实际调整超参数的情况下提高了样本效率和长期性能。

Conclusion: 本文提出了一种新的超参数优化算法MF-PBT，通过采用多频率子群体和迁移过程，提升了样本效率和长期性能。

Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of
instability and inefficiency, creating significant challenges for
practitioners. Hyperparameter Optimization (HPO) algorithms have been developed
to address this issue, among them Population-Based Training (PBT) stands out
for its ability to generate hyperparameters schedules instead of fixed
configurations. PBT trains a population of agents, each with its own
hyperparameters, frequently ranking them and replacing the worst performers
with mutations of the best agents. These intermediate selection steps can cause
PBT to focus on short-term improvements, leading it to get stuck in local
optima and eventually fall behind vanilla Random Search over longer timescales.
This paper studies how this greediness issue is connected to the choice of
evolution frequency, the rate at which the selection is done. We propose
Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm
that addresses greediness by employing sub-populations, each evolving at
distinct frequencies. MF-PBT introduces a migration process to transfer
information between sub-populations, with an asymmetric design to balance short
and long-term optimization. Extensive experiments on the Brax suite demonstrate
that MF-PBT improves sample efficiency and long-term performance, even without
actually tuning hyperparameters.

</details>


### [70] [Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification](https://arxiv.org/abs/2506.03227)
*Abdelrahman Sayed Sayed,Pierre-Jean Meyer,Mohamed Ghazel*

Main category: cs.LG

TL;DR: 研究界定神经ODE和ResNet之间的逼近误差，通过该误差实现模型间的互为验证特性。


<details>
  <summary>Details</summary>
Motivation: 神经ODE和ResNet之间虽然关系紧密，但尚需一种更正式的方法来确定它们之间的逼近误差，以便利用其互为验证的特性。

Method: 在神经ODE与残差网络中进行逼近误差界定，并利用此误差界限进行模型验证。

Result: 当误差界限扩展系统满足其中一个模型的安全性属性时，该属性在另一个模型中同样得到保证。

Conclusion: 通过界定神经ODE和残差网络之间的逼近误差，我们可以使用其中一个模型的验证结果来推断另一个模型的安全性属性。

Abstract: A neural ordinary differential equation (neural ODE) is a machine learning
model that is commonly described as a continuous depth generalization of a
residual network (ResNet) with a single residual block, or conversely, the
ResNet can be seen as the Euler discretization of the neural ODE. These two
models are therefore strongly related in a way that the behaviors of either
model are considered to be an approximation of the behaviors of the other. In
this work, we establish a more formal relationship between these two models by
bounding the approximation error between two such related models. The obtained
error bound then allows us to use one of the models as a verification proxy for
the other, without running the verification tools twice: if the reachable
output set expanded by the error bound satisfies a safety property on one of
the models, this safety property is then guaranteed to be also satisfied on the
other model. This feature is fully reversible, and the initial safety
verification can be run indifferently on either of the two models. This novel
approach is illustrated on a numerical example of a fixed-point attractor
system modeled as a neural ODE.

</details>


### [71] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
*Selcuk Gurses,Aozhong Zhang,Yanxia Deng,Xun Dong,Xin Li,Naigang Wang,Penghang Yin,Zi Yang*

Main category: cs.LG

TL;DR: DiaBlo是一种更新模型权重矩阵对角块的参数高效微调方法，在多个任务中表现优异，无需定制优化策略。


<details>
  <summary>Details</summary>
Motivation: 为了减轻全模型微调的计算和内存成本，同时缩小参数高效微调（PEFT）方法与全模型微调的性能差距。

Method: DiaBlo更新了模型权重矩阵的对角块，从而避免依赖辅助初始化方案或定制优化策略，具有稳定和鲁棒的收敛性。

Result: 进行了一系列任务的广泛实验来评估DiaBlo的有效性和效率，结果表明DiaBlo表现强劲且一致。

Conclusion: DiaBlo在多个基准测试上表现出色且持续表现优异，同时保持高内存效率和快速微调速度。

Abstract: Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [72] [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
*Kaiwen Duan,Hongwei Yao,Yufei Chen,Ziyun Li,Tong Qiao,Zhan Qin,Cong Wang*

Main category: cs.LG

TL;DR: 提出BadReward投毒攻击，破坏多模态RLHF中的奖励模型，实验验证其可以引发T2I模型生成不当图像，呼吁加强系统防御。


<details>
  <summary>Details</summary>
Motivation: 研究表明RLHF中的反馈机制可能被对手利用，因此探讨通过少量偏好数据进行投毒的方法来攻击T2I模型。

Method: 提出了一种称为BadReward的隐秘的、干净标签的投毒攻击方法，该方法通过在视觉矛盾的偏好数据实例之间引发特征碰撞来破坏奖励模型，从而间接破坏T2I模型的完整性。

Result: 实验表明，BadReward能够在T2I模型上持续引导生成不当的输出，例如有偏或暴力的图像。

Conclusion: 多模态RLHF系统中存在的安全隐患是放大的，需要紧急的、强有力的防御措施。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.

</details>


### [73] [On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models](https://arxiv.org/abs/2506.03267)
*Shahbaz Rezaei,Avishai Halev,Xin Liu*

Main category: cs.LG

TL;DR: 该论文通过引入不确定性原理，揭示了时间域与频率域解释的差异，并验证了多域解释的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法仅在单一域进行解释，可能忽略了不同域之间的显著特征，需要多域解释来弥补这种不足。

Method: 引入不确定性原理（UP）来量化时间与频率域归因不匹配的情况，并评估不同深度学习模型和XAI方法的有效性。

Result: 实验表明，在多个数据集和XAI方法中经常出现不确定性原理违反情况，凸显了单一时间域解释方法的局限性并支持多域解释的必要性。

Conclusion: 需要将时间域和频率域的归因同时呈现，以便更全面地解释时间序列模型。

Abstract: A prevailing approach to explain time series models is to generate
attribution in time domain. A recent development in time series XAI is the
concept of explanation spaces, where any model trained in the time domain can
be interpreted with any existing XAI method in alternative domains, such as
frequency. The prevailing approach is to present XAI attributions either in the
time domain or in the domain where the attribution is most sparse. In this
paper, we demonstrate that in certain cases, XAI methods can generate
attributions that highlight fundamentally different features in the time and
frequency domains that are not direct counterparts of one another. This
suggests that both domains' attributions should be presented to achieve a more
comprehensive interpretation. Thus it shows the necessity of multi-domain
explanation. To quantify when such cases arise, we introduce the uncertainty
principle (UP), originally developed in quantum mechanics and later studied in
harmonic analysis and signal processing, to the XAI literature. This principle
establishes a lower bound on how much a signal can be simultaneously localized
in both the time and frequency domains. By leveraging this concept, we assess
whether attributions in the time and frequency domains violate this bound,
indicating that they emphasize distinct features. In other words, UP provides a
sufficient condition that the time and frequency domain explanations do not
match and, hence, should be both presented to the end user. We validate the
effectiveness of this approach across various deep learning models, XAI
methods, and a wide range of classification and forecasting datasets. The
frequent occurrence of UP violations across various datasets and XAI methods
highlights the limitations of existing approaches that focus solely on
time-domain explanations. This underscores the need for multi-domain
explanations as a new paradigm.

</details>


### [74] [Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony](https://arxiv.org/abs/2506.03302)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 多出口 KANs 提高了准确性和可解释性，自动选择合适模型复杂度，并通过新的算法在训练中平衡不同出口的贡献。


<details>
  <summary>Details</summary>
Motivation: 为了结合高精度与可解释性，解决标准 KANs 中深度网络的优化困难及建立网络所需深度不明确的问题。

Method: 引入多出口 KANs 架构，在网络的每一层都包含一个预测分支，允许在多个深度进行准确预测。还开发了一个可微分的'学习退出'算法，用于在训练期间平衡从出口层的贡献。

Result: 多出口 KANs 能自动识别出更小和更易解释的模型，同时保持准确性，且深度监督提升了训练效果。

Conclusion: 多出口的Kolmogorov-Arnold Networks (KANs)在合成函数、动态系统和真实数据集上表现优于标准的单出口版本，并且最好的预测往往来自于较早及更简单的出口层。

Abstract: Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with
interpretability, making them valuable for scientific modeling. However, it is
unclear a priori how deep a network needs to be for any given task, and deeper
KANs can be difficult to optimize. Here we introduce multi-exit KANs, where
each layer includes its own prediction branch, enabling the network to make
accurate predictions at multiple depths simultaneously. This architecture
provides deep supervision that improves training while discovering the right
level of model complexity for each task. Multi-exit KANs consistently
outperform standard, single-exit versions on synthetic functions, dynamical
systems, and real-world datasets. Remarkably, the best predictions often come
from earlier, simpler exits, revealing that these networks naturally identify
smaller, more parsimonious and interpretable models without sacrificing
accuracy. To automate this discovery, we develop a differentiable "learning to
exit" algorithm that balances contributions from exits during training. Our
approach offers scientists a practical way to achieve both high performance and
interpretability, addressing a fundamental challenge in machine learning for
scientific discovery.

</details>


### [75] [Budgeted Online Active Learning with Expert Advice and Episodic Priors](https://arxiv.org/abs/2506.03307)
*Kristen Goebel,William Solow,Paola Pesantez-Cabrera,Markus Keller,Alan Fern*

Main category: cs.LG

TL;DR: 本文提出了一种适用于极限标注预算条件的在线主动学习新方法，结合专家预测与情节性知识，在农业数据预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在农业应用中，数据流可能包括一个生长季节中的每日天气数据，而标签需要昂贵的与天气相关的植物特性测量。为了在极其有限的标注预算下进行有效学习，我们提出了一种新的方法。

Method: 我们的方法结合了两种关键的先验信息来源：一组现有专家的预测和基于无标签数据流的专家之间的情节性行为知识。

Result: 实验结果表明，我们的方法在各种预测问题中均表现出色，特别是在一个现实的农业作物模拟器和多个葡萄品种的真实数据中。

Conclusion: 我们的方法在实验中表现出了优越性，显著优于基线专家预测、均匀查询选择以及现有在预算和有限范围内的其他方法，即使在非常有限的标注预算下也如此。

Abstract: This paper introduces a novel approach to budgeted online active learning
from finite-horizon data streams with extremely limited labeling budgets. In
agricultural applications, such streams might include daily weather data over a
growing season, and labels require costly measurements of weather-dependent
plant characteristics. Our method integrates two key sources of prior
information: a collection of preexisting expert predictors and episodic
behavioral knowledge of the experts based on unlabeled data streams. Unlike
previous research on online active learning with experts, our work
simultaneously considers query budgets, finite horizons, and episodic
knowledge, enabling effective learning in applications with severely limited
labeling capacity. We demonstrate the utility of our approach through
experiments on various prediction problems derived from both a realistic
agricultural crop simulator and real-world data from multiple grape cultivars.
The results show that our method significantly outperforms baseline expert
predictions, uniform query selection, and existing approaches that consider
budgets and limited horizons but neglect episodic knowledge, even under highly
constrained labeling budgets.

</details>


### [76] [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/abs/2506.03320)
*Jack Bell,Luigi Quarantiello,Eric Nuertey Coleman,Lanpei Li,Malio Li,Mauro Madeddu,Elia Piccoli,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 持续学习在深度学习时代尤其是大型语言模型出现后仍具必不可少的价值，强调持续预训练、微调和组合性将推动AI生态系统的发展。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的兴起，尤其是大型语言模型（LLMs）和基础模型的出现，人们开始质疑在这些模型能够利用海量知识处理多样任务的背景下，持续学习是否仍然必要。

Method: 本文通过探讨三个关键理由，论证了持续学习的必要性：(i) 持续预训练以确保基础模型保持更新，减轻知识陈旧性和分布变化；(ii) 持续微调以使模型专门化和个性化，适应特定领域任务、用户偏好和现实约束；(iii) 持续组合性提供智能的可扩展和模块化方法，促进基础模型和代理的动态构建和重组。

Result: 指出在持续预训练和微调这些被视为小众研究方向的同时，持续组合性将成为持续学习的重生标志。

Conclusion: 未来的人工智能将不再由单一静态模型定义，而是由一个不断进化和互动的模型生态系统定义，这使得持续学习比以往任何时候都更为重要。

Abstract: Continual learning--the ability to acquire, retain, and refine knowledge over
time--has always been fundamental to intelligence, both human and artificial.
Historically, different AI paradigms have acknowledged this need, albeit with
varying priorities: early expert and production systems focused on incremental
knowledge consolidation, while reinforcement learning emphasised dynamic
adaptation. With the rise of deep learning, deep continual learning has
primarily focused on learning robust and reusable representations over time to
solve sequences of increasingly complex tasks. However, the emergence of Large
Language Models (LLMs) and foundation models has raised the question: Do we
still need continual learning when centralised, monolithic models can tackle
diverse tasks with access to internet-scale knowledge? We argue that continual
learning remains essential for three key reasons: (i) continual pre-training is
still necessary to ensure foundation models remain up to date, mitigating
knowledge staleness and distribution shifts while integrating new information;
(ii) continual fine-tuning enables models to specialise and personalise,
adapting to domain-specific tasks, user preferences, and real-world constraints
without full retraining, avoiding the need for computationally expensive long
context-windows; (iii) continual compositionality offers a scalable and modular
approach to intelligence, enabling the orchestration of foundation models and
agents to be dynamically composed, recombined, and adapted. While continual
pre-training and fine-tuning are explored as niche research directions, we
argue it is continual compositionality that will mark the rebirth of continual
learning. The future of AI will not be defined by a single static model but by
an ecosystem of continually evolving and interacting models, making continual
learning more relevant than ever.

</details>


### [77] [Optimization of Epsilon-Greedy Exploration](https://arxiv.org/abs/2506.03324)
*Ethan Che,Hakan Ceylan,James McInerney,Nathan Kallus*

Main category: cs.LG

TL;DR: 本文提出了一种优化推荐系统探索策略的方法，利用最小化贝叶斯遗憾和MPC进行动态调整，实验显示其性能优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 探索-利用权衡是推荐系统中的关键问题之一，现有启发式策略在实际操作中存在限制（如批量更新，用户流量变化等），需要更优化的探索策略。

Method: 通过随机梯度下降最小化贝叶斯遗憾，用模型预测控制进行动态探索率调整。

Result: 实验表明，对于不同的时间段，批处理大小的变动显著影响最佳探索策略。提出的方法在不同问题设置下匹配或超过最佳启发式。

Conclusion: 提出了一种通过直接最小化贝叶斯遗憾的原则性的探索时间表框架，并利用MPC进行动态调整。经过优化后，可以自动校准探索策略以适应具体问题设置，性能上比传统启发式方法更加优越。

Abstract: Modern recommendation systems rely on exploration to learn user preferences
for new items, typically implementing uniform exploration policies (e.g.,
epsilon-greedy) due to their simplicity and compatibility with machine learning
(ML) personalization models. Within these systems, a crucial consideration is
the rate of exploration - what fraction of user traffic should receive random
item recommendations and how this should evolve over time. While various
heuristics exist for navigating the resulting exploration-exploitation
tradeoff, selecting optimal exploration rates is complicated by practical
constraints including batched updates, time-varying user traffic, short time
horizons, and minimum exploration requirements. In this work, we propose a
principled framework for determining the exploration schedule based on directly
minimizing Bayesian regret through stochastic gradient descent (SGD), allowing
for dynamic exploration rate adjustment via Model-Predictive Control (MPC).
Through extensive experiments with recommendation datasets, we demonstrate that
variations in the batch size across periods significantly influence the optimal
exploration strategy. Our optimization methods automatically calibrate
exploration to the specific problem setting, consistently matching or
outperforming the best heuristic for each setting.

</details>


### [78] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2209.01205)
*Han Wu,Jie Yin,Bala Rajaratnam,Jianyuan Guo*

Main category: cs.LG

TL;DR: 针对知识图谱的少样本补全问题，提出了HiRe方法，能够有效学习元表示，并在多个数据集上的表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 知识图谱在推理能力方面非常强大，但由于不完整性和关系的长尾分布导致其覆盖范围有限。

Method: 提出一种层次化关系学习方法（HiRe），通过联合捕捉实体级、三元组级和上下文级别的关系信息来进行预测。

Result: 与现有的最先进方法相比，HiRe在基准数据集上的性能更优。

Conclusion: HiRe能有效学习和优化少样本关系的元表示，并能够很好地推广到新的未见关系。

Abstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [79] [A Differential Perspective on Distributional Reinforcement Learning](https://arxiv.org/abs/2506.03333)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 扩展分布式强化学习到平均奖励环境，提出新算法检测和优化长期奖励分布，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 以往的分布式强化学习方法集中在折扣设置中，忽视了平均奖励环境下的分布学习。因此，研究动机是扩展分布式强化学习到平均奖励设置，优化每时间步的奖励。

Method: 使用基于分位数的方法，研发了一套能够成功学习和优化平均奖励马尔可夫决策过程（MDP）的长期每步奖励分布和微分回报分布的算法。

Result: 实验证明，提出的算法在绩效上与非分布式等效算法具有竞争力，同时具有捕捉长期奖励和回报分布的能力。

Conclusion: 提出的新算法在平均奖励环境中稳定收敛，并且在预测和控制任务中表现良好，具有良好的扩展性。

Abstract: To date, distributional reinforcement learning (distributional RL) methods
have exclusively focused on the discounted setting, where an agent aims to
optimize a potentially-discounted sum of rewards over time. In this work, we
extend distributional RL to the average-reward setting, where an agent aims to
optimize the reward received per time-step. In particular, we utilize a
quantile-based approach to develop the first set of algorithms that can
successfully learn and/or optimize the long-run per-step reward distribution,
as well as the differential return distribution of an average-reward MDP. We
derive proven-convergent tabular algorithms for both prediction and control, as
well as a broader family of algorithms that have appealing scaling properties.
Empirically, we find that these algorithms consistently yield competitive
performance when compared to their non-distributional equivalents, while also
capturing rich information about the long-run reward and return distributions.

</details>


### [80] [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/abs/2506.03337)
*Yide Ran,Wentao Guo,Jingwei Sun,Yanzhou Pan,Xiaodong Yu,Hao Wang,Jianwen Xie,Yiran Chen,Denghui Zhang,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: Meerkat通过稀疏零阶优化方法提高联邦学习中大型语言模型的微调效率，并引入机制识别复杂数据分布客户端，提升模型质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在联邦学习中的参数规模巨大，导致内存和通信方面的挑战。为了提高微调的效率，本工作提出一种稀疏零阶优化方法来解决这一问题。

Method: 提出了一种稀疏零阶优化方法，Meerkat，通过限制微调到极稀疏参数子集，实现高效通信。还引入了虚拟路径机制来处理数据分布的非独立同分布漂移问题。

Result: Meerkat在通信频率相同的情况下，比现有稀疏基线显示出更好的性能，Meerkat-vp能够识别极端非独立同分布客户端，并通过早停机制提高模型质量。实验验证了其提升效率和效果的能力。

Conclusion: Meerkat技术通过零阶优化方法提高了联邦学习中大型语言模型的微调效率和性能。实验结果表明，它比现有的稀疏基线具有更好的性能，并通过识别数据异质性强的客户端优化模型质量。

Abstract: Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a sparse
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely sparse subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing sparsity baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.

</details>


### [81] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355)
*Elias Abad Rocamora,Christian Schlarmann,Naman Deep Singh,Yongtao Wu,Matthias Hein,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出LEAF方法，提高了CLIP文本编码器的鲁棒性，增强了对抗环境下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究集中于提高CLIP图像编码器的鲁棒性，但对文本编码器的鲁棒性研究较少。

Method: 提出了一种名为LEAF的高效对抗性微调方法，用于文本域，并且能够扩展到大型CLIP模型。

Result: 在文本域中显著提高了零样本对抗性准确性，同时保持了鲁棒图像编码器提供的视觉性能；在多模态检索任务中提高了召回率；增强了受对抗性噪声影响下文本到图像生成模型的生成质量。

Conclusion: 通过提高文本编码器的鲁棒性，改善了嵌入重构和多模态任务的表现。

Abstract: Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [82] [Probabilistic Factorial Experimental Design for Combinatorial Interventions](https://arxiv.org/abs/2506.03363)
*Divya Shyamal,Jiaqi Zhang,Caroline Uhler*

Main category: cs.LG

TL;DR: 介绍了一种概率因子实验设计以简化组合干预的实验设计，有效估计模型所需的观测值数量。


<details>
  <summary>Details</summary>
Motivation: 组合干预能够在生物医学、工程等领域有重要应用。然而，随着可能处理数p的增加，进行所有可能的组合干预变得繁琐且不可行。因此，需要一种更高效的实验设计方式来处理这一问题。

Method: 提出了一种概率因子实验设计，在该框架中实验设计者选择每种可能处理的剂量，并将其应用于一组单元。每个单元独立接收随机的治疗组合，从由剂量决定的乘积伯努利分布中采样。

Result: 在被动设置中，提供了近似最佳设计的闭式解，证明了对于估计任何k路交互模型而言，每种治疗剂量1/2是最佳的，并且需要O(kp^{3k}ln(p))个观测值。此外，多轮设置中的近似最佳采集函数可以通过数值优化得到。

Conclusion: 该方法能够有效处理组合干预的实验设计问题，并通过模拟验证了其发现。

Abstract: A combinatorial intervention, consisting of multiple treatments applied to a
single unit with potentially interactive effects, has substantial applications
in fields such as biomedicine, engineering, and beyond. Given $p$ possible
treatments, conducting all possible $2^p$ combinatorial interventions can be
laborious and quickly becomes infeasible as $p$ increases. Here we introduce
probabilistic factorial experimental design, formalized from how scientists
perform lab experiments. In this framework, the experimenter selects a dosage
for each possible treatment and applies it to a group of units. Each unit
independently receives a random combination of treatments, sampled from a
product Bernoulli distribution determined by the dosages. Additionally, the
experimenter can carry out such experiments over multiple rounds, adapting the
design in an active manner. We address the optimal experimental design problem
within an intervention model that imposes bounded-degree interactions between
treatments. In the passive setting, we provide a closed-form solution for the
near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each
treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating
any $k$-way interaction model, regardless of $k$, and imply that
$O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate
this model. For the multi-round setting, we provide a near-optimal acquisition
function that can be numerically optimized. We also explore several extensions
of the design problem and finally validate our findings through simulations.

</details>


### [83] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
*Leonid Ryvkin*

Main category: cs.LG

TL;DR: The paper surveys the varied capabilities of UHATs in recognizing formal languages, highlighting differences based on attention mechanisms and establishing theoretical bounds.


<details>
  <summary>Details</summary>
Motivation: To explore and summarize the capabilities of unique hard attention transformers encoders (UHATs) in recognizing formal languages and the influence of different attention mechanisms.

Method: The survey compares and contrasts various configurations of UHATs and derives their capabilities in recognizing formal languages using theoretical bounds from first-order logic and circuit complexity.

Result: The paper establishes a lower bound for UHATs in relation to first-order logic and an upper bound in terms of circuit complexity, detailing their performance and limitations.

Conclusion: UHATs have specific capabilities in recognizing formal languages, which vary based on different configurations like masked vs. non-masked settings and different types of attention score functions.

Abstract: This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [84] [Product Quantization for Surface Soil Similarity](https://arxiv.org/abs/2506.03374)
*Haley Dozier,Althea Henslee,Ashley Abraham,Andrew Strelzoff,Mark Chappell*

Main category: cs.LG

TL;DR: 研究使用机器学习技术，提出了一种新方法来提高土壤分类的精度和灵活性，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的人为土壤分类方法依赖于历史数据理解，局限性较大。研究领域需要一种更数据驱动的方法来提高分类的特异性和准确性。

Method: 使用机器学习技术，并结合产品量化与系统参数和输出评估，创建高维数据集的土壤分类方法。

Result: 通过机器学习管道，能够创建既准确又灵活的土壤分类体系，适用于特定应用需求。

Conclusion: 研究提出了一种能够克服传统土壤分类方法局限性的机器学习方法，可实现更高特异性和灵活性的分类。

Abstract: The use of machine learning (ML) techniques has allowed rapid advancements in
many scientific and engineering fields. One of these problems is that of
surface soil taxonomy, a research area previously hindered by the reliance on
human-derived classifications, which are mostly dependent on dividing a dataset
based on historical understandings of that data rather than data-driven,
statistically observable similarities. Using a ML-based taxonomy allows soil
researchers to move beyond the limitations of human visualization and create
classifications of high-dimension datasets with a much higher level of
specificity than possible with hand-drawn taxonomies. Furthermore, this
pipeline allows for the possibility of producing both highly accurate and
flexible soil taxonomies with classes built to fit a specific application. The
machine learning pipeline outlined in this work combines product quantization
with the systematic evaluation of parameters and output to get the best
available results, rather than accepting sub-optimal results by using either
default settings or best guess settings.

</details>


### [85] [Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons](https://arxiv.org/abs/2506.03392)
*Aref Ghoreishee,Abhishek Mishra,John Walsh,Anup Das,Nagarajan Kandasamy*

Main category: cs.LG

TL;DR: 提出一种新的三元脉冲神经元模型，以改善深度Q学习中二进制神经元的表现，并在多个游戏中取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 为了提高在深度Q学习中二进制脉冲神经元的表示能力，提出了一种新的三元脉冲神经元模型。虽然最近引入了三元神经元模型以克服二进制脉冲神经元的有限表示能力，但其性能在深度Q学习任务中却较差。本文通过数学和经验分析，假设训练过程中梯度估计偏差是其潜在原因。

Method: 提出了一种新的三元脉冲神经元模型，旨在通过减少估计偏差来改善表现。将该三元脉冲神经元作为深度脉冲Q学习网络（DSQN）的基本计算单元，并在Gym环境中的七个Atari游戏中评估网络性能。

Result: 结果表明，新提出的三元脉冲神经元减轻了在Q学习任务中三元神经元的显著性能下降，并与现有二进制神经元相比提高了网络性能，使DSQN成为机载自主决策任务的更实用解决方案。

Conclusion: 本文提出的三元脉冲神经元模型比现有的二进制神经元在深度Q学习任务中表现更好，为机载自主决策任务提供了更实用的解决方案。

Abstract: We propose a new ternary spiking neuron model to improve the representation
capacity of binary spiking neurons in deep Q-learning. Although a ternary
neuron model has recently been introduced to overcome the limited
representation capacity offered by the binary spiking neurons, we show that its
performance is worse than that of binary models in deep Q-learning tasks. We
hypothesize gradient estimation bias during the training process as the
underlying potential cause through mathematical and empirical analysis. We
propose a novel ternary spiking neuron model to mitigate this issue by reducing
the estimation bias. We use the proposed ternary spiking neuron as the
fundamental computing unit in a deep spiking Q-learning network (DSQN) and
evaluate the network's performance in seven Atari games from the Gym
environment. Results show that the proposed ternary spiking neuron mitigates
the drastic performance degradation of ternary neurons in Q-learning tasks and
improves the network performance compared to the existing binary neurons,
making DSQN a more practical solution for on-board autonomous decision-making
tasks.

</details>


### [86] [The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks](https://arxiv.org/abs/2506.03404)
*Walter Mayor,Johan Obando-Ceron,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 并行角色数据收集方法在提升强化学习算法性能上有效，扩展并行环境优于增加展期长度。


<details>
  <summary>Details</summary>
Motivation: 研究数据收集方式对偏差-方差权衡及样本效率与过拟合之间平衡的影响。

Method: 对基于并行角色的数据收集进行实证分析，探讨其与网络可塑性及优化稳定性间的联系。

Result: 较大的数据集规模可在多种设定中提高最终性能，扩展并行环境比增加展期长度更有效。

Conclusion: 数据收集策略在提高代理性能中扮演了关键角色，扩展并行环境比增加展期长度更为有效。

Abstract: The use of parallel actors for data collection has been an effective
technique used in reinforcement learning (RL) algorithms. The manner in which
data is collected in these algorithms, controlled via the number of parallel
environments and the rollout length, induces a form of bias-variance trade-off;
the number of training passes over the collected data, on the other hand, must
strike a balance between sample efficiency and overfitting. We conduct an
empirical analysis of these trade-offs on PPO, one of the most popular RL
algorithms that uses parallel actors, and establish connections to network
plasticity and, more generally, optimization stability. We examine its impact
on network architectures, as well as the hyper-parameter sensitivity when
scaling data. Our analyses indicate that larger dataset sizes can increase
final performance across a variety of settings, and that scaling parallel
environments is more effective than increasing rollout lengths. These findings
highlight the critical role of data collection strategies in improving agent
performance.

</details>


### [87] [A Machine Learning Theory Perspective on Strategic Litigation](https://arxiv.org/abs/2506.03411)
*Melissa Dutz,Han Shao,Avrim Blum,Aloni Cohen*

Main category: cs.LG

TL;DR: 该论文探讨了战略诉讼如何影响法律系统中的决策规则，以及战略诉讼者应如何选择诉讼案件以最大化影响。


<details>
  <summary>Details</summary>
Motivation: 为了研究机器学习理论视角下的战略诉讼，分析战略诉讼如何通过影响高等法院的决策规则来影响未来的案件。

Method: 构建抽象模型，模拟普通法法律系统中高等法院的决策过程，分析战略诉讼者通过选择性提起案件影响决策规则的能力。

Result: 探讨战略诉讼者对高等法院决策规则的影响，哪些案件应提起，及战略诉讼者在明知败诉时提起诉讼的合理性。

Conclusion: 战略诉讼在普通法系统中具有显著的影响力，可以通过选择性提起和处理案件来塑造法律规则导向。

Abstract: Strategic litigation involves bringing a legal case to court with the goal of
having a broader impact beyond resolving the case itself: for example, creating
precedent which will influence future rulings. In this paper, we explore
strategic litigation from the perspective of machine learning theory. We
consider an abstract model of a common-law legal system where a lower court
decides new cases by applying a decision rule learned from a higher court's
past rulings. In this model, we explore the power of a strategic litigator, who
strategically brings cases to the higher court to influence the learned
decision rule, thereby affecting future cases. We explore questions including:
What impact can a strategic litigator have? Which cases should a strategic
litigator bring to court? Does it ever make sense for a strategic litigator to
bring a case when they are sure the court will rule against them?

</details>


### [88] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
*Joonseong Kang,Soojeong Lee,Subeen Park,Sumin Park,Taero Kim,Jihee Kim,Ryunyi Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: ATV解决了ICL的局限，通过动态生成任务向量，提升了模型的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了改善ICL在演示顺序敏感性、上下文长度限制和计算效率低下等问题，提出了自适应任务向量（ATV）框架。

Method: ATV使用一个小型语言模型来生成任务向量，这些向量被转换以匹配目标大规模语言模型的结构，并用于指导模型的输出生成。

Result: ATV展示了强大的性能和泛化能力，即使在未见过的任务上，同时理论分析表明ATV与LoRA具有相同的表现能力，且比Prefix-Tuning更具表现力。

Conclusion: ATV框架通过动态生成与每个输入查询相适应的任务向量，提高了大规模语言模型在看不见任务上的表现和泛化能力。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [89] [Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection](https://arxiv.org/abs/2506.03162)
*Damith Chamalke Senadeera,Xiaoyun Yang,Dimitrios Kollias,Gregory Slabaugh*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dual Branch VideoMamba的高效架构，能够在新基准上实现最先进的性能，表明了SSM在实时监控暴力检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 监控摄像头的快速普及增加了对自动暴力检测的需求。虽然卷积神经网络（CNN）和Transformer在提取时空特征方面取得了成功，但它们在处理长期依赖关系和计算效率方面存在问题。

Method: 我们提出了一种名为Dual Branch VideoMamba的高效架构，结合了双分支设计和状态空间模型（SSM）主干网络，其中一个分支捕捉空间特征，而另一个分支专注于时间动态，通过门控机制进行连续融合。

Result: 我们通过合并RWF-2000、RLVS和VioPeru数据集提出了视频暴力检测的新基准，确保训练和测试集之间严格分离。我们的模型在此基准测试上实现了最先进的性能。

Conclusion: 我们的模型在新的基准测试上实现了最先进的性能，在准确性和计算效率之间提供了最佳平衡，证明了SSM在可扩展、实时监控暴力检测中的前景。

Abstract: The rapid proliferation of surveillance cameras has increased the demand for
automated violence detection. While CNNs and Transformers have shown success in
extracting spatio-temporal features, they struggle with long-term dependencies
and computational efficiency. We propose Dual Branch VideoMamba with Gated
Class Token Fusion (GCTF), an efficient architecture combining a dual-branch
design and a state-space model (SSM) backbone where one branch captures spatial
features, while the other focuses on temporal dynamics, with continuous fusion
via a gating mechanism. We also present a new benchmark by merging RWF-2000,
RLVS, and VioPeru datasets in video violence detection, ensuring strict
separation between training and testing sets. Our model achieves
state-of-the-art performance on this benchmark offering an optimal balance
between accuracy and computational efficiency, demonstrating the promise of
SSMs for scalable, real-time surveillance violence detection.

</details>


### [90] [Improvement of human health lifespan with hybrid group pose estimation methods](https://arxiv.org/abs/2506.03169)
*Arindam Chaudhuri*

Main category: cs.CV

TL;DR: 本文提出了一种基于混合集成的群组姿态估计方法，可增强多人姿态检测的准确性和稳健性，尤其是在遮挡的情况下，并在实时场景中有效应用于改善人体健康。


<details>
  <summary>Details</summary>
Motivation: 人体姿态估计方法利用计算机视觉的进步来跟踪人体在真实生活应用中的动作，这源于通过可用设备录制的视频。姿态估计方法的消费者认为人体姿态内容有助于补充可用的视频。这增加了姿态估计软件对人体姿态的估计使用。为了应对这一问题，我们开发了一种基于混合集成的群组姿态估计方法以改善人体健康。

Method: 我们开发了一种基于混合集成的群组姿态估计方法，以改进人体健康。这种混合集成的群组姿态估计方法旨在使用修改后的群组姿态估计和修改后的实时姿态估计检测多人姿态。集成允许融合所述方法在实时中的性能。图像中的输入姿态被输入到各个方法中。姿态变换方法有助于识别相关特征，以便进行有效的集成训练。经过这一过程，自定义的预训练混合集成在公用基准数据集上进行了训练，并通过测试数据集进行评估。

Result: 该方法在实时姿态估计中提供了最佳优化结果，使姿态估计方法对遮挡更加稳健，并提高了密集回归的准确性。

Conclusion: 通过与现有组姿态估计方法进行比较分析，并在基准数据集上进行实验，验证了所提方法的有效性和可行性，实现了实时姿态估计的最佳优化结果，使得姿态估计方法在遮挡情况下更加稳健，并提高了密集回归的准确性。这些结果证实了该方法在多个实时场景中应用的潜力，改善了人类健康寿命。

Abstract: Human beings rely heavily on estimation of poses in order to access their
body movements. Human pose estimation methods take advantage of computer vision
advances in order to track human body movements in real life applications. This
comes from videos which are recorded through available devices. These
para-digms provide potential to make human movement measurement more accessible
to users. The consumers of pose estimation movements believe that human poses
content tend to supplement available videos. This has increased pose estimation
software usage to estimate human poses. In order to address this problem, we
develop hybrid-ensemble-based group pose estimation method to improve human
health. This proposed hybrid-ensemble-based group pose estimation method aims
to detect multi-person poses using modified group pose estimation and modified
real time pose estimation. This ensemble allows fusion of performance of stated
methods in real time. The input poses from images are fed into individual
meth-ods. The pose transformation method helps to identify relevant features
for en-semble to perform training effectively. After this, customized
pre-trained hybrid ensemble is trained on public benchmarked datasets which is
being evaluated through test datasets. The effectiveness and viability of
proposed method is estab-lished based on comparative analysis of group pose
estimation methods and ex-periments conducted on benchmarked datasets. It
provides best optimized results in real-time pose estimation. It makes pose
estimation method more robust to oc-clusion and improves dense regression
accuracy. These results have affirmed po-tential application of this method in
several real-time situations with improvement in human health life span

</details>


### [91] [PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03170)
*Murthy L,Subarna Tripathi*

Main category: cs.CV

TL;DR: 该研究利用循环纠错码提出了一种用于文本到图像扩散模型的神经指纹标记的方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成模型由于其开放源代码发展，可能被恶意使用，因此需要有效的风险规避策略。

Method: 该研究提出了利用循环纠错码的概念来实现对文本到图像扩散模型的神经指纹标记的方法。

Result: 研究中提出的方法旨在提高神经指纹标记的准确性以实现100%归属准确性。

Conclusion: 现有方法未能实现完全归属准确性，而低于完美准确性的模型实际上不可部署。

Abstract: The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
$100\%$ attribution accuracy. However, any model with less than \emph{perfect}
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.

</details>


### [92] [EdgeVidSum: Real-Time Personalized Video Summarization at the Edge](https://arxiv.org/abs/2506.03171)
*Ghulam Mujtaba,Eun-Seok Ryu*

Main category: cs.CV

TL;DR: EdgeVidSum generates personalized video summaries using thumbnails and operates efficiently on edge devices, ensuring privacy and performance.


<details>
  <summary>Details</summary>
Motivation: To provide a fast, personalized video summarization method for edge devices that preserves user privacy and reduces computational load.

Method: EdgeVidSum uses thumbnail containers to reduce computational complexity through a lightweight 2D CNN model, which identifies user-preferred content from thumbnails and generates timestamps for summaries.

Result: The method successfully generates user-tailored video summaries in real-time on resource-constrained devices like Jetson Nano.

Conclusion: EdgeVidSum is effective in creating personalized video summaries directly on edge devices, balancing computational efficiency, personalization, and privacy.

Abstract: EdgeVidSum is a lightweight method that generates personalized, fast-forward
summaries of long-form videos directly on edge devices. The proposed approach
enables real-time video summarization while safeguarding user privacy through
local data processing using innovative thumbnail-based techniques and efficient
neural architectures. Unlike conventional methods that process entire videos
frame by frame, the proposed method uses thumbnail containers to significantly
reduce computational complexity without sacrificing semantic relevance. The
framework employs a hierarchical analysis approach, where a lightweight 2D CNN
model identifies user-preferred content from thumbnails and generates
timestamps to create fast-forward summaries. Our interactive demo highlights
the system's ability to create tailored video summaries for long-form videos,
such as movies, sports events, and TV shows, based on individual user
preferences. The entire computation occurs seamlessly on resource-constrained
devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical
challenges of computational efficiency, personalization, and privacy in modern
video consumption environments.

</details>


### [93] [FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution](https://arxiv.org/abs/2506.03173)
*Xiaoyi Liu,Hao Tang*

Main category: cs.CV

TL;DR: FOLIAGE是一种物理信息化的多模态世界模型，在表面生长模拟上表现优异，超越了各种专业基准，开辟了新的物理智能途径。


<details>
  <summary>Details</summary>
Motivation: 为了提升下一代世界模型的物理智能，通过部分的多感知观测来预判并塑造世界，对表面生长进行物理信息化的多模态建模是必要的。

Method: 采用物理信息的多模态世界模型，结合映像、网格连接和点云，通过物理感知预测器和累加图网络捕捉动态连接、几何对应融合、交叉补丁屏蔽等技术，实现精细的表面生长模拟。

Result: FOLIAGE在物理智能评估套件中的六项核心任务和四项压力测试表现出色，超过了各类专业基线。

Conclusion: FOLIAGE在动态环境中表现优异，在多模态物理智能领域设立了新的标准。

Abstract: Physical intelligence -- anticipating and shaping the world from partial,
multisensory observations -- is critical for next-generation world models. We
propose FOLIAGE, a physics-informed multimodal world model for unbounded
accretive surface growth. In its Action-Perception loop, a unified context
encoder maps images, mesh connectivity, and point clouds to a shared latent
state. A physics-aware predictor, conditioned on physical control actions,
advances this latent state in time to align with the target latent of the
surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces
with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network
(AGN) captures dynamic connectivity through Age Positional Encoding and
Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch
Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances
global context with local dynamics. We create SURF-GARDEN, a world model
learning platform comprising a Counterfactual Physics Simulator, a Multimodal
Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse
surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation
suite, evaluates six core tasks -- topology recognition, inverse material
estimation, growth-stage classification, latent roll-out, cross-modal
retrieval, and dense correspondence -- and four stress tests -- sensor dropout,
zero-shot modality transfer, long-horizon prediction, and physics ablation --
to probe resilience. FOLIAGE outperforms specialized baselines while remaining
robust across dynamic environments, establishing a new world-model based,
multimodal pathway to physical intelligence.

</details>


### [94] [Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks](https://arxiv.org/abs/2506.03174)
*Koki Matsuishi,Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: AURA-MFM模型通过整合多种模态，实现了对人类活动的详细分析，在各种任务中表现优于现有方法，尤其在零样本分类中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型在提供全身人类活动的详细分析方面存在不足，因此提出了AURA-MFM模型来解决这一问题。

Method: 在AURA-MFM模型中，整合了四种模态：第三人称视频、运动捕捉、IMU和文本，并使用基于Transformer的IMU编码器提高模型性能。

Result: 在零样本分类的动作识别任务中，AURA-MFM模型的F1分数达到0.6226，准确率达到0.7320，而现有方法的F1分数仅为0.0747，准确率为0.1961。

Conclusion: AURA-MFM模型在行为分析中引入了第三人称视频、运动捕捉、IMU和文本四种模态，从而实现了对人类活动的更加详细和多维度的理解。实验结果表明，该模型在检索和活动识别任务上优于现有方法，尤其是在零样本分类的动作识别中表现突出。

Abstract: In recent years, the widespread adoption of wearable devices has highlighted
the growing importance of behavior analysis using IMU. While applications span
diverse fields such as healthcare and robotics, recent studies have
increasingly focused on multimodal analysis, in addition to unimodal analysis.
Several studies have proposed multimodal foundation models that incorporate
first-person video and text data; however, these models still fall short in
providing a detailed analysis of full-body human activity. To address this
limitation, we propose Activity Understanding and Representations Alignment -
Multimodal Foundation Model (AURA-MFM), a foundational model integrating four
modalities: third-person video, motion capture, IMU, and text. By incorporating
third-person video and motion capture data, the model enables a detailed and
multidimensional understanding of human activity, which first-person
perspectives alone fail to capture. Additionally, a Transformer-based IMU
encoder is employed to enhance the model's overall performance. Experimental
evaluations on retrieval and activity recognition tasks demonstrate that our
model surpasses existing methods. Notably, in the zero-shot classification for
action recognition, our method achieved significantly higher performance, with
an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method
recorded an F1-score of 0.0747 and an accuracy of 0.1961.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [95] [From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context](https://arxiv.org/abs/2506.03546)
*Yuanchen Bai,Zijian Ding,Angelique Taylor*

Main category: cs.RO

TL;DR: 分析了生成模型在多代理系统中的应用局限性，并提出设计指南以提高机器人团队的实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型的进步使多代理系统能够执行复杂的虚拟任务，但这些任务并不能很好地推广到物理多代理机器人团队中。

Method: 通过在模拟的急诊科入职场景中重新配置和压力测试基于CrewAI框架的分层多代理机器人团队，分析其在实际应用中的表现。

Result: 我们识别了五种持久的失败模式，包括角色不匹配、工具访问违规、未及时处理故障报告、不遵循预定工作流程以及规避或虚假报告任务完成。

Conclusion: 我们提出了三项设计指南，强调过程透明性、主动故障恢复和上下文接地，以开发更具弹性和鲁棒性的多代理机器人系统。

Abstract: Advancements in generative models have enabled multi-agent systems (MAS) to
perform complex virtual tasks such as writing and code generation, which do not
generalize well to physical multi-agent robotic teams. Current frameworks often
treat agents as conceptual task executors rather than physically embodied
entities, and overlook critical real-world constraints such as spatial context,
robotic capabilities (e.g., sensing and navigation). To probe this gap, we
reconfigure and stress-test a hierarchical multi-agent robotic team built on
the CrewAI framework in a simulated emergency department onboarding scenario.
We identify five persistent failure modes: role misalignment; tool access
violations; lack of in-time handling of failure reports; noncompliance with
prescribed workflows; bypassing or false reporting of task completion. Based on
this analysis, we propose three design guidelines emphasizing process
transparency, proactive failure recovery, and contextual grounding. Our work
informs the development of more resilient and robust multi-agent robotic
systems (MARS), including opportunities to extend virtual multi-agent
frameworks to the real world.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [96] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: 一个高效的深度学习系统用于乳腺癌筛查，在多个临床数据集上取得了卓越的检测和定位性能，展示其在实际临床环境中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个可以帮助放射科医生更有效地检测和定位乳腺癌病灶的系统。

Method: 其使用了一种改进的EfficientNetV2架构与增强的注意力机制来实现，训练和验证包括来自不同医院和数据集的乳腺X光图像。

Result: 模型在不同数据集上的AUROC分别达到了0.89、0.96和0.94，病灶定位能力表现良好，与放射科医生的判定高度一致，并受到临床的普遍接受。

Conclusion: 该研究的深度学习系统在乳腺癌检测中的表现出色，可能改善临床筛查流程。

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [97] [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/abs/2506.03178)
*Md. Zihad Bin Jahangir,Muhammad Ashad Kabir,Sumaiya Akter,Israt Jahan,Minh Chau*

Main category: eess.IV

TL;DR: LLaMA-XR框架结合LLaMA 3.1、DenseNet-121和QLoRA，实现自动放射报告生成，表现优于多种最新方法。


<details>
  <summary>Details</summary>
Motivation: 减轻放射科医生的工作量，提高诊断准确性，但生成精确且具临床意义的报告具有挑战，需解决医学语言复杂性和上下文理解的问题。

Method: 提出LLaMA-XR框架，集成LLaMA 3.1、基于DenseNet-121的图像嵌入和QLoRA微调，优化策略提高参数利用率并降低内存开销。

Result: LLaMA-XR在IU X-ray基准数据集上表现出色，ROUGE-L得分为0.433，METEOR得分为0.336，超过多种最新方法并树立了新的性能基准。

Conclusion: LLaMA-XR在自动放射学报告生成领域表现优异，具有增强的临床实用性和可靠性。

Abstract: Automated radiology report generation holds significant potential to reduce
radiologists' workload and enhance diagnostic accuracy. However, generating
precise and clinically meaningful reports from chest radiographs remains
challenging due to the complexity of medical language and the need for
contextual understanding. Existing models often struggle with maintaining both
accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel
framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings
and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves
improved coherence and clinical accuracy while maintaining computational
efficiency. This efficiency is driven by an optimization strategy that enhances
parameter utilization and reduces memory overhead, enabling faster report
generation with lower computational resource demands. Extensive experiments
conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR
outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L
score of 0.433 and a METEOR score of 0.336, establishing new performance
benchmarks in the domain. These results underscore LLaMA-XR's potential as an
effective and efficient AI system for automated radiology reporting, offering
enhanced clinical utility and reliability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [98] [From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation](https://arxiv.org/abs/2506.03801)
*Peter Pfeiffer,Alexander Rombach,Maxim Majlatow,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 本文探讨了LLM在流程建模、预测和自动化中的四个实际应用案例，强调通过人机协作和领域特定需求来实现其在BPM中的整合。


<details>
  <summary>Details</summary>
Motivation: 传统的业务流程管理（BPM）在动态环境中存在刚性、不透明和可扩展性问题，而新兴的大型语言模型（LLM）则提供了变革性机会和风险，促使本文研究LLM如何在BPM中提供解决方案。

Method: 本文采用了结合工业伙伴的早期研究项目的方法，结合人机协作，在制造、建模、生命科学和设计过程中展示LLM的应用。

Result: 研究发现，LLM可以通过将不确定性感知的可解释机器学习与交互式对话结合，实现制造流程中的透明化，并在流程建模中通过对话界面民主化BPMN设计。药物警戒代理通过知识图谱增强的LLM自动化药物安全监控，而可持续纺织设计利用多代理系统平衡法规与环境的权衡。

Conclusion: 本文通过四个实际案例展示了大型语言模型（LLM）如何通过可信的流程智能重新定义流程建模、预测和自动化，并提供了在各种行业中应用的可操作性洞见。

Abstract: Traditional Business Process Management (BPM) struggles with rigidity,
opacity, and scalability in dynamic environments while emerging Large Language
Models (LLMs) present transformative opportunities alongside risks. This paper
explores four real-world use cases that demonstrate how LLMs, augmented with
trustworthy process intelligence, redefine process modeling, prediction, and
automation. Grounded in early-stage research projects with industrial partners,
the work spans manufacturing, modeling, life-science, and design processes,
addressing domain-specific challenges through human-AI collaboration. In
manufacturing, an LLM-driven framework integrates uncertainty-aware explainable
Machine Learning (ML) with interactive dialogues, transforming opaque
predictions into auditable workflows. For process modeling, conversational
interfaces democratize BPMN design. Pharmacovigilance agents automate drug
safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable
textile design employs multi-agent systems to navigate regulatory and
environmental trade-offs. We intend to examine tensions between transparency
and efficiency, generalization and specialization, and human agency versus
automation. By mapping these trade-offs, we advocate for context-sensitive
integration prioritizing domain needs, stakeholder values, and iterative
human-in-the-loop workflows over universal solutions. This work provides
actionable insights for researchers and practitioners aiming to operationalize
LLMs in critical BPM environments.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [99] [Spatio-spectral light-by-light moulding in multimode fibre](https://arxiv.org/abs/2506.03739)
*Yago Arosa,Tigran Mansuryan,Arnaud Poisson,Wasyhun Asefa Gemechu,Katarzyna Krupa,Mario Ferraro,Fabio Mangini,Benjamin Wetzel,Stefan Wabnitz,Alessandro Tonello,Vincent Couderc*

Main category: physics.optics

TL;DR: 利用多模光纤中的非线性相互作用，通过光与光的控制实现对复杂光波行为的操控。


<details>
  <summary>Details</summary>
Motivation: 控制复杂光波以实现所需行为或特性是一个重大挑战，尤其在操控斑点光束时更加困难。

Method: 通过多模渐变折射率光纤中的弱斑点二次谐波信号与高功率共传播基波泵波的保守相互作用进行操控。

Result: 通过调节泵波功率或模式功率分布，可以增强或降级信号的空间质量，实现了光学模式转换，其相位匹配可通过泵束的模式功率分布进行控制。

Conclusion: 通过物质非线性在多模导波结构中操控复杂光，实现了新可能性。光与光的相互控制引发了增强或部分抑制可见拉曼Stokes级联现象。

Abstract: Controlling complex light waves to achieve desired behaviours or
characteristics on demand presents a significant challenge. This task becomes
even more complicated when manipulating speckled light beams owing to their
inherently fuzzy intensity and phase structures. Here, we demonstrate that a
weak speckled second-harmonic signal in a multimode graded-index fibre can be
manipulated via its conservative interaction with a high-power co-propagating
fundamental pump wave. Specifically, the spatial quality of the signal can be
either enhanced or degraded by varying the pump's power or its modal power
distribution. The underlying physical mechanism is the optically induced mode
conversion, whose phase-matching can be controlled by the mode power
distribution of the pump beam. This phenomenon enables new possibilities for
manipulating complex light via material nonlinearities in multimode guiding
structures. A striking example of this novel light-by-light control is the
experimentally observed enhancement or partial suppression of the visible Raman
Stokes cascade regulated by the second harmonic beam, while modulated by the
mode power distribution of the fundamental beam.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [100] [The size of the sync basin resolved](https://arxiv.org/abs/2506.03419)
*Pablo Groisman,Cecilia De Vita,Julián Fernández Bonder,Yuanzhao Zhang*

Main category: math-ph

TL;DR: 研究稀疏耦合Kuramoto振荡器的扭曲态基异极，支持其规模随绕数的高斯缩放猜想，并揭示背后的动力学机制。


<details>
  <summary>Details</summary>
Motivation: 探讨高维吸引盆因其简单而多稳定的动力学特性的稀疏耦合Kuramoto振荡器的行为，并验证关于扭曲态基异极规模的先前猜想。

Method: 使用了数值和解析方法，通过模型化解的绕数为一组弱相关变量之和，使用中心极限定理导出其基异极规模。

Result: 提供新的数值和分析证据支持猜想，并揭示了高斯缩放背后的动力学机制。

Conclusion: 得出结论是，当从随机初始条件开始时，解的绕数迅速稳定，从而支持关于扭曲态基异极规模呈高斯缩放的猜想。

Abstract: Sparsely coupled Kuramoto oscillators offer a fertile playground for
exploring high-dimensional basins of attraction due to their simple yet
multistable dynamics. For $n$ identical Kuramoto oscillators on cycle graphs,
it is well known that the only attractors are twisted states, whose phases wind
around the circle with a constant gap between neighboring oscillators
($\theta_j = 2\pi q j/n$). It was conjectured in 2006 that basin sizes of these
twisted states scale as $e^{-kq^2}$ to the winding number $q$. Here, we provide
new numerical and analytical evidence supporting the conjecture and uncover the
dynamical mechanism behind the Gaussian scaling. The key idea is that, when
starting with a random initial condition, the winding number of the solution
stabilizes rapidly at $t \propto \log n$, before long-range correlation can
develop among oscillators. This timescale separation allows us to model the
winding number as a sum of weakly dependent variables, leading to a Central
Limit Theorem derivation of the basin scaling.

</details>
