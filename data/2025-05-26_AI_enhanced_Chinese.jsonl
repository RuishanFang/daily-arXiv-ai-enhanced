{"id": "2505.17037", "pdf": "https://arxiv.org/pdf/2505.17037", "abs": "https://arxiv.org/abs/2505.17037", "authors": ["Dimitri Schreiter"], "title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has emerged as a critical component in optimizing large\nlanguage models (LLMs) for domain-specific tasks. However, the role of prompt\nspecificity, especially in domains like STEM (physics, chemistry, biology,\ncomputer science and mathematics), medicine, and law, remains underexplored.\nThis thesis addresses the problem of whether increasing the specificity of\nvocabulary in prompts improves LLM performance in domain-specific\nquestion-answering and reasoning tasks. We developed a synonymization framework\nto systematically substitute nouns, verbs, and adjectives with varying\nspecificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,\nGranite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in\nSTEM, law, and medicine. Our results reveal that while generally increasing the\nspecificity of prompts does not have a significant impact, there appears to be\na specificity range, across all considered models, where the LLM performs the\nbest. Identifying this optimal specificity range offers a key insight for\nprompt design, suggesting that manipulating prompts within this range could\nmaximize LLM performance and lead to more efficient applications in specialized\ndomains.", "AI": {"tldr": "\u589e\u52a0\u63d0\u793a\u8bcd\u5177\u4f53\u6027\u5bf9LLM\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u8303\u56f4\uff0c\u53ef\u4f18\u5316LLM\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u5728STEM\u3001\u533b\u5b66\u548c\u6cd5\u5f8b\u7b49\u9886\u57df\u4e2d\uff0c\u589e\u52a0\u63d0\u793a\u8bcd\u6c47\u7684\u5177\u4f53\u6027\u662f\u5426\u80fd\u591f\u63d0\u9ad8LLM\u5728\u7279\u5b9a\u9886\u57df\u7684\u95ee\u9898\u89e3\u7b54\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540c\u4e49\u8bcd\u66ff\u6362\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u66ff\u6362\u540d\u8bcd\u3001\u52a8\u8bcd\u548c\u5f62\u5bb9\u8bcd\uff0c\u5e76\u6d4b\u91cf\u5176\u5bf9\u56db\u4e2aLLM\uff08Llama-3.1-70B-Instruct\u3001Granite-13B-Instruct-V2\u3001Flan-T5-XL\u548cMistral-Large 2\uff09\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u4e00\u822c\u6765\u8bf4\u63d0\u5347\u63d0\u793a\u5177\u4f53\u6027\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u6709\u4e00\u4e2a\u5177\u4f53\u6027\u8303\u56f4\u53ef\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u8bc6\u522b\u6700\u4f18\u5177\u4f53\u6027\u8303\u56f4\u5bf9\u63d0\u793a\u8bbe\u8ba1\u6709\u91cd\u8981\u610f\u4e49\uff0c\u80fd\u4f18\u5316LLM\u6027\u80fd\u5e76\u63d0\u5347\u5728\u7279\u5b9a\u9886\u57df\u7684\u5e94\u7528\u6548\u7387\u3002", "conclusion": "\u589e\u52a0\u63d0\u793a\u8bcd\u6c47\u7684\u5177\u4f53\u6027\u5bf9LLM\u6027\u80fd\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u5177\u4f53\u6027\u8303\u56f4\u3002\u5728\u8fd9\u4e00\u8303\u56f4\u5185\uff0cLLM\u8868\u73b0\u6700\u597d\u3002\u8bc6\u522b\u8fd9\u4e00\u8303\u56f4\u5bf9\u63d0\u793a\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u4ee5\u4f18\u5316\u7279\u5b9a\u9886\u57df\u7684LLM\u5e94\u7528\u3002"}}
{"id": "2505.17038", "pdf": "https://arxiv.org/pdf/2505.17038", "abs": "https://arxiv.org/abs/2505.17038", "authors": ["Xian Gong", "Paul X. McCarthy", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Massive and diverse web data are increasingly vital for government disaster\nresponse, as demonstrated by the 2022 floods in New South Wales (NSW),\nAustralia. This study examines how X (formerly Twitter) and public inquiry\nsubmissions provide insights into public behaviour during crises. We analyse\nmore than 55,000 flood-related tweets and 1,450 submissions to identify\nbehavioural patterns during extreme weather events. While social media posts\nare short and fragmented, inquiry submissions are detailed, multi-page\ndocuments offering structured insights. Our methodology integrates Latent\nDirichlet Allocation (LDA) for topic modelling with Large Language Models\n(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and\ngeographic patterns, while LLMs improve filtering by identifying flood-relevant\ntweets using public submissions as a reference. This Relevance Index method\nreduces noise and prioritizes actionable content, improving situational\nawareness for emergency responders. By combining these complementary data\nstreams, our approach introduces a novel AI-driven method to refine\ncrisis-related social media content, improve real-time disaster response, and\ninform long-term resilience planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u548c\u8c03\u67e5\u63d0\u4ea4\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6539\u8fdb\u707e\u96be\u54cd\u5e94\u3002", "motivation": "\u653f\u5e9c\u5728\u707e\u96be\u54cd\u5e94\u4e2d\u6108\u53d1\u4f9d\u8d56\u4e8e\u5e9e\u5927\u4e14\u591a\u6837\u5316\u7684\u7f51\u7edc\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u671f\u95f4\u7684\u516c\u4f17\u884c\u4e3a\u5206\u6790\uff0c\u8fd9\u57282022\u5e74\u6fb3\u5927\u5229\u4e9a\u65b0\u5357\u5a01\u5c14\u58eb\u5dde\uff08NSW\uff09\u7684\u6d2a\u6c34\u4e2d\u5c24\u4e3a\u4f53\u73b0\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e3b\u9898\u6a21\u578b\u7684\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u5e03\uff08LDA\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u3002\u4f7f\u7528\u516c\u5171\u8c03\u67e5\u63d0\u4ea4\u4f5c\u4e3a\u53c2\u8003\u6765\u8fc7\u6ee4\u63a8\u6587\uff0c\u63d0\u9ad8\u76f8\u5173\u6027\u6307\u6570\uff0c\u901a\u8fc7\u51cf\u5c11\u566a\u97f3\u548c\u4f18\u5148\u663e\u793a\u53ef\u64cd\u4f5c\u5185\u5bb9\u6765\u6539\u5584\u5e94\u6025\u54cd\u5e94\u7684\u60c5\u5883\u611f\u77e5\u3002", "result": "LDA\u63ed\u793a\u4e86\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u53cd\u6620\u7684\u4e0d\u540c\u610f\u89c1\u548c\u5730\u7406\u6a21\u5f0f\uff0c\u800cLLMs\u5219\u901a\u8fc7\u8bc6\u522b\u4e0e\u6d2a\u6c34\u76f8\u5173\u7684\u63a8\u6587\uff0c\u6539\u8fdb\u4e86\u8fc7\u6ee4\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5e94\u6025\u54cd\u5e94\u7684\u60c5\u5883\u611f\u77e5\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u65b9\u6cd5\uff0c\u7ed3\u5408\u793e\u4ea4\u5a92\u4f53\u548c\u516c\u5171\u8c03\u67e5\u6570\u636e\u6d41\uff0c\u80fd\u591f\u6539\u8fdb\u5371\u673a\u76f8\u5173\u5185\u5bb9\u7684\u7cbe\u70bc\uff0c\u63d0\u9ad8\u5b9e\u65f6\u707e\u5bb3\u54cd\u5e94\uff0c\u5e76\u4e3a\u957f\u671f\u97e7\u6027\u89c4\u5212\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2505.17039", "pdf": "https://arxiv.org/pdf/2505.17039", "abs": "https://arxiv.org/abs/2505.17039", "authors": ["Diego Bonatto"], "title": "A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes", "categories": ["cs.CL"], "comment": "46 pages, 8 figures, 1 table", "summary": "A data-driven quantitative approach was used to develop a novel\nclassification system for beer categories and styles. Sixty-two thousand one\nhundred twenty-one beer recipes were mined and analyzed, considering ingredient\nprofiles, fermentation parameters, and recipe vital statistics. Statistical\nanalyses combined with self-organizing maps (SOMs) identified four major\nsuperclusters that showed distinctive malt and hop usage patterns, style\ncharacteristics, and historical brewing traditions. Cold fermented styles\nshowed a conservative grain and hop composition, whereas hot fermented beers\nexhibited high heterogeneity, reflecting regional preferences and innovation.\nThis new taxonomy offers a reproducible and objective framework beyond\ntraditional sensory-based classifications, providing brewers, researchers, and\neducators with a scalable tool for recipe analysis and beer development. The\nfindings in this work provide an understanding of beer diversity and open\navenues for linking ingredient usage with fermentation profiles and flavor\noutcomes.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u67906\u4e07\u591a\u4efd\u5564\u9152\u914d\u65b9\uff0c\u4f7f\u7528\u81ea\u7ec4\u7ec7\u6620\u5c04\u6cd5\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u98ce\u683c\u95f4\u7684\u539f\u6599\u4f7f\u7528\u6a21\u5f0f\u548c\u5386\u53f2\u4f20\u7edf\uff0c\u4e3a\u5564\u9152\u5206\u6790\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u5de5\u5177\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u4ee5\u8d85\u8d8a\u4f20\u7edf\u7684\u57fa\u4e8e\u611f\u5b98\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5564\u9152\u914d\u65b9\u5206\u6790\u548c\u5f00\u53d1\u63d0\u4f9b\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u5206\u67906\u4e07\u591a\u4efd\u5564\u9152\u914d\u65b9\uff0c\u7ed3\u5408\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOMs\uff09\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u8d85\u7ea7\u7c07\u7fa4\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7684\u9ea6\u82bd\u548c\u5564\u9152\u82b1\u4f7f\u7528\u6a21\u5f0f\u3001\u98ce\u683c\u7279\u5f81\u548c\u5386\u53f2\u917f\u9020\u4f20\u7edf\u3002\u51b7\u53d1\u9175\u98ce\u683c\u5c55\u73b0\u4e86\u4fdd\u5b88\u7684\u7cae\u98df\u548c\u5564\u9152\u82b1\u7ec4\u6210\uff0c\u800c\u70ed\u53d1\u9175\u5564\u9152\u8868\u73b0\u51fa\u9ad8\u5ea6\u5f02\u8d28\u6027\uff0c\u53cd\u6620\u4e86\u5730\u533a\u7684\u504f\u597d\u548c\u521b\u65b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u53ef\u4ee5\u91cd\u73b0\u5e76\u5ba2\u89c2\u5730\u4e3a\u5564\u9152\u914d\u65b9\u5206\u6790\u548c\u5f00\u53d1\u63d0\u4f9b\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042", "abs": "https://arxiv.org/abs/2505.17042", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6848\u4e3b\u8981\u662f\u5355\u6a21\u6001\u7684\uff0c\u53ea\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\uff0c\u672a\u5229\u7528\u5f71\u50cf\u8d44\u6599\uff0c\u5e76\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u957f\u683c\u5f0f\u7684\u653e\u5c04\u5b66\u6570\u636e\u3002\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u653e\u5c04\u5b66\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u7efc\u5408\u5229\u7528\u6587\u672c\u548c\u5f71\u50cf\u6570\u636e\u3002", "result": "\u6211\u4eec\u7684\u591a\u6a21\u6001VLM\u6846\u67b6\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u751f\u6210\u4e86\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ee5\u5f80\u7684\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17064", "pdf": "https://arxiv.org/pdf/2505.17064", "abs": "https://arxiv.org/abs/2505.17064", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. In this work, we present a\nsystematic and reproducible methodology for evaluating how TTI systems depict\ndifferent historical periods. For this purpose, we introduce the HistVis\ndataset, a curated collection of 30,000 synthetic images generated by three\nstate-of-the-art diffusion models using carefully designed prompts depicting\nuniversal human activities across different historical periods. We evaluate\ngenerated imagery across three key aspects: (1) Implicit Stylistic\nAssociations: examining default visual styles associated with specific eras;\n(2) Historical Consistency: identifying anachronisms such as modern artifacts\nin pre-modern contexts; and (3) Demographic Representation: comparing generated\nracial and gender distributions against historically plausible baselines. Our\nfindings reveal systematic inaccuracies in historically themed generated\nimagery, as TTI models frequently stereotype past eras by incorporating\nunstated stylistic cues, introduce anachronisms, and fail to reflect plausible\ndemographic patterns. By offering a scalable methodology and benchmark for\nassessing historical representation in generated imagery, this work provides an\ninitial step toward building more historically accurate and culturally aligned\nTTI models.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7HistVis\u6570\u636e\u96c6\u8bc4\u4f30\u4e86TTI\u7cfb\u7edf\u5728\u63cf\u7ed8\u5386\u53f2\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u98ce\u683c\u3001\u65f6\u4ee3\u548c\u4eba\u53e3\u7edf\u8ba1\u4e0a\u5b58\u5728\u4e0d\u51c6\u786e\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5386\u53f2\u518d\u73b0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6587\u672c\u751f\u6210\u56fe\u50cf(TTI)\u6269\u6563\u6a21\u578b\u5728\u5185\u5bb9\u521b\u4f5c\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u6709\u5f71\u54cd\u529b\uff0c\u4eba\u4eec\u5bf9\u5176\u793e\u4f1a\u548c\u6587\u5316\u5f71\u54cd\u7684\u5173\u6ce8\u4e5f\u5728\u589e\u52a0\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u6b64\u524d\u7684\u7814\u7a76\u4e3b\u8981\u8003\u5bdf\u4e86\u4eba\u53e3\u548c\u6587\u5316\u504f\u89c1\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u51c6\u786e\u518d\u73b0\u5386\u53f2\u80cc\u666f\u65b9\u9762\u7684\u80fd\u529b\u5374\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u4e14\u53ef\u91cd\u73b0\u7684\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30TTI\u7cfb\u7edf\u5982\u4f55\u63cf\u7ed8\u4e0d\u540c\u5386\u53f2\u65f6\u671f\u3002\u4e3a\u6b64\u76ee\u7684\uff0c\u5f15\u5165\u4e86HistVis\u6570\u636e\u96c6\u2014\u2014\u4e00\u4e2a\u5305\u542b\u4e09\u79cd\u5148\u8fdb\u6269\u6563\u6a21\u578b\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u751f\u6210\u768430000\u5e45\u5408\u6210\u56fe\u50cf\u7684\u7cbe\u9009\u96c6\u5408\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u751f\u6210\u7684\u56fe\u50cf\u5728\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u7684\u8868\u73b0\uff1a\u9690\u542b\u98ce\u683c\u5173\u8054\u3001\u5386\u53f2\u4e00\u81f4\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u4ee3\u8868\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5386\u53f2\u4e3b\u9898\u7684\u751f\u6210\u56fe\u50cf\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u51c6\u786e\u6027\uff0cTTI\u6a21\u578b\u5e38\u901a\u8fc7\u672a\u7ecf\u58f0\u660e\u7684\u98ce\u683c\u7ebf\u7d22\u523b\u677f\u5316\u8fc7\u53bb\u7684\u65f6\u4ee3\uff0c\u5f15\u5165\u4e86\u73b0\u4ee3\u80cc\u666f\u4e0b\u4e0d\u5e94\u51fa\u73b0\u7684\u7269\u4f53\uff0c\u5e76\u672a\u80fd\u53cd\u6620\u51fa\u5408\u7406\u7684\u79cd\u65cf\u548c\u6027\u522b\u5206\u5e03\u3002", "conclusion": "\u901a\u8fc7\u8fd9\u9879\u5de5\u4f5c\uff0c\u7814\u7a76\u8005\u63ed\u793a\u51fa\u5f53\u524dTTI\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5386\u53f2\u4e3b\u9898\u56fe\u50cf\u65f6\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u4e0d\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u6a21\u578b\u5e38\u5e38\u4ee5\u672a\u7ecf\u58f0\u660e\u7684\u98ce\u683c\u7ebf\u7d22\u523b\u677f\u5316\u8fc7\u53bb\u7684\u65f6\u4ee3\uff0c\u5f15\u5165\u65f6\u4ee3\u9519\u7f6e\u73b0\u8c61\uff0c\u5e76\u672a\u80fd\u53cd\u6620\u51fa\u5408\u7406\u7684\u4eba\u53e3\u7edf\u8ba1\u6a21\u5f0f\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u5386\u53f2\u518d\u73b0\uff0c\u65e8\u5728\u63a8\u52a8\u6784\u5efa\u66f4\u5177\u5386\u53f2\u51c6\u786e\u6027\u548c\u6587\u5316\u5bf9\u9f50\u7684TTI\u6a21\u578b\u3002"}}
{"id": "2505.17043", "pdf": "https://arxiv.org/pdf/2505.17043", "abs": "https://arxiv.org/abs/2505.17043", "authors": ["Anya Belz"], "title": "QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reproduction studies reported in NLP provide individual data points which in\ncombination indicate worryingly low levels of reproducibility in the field.\nBecause each reproduction study reports quantitative conclusions based on its\nown, often not explicitly stated, criteria for reproduction success/failure,\nthe conclusions drawn are hard to interpret, compare, and learn from. In this\npaper, we present QRA++, a quantitative approach to reproducibility assessment\nthat (i) produces continuous-valued degree of reproducibility assessments at\nthree levels of granularity; (ii) utilises reproducibility measures that are\ndirectly comparable across different studies; and (iii) grounds expectations\nabout degree of reproducibility in degree of similarity between experiments.\nQRA++ enables more informative reproducibility assessments to be conducted, and\nconclusions to be drawn about what causes reproducibility to be better/poorer.\nWe illustrate this by applying QRA++ to three example sets of comparable\nexperiments, revealing clear evidence that degree of reproducibility depends on\nsimilarity of experiment properties, but also system type and evaluation\nmethod.", "AI": {"tldr": "QRA++ \u662f\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30 NLP \u53ef\u91cd\u590d\u6027\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u4f7f\u8bc4\u4f30\u7ed3\u679c\u66f4\u5177\u53ef\u6bd4\u6027\u548c\u4fe1\u606f\u6027\u3002", "motivation": "\u7531\u4e8e\u4e2a\u522b\u518d\u73b0\u7814\u7a76\u96be\u4ee5\u63d0\u4f9b\u660e\u786e\u7684\u7ed3\u8bba\uff0c\u672c\u7814\u7a76\u63d0\u51fa QRA++\uff0c\u4ee5\u4fbf\u80fd\u591f\u8fdb\u884c\u53ef\u6bd4\u8f83\u548c\u53ef\u89e3\u91ca\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a QRA++ \u7684\u5b9a\u91cf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e09\u4e2a\u7c92\u5ea6\u6c34\u5e73\u7684\u8fde\u7eed\u503c\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5e94\u7528 QRA++\uff0c\u63ed\u793a\u4e86\u53ef\u91cd\u590d\u6027\u7a0b\u5ea6\u4e0d\u4ec5\u4e0e\u5b9e\u9a8c\u5c5e\u6027\u7684\u76f8\u4f3c\u6027\u6709\u5173\uff0c\u8fd8\u4e0e\u7cfb\u7edf\u7c7b\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u6709\u5173\u3002", "conclusion": "QRA++ \u80fd\u591f\u5b9e\u73b0\u66f4\u4fe1\u606f\u5316\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\uff0c\u5e76\u5e2e\u52a9\u786e\u5b9a\u5f71\u54cd\u53ef\u91cd\u590d\u6027\u7684\u56e0\u7d20\u3002"}}
{"id": "2505.17090", "pdf": "https://arxiv.org/pdf/2505.17090", "abs": "https://arxiv.org/abs/2505.17090", "authors": ["Phoebe Chua", "Cathy Mengying Fang", "Takehiko Ohkawa", "Raja Kushalnagar", "Suranga Nanayakkara", "Pattie Maes"], "title": "EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language", "categories": ["cs.CV"], "comment": null, "summary": "Unlike spoken languages where the use of prosodic features to convey emotion\nis well studied, indicators of emotion in sign language remain poorly\nunderstood, creating communication barriers in critical settings. Sign\nlanguages present unique challenges as facial expressions and hand movements\nsimultaneously serve both grammatical and emotional functions. To address this\ngap, we introduce EmoSign, the first sign video dataset containing sentiment\nand emotion labels for 200 American Sign Language (ASL) videos. We also collect\nopen-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL\nsigners with professional interpretation experience. Alongside the annotations,\nwe include baseline models for sentiment and emotion classification. This\ndataset not only addresses a critical gap in existing sign language research\nbut also establishes a new benchmark for understanding model capabilities in\nmultimodal emotion recognition for sign languages. The dataset is made\navailable at https://huggingface.co/datasets/catfang/emosign.", "AI": {"tldr": "EmoSign \u6570\u636e\u96c6\u89e3\u51b3\u4e86\u624b\u8bed\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u7531\u4e8e\u624b\u8bed\u4e2d\u7684\u60c5\u611f\u6307\u6807\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u9020\u6210\u4e86\u6279\u5224\u6027\u73af\u5883\u4e2d\u7684\u6c9f\u901a\u969c\u788d\uff0c\u6545\u9700\u5efa\u7acb\u8fd9\u4e00 Emosign \u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5305\u542b\u60c5\u611f\u548c\u60c5\u7eea\u6807\u7b7e\u7684\u7f8e\u56fd\u624b\u8bed\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u7531\u4e13\u4e1a\u89e3\u8bd1\u7ecf\u9a8c\u7684\u4e09\u4f4d\u804b\u4eba\u624b\u8bed\u7528\u6237\u8fdb\u884c\u7684\u5f00\u653e\u5f0f\u60c5\u611f\u63d0\u793a\u63cf\u8ff0\u6ce8\u91ca\uff1b\u5e76\u63d0\u4f9b\u60c5\u611f\u5206\u7c7b\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u4e3a\u7f8e\u56fd\u624b\u8bed\u89c6\u9891\u63d0\u4f9b\u60c5\u611f\u548c\u60c5\u7eea\u6807\u7b7e\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5e76\u5305\u542b\u57fa\u7ebf\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u3002", "conclusion": "EmoSign \u6570\u636e\u96c6\u586b\u8865\u4e86\u624b\u8bed\u60c5\u611f\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u80fd\u529b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2505.17045", "pdf": "https://arxiv.org/pdf/2505.17045", "abs": "https://arxiv.org/abs/2505.17045", "authors": ["Afifah Kashif", "Heer Patel"], "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have separately highlighted significant biases within\nfoundational large language models (LLMs) against certain nationalities and\nstigmatized social groups. This research investigates the ethical implications\nof these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.\nThrough structured prompt series, we evaluate model responses to several\nscenarios involving American and North Korean nationalities with various mental\ndisabilities. Findings reveal significant discrepancies in empathy levels with\nNorth Koreans facing greater negative bias, particularly when mental disability\nis also a factor. This underscores the need for improvements in LLMs designed\nwith a nuanced understanding of intersectional identity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86GPT-3.5/4/4o\u6a21\u578b\u5728\u5904\u7406\u7279\u5b9a\u56fd\u7c4d\u548c\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\u7684\u504f\u89c1\uff0c\u7279\u522b\u662f\u5bf9\u5317\u671d\u9c9c\u4eba\u7684\u8d1f\u9762\u504f\u89c1\uff0c\u5f3a\u8c03\u4e86\u6539\u5584\u6a21\u578b\u516c\u5e73\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u663e\u793a\uff0c\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u5bf9\u67d0\u4e9b\u56fd\u7c4d\u548c\u793e\u4f1a\u7fa4\u4f53\u7684\u504f\u89c1\u3002\u672c\u6587\u7814\u7a76\u8fd9\u4e9b\u504f\u89c1\u5728GPT-3.5/4/4o\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u4f26\u7406\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u63d0\u793a\u7cfb\u5217\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u6d89\u53ca\u7f8e\u56fd\u548c\u671d\u9c9c\u56fd\u7c4d\u4ee5\u53ca\u591a\u79cd\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u573a\u666f\u4e2d\u7684\u54cd\u5e94\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7f8e\u56fd\u4eba\u76f8\u6bd4\uff0c\u5317\u671d\u9c9c\u4eba\u5c24\u5176\u5728\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u60c5\u5883\u4e0b\u906d\u9047\u4e86\u66f4\u5927\u7684\u6d88\u6781\u504f\u89c1\u3002\u8fd9\u53cd\u6620\u4e86\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540c\u7fa4\u4f53\u65f6\u7684\u5171\u60c5\u80fd\u529b\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\uff0cLLMs\u5728\u5904\u7406\u591a\u91cd\u8eab\u4efd\u4ea4\u53c9\u95ee\u9898\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5230\u7279\u5b9a\u56fd\u7c4d\u548c\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\u3002\u8fd9\u9700\u8981\u5728\u6a21\u578b\u8bbe\u8ba1\u4e2d\u5f15\u5165\u66f4\u7ec6\u81f4\u7684\u8eab\u4efd\u6954\u5165\u673a\u5236\uff0c\u786e\u4fdd\u540c\u60c5\u5fc3\u7684\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2505.17097", "pdf": "https://arxiv.org/pdf/2505.17097", "abs": "https://arxiv.org/abs/2505.17097", "authors": ["Yanshu Li", "JianJiang Yang", "Bozheng Li", "Ruixiang Tang"], "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 6 tables", "summary": "Multimodal in-context learning (ICL) enables large vision-language models\n(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of\nreal-world applications. However, multimodal ICL remains unstable, and current\nresearch largely focuses on optimizing sequence configuration while overlooking\nthe internal mechanisms of LVLMs. In this work, we first provide a theoretical\nanalysis of attentional dynamics in multimodal ICL and identify three core\nlimitations of standard attention that ICL impair performance. To address these\nchallenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet\neffective plug-and-play method for directly calibrating LVLM attention logits.\nCAMA is training-free and can be seamlessly applied to various open-source\nLVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its\neffectiveness and generality. CAMA opens new opportunities for deeper\nexploration and targeted utilization of LVLM attention dynamics to advance\nmultimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5CAMA\uff0c\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u5185\u5728\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u7684ICL\u652f\u6301\u5927\u91cf\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5176\u4ecd\u7136\u4e0d\u7a33\u5b9a\uff0c\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f18\u5316\u5e8f\u5217\u914d\u7f6e\uff0c\u800c\u5ffd\u7565\u4e86LVLM\u7684\u5185\u5728\u673a\u5236\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53d1\u73b0\u6807\u51c6\u6ce8\u610f\u529b\u5f71\u54cdICL\u8868\u73b0\u7684\u4e09\u4e2a\u6838\u5fc3\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aContext-Aware Modulated Attention (CAMA)\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u6821\u51c6LVLM\u7684\u6ce8\u610f\u529b\u63a7\u5236\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2aLVLM\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u901a\u8fc7\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86CAMA\u7684\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684CAMA\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u5e94\u7528\u4e8e\u5404\u79cd\u5f00\u6e90LVLM\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2505.17047", "pdf": "https://arxiv.org/pdf/2505.17047", "abs": "https://arxiv.org/abs/2505.17047", "authors": ["Erin Palm", "Astrit Manikantan", "Mark E. Pepin", "Herprit Mahal", "Srikanth Subramanya Belwadi"], "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025", "summary": "In medical practices across the United States, physicians have begun\nimplementing generative artificial intelligence (AI) tools to perform the\nfunction of scribes in order to reduce the burden of documenting clinical\nencounters. Despite their widespread use, no established methods exist to gauge\nthe quality of AI scribes. To address this gap, we developed a blinded study\ncomparing the relative performance of large language model (LLM) generated\nclinical notes with those from field experts based on audio-recorded clinical\nencounters. Quantitative metrics from the Physician Documentation Quality\nInstrument (PDQI9) provided a framework to measure note quality, which we\nadapted to assess relative performance of AI generated notes. Clinical experts\nspanning 5 medical specialties used the PDQI9 tool to evaluate\nspecialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators\nfrom each specialty scored notes drafted from a total of 97 patient visits. We\nfound uniformly high inter rater agreement (RWG greater than 0.7) between\nevaluators in general medicine, orthopedics, and obstetrics and gynecology, and\nmoderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and\ncardiology. We found a modest yet significant difference in the overall note\nquality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes\nscored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9\ninstrument as a practical method to gauge the quality of LLM authored notes, as\ncompared to human-authored notes.", "AI": {"tldr": "The study developed a method to compare AI-generated clinical notes with human experts' notes, finding slightly better quality in human notes but supporting the use of AI in documentation.", "motivation": "To address the lack of established methods for evaluating the quality of AI-generated clinical notes used as scribes, given their increasing use in medical practices.", "method": "A blinded study comparing large language model (LLM) generated clinical notes with those drafted by field experts, using the PDQI9 assessment tool, involving clinical experts from five medical specialties. Notes from 97 patient visits were evaluated.", "result": "There is a modest yet significant difference in quality between AI-generated notes and human-created notes, with human notes slightly scoring higher on average. High inter-rater agreement was found among evaluators in most specialties, supporting the method's reliability.", "conclusion": "The study supports the use of the PDQI9 instrument as a viable method to evaluate the quality of notes generated by LLMs compared to human-written notes."}}
{"id": "2505.17127", "pdf": "https://arxiv.org/pdf/2505.17127", "abs": "https://arxiv.org/abs/2505.17127", "authors": ["Michal Golovanevsky", "William Rudman", "Michael Lepori", "Amir Bar", "Ritambhara Singh", "Carsten Eickhoff"], "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165Visual CounterFact\u6570\u636e\u96c6\u6765\u6d4b\u8bd5\u591a\u6a21\u6001\u6a21\u578b\u5728\u76f4\u89c9\u77e5\u8bc6\u4e0e\u89c6\u89c9\u8bc1\u636e\u51b2\u7a81\u4e2d\u7684\u51b3\u7b56\uff0c\u5e76\u63d0\u51faPvP\u673a\u5236\u4ee5\u63a7\u5236\u8f93\u51fa\uff0c\u53d1\u73b0\u89c6\u89c9\u8bc1\u636e\u5f80\u5f80\u6700\u7ec8\u5360\u636e\u4e3b\u5bfc\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u662f\u66f4\u591a\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u7684\u4e16\u754c\u77e5\u8bc6\u8fd8\u662f\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u5f15\u5165Visual CounterFact\u6570\u636e\u96c6\uff0c\u5305\u542b\u89c6\u89c9\u771f\u5b9e\u7684\u53cd\u4e8b\u5b9e\uff0c\u4ee5\u68c0\u9a8c\u6a21\u578b\u5728\u76f4\u89c9\u77e5\u8bc6\u7279\u6027\u4e0e\u89c6\u89c9\u4fe1\u606f\u7684\u51b2\u7a81\u4e2d\u5982\u4f55\u505a\u51fa\u51b3\u7b56\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u6fc0\u6d3b\u5c42\u6b21\u7684\u5e72\u9884\u6280\u672f\u2014\u2014PvP\u5bfc\u5411\u5411\u91cf\uff0c\u4ee5\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6a21\u578b\u9884\u6d4b\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u9996\u5148\u53cd\u6620\u51fa\u8bb0\u5fc6\u7684\u76f4\u89c9\u77e5\u8bc6\uff0c\u4f46\u5728\u4e2d\u540e\u671f\u5c42\u6b21\u5219\u9010\u6e10\u503e\u5411\u4e8e\u89c6\u89c9\u8bc1\u636e\u3002PvP\u673a\u5236\u80fd\u5c0692.5%\u7684\u989c\u8272\u9884\u6d4b\u548c74.6%\u7684\u5c3a\u5bf8\u9884\u6d4b\u4ece\u76f4\u89c9\u8f6c\u5411\u53cd\u4e8b\u5b9e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u5c42\u6b21\u7684\u5e72\u9884\uff0c\u53ef\u4ee5\u5728\u8f93\u51fa\u7ed3\u679c\u4e2d\u66f4\u503e\u5411\u4e8e\u4e16\u754c\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\u3002PvP\u673a\u5236\u80fd\u591f\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6210\u529f\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2505.17048", "pdf": "https://arxiv.org/pdf/2505.17048", "abs": "https://arxiv.org/abs/2505.17048", "authors": ["Agam Shah", "Siddhant Sukhani", "Huzaifa Pardawala", "Saketh Budideti", "Riya Bhadani", "Rudra Gopal", "Siddhartha Somani", "Michael Galarnyk", "Soungmin Lee", "Arnav Hiray", "Akshar Ravichandran", "Eric Kim", "Pranav Aluru", "Joshua Zhang", "Sebastian Jaskowski", "Veer Guda", "Meghaj Tarte", "Liqin Ye", "Spencer Gosden", "Rutwik Routu", "Rachel Yuh", "Sloka Chava", "Sahasra Chava", "Dylan Patrick Kelly", "Aiden Chiang", "Harsit Mittal", "Sudheer Chava"], "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "Central banks around the world play a crucial role in maintaining economic\nstability. Deciphering policy implications in their communications is\nessential, especially as misinterpretations can disproportionately impact\nvulnerable populations. To address this, we introduce the World Central Banks\n(WCB) dataset, the most comprehensive monetary policy corpus to date,\ncomprising over 380k sentences from 25 central banks across diverse geographic\nregions, spanning 28 years of historical data. After uniformly sampling 1k\nsentences per bank (25k total) across all available years, we annotate and\nreview each sentence using dual annotators, disagreement resolutions, and\nsecondary expert reviews. We define three tasks: Stance Detection, Temporal\nClassification, and Uncertainty Estimation, with each sentence annotated for\nall three. We benchmark seven Pretrained Language Models (PLMs) and nine Large\nLanguage Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on\nthese tasks, running 15,075 benchmarking experiments. We find that a model\ntrained on aggregated data across banks significantly surpasses a model trained\non an individual bank's data, confirming the principle \"the whole is greater\nthan the sum of its parts.\" Additionally, rigorous human evaluations, error\nanalyses, and predictive tasks validate our framework's economic utility. Our\nartifacts are accessible through the HuggingFace and GitHub under the\nCC-BY-NC-SA 4.0 license.", "AI": {"tldr": "\u5f15\u5165WCB\u6570\u636e\u96c6\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6574\u5408\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u5355\u72ec\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7ecf\u6d4e\u6548\u7528\u3002", "motivation": "\u89e3\u6790\u4e2d\u592e\u94f6\u884c\u901a\u4fe1\u4e2d\u7684\u653f\u7b56\u542b\u4e49\uff0c\u4ee5\u907f\u514d\u9519\u8bef\u89e3\u8bfb\u5bf9\u8106\u5f31\u4eba\u7fa4\u7684\u4e0d\u5229\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aWCB\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b380k\u53e5\u5b50\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u53cc\u4eba\u6807\u6ce8\u548c\u4e13\u5bb6\u5ba1\u6838\u3002\u5b9a\u4e49\u4e86\u7acb\u573a\u68c0\u6d4b\u3001\u65f6\u95f4\u5206\u7c7b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e09\u4e2a\u4efb\u52a1\u3002\u4f7f\u7528\u591a\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u8868\u660e\uff0c\u5728\u96c6\u6210\u591a\u4e2a\u94f6\u884c\u7684\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u679c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u94f6\u884c\u6570\u636e\u6a21\u578b\uff0c\u5e76\u7ecf\u8fc7\u4e25\u683c\u7684\u4eba\u7c7b\u8bc4\u4f30\u3001\u9519\u8bef\u5206\u6790\u53ca\u9884\u6d4b\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7ecf\u6d4e\u5b9e\u7528\u6027\u3002", "conclusion": "\u6a21\u578b\u5728\u6574\u5408\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u5355\u72ec\u94f6\u884c\u6570\u636e\uff0c\u8bc1\u5b9e\u4e86\u6574\u4f53\u4f18\u4e8e\u90e8\u5206\u4e4b\u548c\u7684\u539f\u7406\u3002"}}
{"id": "2505.17132", "pdf": "https://arxiv.org/pdf/2505.17132", "abs": "https://arxiv.org/abs/2505.17132", "authors": ["Tanqiu Jiang", "Jiacheng Liang", "Rongyi Zhu", "Jiawei Zhou", "Fenglong Ma", "Ting Wang"], "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86DTR\uff0c\u4e00\u79cd\u901a\u8fc7\u4f18\u5316KV\u7f13\u5b58\u51cf\u8f7b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u653b\u51fb\u7684\u63a8\u7406\u65f6\u95f4\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5229\u7528\u89c6\u89c9\u6587\u672c\u4ea4\u4e92\u7684\u8d8a\u72f1\u653b\u51fb\u7684\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9632\u5fa1\u673a\u5236\u6765\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u6a21\u6001\u5f15\u53d1\u7684\u5b89\u5168\u76f8\u5173\u5206\u5e03\u53d8\u5316\u516c\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u6807\u8bb0\u6743\u91cd\uff0c\u6700\u5c0f\u5316\u5bf9\u6297\u6027\u89c6\u89c9\u8f93\u5165\u7684\u5f71\u54cd\u3002", "result": "DTR\u5728\u653b\u51fb\u9c81\u68d2\u6027\u548c\u5bf9\u826f\u6027\u4efb\u52a1\u7684\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\uff0c\u6210\u529f\u5e94\u7528KV\u7f13\u5b58\u4f18\u5316\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "conclusion": "DTR\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u7684\u5173\u952e\u503c(KV)\u7f13\u5b58\u5b9e\u73b0\u4e86\u63a8\u7406\u65f6\u95f4\u9632\u5fa1\uff0c\u4ee5\u51cf\u8f7b\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049", "abs": "https://arxiv.org/abs/2505.17049", "authors": ["David Rozado"], "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\u00e9sum\u00e9 Evaluations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u804c\u4e1a\u5019\u9009\u4eba\u9009\u62e9\u4e2d\u5c55\u73b0\u51fa\u5bf9\u5973\u6027\u540d\u5b57\u7684\u504f\u597d\uff0c\u5c3d\u7ba1\u5177\u5907\u76f8\u540c\u8d44\u5386\uff0c\u800c\u52a0\u5165\u6027\u522b\u5b57\u6bb5\u53ca\u6027\u522b\u4ee3\u8bcd\u540e\uff0c\u8fd9\u79cd\u504f\u597d\u66f4\u5f3a\u3002\u6b64\u5916\uff0c\u4f4d\u7f6e\u504f\u5dee\u663e\u7740\uff0c\u63d0\u793a\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u4f7f\u7528\u9700\u8981\u8c28\u614e\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6839\u636e\u7b80\u5386\u8bc4\u4f30\u804c\u4e1a\u5019\u9009\u4eba\u65f6\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5206\u6790\u6027\u522b\u504f\u89c1\u7684\u5b58\u5728\u3002", "method": "\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6d89\u53ca22\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6bcf\u4e2a\u6a21\u578b\u88ab\u7cfb\u7edf\u5730\u7ed9\u51fa\u4e00\u4e2a\u804c\u4f4d\u63cf\u8ff0\u548c\u4e00\u5bf9\u804c\u4e1a\u5339\u914d\u7684\u7b80\u5386\uff08\u4e00\u4e2a\u7537\u6027\u540d\u5b57\uff0c\u4e00\u4e2a\u5973\u6027\u540d\u5b57\uff09\uff0c\u8981\u6c42\u9009\u62e9\u66f4\u5408\u9002\u7684\u5019\u9009\u4eba\u3002", "result": "\u6240\u6709LLMs\u572870\u4e2a\u4e0d\u540c\u804c\u4e1a\u4e2d\u4e00\u81f4\u504f\u597d\u9009\u53d6\u5973\u6027\u540d\u5b57\u7684\u5019\u9009\u4eba\uff0c\u5c3d\u7ba1\u5019\u9009\u4eba\u5177\u5907\u76f8\u540c\u7684\u4e13\u4e1a\u8d44\u5386\u3002\u5728\u52a0\u5165\u6027\u522b\u5b57\u6bb5\u540e\uff0c\u8fd9\u79cd\u5bf9\u5973\u6027\u5019\u9009\u4eba\u7684\u504f\u597d\u8fdb\u4e00\u6b65\u589e\u52a0\u3002\u4f7f\u7528\u6027\u522b\u4e2d\u7acb\u6807\u8bc6\u7b26\u65f6\uff0c\u90e8\u5206\u6a21\u578b\u503e\u5411\u9009\u62e9\u201c\u5019\u9009\u4ebaA\u201d\u3002\u4e0d\u540c\u6027\u522b\u8d4b\u4e88\u6027\u522b\u4e2d\u7acb\u6807\u8bc6\u7b26\u53ef\u8fbe\u5230\u6027\u522b\u5e73\u8861\u3002\u72ec\u7acb\u8bc4\u5206\u65f6\uff0c\u5973\u6027\u7b80\u5386\u5f97\u5206\u7565\u9ad8\u4f46\u5f71\u54cd\u8f83\u5c0f\u3002\u52a0\u5165\u6027\u522b\u4ee3\u8bcd\u540e\uff0c\u5019\u9009\u4eba\u88ab\u9009\u4e2d\u7684\u51e0\u7387\u7565\u5fae\u589e\u52a0\u3002\u5927\u591a\u6570\u6a21\u578b\u663e\u793a\u51fa\u9009\u53d6\u9996\u5148\u5217\u51fa\u5019\u9009\u4eba\u7684\u4f4d\u7f6e\u504f\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0cLLMs\u5728\u9009\u62e9\u804c\u4e1a\u5019\u9009\u4eba\u65f6\u503e\u5411\u4e8e\u9009\u62e9\u5177\u6709\u5973\u6027\u540d\u5b57\u7684\u5019\u9009\u4eba\uff0c\u8fd9\u5bf9\u5728\u9ad8\u98ce\u9669\u7684\u81ea\u52a8\u5316\u51b3\u7b56\u73af\u5883\u4e2d\u4f7f\u7528LLMs\u63d0\u51fa\u4e86\u8b66\u793a\u3002"}}
{"id": "2505.17201", "pdf": "https://arxiv.org/pdf/2505.17201", "abs": "https://arxiv.org/abs/2505.17201", "authors": ["Chaim Chai Elchik", "Fatemeh Karimi Nejadasl", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking (MOT) in computer vision has made significant\nadvancements, yet tracking small fish in underwater environments presents\nunique challenges due to complex 3D motions and data noise. Traditional\nsingle-view MOT models often fall short in these settings. This thesis\naddresses these challenges by adapting state-of-the-art single-view MOT models,\nFairMOT and YOLOv8, for underwater fish detecting and tracking in ecological\nstudies. The core contribution of this research is the development of a\nmulti-view framework that utilizes stereo video inputs to enhance tracking\naccuracy and fish behavior pattern recognition. By integrating and evaluating\nthese models on underwater fish video datasets, the study aims to demonstrate\nsignificant improvements in precision and reliability compared to single-view\napproaches. The proposed framework detects fish entities with a relative\naccuracy of 47% and employs stereo-matching techniques to produce a novel 3D\noutput, providing a more comprehensive understanding of fish movements and\ninteractions", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u89c6\u56fe\u6846\u67b6\uff0c\u5229\u7528\u7acb\u4f53\u89c6\u9891\u63d0\u9ad8\u6c34\u4e0b\u9c7c\u7c7b\u7684\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u663e\u793a\u51fa\u6bd4\u5355\u89c6\u56fe\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5355\u89c6\u56feMOT\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u9c7c\u7c7b\u7531\u4e8e\u590d\u67423D\u8fd0\u52a8\u548c\u6570\u636e\u566a\u58f0\u5e26\u6765\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6539\u8fdb\u7684\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9002\u5e94\u6700\u5148\u8fdb\u7684\u5355\u89c6\u56feMOT\u6a21\u578bFairMOT\u548cYOLOv8\uff0c\u4ee5\u53ca\u5f00\u53d1\u591a\u89c6\u56fe\u6846\u67b6\u6765\u8fdb\u884c\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u548c\u8ddf\u8e2a\u3002\u5229\u7528\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u589e\u5f3a\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e76\u4f7f\u7528\u7acb\u4f53\u5339\u914d\u6280\u672f\u751f\u62103D\u8f93\u51fa\u3002", "result": "\u5f00\u53d1\u7684\u6846\u67b6\u5728\u68c0\u6d4b\u9c7c\u7c7b\u5b9e\u4f53\u65b9\u9762\u5177\u670947%\u7684\u76f8\u5bf9\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u76843D\u8f93\u51fa\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u9c7c\u7c7b\u8fd0\u52a8\u548c\u4e92\u52a8\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u63d0\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c7c\u7c7b\u884c\u4e3a\u6a21\u5f0f\u8bc6\u522b\u3002\u96c6\u6210\u548c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u6c34\u4e0b\u9c7c\u7c7b\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u663e\u793a\u51fa\u4e0e\u5355\u89c6\u56fe\u65b9\u6cd5\u76f8\u6bd4\u5728\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050", "abs": "https://arxiv.org/abs/2505.17050", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "AI": {"tldr": "\u5f15\u5165\u4e86PBLBench\u5bf9\u8c61\u57fa\u51c6\u4ee5\u6d4b\u8bd5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u9762\u4e34\u8f83\u5927\u6311\u6218\u3002", "motivation": "\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528AHP\u8fdb\u884c\u4e13\u5bb6\u9a71\u52a8\u7684\u6210\u5bf9\u6bd4\u8f83\u6765\u5efa\u7acb\u7ed3\u6784\u5316\u548c\u52a0\u6743\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728PBLBench\u4e2d\u4ec5\u8868\u73b0\u51fa59%\u7684\u6392\u540d\u51c6\u786e\u6027\uff0c\u8868\u660e\u8be5\u57fa\u51c6\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "PBLBench\u6311\u6218\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u76ee\u6807\u662f\u5728\u6559\u80b2\u73af\u5883\u4e2d\u63d0\u5347\u5176\u8868\u73b0\u4ee5\u51cf\u8f7b\u6559\u5e08\u8d1f\u62c5\u3002"}}
{"id": "2505.17223", "pdf": "https://arxiv.org/pdf/2505.17223", "abs": "https://arxiv.org/abs/2505.17223", "authors": ["Siyang Song", "Micol Spitale", "Xiangyu Kong", "Hengde Zhu", "Cheng Luo", "Cristina Palmero", "German Barquero", "Sergio Escalera", "Michel Valstar", "Mohamed Daoudi", "Tobias Baur", "Fabien Ringeval", "Andrew Howes", "Elisabeth Andre", "Hatice Gunes"], "title": "REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge", "categories": ["cs.CV", "68T40"], "comment": null, "summary": "In dyadic interactions, a broad spectrum of human facial reactions might be\nappropriate for responding to each human speaker behaviour. Following the\nsuccessful organisation of the REACT 2023 and REACT 2024 challenges, we are\nproposing the REACT 2025 challenge encouraging the development and benchmarking\nof Machine Learning (ML) models that can be used to generate multiple\nappropriate, diverse, realistic and synchronised human-style facial reactions\nexpressed by human listeners in response to an input stimulus (i.e.,\naudio-visual behaviours expressed by their corresponding speakers). As a key of\nthe challenge, we provide challenge participants with the first natural and\nlarge-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human\ndyadic interactions containing a total of 2856 interaction sessions covering\nfive different topics. In addition, this paper also presents the challenge\nguidelines and the performance of our baselines on the two proposed\nsub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge\nbaseline code is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2025", "AI": {"tldr": "REACT 2025\u6311\u6218\u65e8\u5728\u5f00\u53d1ML\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u7a7f\u6234\u5f0f\u9762\u90e8\u53cd\u5e94\uff0c\u63d0\u4f9b\u4e86MARS\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7531\u8bb2\u8bdd\u8005\u884c\u4e3a\u5f15\u8d77\u7684\u591a\u6837\u5316\u9762\u90e8\u53cd\u5e94\uff0c\u63d0\u51faREACT 2025\u6311\u6218\uff0c\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u751f\u6210\u591a\u6837\u6027\u548c\u73b0\u5b9e\u6027\u9762\u90e8\u53cd\u5e94\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86REACT 2025\u6311\u6218\uff0c\u9f13\u52b1\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u5927\u89c4\u6a21\u7684MARS\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u4e24\u4e2a\u5b50\u6311\u6218\uff1a\u79bb\u7ebfMAFRG\u548c\u5728\u7ebfMAFRG\uff0c\u5e76\u62a5\u544a\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5176\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u672c\u8bba\u6587\u901a\u8fc7\u63d0\u51faREACT 2025\u6311\u6218\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ee5\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u542c\u8005\u5728\u53cc\u4eba\u4e92\u52a8\u4e2d\u5bf9\u523a\u6fc0\u505a\u51fa\u53cd\u5e94\u65f6\u7684\u9002\u5f53\u9762\u90e8\u8868\u60c5\u3002"}}
{"id": "2505.17051", "pdf": "https://arxiv.org/pdf/2505.17051", "abs": "https://arxiv.org/abs/2505.17051", "authors": ["Bernd Huber", "Ghazal Fazelnia", "Andreas Damianou", "Sebastian Peleato", "Max Lefarov", "Praveen Ravichandran", "Marco De Nadai", "Mounia Lalmas-Roellke", "Paul N. Bennett"], "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating contextually relevant\ncontent. However, tailoring these outputs to individual users for effective\npersonalization is a significant challenge. While rich user-specific\ninformation often exists as pre-existing user representations, such as\nembeddings learned from preferences or behaviors, current methods to leverage\nthese for LLM personalization typically require costly fine-tuning or\ntoken-heavy prompting. We propose Embedding-to-Prefix (E2P), a\nparameter-efficient method that injects pre-computed context embeddings into an\nLLM's hidden representation space through a learned projection to a single soft\ntoken prefix. This enables effective personalization while keeping the backbone\nmodel frozen and avoiding expensive adaptation techniques. We evaluate E2P\nacross two public datasets and in a production setting: dialogue\npersonalization on Persona-Chat, contextual headline generation on PENS, and\nlarge-scale personalization for music and podcast consumption. Results show\nthat E2P preserves contextual signals and achieves strong performance with\nminimal computational overhead, offering a scalable, efficient solution for\ncontextualizing generative AI systems.", "AI": {"tldr": "Embedding-to-Prefix (E2P)\u65b9\u6cd5\u901a\u8fc7\u6ce8\u5165\u4e0a\u4e0b\u6587\u5d4c\u5165\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e2a\u6027\u5316\uff0c\u800c\u4e0d\u9700\u8981\u6602\u8d35\u7684\u9002\u914d\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u5229\u7528LLM\u8fdb\u884c\u4e2a\u6027\u5316\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u6216\u5927\u91cf\u7684\u63d0\u793a\uff0c\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEmbedding-to-Prefix (E2P)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6295\u5c04\u5c06\u9884\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6ce8\u5165\u5230LLM\u7684\u9690\u85cf\u8868\u793a\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u3002", "result": "E2P\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u4fe1\u53f7\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9884\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6ce8\u5165\u5230LLM\u7684\u9690\u85cf\u8868\u793a\u7a7a\u95f4\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5728\u4fdd\u6301\u9aa8\u5e72\u6a21\u578b\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6709\u6548\u7684\u4e2a\u6027\u5316\u3002"}}
{"id": "2505.17235", "pdf": "https://arxiv.org/pdf/2505.17235", "abs": "https://arxiv.org/abs/2505.17235", "authors": ["Omar Moured", "Yufan Chen", "Ruiping Liu", "Simon Rei\u00df", "Philip Torr", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "CHAOS: Chart Analysis with Outlier Samples", "categories": ["cs.CV", "cs.CL"], "comment": "Data and code are publicly available at:\n  http://huggingface.co/datasets/omoured/CHAOS", "summary": "Charts play a critical role in data analysis and visualization, yet\nreal-world applications often present charts with challenging or noisy\nfeatures. However, \"outlier charts\" pose a substantial challenge even for\nMultimodal Large Language Models (MLLMs), which can struggle to interpret\nperturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier\nSamples), a robustness benchmark to systematically evaluate MLLMs against chart\nperturbations. CHAOS encompasses five types of textual and ten types of visual\nperturbations, each presented at three levels of severity (easy, mid, hard)\ninspired by the study result of human evaluation. The benchmark includes 13\nstate-of-the-art MLLMs divided into three groups (i.e., general-, document-,\nand chart-specific models) according to the training scope and data.\nComprehensive analysis involves two downstream tasks (ChartQA and\nChart-to-Text). Extensive experiments and case studies highlight critical\ninsights into robustness of models across chart perturbations, aiming to guide\nfuture research in chart understanding domain. Data and code are publicly\navailable at: http://huggingface.co/datasets/omoured/CHAOS.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CHAOS\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u8fdb\u884c\u56fe\u8868\u5206\u6790\u548c\u53ef\u89c6\u5316\uff0c\u73b0\u5b9e\u5e94\u7528\u4e2d\u56fe\u8868\u5e38\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\u6216\u566a\u58f0\u7279\u5f81\uff0c\u5bf9\u2018\u5f02\u5e38\u56fe\u8868\u2019\u7684\u89e3\u8bfb\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "CHAOS\u57fa\u51c6\u6d4b\u8bd5\u5305\u62ec\u4e94\u79cd\u6587\u672c\u548c\u5341\u79cd\u89c6\u89c9\u6270\u52a8\uff0c\u6bcf\u79cd\u6270\u52a8\u5206\u4e3a\u4e09\u4e2a\u7ea7\u522b\uff08\u7b80\u5355\uff0c\u4e2d\u7b49\uff0c\u56f0\u96be\uff09\uff0c\u8fd9\u4e9b\u7ea7\u522b\u7075\u611f\u6765\u81ea\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u7684\u7814\u7a76\u7ed3\u679c\u3002\u6d4b\u8bd5\u5305\u62ec13\u79cd\u6700\u5148\u8fdb\u7684MLLMs\u5206\u4e3a\u4e09\u7ec4\uff08\u4e00\u822c\u6a21\u578b\u3001\u6587\u6863\u6a21\u578b\u548c\u4e13\u95e8\u9488\u5bf9\u56fe\u8868\u7684\u6a21\u578b\uff09\uff0c\u6839\u636e\u5176\u8bad\u7ec3\u8303\u56f4\u548c\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u548c\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u65b9\u9762\u7684\u9c81\u68d2\u6027\u7684\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u65e8\u5728\u6307\u5bfc\u672a\u6765\u56fe\u8868\u7406\u89e3\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "\u6211\u4eec\u5f15\u5165\u4e86CHAOS\u4f5c\u4e3a\u4e00\u4e2a\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30MLLMs\u5728\u9762\u5bf9\u56fe\u8868\u6270\u52a8\u65f6\u7684\u8868\u73b0\u3002"}}
{"id": "2505.17052", "pdf": "https://arxiv.org/pdf/2505.17052", "abs": "https://arxiv.org/abs/2505.17052", "authors": ["Jinwoo Park", "Seunggeun Cho", "Dongsu Han"], "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) power many modern applications, but serving them\nat scale remains costly and resource-intensive. Current server-centric systems\noverlook consumer-grade GPUs at the edge. We introduce SpecEdge, an\nedge-assisted inference framework that splits LLM workloads between edge and\nserver GPUs using a speculative decoding scheme, exchanging only token outputs\nover the network. SpecEdge employs proactive edge drafting to overlap edge\ntoken creation with server verification and pipeline-aware scheduling that\ninterleaves multiple user requests to increase server-side throughput.\nExperiments show SpecEdge enhances overall cost efficiency by 1.91x through\nachieving 2.22x server throughput, and reduces inter token latency by 11.24%\ncompared to a server-only baseline, introducing a scalable, cost-effective\nparadigm for LLM serving.", "AI": {"tldr": "SpecEdge\u901a\u8fc7\u8fb9\u7f18\u548c\u670d\u52a1\u5668\u534f\u4f5c\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u63a8\u7406\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u670d\u52a1\u6210\u672c\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u4ee5\u670d\u52a1\u5668\u4e3a\u4e2d\u5fc3\u7684\u7cfb\u7edf\u5ffd\u89c6\u4e86\u8fb9\u7f18\u7684\u6d88\u8d39\u7ea7GPU\u8d44\u6e90\uff0c\u800c\u5728\u8fb9\u7f18\u53c2\u4e0e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u66f4\u7ecf\u6d4e\u7684\u65b9\u5f0f\u8fdb\u884c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u662f\u503c\u5f97\u63a2\u7d22\u7684\u65b9\u5411\u3002", "method": "SpecEdge\u5229\u7528\u4e86\u4e00\u79cd\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\uff0c\u5c06\u8fb9\u7f18\u548c\u670d\u52a1\u5668GPUs\u8fdb\u884c\u8d1f\u8f7d\u5206\u62c5\uff0c\u5e76\u4f7f\u7528\u7ba1\u9053\u611f\u77e5\u8c03\u5ea6\u6765\u4ea4\u9519\u5904\u7406\u591a\u4e2a\u7528\u6237\u8bf7\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpecEdge\u901a\u8fc7\u5b9e\u73b02.22\u500d\u7684\u670d\u52a1\u5668\u541e\u5410\u91cf\uff0c\u63d0\u9ad8\u4e861.91\u500d\u7684\u6574\u4f53\u6210\u672c\u6548\u7387\uff0c\u5e76\u5c06\u6bcf\u4e2atoken\u7684\u5ef6\u8fdf\u964d\u4f4e\u4e8611.24%\u3002", "conclusion": "SpecEdge\u5728\u6574\u4f53\u6210\u672c\u6548\u7387\u548c\u5ef6\u8fdf\u65b9\u9762\u90fd\u4f18\u4e8e\u4ec5\u670d\u52a1\u5668\u7684\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u4e3aLLM\u670d\u52a1\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2505.17245", "pdf": "https://arxiv.org/pdf/2505.17245", "abs": "https://arxiv.org/abs/2505.17245", "authors": ["Ryota Yagi"], "title": "Extending Dataset Pruning to Object Detection: A Variance-based Approach", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset pruning -- selecting a small yet informative subset of training data\n-- has emerged as a promising strategy for efficient machine learning, offering\nsignificant reductions in computational cost and storage compared to\nalternatives like dataset distillation. While pruning methods have shown strong\nperformance in image classification, their extension to more complex computer\nvision tasks, particularly object detection, remains relatively underexplored.\nIn this paper, we present the first principled extension of classification\npruning techniques to the object detection domain, to the best of our\nknowledge. We identify and address three key challenges that hinder this\ntransition: the Object-Level Attribution Problem, the Scoring Strategy Problem,\nand the Image-Level Aggregation Problem. To overcome these, we propose tailored\nsolutions, including a novel scoring method called Variance-based Prediction\nScore (VPS). VPS leverages both Intersection over Union (IoU) and confidence\nscores to effectively identify informative training samples specific to\ndetection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate\nthat our approach consistently outperforms prior dataset pruning methods in\nterms of mean Average Precision (mAP). We also show that annotation count and\nclass distribution shift can influence detection performance, but selecting\ninformative examples is a more critical factor than dataset size or balance.\nOur work bridges dataset pruning and object detection, paving the way for\ndataset pruning in complex vision tasks.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u9488\u5bf9\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4fee\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u8bc4\u5206\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u66f4\u590d\u6742\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5e94\u7528\u6570\u636e\u96c6\u4fee\u526a\u6280\u672f\uff0c\u5982\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5--\u57fa\u4e8e\u65b9\u5dee\u7684\u9884\u6d4b\u8bc4\u5206 (VPS)\uff0c\u7ed3\u5408\u4e86IoU\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u8bc6\u522b\u7279\u5b9a\u4e8e\u68c0\u6d4b\u4efb\u52a1\u7684\u4fe1\u606f\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728PASCAL VOC\u548cMS COCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u5e73\u5747\u7cbe\u5ea6(mAP)\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u6570\u636e\u96c6\u4fee\u526a\u65b9\u6cd5\u3002\u8fd8\u53d1\u73b0\uff0c\u6ce8\u91ca\u6570\u91cf\u548c\u7c7b\u5206\u5e03\u53d8\u5316\u4f1a\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\uff0c\u4f46\u9009\u62e9\u4fe1\u606f\u793a\u4f8b\u6bd4\u6570\u636e\u96c6\u5927\u5c0f\u6216\u5e73\u8861\u6027\u66f4\u4e3a\u5173\u952e\u3002", "conclusion": "\u6210\u529f\u5c06\u6570\u636e\u96c6\u4fee\u526a\u6280\u672f\u5e94\u7528\u5230\u4e86\u5bf9\u8c61\u68c0\u6d4b\u9886\u57df\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u96c6\u4fee\u526a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.17053", "pdf": "https://arxiv.org/pdf/2505.17053", "abs": "https://arxiv.org/abs/2505.17053", "authors": ["Ou Jiamin", "Eikmans Emile", "Buskens Vincent", "Pankowska Paulina", "Shan Yuli"], "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures, 2 tables", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin understanding human languages, this study explores how they translate this\nunderstanding into social exchange contexts that capture certain essences of\nreal world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were\nplaced in economic trust games where players balance self-interest with trust\nand reciprocity, making decisions that reveal their social preferences and\ninteractive reasoning abilities. Our study shows that LLMs deviate from pure\nself-interest and exhibit trust and reciprocity even without being prompted to\nadopt a specific persona. In the simplest one-shot interaction, LLMs emulated\nhow human players place trust at the beginning of such a game. Larger\nhuman-machine divergences emerged in scenarios involving trust repayment or\nmulti-round interactions, where decisions were influenced by both social\npreferences and interactive reasoning. LLMs responses varied significantly when\nprompted to adopt personas like selfish or unselfish players, with the impact\noutweighing differences between models or game types. Response of ChatGPT-4, in\nan unselfish or neutral persona, resembled the highest trust and reciprocity,\nsurpassing humans, Claude, and Bard. Claude and Bard displayed trust and\nreciprocity levels that sometimes exceeded and sometimes fell below human\nchoices. When given selfish personas, all LLMs showed lower trust and\nreciprocity than humans. Interactive reasoning to the actions of counterparts\nor changing game mechanics appeared to be random rather than stable,\nreproducible characteristics in the response of LLMs, though some improvements\nwere observed when ChatGPT-4 responded in selfish or unselfish personas.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u6a21\u62df\u4eba\u7c7b\u7684\u4e92\u52a8\u884c\u4e3a\uff0c\u53d1\u73b0LLMs\u5728\u6e38\u620f\u4e2d\u80fd\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\u3002\u8fd9\u79cd\u80fd\u529b\u5728\u6ca1\u6709\u63d0\u793a\u7279\u5b9a\u4eba\u8bbe\u65f6\u5df2\u7ecf\u663e\u73b0\uff0c\u4e14\u5728\u88ab\u63d0\u793a\u540e\u5dee\u5f02\u66f4\u5927\u3002", "motivation": "\u8fd9\u9879\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u5176\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u8f6c\u5316\u4e3a\u793e\u4f1a\u4ea4\u4e92\u8bed\u5883\uff0c\u7279\u522b\u662f\u6a21\u62df\u4eba\u7c7b\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u7684\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793aLLMs\u5728\u5904\u7406\u4fe1\u4efb\u548c\u4e92\u60e0\u65f6\u7684\u793e\u4f1a\u504f\u597d\u548c\u4e92\u52a8\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u2014\u2014ChatGPT-4\u3001Claude\u548cBard\u2014\u2014\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002\u6e38\u620f\u4e2d\uff0c\u73a9\u5bb6\u9700\u8981\u5728\u81ea\u6211\u5229\u76ca\u4e0e\u4fe1\u4efb\u548c\u4e92\u60e0\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u505a\u51fa\u80fd\u591f\u63ed\u793a\u5176\u793e\u4f1a\u504f\u597d\u548c\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u7684\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u6e38\u620f\u4e2d\u504f\u79bb\u7eaf\u7cb9\u7684\u81ea\u6211\u5229\u76ca\uff0c\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\u3002\u5728\u7b80\u5355\u7684\u5355\u6b21\u4ea4\u4e92\u4e2d\uff0cLLMs\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u73a9\u5bb6\u5728\u6e38\u620f\u5f00\u59cb\u65f6\u7684\u4fe1\u4efb\u884c\u4e3a\u3002\u5728\u6d89\u53ca\u4fe1\u4efb\u56de\u62a5\u6216\u591a\u8f6e\u4ea4\u4e92\u7684\u573a\u666f\u4e2d\uff0cLLMs\u7684\u51b3\u7b56\u53d7\u793e\u4f1a\u504f\u597d\u548c\u4e92\u52a8\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u66f4\u5927\u7684\u4e0e\u4eba\u7c7b\u7684\u5dee\u5f02\u3002\u5f53\u88ab\u63d0\u793a\u91c7\u7528\u81ea\u79c1\u6216\u65e0\u79c1\u4eba\u8bbe\u65f6\uff0cChatGPT-4\u5728\u65e0\u79c1\u6216\u4e2d\u7acb\u4eba\u8bbe\u4e0b\u7684\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\u6700\u9ad8\uff0c\u8d85\u8fc7\u4e86\u4eba\u7c7b\u548c\u5176\u4ed6\u6a21\u578b\u3002\u800c\u5728\u88ab\u8d4b\u4e88\u81ea\u79c1\u4eba\u8bbe\u65f6\uff0c\u6240\u6709LLMs\u7684\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\u5747\u4f4e\u4e8e\u4eba\u7c7b\u3002\u5728\u5e94\u5bf9\u5bf9\u624b\u884c\u52a8\u6216\u6e38\u620f\u673a\u5236\u53d8\u5316\u65f6\uff0cLLMs\u7684\u4e92\u52a8\u63a8\u7406\u8868\u73b0\u51fa\u968f\u673a\u800c\u975e\u7a33\u5b9a\u518d\u73b0\u7684\u7279\u5f81\uff0c\u5c3d\u7ba1ChatGPT-4\u5728\u81ea\u79c1\u6216\u65e0\u79c1\u4eba\u8bbe\u4e0b\u6709\u4e00\u4e9b\u6539\u5584\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u51fa\u4e8e\u81ea\u8eab\u5229\u76ca\u7684\u8003\u8651\u3002\u5373\u4f7f\u5728\u672a\u88ab\u63d0\u793a\u91c7\u7528\u7279\u5b9a\u4eba\u8bbe\u65f6\uff0cLLMs\u4e5f\u80fd\u6a21\u62df\u4eba\u7c7b\u5728\u4fe1\u4efb\u535a\u5f08\u4e2d\u7684\u884c\u4e3a\uff0c\u4f46\u5728\u4fe1\u4efb\u56de\u62a5\u6216\u591a\u8f6e\u4ea4\u4e92\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u4eba\u7c7b\u7684\u884c\u4e3a\u5dee\u5f02\u66f4\u5927\u3002\u6b64\u5916\uff0c\u5f53\u88ab\u63d0\u793a\u91c7\u7528\u7279\u5b9a\u4eba\u8bbe\u65f6\uff0cLLMs\u7684\u53cd\u5e94\u5dee\u5f02\u663e\u8457\uff0c\u8d85\u8fc7\u4e86\u4e0d\u540c\u6a21\u578b\u6216\u6e38\u620f\u7c7b\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002"}}
{"id": "2505.17256", "pdf": "https://arxiv.org/pdf/2505.17256", "abs": "https://arxiv.org/abs/2505.17256", "authors": ["Liang Shi", "Yun Fu"], "title": "ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved text-to-face\ngeneration, but achieving fine-grained control over facial features remains a\nchallenge. Existing methods often require training additional modules to handle\nspecific controls such as identity, attributes, or age, making them inflexible\nand resource-intensive. We propose ExpertGen, a training-free framework that\nleverages pre-trained expert models such as face recognition, facial attribute\nrecognition, and age estimation networks to guide generation with fine control.\nOur approach uses a latent consistency model to ensure realistic and\nin-distribution predictions at each diffusion step, enabling accurate guidance\nsignals to effectively steer the diffusion process. We show qualitatively and\nquantitatively that expert models can guide the generation process with high\nprecision, and multiple experts can collaborate to enable simultaneous control\nover diverse facial aspects. By allowing direct integration of off-the-shelf\nexpert models, our method transforms any such model into a plug-and-play\ncomponent for controllable face generation.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6ExpertGen\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u591a\u4e13\u5bb6\u534f\u4f5c\u63a7\u5236\u7684\u6587\u672c\u5230\u9762\u90e8\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u7684\u6a21\u5757\u8bad\u7ec3\u4ee5\u5904\u7406\u7279\u5b9a\u63a7\u5236\uff0c\u5982\u8eab\u4efd\u6216\u5e74\u9f84\uff0c\u4f7f\u5176\u4e0d\u7075\u6d3b\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u6211\u4eec\u63d0\u51faExpertGen\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u5f15\u5bfc\u751f\u6210\uff0c\u901a\u8fc7\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u786e\u4fdd\u6bcf\u4e2a\u6269\u6563\u6b65\u9aa4\u7684\u9884\u6d4b\u771f\u5b9e\u4e14\u7b26\u5408\u5206\u5e03\u3002", "result": "\u4e13\u5bb6\u6a21\u578b\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u591a\u4e2a\u4e13\u5bb6\u53ef\u4ee5\u534f\u4f5c\u5b9e\u73b0\u5bf9\u4e0d\u540c\u9762\u90e8\u7279\u5f81\u7684\u540c\u65f6\u63a7\u5236\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5141\u8bb8\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u76f4\u63a5\u96c6\u6210\uff0c\u5e76\u80fd\u591f\u4f5c\u4e3a\u53ef\u63a7\u9762\u90e8\u751f\u6210\u7684\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\u3002"}}
{"id": "2505.17054", "pdf": "https://arxiv.org/pdf/2505.17054", "abs": "https://arxiv.org/abs/2505.17054", "authors": ["Linglong Qian", "Zina Ibrahim"], "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.", "AI": {"tldr": "The paper introduces \\METHOD~, a novel transformer architecture that outperforms existing models in clinical sequence modelling and maintains clinical relevance and computational efficiency.", "motivation": "The unique challenges in healthcare domains such as irregular sampling, variable temporal dependencies, and complex contextual relationships in patient timelines necessitate a specialized transformer architecture for clinical sequence modelling in electronic health records.", "method": "Introduces \\METHOD~ (Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture with three key innovations: a patient-aware attention mechanism, an adaptive sliding window attention scheme, and a U-Net inspired architecture with dynamic skip connections.", "result": "\\METHOD~ consistently outperforms the state-of-the-art \\ETHOS~ model, especially in predicting high-severity cases requiring urgent clinical intervention. It shows stable performance across varying inference lengths and better preserves clinical hierarchies and relationships between medical concepts.", "conclusion": "\\METHOD~ represents a significant advancement in transformer architectures optimized for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency."}}
{"id": "2505.17280", "pdf": "https://arxiv.org/pdf/2505.17280", "abs": "https://arxiv.org/abs/2505.17280", "authors": ["Pushkar Shukla", "Aditya Chinchure", "Emily Diana", "Alexander Tolbert", "Kartik Hosanagar", "Vineeth N Balasubramanian", "Leonid Sigal", "Matthew Turk"], "title": "Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "The biases exhibited by text-to-image (TTI) models are often treated as\nindependent, though in reality, they may be deeply interrelated. Addressing\nbias along one dimension - such as ethnicity or age - can inadvertently affect\nanother, like gender, either mitigating or exacerbating existing disparities.\nUnderstanding these interdependencies is crucial for designing fairer\ngenerative models, yet measuring such effects quantitatively remains a\nchallenge. To address this, we introduce BiasConnect, a novel tool for\nanalyzing and quantifying bias interactions in TTI models. BiasConnect uses\ncounterfactual interventions along different bias axes to reveal the underlying\nstructure of these interactions and estimates the effect of mitigating one bias\naxis on another. These estimates show strong correlation (+0.65) with observed\npost-mitigation outcomes. Building on BiasConnect, we propose InterMit, an\nintersectional bias mitigation algorithm guided by user-defined target\ndistributions and priority weights. InterMit achieves lower bias (0.33 vs.\n0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields\nsuperior image quality compared to traditional techniques. Although our\nimplementation is training-free, InterMit is modular and can be integrated with\nmany existing debiasing approaches for TTI models, making it a flexible and\nextensible solution.", "AI": {"tldr": "BiasConnect\u901a\u8fc7\u91cf\u5316\u504f\u89c1\u4e4b\u95f4\u7684\u4e92\u52a8\u5173\u7cfb\u5e2e\u52a9TTI\u6a21\u578b\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u751f\u6210\uff1bInterMit\u5728\u8f83\u5c11\u6b65\u9aa4\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u7684\u504f\u89c1\u4e14\u56fe\u50cf\u8d28\u91cf\u66f4\u4f18\u3002", "motivation": "\u7406\u89e3\u504f\u89c1\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u662f\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\uff0c\u800c\u91cf\u5316\u8fd9\u4e9b\u5f71\u54cd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "BiasConnect\u4f7f\u7528\u53cd\u4e8b\u5b9e\u5e72\u9884\u5206\u6790\u4e0d\u540c\u504f\u89c1\u8f74\u4e0a\u7684\u504f\u89c1\u4e92\u52a8\u7ed3\u6784\uff0c\u5e76\u4f30\u8ba1\u8c03\u8282\u4e00\u4e2a\u504f\u89c1\u8f74\u5bf9\u53e6\u4e00\u4e2a\u4ea7\u751f\u7684\u5f71\u54cd\uff1bInterMit\u5219\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u76ee\u6807\u5206\u5e03\u53ca\u4f18\u5148\u6743\u91cd\u6765\u5f15\u5bfc\u504f\u89c1\u7f13\u89e3\u3002", "result": "BiasConnect\u7684\u4f30\u7b97\u4e0e\u89c2\u5bdf\u5230\u7684\u504f\u89c1\u7f13\u89e3\u540e\u7684\u7ed3\u679c\u5448\u73b0\u51fa\u5f3a\u76f8\u5173\u6027\uff08+0.65\uff09\uff0c\u800cInterMit\u5728\u964d\u4f4e\u504f\u89c1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff080.33\u5bf9\u6bd40.52\uff09\uff0c\u9700\u8981\u7684\u7f13\u89e3\u6b65\u9aa4\u66f4\u5c11\uff08\u5e73\u5747\u6b65\u9aa42.38\u5bf9\u6bd43.15\uff09\uff0c\u4e14\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002", "conclusion": "BiasConnect\u548cInterMit\u4e3aTTI\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u504f\u89c1\u91cf\u5316\u548c\u8c03\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u8f83\u4f4e\u7684\u504f\u89c1\u6c34\u5e73\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055", "abs": "https://arxiv.org/abs/2505.17055", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a97.59%\u51c6\u786e\u7387\u7684\u5df4\u52d2\u65af\u5766\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u4ee5\u4fc3\u8fdb\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\uff0c\u63d0\u9ad8\u4ed6\u4eec\u7684\u5b66\u4e60\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u7531\u4e8e\u5df4\u52d2\u65af\u5766\u624b\u8bed\u7684\u6570\u5b57\u8d44\u6e90\u7f3a\u4e4f\uff0c\u7814\u7a76\u7684\u52a8\u673a\u662f\u901a\u8fc7\u5148\u8fdb\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6765\u589e\u5f3a\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u4f7f\u7528Vision Transformer ViTModel\u8fdb\u884c\u624b\u52bf\u5206\u7c7b\uff0c\u8be5\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u4ee5\u63d0\u9ad8\u5176\u5bf9\u6570\u5b66\u624b\u52bf\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e8697.59%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bc6\u522b\u6570\u5b66\u624b\u52bf\u65b9\u9762\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u51c6\u786e\u7684\u5df4\u52d2\u65af\u5766\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6765\u589e\u5f3a\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\u3002\u8be5\u7cfb\u7edf\u53d6\u5f97\u9ad8\u8fbe97.59%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u9760\u6027\u548c\u7cbe\u786e\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u6559\u80b2\u5de5\u5177\u53ef\u4ee5\u901a\u8fc7\u4e92\u52a8\u89e3\u51b3\u65b9\u6848\u6765\u7f29\u5c0f\u542c\u969c\u5b66\u751f\u7684\u5b66\u4e60\u5dee\u8ddd\u3002"}}
{"id": "2505.17311", "pdf": "https://arxiv.org/pdf/2505.17311", "abs": "https://arxiv.org/abs/2505.17311", "authors": ["Harim Kim", "Yuhan Wang", "Minkyu Ahn", "Heeyoul Choi", "Yuyin Zhou", "Charmgil Hong"], "title": "Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI 2025 early accept", "summary": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for\nidentifying pathological abnormalities without requiring extensive labeled\ndata. However, existing diffusion-based UAD models rely solely on imaging\nfeatures, limiting their ability to distinguish between normal anatomical\nvariations and pathological anomalies. To address this, we propose Diff3M, a\nmulti-modal diffusion-based framework that integrates chest X-rays and\nstructured Electronic Health Records (EHRs) for enhanced anomaly detection.\nSpecifically, we introduce a novel image-EHR cross-attention module to\nincorporate structured clinical context into the image generation process,\nimproving the model's ability to differentiate normal from abnormal features.\nAdditionally, we develop a static masking strategy to enhance the\nreconstruction of normal-like images from anomalies. Extensive evaluations on\nCheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art\nperformance, outperforming existing UAD methods in medical imaging. Our code is\navailable at this http URL https://github.com/nth221/Diff3M", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6269\u6563\u6846\u67b6Diff3M\uff0c\u901a\u8fc7\u6574\u5408X\u5149\u7247\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684UAD\u6a21\u578b\u4ec5\u4f9d\u8d56\u4e8e\u5f71\u50cf\u7279\u5f81\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6b63\u5e38\u7684\u89e3\u5256\u53d8\u5f02\u548c\u75c5\u7406\u5f02\u5e38\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf-EHR\u8de8\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5c06\u7ed3\u6784\u5316\u4e34\u5e8a\u80cc\u666f\u7eb3\u5165\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u9759\u6001\u63a9\u7801\u7b56\u7565\u6765\u589e\u5f3a\u4ece\u5f02\u5e38\u5230\u6b63\u5e38\u6837\u8c8c\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "result": "Diff3M\u5728CheXpert\u548cMIMIC-CXR/IV\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684UAD\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Diff3M\u6846\u67b6\u901a\u8fc7\u6574\u5408\u80f8\u90e8X\u5149\u7247\u548c\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728CheXpert\u548cMIMIC-CXR/IV\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u8868\u73b0\u3002"}}
{"id": "2505.17056", "pdf": "https://arxiv.org/pdf/2505.17056", "abs": "https://arxiv.org/abs/2505.17056", "authors": ["Luoxi Tang", "Tharunya Sundar", "Shuai Yang", "Ankita Patra", "Manohar Chippada", "Giqi Zhao", "Yi Li", "Riteng Zhang", "Tunan Zhao", "Ting Yang", "Yuqiao Meng", "Weicheng Ma", "Zhaohan Xi"], "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u82f1\u8bed\u6807\u51c6\u5316\u8003\u8bd5\u51c6\u5907\u4e2d\u7684\u6f5c\u529b\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6ESTBOOK\u7528\u4e8e\u8bc4\u4f30\u5176\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u5206\u89e3\u5206\u6790\u6846\u67b6\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u8c03\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u652f\u6301\u82f1\u8bed\u6807\u51c6\u5316\u8003\u8bd5\u51c6\u5907\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u5176\u5728\u751f\u6210\u51c6\u786e\u548c\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aESTBOOK\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u82f1\u8bed\u6807\u51c6\u5316\u8003\u8bd5\uff08EST\uff09\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u3002ESTBOOK\u6c47\u96c6\u4e86\u4e94\u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u7684\u6d4b\u8bd5\uff0c\u6db5\u76d629\u79cd\u95ee\u9898\u7c7b\u578b\u548c\u8d85\u8fc710576\u4e2a\u95ee\u9898\uff0c\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u8868\u683c\u548c\u6570\u5b66\u7b26\u53f7\u7b49\u591a\u79cd\u5f62\u5f0f\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u5206\u6790\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684EST\u95ee\u9898\u62c6\u5206\u4e3a\u7279\u5b9a\u4efb\u52a1\u7684\u89e3\u51b3\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u4f7f\u7528ESTBOOK\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u63a8\u65ad\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u5206\u6790\u6846\u67b6\u8bc4\u4f30\u5176\u5728\u5404\u4e2a\u63a8\u7406\u73af\u8282\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u6f5c\u5728\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u9700\u8981\u91c7\u53d6\u9488\u5bf9\u6027\u7684\u7b56\u7565\u6765\u63d0\u9ad8\u5176\u4f5c\u4e3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models.", "AI": {"tldr": "\u63d0\u51fa\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u4ee5\u589e\u5f3a\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6539\u5584\u89c6\u89c9\u5d4c\u5165\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u9996\u5148\u8c03\u67e5\u4e86\u6295\u5f71\u5668\u5728\u538b\u7f29\u89c6\u89c9\u5d4c\u5165\u548c\u4e0e\u8bcd\u5d4c\u5165\u5bf9\u9f50\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u4ee5\u589e\u5f3a\u8865\u4e01\u7ea7\u5bf9\u9f50\u3002", "result": "\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u63d0\u9ad8\u4e86\u8865\u4e01\u7ea7\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u538b\u7f29\u80fd\u529b\u4ee5\u53ca\u66f4\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u5185\u5bb9\u751f\u6210\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u89c6\u89c9\u8865\u4e01\u4e0e\u8bed\u4e49\u8bcd\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.17058", "pdf": "https://arxiv.org/pdf/2505.17058", "abs": "https://arxiv.org/abs/2505.17058", "authors": ["David Osei Opoku", "Ming Sheng", "Yong Zhang"], "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 5 figures;", "summary": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.", "AI": {"tldr": "DO-RAG, a scalable hybrid QA system, enhances factual accuracy by integrating knowledge graphs and vector retrieval, achieving high recall and relevancy.", "motivation": "Existing RAG frameworks struggle to integrate heterogeneous data while maintaining consistency in reasoning. There is a need for systems with high factual accuracy grounded in structured knowledge.", "method": "Proposes a hybrid QA framework called DO-RAG that integrates multi-level knowledge graph construction with semantic vector retrieval, employing an agentic chain-of-thought architecture for extracting structured relationships and constructing dynamic knowledge graphs.", "result": "Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%.", "conclusion": "DO-RAG represents a significant improvement in the field of domain-specific QA systems by enhancing recall and answer relevancy through the integration of knowledge graphs and semantic vector retrieval, outperforming existing frameworks."}}
{"id": "2505.17317", "pdf": "https://arxiv.org/pdf/2505.17317", "abs": "https://arxiv.org/abs/2505.17317", "authors": ["Alyson East", "Elizabeth G. Campolongo", "Luke Meyers", "S M Rayeed", "Samuel Stevens", "Iuliia Zarubiieva", "Isadora E. Fluck", "Jennifer C. Gir\u00f3n", "Maximiliane Jousse", "Scott Lowe", "Kayla I Perry", "Isabelle Betancourt", "Noah Charney", "Evan Donoso", "Nathan Fox", "Kim J. Landsbergen", "Ekaterina Nepovinnykh", "Michelle Ramirez", "Parkash Singh", "Khum Thapa-Magar", "Matthew Thompson", "Evan Waite", "Tanya Berger-Wolf", "Hilmar Lapp", "Paula Mabee", "Graham Taylor", "Sydne Record"], "title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Biological collections house millions of specimens documenting Earth's\nbiodiversity, with digital images increasingly available through open-access\nplatforms. Most imaging protocols were developed for human visual\ninterpretation without considering computational analysis requirements. This\npaper aims to bridge the gap between current imaging practices and the\npotential for automated analysis by presenting key considerations for creating\nbiological specimen images optimized for computer vision applications. We\nprovide conceptual computer vision topics for context, addressing fundamental\nconcerns including model generalization, data leakage, and comprehensive\nmetadata documentation, and outline practical guidance on specimen imagine, and\ndata storage. These recommendations were synthesized through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists. Through this synthesis, we have identified ten\ninterconnected considerations that form a framework for successfully\nintegrating biological specimen images into computer vision pipelines. The key\nelements include: (1) comprehensive metadata documentation, (2) standardized\nspecimen positioning, (3) consistent size and color calibration, (4) protocols\nfor handling multiple specimens in one image, (5) uniform background selection,\n(6) controlled lighting, (7) appropriate resolution and magnification, (8)\noptimal file formats, (9) robust data archiving strategies, and (10) accessible\ndata sharing practices. By implementing these recommendations, collection\nmanagers, taxonomists, and biodiversity informaticians can generate images that\nsupport automated trait extraction, species identification, and novel\necological and evolutionary analyses at unprecedented scales. Successful\nimplementation lies in thorough documentation of methodological choices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5341\u4e2a\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u4ee5\u4f18\u5316\u751f\u7269\u6807\u672c\u56fe\u50cf\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\uff0c\u5e76\u5f3a\u8c03\u8be6\u7ec6\u8bb0\u5f55\u65b9\u6cd5\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f25\u5408\u73b0\u6709\u6210\u50cf\u5b9e\u8df5\u4e0e\u81ea\u52a8\u5316\u5206\u6790\u6f5c\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f18\u5316\u751f\u7269\u6807\u672c\u56fe\u50cf\u4ee5\u9002\u5e94\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8de8\u5b66\u79d1\u7684\u5408\u4f5c\uff0c\u5236\u5b9a\u4e86\u5341\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8003\u8651\u56e0\u7d20\uff0c\u4e3a\u751f\u7269\u6807\u672c\u56fe\u50cf\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u4e2d\u7684\u6210\u529f\u6574\u5408\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "result": "\u751f\u6210\u652f\u6301\u81ea\u52a8\u6027\u72b6\u63d0\u53d6\u3001\u7269\u79cd\u8bc6\u522b\u53ca\u65b0\u9896\u751f\u6001\u548c\u8fdb\u5316\u5206\u6790\u7684\u56fe\u50cf\u3002", "conclusion": "\u6210\u529f\u7684\u751f\u7269\u6807\u672c\u56fe\u50cf\u521b\u5efa\u9700\u8981\u8be6\u7ec6\u8bb0\u5f55\u65b9\u6cd5\u9009\u62e9\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u7684\u81ea\u52a8\u6027\u72b6\u63d0\u53d6\u3001\u7269\u79cd\u8bc6\u522b\u53ca\u65b0\u9896\u7684\u751f\u6001\u548c\u8fdb\u5316\u5206\u6790\u3002"}}
{"id": "2505.17059", "pdf": "https://arxiv.org/pdf/2505.17059", "abs": "https://arxiv.org/abs/2505.17059", "authors": ["Van-Tinh Nguyen", "Hoang-Duong Pham", "Thanh-Hai To", "Cong-Tuan Hung Do", "Thi-Thu-Trang Dong", "Vu-Trung Duong Le", "Van-Phuc Hoang"], "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary\n  version posted for early dissemination and feedback", "summary": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.", "AI": {"tldr": "Medalyze, an application using FLAN-T5-Large models, excels in summarizing medical texts and improves healthcare information access.", "motivation": "To address the challenges in understanding complex medical texts due to terminology and context-specific language.", "method": "This paper utilizes three specialized FLAN-T5-Large models, fine-tuned for different tasks related to medical texts. The system is deployed on web and mobile platforms using a scalable API and YugabyteDB for real-time inference.", "result": "The system shows superior performance in summarization tasks over GPT-4, supported by BLEU, ROUGE-L, BERTScore, and SpaCy Similarity metrics.", "conclusion": "Medalyze provides an effective solution for accessing healthcare information, with superior performance over GPT-4 in specific tasks."}}
{"id": "2505.17328", "pdf": "https://arxiv.org/pdf/2505.17328", "abs": "https://arxiv.org/abs/2505.17328", "authors": ["Dylan Kline"], "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning", "categories": ["cs.CV"], "comment": null, "summary": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames.", "AI": {"tldr": "\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\uff0c\u6210\u529f\u5b66\u4e60\u4e86\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u6e38\u620f\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u6e38\u620f\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6e38\u620f\u56fe\u50cf\u7f16\u7801\u5668\u5f80\u5f80\u8fc7\u62df\u5408\u4e8e\u7279\u5b9a\u6e38\u620f\u7684\u89c6\u89c9\u98ce\u683c\uff0c\u9650\u5236\u4e86\u5728\u65b0\u6e38\u620f\u4e2d\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5b66\u4e60\u80fd\u591f\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u901a\u7528\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u6539\u5584\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\uff0c\u901a\u8fc7\u4e00\u4e2a\u5bf9\u6297\u6027\u7684\u57df\u5206\u7c7b\u5668\u540c\u65f6\u9f13\u52b1\u76f8\u4f3c\u5185\u5bb9\u805a\u7c7b\u5e76\u6291\u5236\u6e38\u620f\u7279\u5b9a\u7684\u89c6\u89c9\u7ebf\u7d22\u3002\u8fd9\u79cd\u65b9\u6cd5\u4ea7\u751f\u7684\u5d4c\u5165\u5728\u4e0d\u540c\u7684\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5728Bingsu\u6e38\u620f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u77ed\u65f6\u95f4\u7684\u8bad\u7ec3\u540e\uff0c\u7279\u5f81\u4e0d\u518d\u6309\u6e38\u620f\u805a\u7c7b\uff0c\u8bf4\u660e\u5b9e\u73b0\u4e86\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u53ef\u4ee5\u63d0\u5347\u8de8\u6e38\u620f\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u8de8\u6e38\u620f\u7684\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u663e\u793a\u51fa\u6539\u8fdb\u8de8\u6e38\u620f\u8fc1\u79fb\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u5728\u5c3d\u53ef\u80fd\u5c11\u7684\u5fae\u8c03\u4e0b\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u3002"}}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060", "abs": "https://arxiv.org/abs/2505.17060", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "AI": {"tldr": "Introduces SALMONN-omni, a full-duplex speech LLM improving conversational AI efficiency, achieving notable performance gains without extensive training data.", "motivation": "To address error accumulation, context-dependent barge-in, and echo cancellation challenges in modular conversational systems by creating a more efficient single-model solution.", "method": "Introduces a standalone full-duplex speech LLM with a novel dynamic thinking mechanism, avoiding audio codecs in the token space and improving through reinforcement learning.", "result": "SALMONN-omni achieves at least 30% relative performance improvement over existing models in benchmarks and excels in complex conversational scenarios.", "conclusion": "SALMONN-omni significantly improves full-duplex conversational systems by integrating a dynamic thinking mechanism, outperforming existing models with less training data."}}
{"id": "2505.17330", "pdf": "https://arxiv.org/pdf/2505.17330", "abs": "https://arxiv.org/abs/2505.17330", "authors": ["Amit Agarwal", "Srikant Panda", "Kulbhushan Pachauri"], "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; I.7"], "comment": "Published in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Industry Track, pages 100-114", "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag", "AI": {"tldr": "FS-DAG\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5728\u5c11\u6837\u672c\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u5ea6\u7684\u6027\u80fd\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u5c11\u6837\u672c\u73af\u5883\u4e2d\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9OCR\u9519\u8bef\u3001\u62fc\u5199\u9519\u8bef\u548c\u9886\u57df\u53d8\u5316\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "FS-DAG\u4f7f\u7528\u6a21\u5757\u5316\u6846\u67b6\u5c06\u9886\u57df\u7279\u5b9a\u548c\u8bed\u8a00/\u89c6\u89c9\u7279\u5b9a\u7684\u9aa8\u5e72\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u6587\u6863\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u8fdb\u884c\u9002\u5e94\u3002", "result": "FS-DAG\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u5feb\u901f\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u63d0\u5347\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u6027\u80fd\u663e\u8457\u3002", "conclusion": "FS-DAG\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061", "abs": "https://arxiv.org/abs/2505.17061", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565MoD\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u89e3\u7801\u7b56\u7565\u6765\u8bc4\u4f30\u56fe\u50cftoken\u7684\u6b63\u786e\u6027\uff0c\u4ece\u800c\u663e\u8457\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136LVLM\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u65b9\u6cd5\u3002", "method": "Mixture of Decoding\uff08MoD\uff09\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u89e3\u7801\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5bf9\u56fe\u50cftoken\u6ce8\u610f\u529b\u7684\u6b63\u786e\u6027\u6765\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u4e0a\uff0cMoD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u89e3\u7801\u65b9\u6cd5\u5e76\u6709\u6548\u7f13\u89e3LVLM\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "MoD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u51cf\u8f7bLVLMs\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2505.17333", "pdf": "https://arxiv.org/pdf/2505.17333", "abs": "https://arxiv.org/abs/2505.17333", "authors": ["Xin You", "Minghui Zhang", "Hanxiao Zhang", "Jie Yang", "Nassir Navab"], "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis", "categories": ["cs.CV"], "comment": "early accepted by MICCAI", "summary": "Temporal modeling on regular respiration-induced motions is crucial to\nimage-guided clinical applications. Existing methods cannot simulate temporal\nmotions unless high-dose imaging scans including starting and ending frames\nexist simultaneously. However, in the preoperative data acquisition stage, the\nslight movement of patients may result in dynamic backgrounds between the first\nand last frames in a respiratory period. This additional deviation can hardly\nbe removed by image registration, thus affecting the temporal modeling. To\naddress that limitation, we pioneeringly simulate the regular motion process\nvia the image-to-video (I2V) synthesis framework, which animates with the first\nframe to forecast future frames of a given length. Besides, to promote the\ntemporal consistency of animated videos, we devise the Temporal Differential\nDiffusion Model to generate temporal differential fields, which measure the\nrelative differential representations between adjacent frames. The prompt\nattention layer is devised for fine-grained differential fields, and the field\naugmented layer is adopted to better interact these fields with the I2V\nframework, promoting more accurate temporal variation of synthesized videos.\nExtensive results on ACDC cardiac and 4D Lung datasets reveal that our approach\nsimulates 4D videos along the intrinsic motion trajectory, rivaling other\ncompetitive methods on perceptual similarity and temporal consistency. Codes\nwill be available soon.", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\u6765\u6539\u5584\u65f6\u95f4\u8fd0\u52a8\u7684\u5efa\u6a21\uff0c\u4ee5\u6a21\u62df4D\u89c6\u9891\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u663e\u793a\u4e86\u4f18\u8d8a\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u65e0\u6cd5\u6a21\u62df\u65f6\u95f4\u8fd0\u52a8\uff0c\u9664\u975e\u540c\u65f6\u5b58\u5728\u9ad8\u5242\u91cf\u6210\u50cf\u626b\u63cf\u7684\u8d77\u59cb\u548c\u7ed3\u675f\u5e27\u3002\u5728\u672f\u524d\u6570\u636e\u91c7\u96c6\u9636\u6bb5\uff0c\u8f7b\u5fae\u7684\u60a3\u8005\u8fd0\u52a8\u53ef\u80fd\u5bfc\u81f4\u547c\u5438\u5468\u671f\u5185\u7684\u52a8\u6001\u80cc\u666f\uff0c\u4ece\u800c\u5f71\u54cd\u65f6\u95f4\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6b64\u9650\u5236\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u5fae\u5206\u573a\uff0c\u91c7\u7528\u63d0\u793a\u6ce8\u610f\u5c42\u548c\u573a\u589e\u5f3a\u5c42\u6765\u4fc3\u8fdb\u5fae\u5206\u573a\u4e0eI2V\u6846\u67b6\u7684\u4ea4\u4e92\uff0c\u63d0\u9ad8\u5408\u6210\u89c6\u9891\u7684\u65f6\u95f4\u53d8\u5316\u51c6\u786e\u6027\u3002", "result": "\u5728ACDC\u5fc3\u810f\u548c4D\u80ba\u90e8\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6cbf\u7740\u5185\u5728\u8fd0\u52a8\u8f68\u8ff9\u6a21\u62df4D\u89c6\u9891\uff0c\u5728\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u4e0e\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\u6210\u529f\u5730\u6539\u5584\u4e86\u4e34\u65f6\u5dee\u5f02\u573a\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u5728\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u76844D\u89c6\u9891\u6a21\u62df\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2505.17037", "pdf": "https://arxiv.org/pdf/2505.17037", "abs": "https://arxiv.org/abs/2505.17037", "authors": ["Dimitri Schreiter"], "title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has emerged as a critical component in optimizing large\nlanguage models (LLMs) for domain-specific tasks. However, the role of prompt\nspecificity, especially in domains like STEM (physics, chemistry, biology,\ncomputer science and mathematics), medicine, and law, remains underexplored.\nThis thesis addresses the problem of whether increasing the specificity of\nvocabulary in prompts improves LLM performance in domain-specific\nquestion-answering and reasoning tasks. We developed a synonymization framework\nto systematically substitute nouns, verbs, and adjectives with varying\nspecificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,\nGranite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in\nSTEM, law, and medicine. Our results reveal that while generally increasing the\nspecificity of prompts does not have a significant impact, there appears to be\na specificity range, across all considered models, where the LLM performs the\nbest. Identifying this optimal specificity range offers a key insight for\nprompt design, suggesting that manipulating prompts within this range could\nmaximize LLM performance and lead to more efficient applications in specialized\ndomains.", "AI": {"tldr": "\u7814\u7a76\u63d0\u793a\u4e13\u95e8\u6027\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u4e13\u95e8\u6027\u8303\u56f4\u53ef\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u589e\u52a0\u63d0\u793a\u8bcd\u6c47\u4e13\u95e8\u6027\u662f\u5426\u80fd\u5728\u9886\u57df\u7279\u5b9a\u95ee\u9898\u56de\u7b54\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u9ad8LLM\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540c\u4e49\u5316\u6846\u67b6\uff0c\u5c06\u540d\u8bcd\u3001\u52a8\u8bcd\u548c\u5f62\u5bb9\u8bcd\u66ff\u6362\u4e3a\u4e0d\u540c\u7684\u4e13\u95e8\u6027\u6c34\u5e73\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u56db\u4e2aLLM\u7684\u5f71\u54cd\u3002", "result": "\u4e00\u822c\u6765\u8bf4\uff0c\u589e\u52a0\u63d0\u793a\u7684\u4e13\u95e8\u6027\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u4e13\u95e8\u6027\u8303\u56f4\u3002", "conclusion": "\u867d\u7136\u589e\u52a0\u63d0\u793a\u8bcd\u7684\u4e13\u95e8\u6027\u603b\u4f53\u4e0a\u5bf9\u63d0\u9ad8LLM\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u4e13\u95e8\u6027\u8303\u56f4\uff0c\u5728\u8be5\u8303\u56f4\u5185LLM\u8868\u73b0\u6700\u4f73\u3002\u627e\u5230\u8fd9\u4e2a\u6700\u4f73\u8303\u56f4\u4e3a\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.17038", "pdf": "https://arxiv.org/pdf/2505.17038", "abs": "https://arxiv.org/abs/2505.17038", "authors": ["Xian Gong", "Paul X. McCarthy", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Massive and diverse web data are increasingly vital for government disaster\nresponse, as demonstrated by the 2022 floods in New South Wales (NSW),\nAustralia. This study examines how X (formerly Twitter) and public inquiry\nsubmissions provide insights into public behaviour during crises. We analyse\nmore than 55,000 flood-related tweets and 1,450 submissions to identify\nbehavioural patterns during extreme weather events. While social media posts\nare short and fragmented, inquiry submissions are detailed, multi-page\ndocuments offering structured insights. Our methodology integrates Latent\nDirichlet Allocation (LDA) for topic modelling with Large Language Models\n(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and\ngeographic patterns, while LLMs improve filtering by identifying flood-relevant\ntweets using public submissions as a reference. This Relevance Index method\nreduces noise and prioritizes actionable content, improving situational\nawareness for emergency responders. By combining these complementary data\nstreams, our approach introduces a novel AI-driven method to refine\ncrisis-related social media content, improve real-time disaster response, and\ninform long-term resilience planning.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u548c\u67e5\u8be2\u63d0\u4ea4\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528LDA\u548cLLMs\u7684\u65b0\u9896AI\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u707e\u5bb3\u54cd\u5e94\u548c\u97e7\u6027\u89c4\u5212\u3002", "motivation": "\u653f\u5e9c\u5728\u707e\u5bb3\u5e94\u5bf9\u8fc7\u7a0b\u4e2d\u8d8a\u6765\u8d8a\u9700\u8981\u5206\u6790\u5927\u91cf\u591a\u6837\u5316\u7684\u7f51\u7edc\u6570\u636e\u3002\u7814\u7a76\u4ee52022\u5e74\u65b0\u5357\u5a01\u5c14\u58eb\u5dde\u7684\u6d2a\u6c34\u4e3a\u4f8b\uff0c\u63a2\u8ba8\u793e\u4ea4\u5a92\u4f53\uff08\u4f8b\u5982X\uff0c\u524d\u79f0Twitter\uff09\u548c\u516c\u5171\u67e5\u8be2\u5982\u4f55\u63d0\u4f9b\u5371\u673a\u671f\u95f4\u7684\u516c\u4f17\u884c\u4e3a\u6d1e\u5bdf\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86Latent Dirichlet Allocation\uff08LDA\uff09\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u3002LDA\u7528\u4e8e\u63ed\u793a\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u4e0d\u540c\u89c2\u70b9\u548c\u5730\u7406\u6a21\u5f0f\uff0c\u800cLLMs\u5229\u7528\u516c\u5171\u67e5\u8be2\u63d0\u4ea4\u4f5c\u4e3a\u53c2\u8003\u6765\u8fc7\u6ee4\u548c\u8bc6\u522b\u4e0e\u6d2a\u6c34\u76f8\u5173\u7684\u63a8\u6587\u3002\u6b64\u76f8\u5173\u6307\u6570\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u566a\u58f0\uff0c\u4f18\u5148\u5904\u7406\u53ef\u64cd\u4f5c\u5185\u5bb9\u3002", "result": "\u7814\u7a76\u5206\u679055,000\u591a\u6761\u4e0e\u6d2a\u6c34\u76f8\u5173\u7684\u63a8\u6587\u548c1,450\u4efd\u63d0\u4ea4\uff0c\u6210\u529f\u8bc6\u522b\u51fa\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u671f\u95f4\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u901a\u8fc7\u4f7f\u7528\u76f8\u5173\u6307\u6570\u65b9\u6cd5\u589e\u5f3a\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u7684\u6001\u52bf\u611f\u77e5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u548c\u516c\u5171\u67e5\u8be2\u63d0\u4ea4\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u4e0e\u5371\u673a\u76f8\u5173\u7684\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\uff0c\u5e76\u5e2e\u52a9\u6539\u8fdb\u5b9e\u65f6\u707e\u5bb3\u54cd\u5e94\u548c\u957f\u671f\u97e7\u6027\u89c4\u5212\u3002"}}
{"id": "2505.17039", "pdf": "https://arxiv.org/pdf/2505.17039", "abs": "https://arxiv.org/abs/2505.17039", "authors": ["Diego Bonatto"], "title": "A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes", "categories": ["cs.CL"], "comment": "46 pages, 8 figures, 1 table", "summary": "A data-driven quantitative approach was used to develop a novel\nclassification system for beer categories and styles. Sixty-two thousand one\nhundred twenty-one beer recipes were mined and analyzed, considering ingredient\nprofiles, fermentation parameters, and recipe vital statistics. Statistical\nanalyses combined with self-organizing maps (SOMs) identified four major\nsuperclusters that showed distinctive malt and hop usage patterns, style\ncharacteristics, and historical brewing traditions. Cold fermented styles\nshowed a conservative grain and hop composition, whereas hot fermented beers\nexhibited high heterogeneity, reflecting regional preferences and innovation.\nThis new taxonomy offers a reproducible and objective framework beyond\ntraditional sensory-based classifications, providing brewers, researchers, and\neducators with a scalable tool for recipe analysis and beer development. The\nfindings in this work provide an understanding of beer diversity and open\navenues for linking ingredient usage with fermentation profiles and flavor\noutcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u8bc6\u522b\u51fa\u4e86\u56db\u4e2a\u4e3b\u8981\u8d85\u7ea7\u7c07\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u548c\u5ba2\u89c2\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5564\u9152\u7684\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4ee5\u611f\u5b98\u4e3a\u57fa\u7840\u7684\u5564\u9152\u5206\u7c7b\u4f53\u7cfb\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u7cfb\u7edf\uff0c\u4ee5\u5ba2\u89c2\u8bc4\u4f30\u5564\u9152\u7c7b\u522b\u548c\u98ce\u683c\u3002", "method": "\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684\u5b9a\u91cf\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf962,121\u4e2a\u5564\u9152\u914d\u65b9\u8fdb\u884c\u6316\u6398\u548c\u5206\u6790\uff0c\u7ed3\u5408\u540c\u6b65\u7ec4\u7ec7\u6620\u5c04\uff08SOMs\uff09\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u4e3b\u8981\u8d85\u7ea7\u7c07\uff0c\u5b83\u4eec\u5c55\u793a\u4e86\u4e0d\u540c\u7684\u9ea6\u82bd\u548c\u5564\u9152\u82b1\u4f7f\u7528\u6a21\u5f0f\u3001\u98ce\u683c\u7279\u5f81\u548c\u5386\u53f2\u917f\u9020\u4f20\u7edf\u3002\u51b7\u53d1\u9175\u98ce\u683c\u8868\u73b0\u51fa\u4fdd\u5b88\u7684\u8c37\u7269\u548c\u5564\u9152\u82b1\u7ec4\u6210\uff0c\u800c\u70ed\u53d1\u9175\u5564\u9152\u8868\u73b0\u51fa\u9ad8\u5ea6\u5f02\u8d28\u6027\uff0c\u53cd\u6620\u4e86\u5730\u533a\u504f\u597d\u548c\u521b\u65b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e00\u8d21\u732e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u4ee5\u91cd\u590d\u548c\u5ba2\u89c2\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u917f\u9152\u5e08\u3001\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u8005\u8fdb\u884c\u914d\u65b9\u5206\u6790\u548c\u5564\u9152\u5f00\u53d1\u3002"}}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042", "abs": "https://arxiv.org/abs/2505.17042", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u662f\u9996\u4e2a\u591a\u6a21\u6001\u653e\u5c04\u5b66\u56fe\u8c31\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6587\u672c\u4fe1\u606f\uff0c\u7f3a\u4e4f\u5bf9\u4e8e\u653e\u5c04\u56fe\u50cf\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u89e3\u6790\u957f\u7bc7\u5e45\u653e\u5c04\u5b66\u6570\u636e\u65f6\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u4e86\u63d0\u9ad8\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u751f\u6210\u8d28\u91cf\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6574\u5408\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001VLM\u7684\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u8bed\u8a00\u548c\u56fe\u50cf\u4fe1\u606f\uff0c\u66f4\u6709\u6548\u5730\u5904\u7406\u957f\u7bc7\u5e45\u7684\u653e\u5c04\u5b66\u6570\u636e\u3002", "result": "\u8be5\u591a\u6a21\u6001VLM\u6846\u67b6\u5728\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u65b9\u9762\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u9996\u6b21\u7ed3\u5408\u4e86\u56fe\u50cf\u4e0e\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u66f4\u5168\u9762\u7684\u77e5\u8bc6\u63d0\u53d6\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u751f\u6210\u6548\u679c\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.17064", "pdf": "https://arxiv.org/pdf/2505.17064", "abs": "https://arxiv.org/abs/2505.17064", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. In this work, we present a\nsystematic and reproducible methodology for evaluating how TTI systems depict\ndifferent historical periods. For this purpose, we introduce the HistVis\ndataset, a curated collection of 30,000 synthetic images generated by three\nstate-of-the-art diffusion models using carefully designed prompts depicting\nuniversal human activities across different historical periods. We evaluate\ngenerated imagery across three key aspects: (1) Implicit Stylistic\nAssociations: examining default visual styles associated with specific eras;\n(2) Historical Consistency: identifying anachronisms such as modern artifacts\nin pre-modern contexts; and (3) Demographic Representation: comparing generated\nracial and gender distributions against historically plausible baselines. Our\nfindings reveal systematic inaccuracies in historically themed generated\nimagery, as TTI models frequently stereotype past eras by incorporating\nunstated stylistic cues, introduce anachronisms, and fail to reflect plausible\ndemographic patterns. By offering a scalable methodology and benchmark for\nassessing historical representation in generated imagery, this work provides an\ninitial step toward building more historically accurate and culturally aligned\nTTI models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76TTI\u6a21\u578b\u7684\u5386\u53f2\u65f6\u671f\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5e38\u523b\u677f\u5316\u5386\u53f2\u98ce\u683c\uff0c\u5f15\u5165\u9519\u4f4d\u53ca\u4eba\u53e3\u504f\u5dee\uff0c\u63d0\u51fa\u65b9\u6cd5\u6539\u5584\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u5f71\u54cd\u529b\u65e5\u76ca\u589e\u52a0\uff0c\u4eba\u4eec\u5f00\u59cb\u5173\u6ce8\u5176\u793e\u4f1a\u548c\u6587\u5316\u5f71\u54cd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u5426\u51c6\u786e\u5730\u518d\u73b0\u5386\u53f2\u80cc\u666f\u8fd9\u4e00\u95ee\u9898\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u5f15\u5165HistVis\u6570\u636e\u96c6\uff0c\u7531\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u6839\u636e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u751f\u62103\u4e07\u5f20\u5408\u6210\u56fe\u50cf\uff0c\u63cf\u7ed8\u4e86\u8de8\u8d8a\u4e0d\u540c\u5386\u53f2\u65f6\u671f\u7684\u666e\u904d\u4eba\u7c7b\u6d3b\u52a8\u3002\u8bc4\u4f30\u5206\u4e3a\u4e09\u4e2a\u65b9\u9762\uff1a\u9690\u542b\u7684\u98ce\u683c\u5173\u8054\u3001\u5386\u53f2\u4e00\u81f4\u6027\u53ca\u4eba\u53e3\u4ee3\u8868\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5728\u751f\u6210\u7684\u5386\u53f2\u4e3b\u9898\u56fe\u50cf\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u6a21\u578b\u5e38\u901a\u8fc7\u4e0d\u660e\u786e\u7684\u98ce\u683c\u7ebf\u7d22\u523b\u677f\u5316\u8fc7\u53bb\u7684\u65f6\u4ee3\uff0c\u5f15\u5165\u65f6\u4ee3\u9519\u4f4d\uff0c\u5e76\u672a\u80fd\u53cd\u6620\u5408\u7406\u7684\u4eba\u53e3\u7279\u5f81\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165HistVis\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u548c\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30TTI\u7cfb\u7edf\u5982\u4f55\u523b\u753b\u4e0d\u540c\u7684\u5386\u53f2\u65f6\u671f\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0cTTI\u6a21\u578b\u5728\u751f\u6210\u5386\u53f2\u4e3b\u9898\u7684\u56fe\u50cf\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u5e38\u901a\u8fc7\u4e0d\u660e\u786e\u7684\u98ce\u683c\u7ebf\u7d22\u523b\u677f\u5316\u8fc7\u53bb\u7684\u65f6\u4ee3\uff0c\u5f15\u5165\u65f6\u4ee3\u9519\u4f4d\uff0c\u5e76\u672a\u80fd\u53cd\u6620\u5408\u7406\u7684\u4eba\u53e3\u7279\u5f81\u3002\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5efa\u7acb\u66f4\u5177\u5386\u53f2\u51c6\u786e\u6027\u548c\u6587\u5316\u4e00\u81f4\u6027\u7684TTI\u6a21\u578b\u63d0\u4f9b\u4e86\u521d\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.17043", "pdf": "https://arxiv.org/pdf/2505.17043", "abs": "https://arxiv.org/abs/2505.17043", "authors": ["Anya Belz"], "title": "QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reproduction studies reported in NLP provide individual data points which in\ncombination indicate worryingly low levels of reproducibility in the field.\nBecause each reproduction study reports quantitative conclusions based on its\nown, often not explicitly stated, criteria for reproduction success/failure,\nthe conclusions drawn are hard to interpret, compare, and learn from. In this\npaper, we present QRA++, a quantitative approach to reproducibility assessment\nthat (i) produces continuous-valued degree of reproducibility assessments at\nthree levels of granularity; (ii) utilises reproducibility measures that are\ndirectly comparable across different studies; and (iii) grounds expectations\nabout degree of reproducibility in degree of similarity between experiments.\nQRA++ enables more informative reproducibility assessments to be conducted, and\nconclusions to be drawn about what causes reproducibility to be better/poorer.\nWe illustrate this by applying QRA++ to three example sets of comparable\nexperiments, revealing clear evidence that degree of reproducibility depends on\nsimilarity of experiment properties, but also system type and evaluation\nmethod.", "AI": {"tldr": "QRA++\u662f\u4e00\u79cd\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\uff0c\u901a\u8fc7\u8fde\u7eed\u503c\u8bc4\u4f30\u548c\u8de8\u7814\u7a76\u6bd4\u8f83\u63aa\u65bd\u6765\u8bc4\u4f30\u5b9e\u9a8c\u76f8\u4f3c\u6027\u5bf9\u53ef\u91cd\u590d\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff0c\u5404\u4e2a\u91cd\u73b0\u7814\u7a76\u63d0\u4f9b\u7684\u4e2a\u522b\u6570\u636e\u70b9\u8868\u660e\u9886\u57df\u5185\u7684\u53ef\u91cd\u590d\u6027\u4ee4\u4eba\u62c5\u5fe7\u3002\u7531\u4e8e\u6bcf\u4e2a\u91cd\u73b0\u7814\u7a76\u6839\u636e\u81ea\u5df1\u4e0d\u660e\u786e\u7684\u6807\u51c6\u62a5\u544a\u7ed3\u8bba\uff0c\u56e0\u6b64\u5f88\u96be\u89e3\u91ca\u3001\u6bd4\u8f83\u548c\u5b66\u4e60\u5176\u4e2d\u7684\u7ed3\u8bba\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b9a\u91cf\u65b9\u6cd5QRA++\u7528\u4e8e\u8bc4\u4f30\u53ef\u91cd\u590d\u6027\uff0c\u80fd\u591f\u5728\u4e09\u4e2a\u7c92\u5ea6\u6c34\u5e73\u4e0a\u63d0\u4f9b\u8fde\u7eed\u503c\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5c06QRA++\u5e94\u7528\u4e8e\u4e09\u4e2a\u5b9e\u9a8c\u96c6\uff0c\u63ed\u793a\u4e86\u5b9e\u9a8c\u5c5e\u6027\u76f8\u4f3c\u7a0b\u5ea6\u4ee5\u53ca\u7cfb\u7edf\u7c7b\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u53ef\u91cd\u590d\u6027\u7a0b\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "QRA++\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9a\u91cf\u65b9\u6cd5\u6765\u8fdb\u884c\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\uff0c\u5b83\u80fd\u591f\u5728\u4e0d\u540c\u7c92\u5ea6\u6c34\u5e73\u4e0a\u4ea7\u751f\u8fde\u7eed\u503c\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u5728\u4e0d\u540c\u7814\u7a76\u4e2d\u53ef\u76f4\u63a5\u6bd4\u8f83\u7684\u53ef\u91cd\u590d\u6027\u63aa\u65bd\u3002"}}
{"id": "2505.17090", "pdf": "https://arxiv.org/pdf/2505.17090", "abs": "https://arxiv.org/abs/2505.17090", "authors": ["Phoebe Chua", "Cathy Mengying Fang", "Takehiko Ohkawa", "Raja Kushalnagar", "Suranga Nanayakkara", "Pattie Maes"], "title": "EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language", "categories": ["cs.CV"], "comment": null, "summary": "Unlike spoken languages where the use of prosodic features to convey emotion\nis well studied, indicators of emotion in sign language remain poorly\nunderstood, creating communication barriers in critical settings. Sign\nlanguages present unique challenges as facial expressions and hand movements\nsimultaneously serve both grammatical and emotional functions. To address this\ngap, we introduce EmoSign, the first sign video dataset containing sentiment\nand emotion labels for 200 American Sign Language (ASL) videos. We also collect\nopen-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL\nsigners with professional interpretation experience. Alongside the annotations,\nwe include baseline models for sentiment and emotion classification. This\ndataset not only addresses a critical gap in existing sign language research\nbut also establishes a new benchmark for understanding model capabilities in\nmultimodal emotion recognition for sign languages. The dataset is made\navailable at https://huggingface.co/datasets/catfang/emosign.", "AI": {"tldr": "EmoSign\u662f\u7b2c\u4e00\u4e2a\u5305\u542b\u60c5\u611f\u4e0e\u60c5\u7eea\u6807\u6ce8\u7684ASL\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u624b\u8bed\u60c5\u611f\u8bc6\u522b\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u8bbe\u7acb\u4e86\u65b0\u7684\u7814\u7a76\u57fa\u51c6\u3002", "motivation": "\u5c3d\u7ba1\u5728\u53e3\u8bed\u4e2d\u8868\u8fbe\u60c5\u611f\u7684\u97f5\u5f8b\u7279\u5f81\u7814\u7a76\u8f83\u591a\uff0c\u624b\u8bed\u4e2d\u7684\u60c5\u611f\u6307\u793a\u4ecd\u7136\u4e0d\u592a\u660e\u6717\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u573a\u5408\u4e2d\u5bfc\u81f4\u6c9f\u901a\u969c\u788d\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86EmoSign\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e86200\u4e2aASL\u89c6\u9891\u7684\u60c5\u611f\u548c\u60c5\u7eea\u6807\u6ce8\uff0c\u5e76\u6536\u96c6\u4e86\u60c5\u7eea\u7ebf\u7d22\u7684\u5f00\u653e\u5f0f\u63cf\u8ff0\u3002\u6807\u6ce8\u8fc7\u7a0b\u75313\u4f4d\u5177\u6709\u4e13\u4e1a\u7ffb\u8bd1\u7ecf\u9a8c\u7684\u542c\u969cASL\u624b\u8bed\u4eba\u58eb\u5b8c\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u60c5\u611f\u548c\u60c5\u7eea\u5206\u7c7b\u7684\u57fa\u51c6\u6a21\u578b\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u7684EmoSign\u6570\u636e\u96c6\u6210\u4e3a\u73b0\u6709\u624b\u8bed\u7814\u7a76\u7684\u91cd\u8981\u8865\u5145\uff0c\u5e76\u4e3a\u624b\u8bed\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u80fd\u529b\u7684\u7406\u89e3\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u60c5\u611f\u548c\u60c5\u7eea\u6807\u6ce8\u7684ASL\u89c6\u9891\u6570\u636e\u96c6EmoSign\uff0c\u586b\u8865\u4e86\u624b\u8bed\u60c5\u611f\u8bc6\u522b\u9886\u57df\u7684\u4e00\u4e2a\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2505.17045", "pdf": "https://arxiv.org/pdf/2505.17045", "abs": "https://arxiv.org/abs/2505.17045", "authors": ["Afifah Kashif", "Heer Patel"], "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have separately highlighted significant biases within\nfoundational large language models (LLMs) against certain nationalities and\nstigmatized social groups. This research investigates the ethical implications\nof these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.\nThrough structured prompt series, we evaluate model responses to several\nscenarios involving American and North Korean nationalities with various mental\ndisabilities. Findings reveal significant discrepancies in empathy levels with\nNorth Koreans facing greater negative bias, particularly when mental disability\nis also a factor. This underscores the need for improvements in LLMs designed\nwith a nuanced understanding of intersectional identity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u67d0\u4e9b\u56fd\u5bb6\u548c\u793e\u4f1a\u7fa4\u4f53\u5b58\u5728\u504f\u89c1\uff0c\u7279\u522b\u662f\u5bf9\u671d\u9c9c\u56fd\u7c4d\u548c\u7cbe\u795e\u6b8b\u75be\u8005\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5b9a\u56fd\u5bb6\uff08\u5982\u671d\u9c9c\uff09\u548c\u6709\u6c61\u540d\u5316\u7684\u793e\u4f1a\u7fa4\u4f53\u504f\u89c1\u7684\u4f26\u7406\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7cfb\u5217\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u4e0d\u540c\u56fd\u7c4d\u548c\u7cbe\u795e\u6b8b\u75be\u60c5\u5883\u7684\u54cd\u5e94\u3002", "result": "\u53d1\u73b0\u5bf9\u671d\u9c9c\u4eba\u8868\u73b0\u51fa\u66f4\u5927\u7684\u8d1f\u9762\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u7cbe\u795e\u6b8b\u75be\u65b9\u9762\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u4ea4\u53c9\u8eab\u4efd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.17097", "pdf": "https://arxiv.org/pdf/2505.17097", "abs": "https://arxiv.org/abs/2505.17097", "authors": ["Yanshu Li", "JianJiang Yang", "Bozheng Li", "Ruixiang Tang"], "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 6 tables", "summary": "Multimodal in-context learning (ICL) enables large vision-language models\n(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of\nreal-world applications. However, multimodal ICL remains unstable, and current\nresearch largely focuses on optimizing sequence configuration while overlooking\nthe internal mechanisms of LVLMs. In this work, we first provide a theoretical\nanalysis of attentional dynamics in multimodal ICL and identify three core\nlimitations of standard attention that ICL impair performance. To address these\nchallenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet\neffective plug-and-play method for directly calibrating LVLM attention logits.\nCAMA is training-free and can be seamlessly applied to various open-source\nLVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its\neffectiveness and generality. CAMA opens new opportunities for deeper\nexploration and targeted utilization of LVLM attention dynamics to advance\nmultimodal reasoning.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86CAMA\uff0c\u4e00\u79cd\u6539\u8fdbLVLMs\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6a21\u6001ICL\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u591a\u6a21\u6001ICL\u5728LVLMs\u4e2d\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e3aLVLM\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e00\u4e2a\u7406\u8bba\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAMA\u7684\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u76f4\u63a5\u6821\u51c6LVLMs\u7684\u6ce8\u610f\u529blogit\u3002", "result": "CAMA\u5728\u56db\u4e2aLVLMs\u7684\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4ef7\uff0c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236CAMA\uff0c\u53ef\u4ee5\u6539\u8fdbLVLMs\u5728\u591a\u6a21\u6001\u60c5\u51b5\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u80fd\u3002"}}
{"id": "2505.17047", "pdf": "https://arxiv.org/pdf/2505.17047", "abs": "https://arxiv.org/abs/2505.17047", "authors": ["Erin Palm", "Astrit Manikantan", "Mark E. Pepin", "Herprit Mahal", "Srikanth Subramanya Belwadi"], "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025", "summary": "In medical practices across the United States, physicians have begun\nimplementing generative artificial intelligence (AI) tools to perform the\nfunction of scribes in order to reduce the burden of documenting clinical\nencounters. Despite their widespread use, no established methods exist to gauge\nthe quality of AI scribes. To address this gap, we developed a blinded study\ncomparing the relative performance of large language model (LLM) generated\nclinical notes with those from field experts based on audio-recorded clinical\nencounters. Quantitative metrics from the Physician Documentation Quality\nInstrument (PDQI9) provided a framework to measure note quality, which we\nadapted to assess relative performance of AI generated notes. Clinical experts\nspanning 5 medical specialties used the PDQI9 tool to evaluate\nspecialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators\nfrom each specialty scored notes drafted from a total of 97 patient visits. We\nfound uniformly high inter rater agreement (RWG greater than 0.7) between\nevaluators in general medicine, orthopedics, and obstetrics and gynecology, and\nmoderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and\ncardiology. We found a modest yet significant difference in the overall note\nquality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes\nscored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9\ninstrument as a practical method to gauge the quality of LLM authored notes, as\ncompared to human-authored notes.", "AI": {"tldr": "\u533b\u5e08\u4f7f\u7528AI\u5de5\u5177\u8fdb\u884c\u8bb0\u5f55\uff0c\u4f46\u5c1a\u65e0\u8bc4\u4f30\u5176\u8d28\u91cf\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u6bd4\u8f83\u4e86LLM\u751f\u6210\u7684\u4e34\u5e8a\u7b14\u8bb0\u4e0e\u4e13\u5bb6\u7b14\u8bb0\uff0c\u4f7f\u7528PDQI9\u5de5\u5177\u8bc4\u4f30\u5176\u8d28\u91cf\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e8c\u8005\u8d28\u91cf\u5dee\u5f02\u4e0d\u663e\u8457\uff0c\u4f46\u652f\u6301\u4f7f\u7528PDQI9\u8bc4\u4f30LLM\u7b14\u8bb0\u3002", "motivation": "\u7f8e\u56fd\u5404\u5730\u7684\u533b\u5e08\u5f00\u59cb\u4f7f\u7528\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd(AI)\u5de5\u5177\u5145\u5f53\u8bb0\u5f55\u5458\u4ee5\u51cf\u8f7b\u8bb0\u5f55\u4e34\u5e8a\u4f1a\u8bdd\u7684\u8d1f\u62c5\u3002\u7136\u800c\uff0c\u5c1a\u65e0\u65e2\u5b9a\u65b9\u6cd5\u6765\u8861\u91cfAI\u8bb0\u5f55\u5458\u7684\u8d28\u91cf\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u9879\u76f2\u6cd5\u7814\u7a76\uff0c\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u7684\u4e34\u5e8a\u7b14\u8bb0\u4e0e\u901a\u8fc7\u97f3\u9891\u8bb0\u5f55\u7684\u4e34\u5e8a\u4f1a\u8bdd\u5f97\u5230\u7684\u9886\u57df\u4e13\u5bb6\u7b14\u8bb0\u7684\u76f8\u5bf9\u8868\u73b0\u3002\u4e34\u5e8a\u4e13\u5bb6\u4f7f\u7528PDQI9\u5de5\u5177\u5bf9\u4e13\u5bb6\u64b0\u5199\u7684\u9ec4\u91d1\u7b14\u8bb0\u548cLLM\u64b0\u5199\u7684\u73af\u5883\u7b14\u8bb0\u8fdb\u884c\u8bc4\u4f30\u3002\u6765\u81ea5\u4e2a\u533b\u5b66\u4e13\u4e1a\u7684\u4e24\u4f4d\u8bc4\u4f30\u8005\u5bf9\u4ece97\u6b21\u60a3\u8005\u8bbf\u95ee\u4e2d\u64b0\u5199\u7684\u7b14\u8bb0\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728\u5185\u79d1\u3001\u9aa8\u79d1\u548c\u5987\u4ea7\u79d1\u8bc4\u4f30\u8005\u4e4b\u95f4\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u81f4\u6027\u8f83\u9ad8\u7684\u8bc4\u5206\u4e00\u81f4\u6027(RWG\u5927\u4e8e0.7)\uff0c\u5728\u513f\u79d1\u548c\u5fc3\u810f\u75c5\u5b66\u5219\u4e3a\u4e2d\u7b49(RWG 0.5\u52300.7)\u81f3\u9ad8\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u53d1\u73b0\u6574\u4f53\u7b14\u8bb0\u8d28\u91cf\u7565\u6709\u5dee\u5f02\uff0c\u5176\u4e2d\u9ec4\u91d1\u7b14\u8bb0\u5f97\u5206\u4e3a4.25\uff08\u6ee1\u52065\uff09\uff0c\u73af\u5883\u7b14\u8bb0\u5f97\u5206\u4e3a4.20\uff08p=0.04\uff09\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4f7f\u7528PDQI9\u5de5\u5177\u4f5c\u4e3a\u8bc4\u4f30LLM\u64b0\u5199\u7684\u4e34\u5e8a\u7b14\u8bb0\u8d28\u91cf\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u7b14\u8bb0\u8fdb\u884c\u6bd4\u8f83\u3002"}}
{"id": "2505.17127", "pdf": "https://arxiv.org/pdf/2505.17127", "abs": "https://arxiv.org/abs/2505.17127", "authors": ["Michal Golovanevsky", "William Rudman", "Michael Lepori", "Amir Bar", "Ritambhara Singh", "Carsten Eickhoff"], "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.", "AI": {"tldr": "\u7814\u7a76\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u63a8\u7406\u4f9d\u8d56\uff0c\u901a\u8fc7Visual CounterFact\u6570\u636e\u96c6\u53d1\u73b0\u89c6\u89c9\u8f93\u5165\u5728\u4e2d\u540e\u671f\u5360\u636e\u4e3b\u5bfc\uff0c\u5e76\u63d0\u51faPvP\u5f15\u5bfc\u5411\u91cf\u6709\u6548\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u63a8\u7406\u65f6\u662f\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u7684\u4e16\u754c\u77e5\u8bc6\u8fd8\u662f\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u5e76\u4f7f\u7528Visual CounterFact\u6570\u636e\u96c6\uff0c\u5206\u6790\u6a21\u578b\u9884\u6d4b\u7684\u53d8\u5316\uff0c\u5e76\u5f15\u5165Pixels Versus Priors (PvP) \u5f15\u5bfc\u5411\u91cf\u8fdb\u884c\u6fc0\u6d3b\u7ea7\u5e72\u9884\u4ee5\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002", "result": "PvP\u5f15\u5bfc\u5411\u91cf\u5e73\u5747\u6210\u529f\u5c0692.5%\u7684\u989c\u8272\u9884\u6d4b\u548c74.6%\u7684\u5927\u5c0f\u9884\u6d4b\u4ece\u8bb0\u5fc6\u7684\u5148\u9a8c\u77e5\u8bc6\u8f6c\u5411\u89c6\u89c9\u8f93\u5165\u7684\u53cd\u4e8b\u5b9e\u3002", "conclusion": "\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e2d\uff0c\u89c6\u89c9\u8f93\u5165\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u6700\u7ec8\u8986\u76d6\u4e86\u5148\u9a8c\u77e5\u8bc6\uff0c\u540c\u65f6PvP\u5f15\u5bfc\u5411\u91cf\u80fd\u591f\u6709\u6548\u63a7\u5236\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4f7f\u4e4b\u504f\u5411\u4e8e\u4e16\u754c\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\u3002"}}
{"id": "2505.17048", "pdf": "https://arxiv.org/pdf/2505.17048", "abs": "https://arxiv.org/abs/2505.17048", "authors": ["Agam Shah", "Siddhant Sukhani", "Huzaifa Pardawala", "Saketh Budideti", "Riya Bhadani", "Rudra Gopal", "Siddhartha Somani", "Michael Galarnyk", "Soungmin Lee", "Arnav Hiray", "Akshar Ravichandran", "Eric Kim", "Pranav Aluru", "Joshua Zhang", "Sebastian Jaskowski", "Veer Guda", "Meghaj Tarte", "Liqin Ye", "Spencer Gosden", "Rutwik Routu", "Rachel Yuh", "Sloka Chava", "Sahasra Chava", "Dylan Patrick Kelly", "Aiden Chiang", "Harsit Mittal", "Sudheer Chava"], "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "Central banks around the world play a crucial role in maintaining economic\nstability. Deciphering policy implications in their communications is\nessential, especially as misinterpretations can disproportionately impact\nvulnerable populations. To address this, we introduce the World Central Banks\n(WCB) dataset, the most comprehensive monetary policy corpus to date,\ncomprising over 380k sentences from 25 central banks across diverse geographic\nregions, spanning 28 years of historical data. After uniformly sampling 1k\nsentences per bank (25k total) across all available years, we annotate and\nreview each sentence using dual annotators, disagreement resolutions, and\nsecondary expert reviews. We define three tasks: Stance Detection, Temporal\nClassification, and Uncertainty Estimation, with each sentence annotated for\nall three. We benchmark seven Pretrained Language Models (PLMs) and nine Large\nLanguage Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on\nthese tasks, running 15,075 benchmarking experiments. We find that a model\ntrained on aggregated data across banks significantly surpasses a model trained\non an individual bank's data, confirming the principle \"the whole is greater\nthan the sum of its parts.\" Additionally, rigorous human evaluations, error\nanalyses, and predictive tasks validate our framework's economic utility. Our\nartifacts are accessible through the HuggingFace and GitHub under the\nCC-BY-NC-SA 4.0 license.", "AI": {"tldr": "\u5f15\u5165\u4e86WCB\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86380,000\u4e2a\u53e5\u5b50\u4ee5\u5f00\u5c55\u591a\u9879\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8de8\u94f6\u884c\u7684\u6570\u636e\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u5355\u94f6\u884c\u6570\u636e\u3002", "motivation": "\u8bef\u89e3\u4e2d\u592e\u94f6\u884c\u7684\u653f\u7b56\u53ef\u80fd\u5bf9\u5f31\u52bf\u7fa4\u4f53\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u51c6\u786e\u89e3\u8bfb\u5176\u901a\u4fe1\u5185\u5bb9\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u5e2e\u52a9\u7814\u7a76\u8fd9\u4e9b\u653f\u7b56\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u62ec380,000\u4e2a\u53e5\u5b50\u7684WCB\u6570\u636e\u96c6\uff0c\u91c7\u7528\u53cc\u4eba\u6807\u6ce8\u3001\u5206\u6b67\u89e3\u51b3\u548c\u4e13\u5bb6\u5ba1\u67e5\u5bf9\u6bcf\u4e2a\u53e5\u5b50\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u5b9a\u4e49\u4e86\u6001\u5ea6\u68c0\u6d4b\u3001\u65f6\u95f4\u5206\u7c7b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e09\u4e2a\u4efb\u52a1\u3002\u4f7f\u7528\u4e03\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u4e5d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u5728\u591a\u4e2a\u94f6\u884c\u6570\u636e\u4e0a\u7684\u6a21\u578b\u4f1a\u4f18\u4e8e\u53ea\u5728\u5355\u4e00\u94f6\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u7c7b\u8bc4\u4f30\u548c\u9519\u8bef\u5206\u6790\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7ecf\u6d4e\u6548\u7528\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u591a\u4e2a\u4e2d\u592e\u94f6\u884c\u7684\u6570\u636e\uff0c\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u663e\u8457\u8d85\u8fc7\u53ea\u901a\u8fc7\u5355\u4e2a\u94f6\u884c\u6570\u636e\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u3002"}}
{"id": "2505.17132", "pdf": "https://arxiv.org/pdf/2505.17132", "abs": "https://arxiv.org/abs/2505.17132", "authors": ["Tanqiu Jiang", "Jiacheng Liang", "Rongyi Zhu", "Jiawei Zhou", "Fenglong Ma", "Ting Wang"], "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)", "AI": {"tldr": "The paper introduces DTR, a novel defense optimizing KV caches to protect vision-language models from jailbreak attacks, outperforming existing defenses in robustness and performance.", "motivation": "To address the vulnerability of large vision-language models to jailbreak attacks that exploit visual-textual interactions, the paper aims to enhance safety without relying on curated safety-specific data or costly image-to-text conversion.", "method": "The paper proposes DTR, a defense mechanism that optimizes the model's key-value caches to mitigate multimodal jailbreak attacks by dynamically adjusting visual token weights.", "result": "DTR outperforms existing defenses in terms of attack robustness and benign task performance, demonstrating its effectiveness through extensive evaluation across various models and attack benchmarks.", "conclusion": "DTR significantly improves the robustness of vision-language models against jailbreak attacks while maintaining performance on benign tasks, marking a novel application of KV cache optimization for safety."}}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049", "abs": "https://arxiv.org/abs/2505.17049", "authors": ["David Rozado"], "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\u00e9sum\u00e9 Evaluations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u7b80\u5386\u8bc4\u4f30\u4e2d\u5bf9\u6027\u522b\u548c\u7b80\u5386\u987a\u5e8f\u6709\u504f\u597d\uff0c\u63d0\u793a\u5728\u81ea\u52a8\u5316\u51b3\u7b56\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u804c\u4e1a\u5019\u9009\u8005\u8bc4\u4f30\u4e2d\u7684\u6027\u522b\u504f\u597d\u95ee\u9898\uff0c\u4e86\u89e3\u5176\u662f\u5426\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u8fd0\u7528\u539f\u5219\u6027\u63a8\u7406\u3002", "method": "\u5229\u752822\u4e2aLLM\uff0c\u901a\u8fc7\u63d0\u4f9b\u804c\u4f4d\u63cf\u8ff0\u548c\u4e00\u5bf9\u804c\u914d\u7b80\u5386\u8fdb\u884c\u5b9e\u9a8c\u3002\u6bcf\u5bf9\u7b80\u5386\u5728\u4e24\u79cd\u540d\u5b57\u4e92\u6362\u7684\u60c5\u51b5\u4e0b\uff0c\u6bcf\u6b21\u63d0\u4f9b\u4e24\u6b21\uff0c\u4ee5\u89c2\u5bdf\u6027\u522b\u504f\u597d\u5bf9\u5019\u9009\u4eba\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u5973\u6027\u540d\u5b57\u7684\u7b80\u5386\u901a\u5e38\u88ab\u9009\u4e2d\uff0c\u8bf4\u660eLLMs\u5728\u8bc4\u4f30\u4e2d\u5b58\u5728\u6027\u522b\u504f\u597d\u3002\u6dfb\u52a0\u660e\u786e\u7684\u6027\u522b\u5b57\u6bb5\u540e\uff0c\u8fd9\u79cd\u504f\u597d\u66f4\u52a0\u5f3a\u70c8\u3002\u5f53\u4f7f\u7528\u6027\u522b\u4e2d\u7acb\u7684\u6807\u8bc6\u7b26\u65f6\uff0c\u504f\u597d\u6709\u6240\u51cf\u5c11\u4f46\u4ecd\u5b58\u5728\u3002\u7b80\u5386\u7684\u987a\u5e8f\u5bf9\u5019\u9009\u9009\u62e9\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u8bc4\u4f30\u6c42\u804c\u8005\u65f6\u5b58\u5728\u6027\u522b\u504f\u597d\uff0c\u5e76\u4e14\u5bf9\u804c\u4f4d\u4e0a\u5448\u73b0\u7684\u987a\u5e8f\u4e5f\u6709\u9009\u62e9\u504f\u5dee\u3002\u867d\u7136\u5973\u6027\u540d\u5b57\u7684\u7b80\u5386\u901a\u5e38\u88ab\u503e\u5411\u9009\u62e9\uff0c\u4f46\u6dfb\u52a0\u6027\u522b\u4fe1\u606f\u6216\u4f7f\u7528\u6027\u522b\u4e2d\u7acb\u7684\u8bc6\u522b\u7b26\u53ef\u4ee5\u6539\u53d8\u8fd9\u79cd\u7ed3\u679c\u3002\u8fd9\u8868\u660e\u5728\u81ea\u52a8\u5316\u51b3\u7b56\u4e2d\u4f7f\u7528LLMs\u65f6\u9700\u8981\u8c28\u614e\u3002"}}
{"id": "2505.17201", "pdf": "https://arxiv.org/pdf/2505.17201", "abs": "https://arxiv.org/abs/2505.17201", "authors": ["Chaim Chai Elchik", "Fatemeh Karimi Nejadasl", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking (MOT) in computer vision has made significant\nadvancements, yet tracking small fish in underwater environments presents\nunique challenges due to complex 3D motions and data noise. Traditional\nsingle-view MOT models often fall short in these settings. This thesis\naddresses these challenges by adapting state-of-the-art single-view MOT models,\nFairMOT and YOLOv8, for underwater fish detecting and tracking in ecological\nstudies. The core contribution of this research is the development of a\nmulti-view framework that utilizes stereo video inputs to enhance tracking\naccuracy and fish behavior pattern recognition. By integrating and evaluating\nthese models on underwater fish video datasets, the study aims to demonstrate\nsignificant improvements in precision and reliability compared to single-view\napproaches. The proposed framework detects fish entities with a relative\naccuracy of 47% and employs stereo-matching techniques to produce a novel 3D\noutput, providing a more comprehensive understanding of fish movements and\ninteractions", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u591a\u89c6\u89d2\u6846\u67b6\u6539\u8fdb\u4e86\u5355\u89c6\u56feMOT\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u76843D\u8fd0\u52a8\u548c\u6570\u636e\u566a\u58f0\uff0c\u4f20\u7edf\u7684\u5355\u89c6\u56feMOT\u6a21\u578b\u5728\u8ddf\u8e2a\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u9c7c\u65f6\u5f80\u5f80\u4e0d\u8db3\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6539\u8fdb\u6700\u5148\u8fdb\u7684\u5355\u89c6\u56feMOT\u6a21\u578bFairMOT\u548cYOLOv8\u6765\u9002\u5e94\u751f\u6001\u7814\u7a76\u4e2d\u6c34\u4e0b\u9c7c\u7c7b\u7684\u68c0\u6d4b\u548c\u8ddf\u8e2a\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u68c0\u6d4b\u9c7c\u7c7b\u5b9e\u4f53\u65f6\u5177\u6709\u76f8\u5bf947%\u7684\u7cbe\u786e\u5ea6\uff0c\u5e76\u901a\u8fc7\u7acb\u4f53\u5339\u914d\u6280\u672f\u751f\u6210\u65b0\u76843D\u8f93\u51fa\uff0c\u66f4\u5168\u9762\u5730\u7406\u89e3\u9c7c\u7c7b\u8fd0\u52a8\u548c\u4e92\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u63d0\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c7c\u7c7b\u884c\u4e3a\u6a21\u5f0f\u8bc6\u522b\uff0c\u4e0e\u5355\u89c6\u56fe\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050", "abs": "https://arxiv.org/abs/2505.17050", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPBLBench\uff0c\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u8bc4\u4f30MLLMs\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728PBLBench\u4e0a\u7684\u51c6\u786e\u7387\u4ec559%\uff0c\u663e\u793a\u51fa\u91cd\u5927\u6311\u6218\u3002PBLBench\u65e8\u5728\u4fc3\u8fdbAI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u4ee5\u51cf\u8f7b\u6559\u5e08\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u6559\u80b2\u6548\u7387\u3002", "motivation": "\u76ee\u524d\u73b0\u6709\u7684\u57fa\u51c6\u65e0\u6cd5\u63d0\u4f9b\u81ea\u7531\u5f62\u5f0f\u7684\u8f93\u51fa\u7ed3\u6784\u548c\u4e25\u683c\u7684\u4eba\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u6548\u529b\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6a21\u578b\u5e7b\u89c9\u548c\u4e0d\u7a33\u5b9a\u6027\uff0c\u7f3a\u5c11\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u6765\u5e2e\u52a9\u6559\u5e08\u7ba1\u7406MLLMs\u7684\u590d\u6742\u804c\u8d23\u3002\u56e0\u6b64\u5f62\u6210\u8fd9\u4e2a\u65b0\u57fa\u51c6\uff08PBLBench\uff09\uff0c\u4ee5\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5904\u7406\u4efb\u52a1\u9ad8\u5ea6\u76f8\u4f3c\u7684\u6a21\u578b\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u6211\u4eec\u91c7\u7528\u5c42\u6b21\u5206\u6790\u6cd5\uff08AHP\uff09\uff0c\u5229\u7528\u4e13\u5bb6\u9a71\u52a8\u7684\u6210\u5bf9\u6bd4\u8f83\u6765\u63a8\u5bfc\u7ed3\u6784\u5316\u548c\u52a0\u6743\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e3a\u5efa\u7acb\u53ef\u9760\u7684\u4e8b\u5b9e\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs/LLMs\u5728PBLBench\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5229\u7528PBLBench\u8bc4\u4f30\u4e8615\u4e2a\u5148\u8fdb\u7684MLLMs/LLMs\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u4ec5\u8fbe\u523059%\u7684\u6392\u540d\u51c6\u786e\u7387\uff0c\u8fd9\u7a81\u663e\u4e86\u8be5\u57fa\u51c6\u6240\u63d0\u51fa\u7684\u91cd\u5927\u6311\u6218\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86PBLBench\uff0c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u6765\u8bc4\u4f30\u590d\u6742\u63a8\u7406\u3002\u8fd9\u8868\u660e\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs/LLMs\u5728PBLBench\u4e0a\u7684\u8868\u73b0\u4ec5\u8fbe\u523059%\u7684\u6392\u540d\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u8fd9\u4e2a\u57fa\u51c6\u63d0\u51fa\u7684\u91cd\u5927\u6311\u6218\u3002\u6211\u4eec\u76f8\u4fe1\uff0cPBLBench\u5c06\u4fc3\u8fdb\u66f4\u5f3a\u5927\u7684AI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u6700\u7ec8\u65e8\u5728\u51cf\u8f7b\u6559\u5e08\u7684\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u6559\u80b2\u751f\u4ea7\u529b\u3002"}}
{"id": "2505.17223", "pdf": "https://arxiv.org/pdf/2505.17223", "abs": "https://arxiv.org/abs/2505.17223", "authors": ["Siyang Song", "Micol Spitale", "Xiangyu Kong", "Hengde Zhu", "Cheng Luo", "Cristina Palmero", "German Barquero", "Sergio Escalera", "Michel Valstar", "Mohamed Daoudi", "Tobias Baur", "Fabien Ringeval", "Andrew Howes", "Elisabeth Andre", "Hatice Gunes"], "title": "REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge", "categories": ["cs.CV", "68T40"], "comment": null, "summary": "In dyadic interactions, a broad spectrum of human facial reactions might be\nappropriate for responding to each human speaker behaviour. Following the\nsuccessful organisation of the REACT 2023 and REACT 2024 challenges, we are\nproposing the REACT 2025 challenge encouraging the development and benchmarking\nof Machine Learning (ML) models that can be used to generate multiple\nappropriate, diverse, realistic and synchronised human-style facial reactions\nexpressed by human listeners in response to an input stimulus (i.e.,\naudio-visual behaviours expressed by their corresponding speakers). As a key of\nthe challenge, we provide challenge participants with the first natural and\nlarge-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human\ndyadic interactions containing a total of 2856 interaction sessions covering\nfive different topics. In addition, this paper also presents the challenge\nguidelines and the performance of our baselines on the two proposed\nsub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge\nbaseline code is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2025", "AI": {"tldr": "REACT 2025\u6311\u6218\u4fc3\u8fdbML\u6a21\u578b\u751f\u6210\u591a\u6837\u548c\u540c\u6b65\u7684\u4eba\u7c7b\u9762\u90e8\u53cd\u5e94\uff0c\u65b0\u6570\u636e\u96c6MARS\u7528\u4e8e\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5728\u7075\u6d3b\u591a\u6837\u7684\u4eba\u7c7b\u4ea4\u4e92\u4e2d\uff0c\u5bf9\u6bcf\u79cd\u884c\u4e3a\u7684\u56de\u5e94\u90fd\u6709\u8bb8\u591a\u53ef\u80fd\u7684\u9762\u90e8\u53cd\u5e94\uff0c\u9f13\u52b1\u5f00\u53d1\u80fd\u591a\u6837\u5316\u54cd\u5e94\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u63d0\u4f9b\u4e00\u4e2a\u65b0\u7684\u81ea\u7136\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6(MARS)\uff0c\u5305\u62ec137\u6b21\u4eba\u7c7b\u4e92\u52a8\uff0c\u6db5\u76d62856\u4e2a\u4f1a\u8bdd\uff0c\u901a\u8fc7\u7ebf\u4e0b\u548c\u5728\u7ebfMAFRG\u7684\u57fa\u7ebf\u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u7ebf\u4ee3\u7801\uff0c\u5c55\u793a\u4e86\u79bb\u7ebf\u548c\u5728\u7ebfMAFRG\u5b50\u6311\u6218\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86REACT 2025\u6311\u6218\uff0c\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u3001\u771f\u5b9e\u4e14\u540c\u6b65\u7684\u4eba\u7c7b\u9762\u90e8\u53cd\u5e94\uff0c\u4ee5\u54cd\u5e94\u8f93\u5165\u523a\u6fc0\u3002"}}
{"id": "2505.17051", "pdf": "https://arxiv.org/pdf/2505.17051", "abs": "https://arxiv.org/abs/2505.17051", "authors": ["Bernd Huber", "Ghazal Fazelnia", "Andreas Damianou", "Sebastian Peleato", "Max Lefarov", "Praveen Ravichandran", "Marco De Nadai", "Mounia Lalmas-Roellke", "Paul N. Bennett"], "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating contextually relevant\ncontent. However, tailoring these outputs to individual users for effective\npersonalization is a significant challenge. While rich user-specific\ninformation often exists as pre-existing user representations, such as\nembeddings learned from preferences or behaviors, current methods to leverage\nthese for LLM personalization typically require costly fine-tuning or\ntoken-heavy prompting. We propose Embedding-to-Prefix (E2P), a\nparameter-efficient method that injects pre-computed context embeddings into an\nLLM's hidden representation space through a learned projection to a single soft\ntoken prefix. This enables effective personalization while keeping the backbone\nmodel frozen and avoiding expensive adaptation techniques. We evaluate E2P\nacross two public datasets and in a production setting: dialogue\npersonalization on Persona-Chat, contextual headline generation on PENS, and\nlarge-scale personalization for music and podcast consumption. Results show\nthat E2P preserves contextual signals and achieves strong performance with\nminimal computational overhead, offering a scalable, efficient solution for\ncontextualizing generative AI systems.", "AI": {"tldr": "E2P efficiently personalizes LLMs using pre-computed embeddings without modifying the main model, proving effective and computationally light across multiple contexts.", "motivation": "The motivation is to overcome the challenges of personalizing LLMs by using existing user-specific information like embeddings without incurring the high costs of fine-tuning or extensive prompting.", "method": "The method, Embedding-to-Prefix (E2P), injects pre-computed context embeddings into the LLM's hidden representation through a learned projection to a soft token prefix. This approach allows for personalization without modifying the backbone model.", "result": "E2P is effective in maintaining contextual signals and performs well on publicly available datasets like Persona-Chat and PENS, as well as in large-scale settings such as music and podcast personalization, all with minimal computational cost.", "conclusion": "E2P method provides a scalable and efficient solution for personalizing LLM-based systems by embedding user-specific context without costly model adaptations."}}
{"id": "2505.17235", "pdf": "https://arxiv.org/pdf/2505.17235", "abs": "https://arxiv.org/abs/2505.17235", "authors": ["Omar Moured", "Yufan Chen", "Ruiping Liu", "Simon Rei\u00df", "Philip Torr", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "CHAOS: Chart Analysis with Outlier Samples", "categories": ["cs.CV", "cs.CL"], "comment": "Data and code are publicly available at:\n  http://huggingface.co/datasets/omoured/CHAOS", "summary": "Charts play a critical role in data analysis and visualization, yet\nreal-world applications often present charts with challenging or noisy\nfeatures. However, \"outlier charts\" pose a substantial challenge even for\nMultimodal Large Language Models (MLLMs), which can struggle to interpret\nperturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier\nSamples), a robustness benchmark to systematically evaluate MLLMs against chart\nperturbations. CHAOS encompasses five types of textual and ten types of visual\nperturbations, each presented at three levels of severity (easy, mid, hard)\ninspired by the study result of human evaluation. The benchmark includes 13\nstate-of-the-art MLLMs divided into three groups (i.e., general-, document-,\nand chart-specific models) according to the training scope and data.\nComprehensive analysis involves two downstream tasks (ChartQA and\nChart-to-Text). Extensive experiments and case studies highlight critical\ninsights into robustness of models across chart perturbations, aiming to guide\nfuture research in chart understanding domain. Data and code are publicly\navailable at: http://huggingface.co/datasets/omoured/CHAOS.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165 CHAOS \u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f02\u5e38\u56fe\u8868\u6270\u52a8\u4e0b\u7684\u7a33\u5065\u6027\uff0c\u53d1\u73b0\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u4e0d\u540c\u6270\u52a8\u65f6\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u56fe\u8868\u901a\u5e38\u5177\u6709\u6311\u6218\u6027\u6216\u566a\u58f0\u7279\u5f81\uff0c\u5c24\u5176\u662f\u201c\u5f02\u5e38\u56fe\u8868\u201d\u5bf9\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7 CHAOS \u57fa\u51c6\u7cfb\u7edf\u5730\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u7a33\u5065\u6027\u3002", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a CHAOS \u7684\u7a33\u5065\u6027\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002CHAOS \u5305\u542b 5 \u79cd\u6587\u672c\u548c 10 \u79cd\u89c6\u89c9\u6270\u52a8\uff0c\u6bcf\u79cd\u6270\u52a8\u5747\u6709 3 \u4e2a\u4e0d\u540c\u7684\u4e25\u91cd\u7b49\u7ea7\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u5f97\u51fa\u5173\u4e8e\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u4e0b\u7684\u5173\u952e\u7a33\u5065\u6027\u89c1\u89e3\u3002\u8fd9\u4e9b\u89c1\u89e3\u65e8\u5728\u6307\u5bfc\u672a\u6765\u56fe\u8868\u7406\u89e3\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408\u5206\u6790\uff0c\u7814\u7a76\u8868\u660e\u5f53\u9762\u5bf9\u56fe\u8868\u6270\u52a8\u65f6\uff0c\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u7a33\u5065\u6027\u5dee\u5f02\u663e\u8457\u3002\u7814\u7a76\u63d0\u4f9b\u7684\u57fa\u51c6\u6709\u52a9\u4e8e\u672a\u6765\u56fe\u8868\u7406\u89e3\u9886\u57df\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2505.17052", "pdf": "https://arxiv.org/pdf/2505.17052", "abs": "https://arxiv.org/abs/2505.17052", "authors": ["Jinwoo Park", "Seunggeun Cho", "Dongsu Han"], "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) power many modern applications, but serving them\nat scale remains costly and resource-intensive. Current server-centric systems\noverlook consumer-grade GPUs at the edge. We introduce SpecEdge, an\nedge-assisted inference framework that splits LLM workloads between edge and\nserver GPUs using a speculative decoding scheme, exchanging only token outputs\nover the network. SpecEdge employs proactive edge drafting to overlap edge\ntoken creation with server verification and pipeline-aware scheduling that\ninterleaves multiple user requests to increase server-side throughput.\nExperiments show SpecEdge enhances overall cost efficiency by 1.91x through\nachieving 2.22x server throughput, and reduces inter token latency by 11.24%\ncompared to a server-only baseline, introducing a scalable, cost-effective\nparadigm for LLM serving.", "AI": {"tldr": "SpecEdge, an edge-assisted inference framework, splits LLM workloads to utilize edge GPUs, enhancing cost efficiency and server throughput for scalable large language model serving.", "motivation": "The motivation behind this paper is to reduce the cost and resource intensity of serving large language models at scale by utilizing consumer-grade GPUs at the edge, which are often overlooked.", "method": "The method involves splitting LLM workloads between edge and server GPUs using a speculative decoding scheme, proactive edge drafting, and pipeline-aware scheduling to improve cost efficiency and throughput.", "result": "Experiments demonstrate that SpecEdge enhances overall cost efficiency by 1.91 times, achieves 2.22 times server throughput, and reduces inter-token latency by 11.24% compared to a server-only baseline.", "conclusion": "SpecEdge introduces a scalable and cost-effective paradigm for serving large language models by efficiently utilizing edge and server resources."}}
{"id": "2505.17245", "pdf": "https://arxiv.org/pdf/2505.17245", "abs": "https://arxiv.org/abs/2505.17245", "authors": ["Ryota Yagi"], "title": "Extending Dataset Pruning to Object Detection: A Variance-based Approach", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset pruning -- selecting a small yet informative subset of training data\n-- has emerged as a promising strategy for efficient machine learning, offering\nsignificant reductions in computational cost and storage compared to\nalternatives like dataset distillation. While pruning methods have shown strong\nperformance in image classification, their extension to more complex computer\nvision tasks, particularly object detection, remains relatively underexplored.\nIn this paper, we present the first principled extension of classification\npruning techniques to the object detection domain, to the best of our\nknowledge. We identify and address three key challenges that hinder this\ntransition: the Object-Level Attribution Problem, the Scoring Strategy Problem,\nand the Image-Level Aggregation Problem. To overcome these, we propose tailored\nsolutions, including a novel scoring method called Variance-based Prediction\nScore (VPS). VPS leverages both Intersection over Union (IoU) and confidence\nscores to effectively identify informative training samples specific to\ndetection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate\nthat our approach consistently outperforms prior dataset pruning methods in\nterms of mean Average Precision (mAP). We also show that annotation count and\nclass distribution shift can influence detection performance, but selecting\ninformative examples is a more critical factor than dataset size or balance.\nOur work bridges dataset pruning and object detection, paving the way for\ndataset pruning in complex vision tasks.", "AI": {"tldr": "The paper extends dataset pruning to object detection, proposes a novel scoring method, and shows improved performance over previous methods on benchmark datasets.", "motivation": "Dataset pruning has shown promise in efficient machine learning but its application to complex computer vision tasks, like object detection, is underexplored. This paper aims to extend pruning techniques from image classification to object detection.", "method": "We proposed tailored solutions for dataset pruning in object detection, including a novel scoring method named Variance-based Prediction Score (VPS) that uses Intersection over Union (IoU) and confidence scores.", "result": "Our approach consistently outperforms prior dataset pruning methods in terms of mean Average Precision (mAP) on PASCAL VOC and MS COCO datasets.", "conclusion": "Our work bridges dataset pruning and object detection, paving the way for dataset pruning in complex vision tasks."}}
{"id": "2505.17053", "pdf": "https://arxiv.org/pdf/2505.17053", "abs": "https://arxiv.org/abs/2505.17053", "authors": ["Ou Jiamin", "Eikmans Emile", "Buskens Vincent", "Pankowska Paulina", "Shan Yuli"], "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures, 2 tables", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin understanding human languages, this study explores how they translate this\nunderstanding into social exchange contexts that capture certain essences of\nreal world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were\nplaced in economic trust games where players balance self-interest with trust\nand reciprocity, making decisions that reveal their social preferences and\ninteractive reasoning abilities. Our study shows that LLMs deviate from pure\nself-interest and exhibit trust and reciprocity even without being prompted to\nadopt a specific persona. In the simplest one-shot interaction, LLMs emulated\nhow human players place trust at the beginning of such a game. Larger\nhuman-machine divergences emerged in scenarios involving trust repayment or\nmulti-round interactions, where decisions were influenced by both social\npreferences and interactive reasoning. LLMs responses varied significantly when\nprompted to adopt personas like selfish or unselfish players, with the impact\noutweighing differences between models or game types. Response of ChatGPT-4, in\nan unselfish or neutral persona, resembled the highest trust and reciprocity,\nsurpassing humans, Claude, and Bard. Claude and Bard displayed trust and\nreciprocity levels that sometimes exceeded and sometimes fell below human\nchoices. When given selfish personas, all LLMs showed lower trust and\nreciprocity than humans. Interactive reasoning to the actions of counterparts\nor changing game mechanics appeared to be random rather than stable,\nreproducible characteristics in the response of LLMs, though some improvements\nwere observed when ChatGPT-4 responded in selfish or unselfish personas.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLMs\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u8d85\u8d8a\u81ea\u6211\u5229\u76ca\u7684\u884c\u4e3a\uff0c\u53d1\u73b0ChatGPT-4\u5728\u7279\u5b9a\u89d2\u8272\u8868\u73b0\u4e0b\u8d85\u8d8a\u4eba\u7c7b\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\u3002", "motivation": "\u63a2\u7a76LLMs\u5982\u4f55\u5728\u793e\u4ea4\u4e92\u52a8\u4e0a\u4e0b\u6587\u4e2d\u5e94\u7528\u5176\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5c24\u5176\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u5c55\u73b0\u51fa\u7684\u793e\u4ea4\u504f\u597d\u548c\u4e92\u52a8\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06LLMs\u7f6e\u4e8e\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4e92\u52a8\u7684\u573a\u666f\uff0c\u89c2\u5bdf\u5176\u5728\u4fe1\u4efb\u548c\u4e92\u60e0\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "ChatGPT-4\u5728\u65e0\u79c1\u6216\u4e2d\u7acb\u89d2\u8272\u4e0b\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\uff0c\u8d85\u8fc7\u4eba\u7c7b\u3001Claude\u548cBard\u7684\u8868\u73b0\u3002Claude\u548cBard\u5219\u5728\u4e0d\u540c\u60c5\u51b5\u4e0b\u5176\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\u6709\u65f6\u8d85\u51fa\uff0c\u6709\u65f6\u4f4e\u4e8e\u4eba\u7c7b\u9009\u62e9\u3002\u8d4b\u4e88\u81ea\u79c1\u89d2\u8272\u65f6\uff0c\u6240\u6709LLMs\u5747\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u4f4e\u7684\u4fe1\u4efb\u548c\u4e92\u60e0\u3002", "conclusion": "\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\uff0cLLMs\u5c55\u73b0\u51fa\u8d85\u8d8a\u7eaf\u7cb9\u81ea\u6211\u5229\u76ca\u7684\u884c\u4e3a\uff0c\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\uff0c\u5728\u6700\u7b80\u5355\u7684\u4e00\u6b21\u6027\u4e92\u52a8\u4e2d\u6a21\u62df\u4eba\u7c7b\u73a9\u5bb6\u7684\u4fe1\u4efb\u51b3\u7b56\u3002\u5728\u6d89\u53ca\u4fe1\u4efb\u507f\u8fd8\u6216\u591a\u8f6e\u4e92\u52a8\u7684\u573a\u666f\u4e2d\u51fa\u73b0\u66f4\u5927\u7684\u4eba\u673a\u504f\u5dee\u3002"}}
{"id": "2505.17256", "pdf": "https://arxiv.org/pdf/2505.17256", "abs": "https://arxiv.org/abs/2505.17256", "authors": ["Liang Shi", "Yun Fu"], "title": "ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved text-to-face\ngeneration, but achieving fine-grained control over facial features remains a\nchallenge. Existing methods often require training additional modules to handle\nspecific controls such as identity, attributes, or age, making them inflexible\nand resource-intensive. We propose ExpertGen, a training-free framework that\nleverages pre-trained expert models such as face recognition, facial attribute\nrecognition, and age estimation networks to guide generation with fine control.\nOur approach uses a latent consistency model to ensure realistic and\nin-distribution predictions at each diffusion step, enabling accurate guidance\nsignals to effectively steer the diffusion process. We show qualitatively and\nquantitatively that expert models can guide the generation process with high\nprecision, and multiple experts can collaborate to enable simultaneous control\nover diverse facial aspects. By allowing direct integration of off-the-shelf\nexpert models, our method transforms any such model into a plug-and-play\ncomponent for controllable face generation.", "AI": {"tldr": "ExpertGen\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7684\u53ef\u63a7\u4eba\u8138\u751f\u6210\uff0c\u901a\u8fc7\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u5f15\u5bfc\uff0c\u5e76\u652f\u6301\u591a\u4e13\u5bb6\u534f\u4f5c\u63a7\u5236\u591a\u4e2a\u9762\u90e8\u7279\u5f81\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u9762\u90e8\u751f\u6210\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u63a7\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5bf9\u7279\u5b9a\u63a7\u5236\u6a21\u5757\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u5e76\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u5728\u6bcf\u4e2a\u6269\u6563\u6b65\u9aa4\u4e2d\u5b9e\u73b0\u771f\u5b9e\u4e14\u7b26\u5408\u5206\u5e03\u7684\u9884\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u7cbe\u786e\u5f15\u5bfc\u3002\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u5982\u4eba\u8138\u8bc6\u522b\u3001\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u548c\u5e74\u9f84\u4f30\u8ba1\u7f51\u7edc\u8fdb\u884c\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e13\u5bb6\u6a21\u578b\u80fd\u591f\u4ee5\u9ad8\u7cbe\u5ea6\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u4e14\u591a\u4e2a\u4e13\u5bb6\u53ef\u4ee5\u534f\u4f5c\uff0c\u5b9e\u73b0\u5bf9\u591a\u4e2a\u9762\u90e8\u7279\u5f81\u7684\u540c\u65f6\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684ExpertGen\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u9762\u90e8\u751f\u6210\u7684\u7cbe\u786e\u5ea6\uff0c\u901a\u8fc7\u6574\u5408\u79bb\u7ebf\u4e13\u5bb6\u6a21\u578b\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7684\u53ef\u63a7\u4eba\u8138\u751f\u6210\u3002"}}
{"id": "2505.17054", "pdf": "https://arxiv.org/pdf/2505.17054", "abs": "https://arxiv.org/abs/2505.17054", "authors": ["Linglong Qian", "Zina Ibrahim"], "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.", "AI": {"tldr": "This paper introduces METHOD, a transformer architecture for clinical sequence modeling in EHRs, outperforming the ETHOS model, capturing complex clinical data effectively.", "motivation": "Applying transformer architectures to healthcare domains presents unique challenges due to patient timelines being characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships different from traditional language tasks.", "method": "\\METHOD integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing.", "result": "Evaluations on the MIMIC-IV database demonstrate that \\METHOD consistently outperforms the state-of-the-art \\ETHOS model, particularly in predicting high-severity cases that require urgent clinical intervention. It exhibits stable performance across varying inference lengths, and better preserves clinical hierarchies and relationships between medical concepts.", "conclusion": "\\METHOD represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency."}}
{"id": "2505.17280", "pdf": "https://arxiv.org/pdf/2505.17280", "abs": "https://arxiv.org/abs/2505.17280", "authors": ["Pushkar Shukla", "Aditya Chinchure", "Emily Diana", "Alexander Tolbert", "Kartik Hosanagar", "Vineeth N Balasubramanian", "Leonid Sigal", "Matthew Turk"], "title": "Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "The biases exhibited by text-to-image (TTI) models are often treated as\nindependent, though in reality, they may be deeply interrelated. Addressing\nbias along one dimension - such as ethnicity or age - can inadvertently affect\nanother, like gender, either mitigating or exacerbating existing disparities.\nUnderstanding these interdependencies is crucial for designing fairer\ngenerative models, yet measuring such effects quantitatively remains a\nchallenge. To address this, we introduce BiasConnect, a novel tool for\nanalyzing and quantifying bias interactions in TTI models. BiasConnect uses\ncounterfactual interventions along different bias axes to reveal the underlying\nstructure of these interactions and estimates the effect of mitigating one bias\naxis on another. These estimates show strong correlation (+0.65) with observed\npost-mitigation outcomes. Building on BiasConnect, we propose InterMit, an\nintersectional bias mitigation algorithm guided by user-defined target\ndistributions and priority weights. InterMit achieves lower bias (0.33 vs.\n0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields\nsuperior image quality compared to traditional techniques. Although our\nimplementation is training-free, InterMit is modular and can be integrated with\nmany existing debiasing approaches for TTI models, making it a flexible and\nextensible solution.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faBiasConnect\u5de5\u5177\u5206\u6790TTI\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u4ea4\u4e92\uff0cInterMit\u7b97\u6cd5\u6709\u6548\u51cf\u504f\u5e76\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u7406\u89e3\u504f\u5dee\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u662f\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5b9a\u91cf\u5206\u6790\u8fd9\u4e9b\u5f71\u54cd\u3002", "method": "\u5f15\u5165BiasConnect\u5de5\u5177\u8fdb\u884c\u53cd\u4e8b\u5b9e\u5e72\u9884\u5206\u6790\u504f\u5dee\u4ea4\u4e92\uff0c\u63d0\u51faInterMit\u7b97\u6cd5\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u76ee\u6807\u5206\u5e03\u548c\u4f18\u5148\u6743\u91cd\u6765\u6307\u5bfc\u504f\u5dee\u7f13\u89e3\u3002", "result": "InterMit\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u504f\u5dee\uff080.33\u4e0e0.52\u76f8\u6bd4\u8f83\uff09\uff0c\u6240\u9700\u7f13\u89e3\u6b65\u6570\u66f4\u5c11\uff08\u5e73\u5747\u6b65\u65702.38 vs. 3.15\uff09\uff0c\u5e76\u63d0\u4f9b\u6bd4\u4f20\u7edf\u6280\u672f\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002", "conclusion": "InterMit\u80fd\u6709\u6548\u964d\u4f4e\u504f\u5dee\uff0c\u540c\u65f6\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002BiasConnect\u80fd\u591f\u4f30\u7b97\u504f\u5dee\u4ea4\u4e92\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u89e3\u51b3\u504f\u5dee\u540e\u7684\u7ed3\u679c\u8868\u73b0\u51fa\u5f3a\u76f8\u5173\u6027\u3002"}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055", "abs": "https://arxiv.org/abs/2505.17055", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "\u5229\u7528\u5148\u8fdbAI\u6280\u672f\u5f00\u53d1\u4e86PSL\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u5efa41\u4e2a\u624b\u52bf\u7c7b\u522b\u7684\u6570\u636e\u96c6\u548c\u5fae\u8c03ViT\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8697.59%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\u53ef\u53ca\u6027\u3002", "motivation": "\u6839\u636e\u542c\u969c\u5b66\u751f\u6570\u5b66\u6559\u80b2\u96be\u4ee5\u83b7\u5f97\u7684\u73b0\u72b6\uff0c\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u7cbe\u786e\u7684\u5df4\u52d2\u65af\u5766\u624b\u8bed(PSL)\u8bc6\u522b\u7cfb\u7edf\u6765\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u7279\u522b\u662f\u5fae\u8c03\u7684Vision Transformer (ViT) \u6a21\u578b\u8fdb\u884c\u624b\u52bf\u5206\u7c7b\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b41\u4e2a\u6570\u5b66\u624b\u52bf\u7c7b\u522b\u7684\u5b9a\u5236\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u8fbe\u5230\u4e8697.59%\u7684\u51c6\u786e\u7387\uff0c\u6709\u6548\u8bc6\u522b\u6570\u5b66\u7b26\u53f7\uff0c\u4fdd\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684PSL\u8bc6\u522b\u7cfb\u7edf\u6709\u6548\u5730\u8bc6\u522b\u6570\u5b66\u624b\u52bf\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5f00\u53d1\u667a\u80fd\u6559\u80b2\u5de5\u5177\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u80fd\u591f\u4e3a\u542c\u969c\u5b66\u751f\u63d0\u4f9bAI\u9a71\u52a8\u7684\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\u4ee5\u589e\u5f3a\u6570\u5b66\u7406\u89e3\u3002"}}
{"id": "2505.17311", "pdf": "https://arxiv.org/pdf/2505.17311", "abs": "https://arxiv.org/abs/2505.17311", "authors": ["Harim Kim", "Yuhan Wang", "Minkyu Ahn", "Heeyoul Choi", "Yuyin Zhou", "Charmgil Hong"], "title": "Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI 2025 early accept", "summary": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for\nidentifying pathological abnormalities without requiring extensive labeled\ndata. However, existing diffusion-based UAD models rely solely on imaging\nfeatures, limiting their ability to distinguish between normal anatomical\nvariations and pathological anomalies. To address this, we propose Diff3M, a\nmulti-modal diffusion-based framework that integrates chest X-rays and\nstructured Electronic Health Records (EHRs) for enhanced anomaly detection.\nSpecifically, we introduce a novel image-EHR cross-attention module to\nincorporate structured clinical context into the image generation process,\nimproving the model's ability to differentiate normal from abnormal features.\nAdditionally, we develop a static masking strategy to enhance the\nreconstruction of normal-like images from anomalies. Extensive evaluations on\nCheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art\nperformance, outperforming existing UAD methods in medical imaging. Our code is\navailable at this http URL https://github.com/nth221/Diff3M", "AI": {"tldr": "\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408X\u5149\u5f71\u50cf\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u591a\u6a21\u6001\u6269\u6563\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38\u7279\u5f81\u65f6\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u4ec5\u4f9d\u8d56\u4e8e\u6210\u50cf\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u533a\u5206\u6b63\u5e38\u89e3\u5256\u53d8\u5f02\u548c\u75c5\u7406\u5f02\u5e38\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u7684\u6846\u67b6\u4ee5\u6539\u5584\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf-EHR\u4ea4\u53c9\u6ce8\u610f\u6a21\u5757\uff0c\u5c06\u7ed3\u6784\u5316\u4e34\u5e8a\u4e0a\u4e0b\u6587\u7ed3\u5408\u5230\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u9759\u6001\u906e\u853d\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u4ece\u5f02\u5e38\u4e2d\u91cd\u5efa\u6b63\u5e38\u56fe\u50cf\u7684\u80fd\u529b\u3002", "result": "\u5728CheXpert\u548cMIMIC-CXR/IV\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cDiff3M\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6b64\u7814\u7a76\u63d0\u51fa\u4e86Diff3M\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u80f8\u90e8X\u5c04\u7ebf\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8fdb\u884c\u589e\u5f3a\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u80cc\u666f\u4fe1\u606f\u4e0e\u5f71\u50cf\u751f\u6210\uff0c\u63d0\u5347\u6a21\u578b\u5728\u8bc6\u522b\u6b63\u5e38\u4e0e\u5f02\u5e38\u7279\u5f81\u4e0a\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728CheXpert\u548cMIMIC-CXR/IV\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2505.17056", "pdf": "https://arxiv.org/pdf/2505.17056", "abs": "https://arxiv.org/abs/2505.17056", "authors": ["Luoxi Tang", "Tharunya Sundar", "Shuai Yang", "Ankita Patra", "Manohar Chippada", "Giqi Zhao", "Yi Li", "Riteng Zhang", "Tunan Zhao", "Ting Yang", "Yuqiao Meng", "Weicheng Ma", "Zhaohan Xi"], "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.", "AI": {"tldr": "The study evaluates LLMs' capabilities in standardized test preparation using a benchmark called ESTBOOK, revealing insights and strategies for their use in educational contexts.", "motivation": "To investigate LLMs' potential in supporting standardized test preparation and enhancing educational experiences.", "method": "The study introduces ESTBOOK, a benchmark aggregating various standardized tests to systematically evaluate LLMs' accuracy and inference efficiency.", "result": "The evaluation shows insights into LLMs' abilities and suggests strategies for improving their reliability in intelligent tutoring systems.", "conclusion": "LLMs exhibit promising potential to enhance educational contexts, specifically in standardized test preparation."}}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6539\u5584\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u597d\u5730\u5bf9\u9f50\u89c6\u89c9\u5d4c\u5165\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u6539\u5584\u8fd9\u79cd\u5bf9\u9f50\u3002", "method": "\u7814\u7a76\u4e2d\u63d0\u51fa\u4e86\u201cpatch-aligned training\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd9\u4e00\u65b9\u6cd5\u589e\u5f3a\u89c6\u89c9\u788e\u7247\u4e0e\u8bed\u4e49\u8bcd\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u5f97\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u79f0\u8868\u8fbe\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u63d0\u5347\u4e8616%\uff0c\u5728\u95ee\u7b54\u4efb\u52a1\u4e0a\u63d0\u53474%\uff0c\u5728\u73b0\u4ee3\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u4e0a\u63d0\u53473%\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u89c6\u89c9\u5d4c\u5165\u4e0e\u8bcd\u5d4c\u5165\u7684\u5bf9\u9f50\u65b9\u5f0f\uff0c\u5c24\u5176\u662f\u91c7\u7528\u201cpatch-aligned training\u201d\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.17058", "pdf": "https://arxiv.org/pdf/2505.17058", "abs": "https://arxiv.org/abs/2505.17058", "authors": ["David Osei Opoku", "Ming Sheng", "Yong Zhang"], "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 5 figures;", "summary": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.", "AI": {"tldr": "DO-RAG\u662f\u4e00\u79cd\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u68c0\u7d22\u7684\u9ad8\u6548\u95ee\u7b54\u6846\u67b6\uff0c\u80fd\u63d0\u9ad8\u7cbe\u51c6\u5ea6\u53ca\u7b54\u6848\u76f8\u5173\u6027\u3002", "motivation": "\u63d0\u9ad8\u9886\u57df\u4e13\u5c5e\u95ee\u7b54\u7cfb\u7edf\u7684\u751f\u6210\u6d41\u7545\u6027\u53ca\u57fa\u4e8e\u7ed3\u6784\u5316\u4e13\u5bb6\u77e5\u8bc6\u7684\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u96c6\u6210\u591a\u7ea7\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e0e\u8bed\u4e49\u5411\u91cf\u68c0\u7d22\u7684\u6df7\u5408\u95ee\u7b54\u6846\u67b6\u3002", "result": "\u5728\u6570\u636e\u5e93\u548c\u7535\u529b\u9886\u57df\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cDO-RAG\u51e0\u4e4e\u8fbe\u5230\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\u548c\u8d85\u8fc794%\u7684\u7b54\u6848\u76f8\u5173\u6027\uff0c\u6bd4\u57fa\u51c6\u6846\u67b6\u63d0\u9ad8\u4e86\u6700\u591a33.38%\u3002", "conclusion": "DO-RAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u591a\u9886\u57df\u9ad8\u7cbe\u5ea6\u95ee\u7b54\u6846\u67b6\uff0c\u5177\u5907\u53ef\u8ffd\u6eaf\u6027\u3001\u9002\u5e94\u6027\u548c\u6027\u80fd\u6548\u7387\u3002"}}
{"id": "2505.17317", "pdf": "https://arxiv.org/pdf/2505.17317", "abs": "https://arxiv.org/abs/2505.17317", "authors": ["Alyson East", "Elizabeth G. Campolongo", "Luke Meyers", "S M Rayeed", "Samuel Stevens", "Iuliia Zarubiieva", "Isadora E. Fluck", "Jennifer C. Gir\u00f3n", "Maximiliane Jousse", "Scott Lowe", "Kayla I Perry", "Isabelle Betancourt", "Noah Charney", "Evan Donoso", "Nathan Fox", "Kim J. Landsbergen", "Ekaterina Nepovinnykh", "Michelle Ramirez", "Parkash Singh", "Khum Thapa-Magar", "Matthew Thompson", "Evan Waite", "Tanya Berger-Wolf", "Hilmar Lapp", "Paula Mabee", "Graham Taylor", "Sydne Record"], "title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Biological collections house millions of specimens documenting Earth's\nbiodiversity, with digital images increasingly available through open-access\nplatforms. Most imaging protocols were developed for human visual\ninterpretation without considering computational analysis requirements. This\npaper aims to bridge the gap between current imaging practices and the\npotential for automated analysis by presenting key considerations for creating\nbiological specimen images optimized for computer vision applications. We\nprovide conceptual computer vision topics for context, addressing fundamental\nconcerns including model generalization, data leakage, and comprehensive\nmetadata documentation, and outline practical guidance on specimen imagine, and\ndata storage. These recommendations were synthesized through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists. Through this synthesis, we have identified ten\ninterconnected considerations that form a framework for successfully\nintegrating biological specimen images into computer vision pipelines. The key\nelements include: (1) comprehensive metadata documentation, (2) standardized\nspecimen positioning, (3) consistent size and color calibration, (4) protocols\nfor handling multiple specimens in one image, (5) uniform background selection,\n(6) controlled lighting, (7) appropriate resolution and magnification, (8)\noptimal file formats, (9) robust data archiving strategies, and (10) accessible\ndata sharing practices. By implementing these recommendations, collection\nmanagers, taxonomists, and biodiversity informaticians can generate images that\nsupport automated trait extraction, species identification, and novel\necological and evolutionary analyses at unprecedented scales. Successful\nimplementation lies in thorough documentation of methodological choices.", "AI": {"tldr": "\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u5341\u9879\u5efa\u8bae\uff0c\u4ee5\u4f18\u5316\u751f\u7269\u6807\u672c\u56fe\u50cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\uff0c\u4fc3\u8fdb\u81ea\u52a8\u5316\u7279\u5f81\u63d0\u53d6\u548c\u7269\u79cd\u8bc6\u522b\u7b49\u5206\u6790\u3002", "motivation": "\u4e3a\u7f29\u5c0f\u5f53\u524d\u6210\u50cf\u5b9e\u8df5\u4e0e\u81ea\u52a8\u5316\u5206\u6790\u6f5c\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u751f\u7269\u6807\u672c\u56fe\u50cf\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u3002\u56e0\u4e3a\u5927\u591a\u6570\u6210\u50cf\u534f\u8bae\u662f\u4e3a\u4eba\u7c7b\u89c6\u89c9\u89e3\u8bfb\u800c\u5f00\u53d1\u7684\uff0c\u6ca1\u6709\u8003\u8651\u5230\u8ba1\u7b97\u5206\u6790\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u7684\u5408\u4f5c\uff08\u5305\u62ec\u5206\u7c7b\u5b66\u5bb6\u3001\u6536\u85cf\u7ba1\u7406\u8005\u3001\u751f\u6001\u5b66\u5bb6\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\uff09\uff0c\u8fdb\u884c\u4e86\u5efa\u8bae\u7684\u7efc\u5408\uff0c\u63d0\u51fa\u4e86\u521b\u5efa\u9002\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u751f\u7269\u6807\u672c\u56fe\u50cf\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\u3002", "result": "\u5236\u5b9a\u4e86\u5305\u542b\u5341\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8003\u8651\u56e0\u7d20\u7684\u6846\u67b6\uff0c\u4ee5\u6210\u529f\u5c06\u751f\u7269\u6807\u672c\u56fe\u50cf\u6574\u5408\u5230\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u65bd\u8fd9\u4e9b\u5efa\u8bae\uff0c\u6536\u85cf\u7ba1\u7406\u8005\u3001\u5206\u7c7b\u5b66\u5bb6\u548c\u751f\u7269\u591a\u6837\u6027\u4fe1\u606f\u5b66\u5bb6\u53ef\u4ee5\u751f\u6210\u652f\u6301\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u3001\u7269\u79cd\u8bc6\u522b\u548c\u524d\u6240\u672a\u6709\u89c4\u6a21\u7684\u65b0\u751f\u6001\u548c\u8fdb\u5316\u5206\u6790\u7684\u56fe\u50cf\u3002\u6210\u529f\u5b9e\u65bd\u7684\u5173\u952e\u5728\u4e8e\u5bf9\u65b9\u6cd5\u9009\u62e9\u8fdb\u884c\u5f7b\u5e95\u8bb0\u5f55\u3002"}}
{"id": "2505.17059", "pdf": "https://arxiv.org/pdf/2505.17059", "abs": "https://arxiv.org/abs/2505.17059", "authors": ["Van-Tinh Nguyen", "Hoang-Duong Pham", "Thanh-Hai To", "Cong-Tuan Hung Do", "Thi-Thu-Trang Dong", "Vu-Trung Duong Le", "Van-Phuc Hoang"], "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary\n  version posted for early dissemination and feedback", "summary": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.", "AI": {"tldr": "Medalyze\u662f\u4e00\u4e2a\u7528\u4e8e\u589e\u5f3a\u533b\u5b66\u6587\u672c\u7406\u89e3\u7684AI\u5e94\u7528\uff0c\u4f7f\u7528\u4e09\u4e2a\u4e13\u95e8\u7684FLAN-T5-Large\u6a21\u578b\uff0c\u88ab\u8bc1\u660e\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e0a\u6027\u80fd\u4f18\u4e8eGPT-4\u3002", "motivation": "\u7531\u4e8e\u533b\u5b66\u6587\u672c\u7684\u590d\u6742\u672f\u8bed\u548c\u7279\u5b9a\u8bed\u5883\uff0c\u4f7f\u5f97\u7406\u89e3\u533b\u5b66\u6587\u672c\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u8bbe\u8ba1\u4e86Medalyze\u5e94\u7528\u4ee5\u63d0\u9ad8\u533b\u5b66\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u7279\u6b8a\u7684FLAN-T5-Large\u6a21\u578b\uff0c\u5206\u522b\u8c03\u6574\u7528\u4e8e\u603b\u7ed3\u533b\u7597\u62a5\u544a\u3001\u4ece\u75c5\u4eba\u533b\u751f\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u5065\u5eb7\u95ee\u9898\u4ee5\u53ca\u8bc6\u522b\u6bb5\u843d\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u603b\u7ed3\u6027\u80fd\u4f18\u4e8eGPT-4\uff0c\u57fa\u4e8eBLEU\u3001ROUGE-L\u3001BERTScore\u548cSpaCy Similarity\u7b49\u6307\u6807\u3002", "conclusion": "Medalyze provides a privacy-preserving and lightweight solution for improving information accessibility in healthcare."}}
{"id": "2505.17328", "pdf": "https://arxiv.org/pdf/2505.17328", "abs": "https://arxiv.org/abs/2505.17328", "authors": ["Dylan Kline"], "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning", "categories": ["cs.CV"], "comment": null, "summary": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6e38\u620f\u56fe\u50cf\u7684\u89c6\u89c9\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u6709\u671b\u5f00\u53d1\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u4e8e\u65b0\u6e38\u620f\u7684\u89c6\u89c9\u6a21\u578b\u3002", "motivation": "\u57fa\u7840\u7684\u6e38\u620f\u56fe\u50cf\u7f16\u7801\u5668\u5e38\u5e38\u56e0\u8fc7\u5ea6\u62df\u5408\u4e8e\u6e38\u620f\u7279\u5b9a\u7684\u89c6\u89c9\u98ce\u683c\u800c\u5728\u5e94\u7528\u4e8e\u65b0\u6e38\u620f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u57df\u5bf9\u6297\u8bad\u7ec3\u7684\u65b9\u6cd5\u4ee5\u5b66\u4e60\u6e38\u620f\u4e0d\u53d8\u7684\u89c6\u89c9\u7279\u5f81\u3002\u901a\u8fc7\u540c\u65f6\u9f13\u52b1\u76f8\u4f3c\u5185\u5bb9\u805a\u7c7b\u5e76\u901a\u8fc7\u5bf9\u6297\u57df\u5206\u7c7b\u5668\u6765\u6291\u5236\u6e38\u620f\u7279\u5b9a\u7ebf\u7d22\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u53ef\u4ee5\u5728\u4e0d\u540c\u6e38\u620f\u95f4\u6cdb\u5316\u7684\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u4e86Bingsu\u6e38\u620f\u56fe\u50cf\u6570\u636e\u96c6\uff08\u6765\u81ea10\u4e2a\u6e38\u620f\u768410,000\u5f20\u622a\u56fe\uff09\uff0c\u5c55\u793a\u4ec5\u9700\u51e0\u4e2a\u8bad\u7ec3\u5468\u671f\u540e\uff0c\u6211\u4eec\u6a21\u578b\u7684\u7279\u5f81\u4e0d\u518d\u6309\u6e38\u620f\u805a\u7c7b\uff0c\u8868\u660e\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u4e0d\u53d8\u91cf\u6027\u5e76\u6709\u6f5c\u529b\u901a\u8fc7\u6700\u5c0f\u7684\u5fae\u8c03\u6765\u6539\u5584\u8de8\u6e38\u620f\u8fc1\u79fb\uff08\u4f8b\u5982\uff0c\u6545\u969c\u68c0\u6d4b\uff09\u3002", "conclusion": "\u8fd9\u79cd\u80fd\u529b\u4e3a\u5f00\u53d1\u66f4\u5177\u666e\u904d\u6027\u7684\u6e38\u620f\u89c6\u89c9\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u65b0\u6e38\u620f\u4e0a\u51e0\u4e4e\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060", "abs": "https://arxiv.org/abs/2505.17060", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "AI": {"tldr": "SALMONN-omni significantly improves performance in human-machine speech interaction without codecs, using dynamic thinking in an LLM.", "motivation": "Address error accumulation and challenges like barge-in and echo cancellation in full-duplex systems.", "method": "A dynamic thinking mechanism within the LLM backbone, replacing audio codecs with standalone operation.", "result": "30% relative performance improvement over existing models with less training data, strong performance in complex scenarios, enhanced through reinforcement learning.", "conclusion": "SALMONN-omni exhibits significant performance improvements and competitive behaviour compared to existing systems in full-duplex speech interaction."}}
{"id": "2505.17330", "pdf": "https://arxiv.org/pdf/2505.17330", "abs": "https://arxiv.org/abs/2505.17330", "authors": ["Amit Agarwal", "Srikant Panda", "Kulbhushan Pachauri"], "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; I.7"], "comment": "Published in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Industry Track, pages 100-114", "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag", "AI": {"tldr": "FS-DAG is a scalable and efficient model for document understanding in few-shot settings, improving upon state-of-the-art performance with less than 90M parameters. It handles OCR errors, misspellings, and domain shifts effectively in real-world applications.", "motivation": "Current models for visually rich document understanding require large datasets and computational resources, which can be challenging in real-world settings. The goal is to develop a model that is both efficient and capable of handling practical challenges like OCR errors, misspellings, and domain shifts in few-shot settings.", "method": "The proposed model, FS-DAG, employs a scalable architecture that uses domain-specific and language/vision specific backbones within a modular framework. This allows the model to adapt to diverse document types with minimal data.", "result": "FS-DAG performs robustly in Information Extraction tasks under practical conditions. The model is lightweight with less than 90 million parameters and shows significant improvements in convergence speed and performance under few-shot learning conditions.", "conclusion": "FS-DAG shows significant improvements in convergence speed and performance in information extraction tasks compared to current state-of-the-art methods, demonstrating that smaller and more efficient models can be developed without compromising performance."}}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061", "abs": "https://arxiv.org/abs/2505.17061", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.", "AI": {"tldr": "Mixture of Decoding (MoD) is proposed to mitigate hallucinations in LVLMs by adapting decoding strategies based on attention correctness, achieving superior results over existing methods.", "motivation": "The motivation is to address the persistent challenge of hallucinations in Large Vision-Language Models (LVLMs) to improve their performance across various visual tasks.", "method": "The Mixture of Decoding (MoD) approach evaluates the correctness of the model's attention on image tokens by measuring the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens. Depending on this consistency, it adapts its decoding strategy either by amplifying critical information (when consistent) or suppressing misleading information (when inconsistent).", "result": "MoD achieves significant improvements over existing decoding methods in mitigating hallucinations, as demonstrated by extensive experiments on multiple mainstream benchmarks.", "conclusion": "Mixture of Decoding (MoD) significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in Large Vision-Language Models (LVLMs)."}}
{"id": "2505.17333", "pdf": "https://arxiv.org/pdf/2505.17333", "abs": "https://arxiv.org/abs/2505.17333", "authors": ["Xin You", "Minghui Zhang", "Hanxiao Zhang", "Jie Yang", "Nassir Navab"], "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis", "categories": ["cs.CV"], "comment": "early accepted by MICCAI", "summary": "Temporal modeling on regular respiration-induced motions is crucial to\nimage-guided clinical applications. Existing methods cannot simulate temporal\nmotions unless high-dose imaging scans including starting and ending frames\nexist simultaneously. However, in the preoperative data acquisition stage, the\nslight movement of patients may result in dynamic backgrounds between the first\nand last frames in a respiratory period. This additional deviation can hardly\nbe removed by image registration, thus affecting the temporal modeling. To\naddress that limitation, we pioneeringly simulate the regular motion process\nvia the image-to-video (I2V) synthesis framework, which animates with the first\nframe to forecast future frames of a given length. Besides, to promote the\ntemporal consistency of animated videos, we devise the Temporal Differential\nDiffusion Model to generate temporal differential fields, which measure the\nrelative differential representations between adjacent frames. The prompt\nattention layer is devised for fine-grained differential fields, and the field\naugmented layer is adopted to better interact these fields with the I2V\nframework, promoting more accurate temporal variation of synthesized videos.\nExtensive results on ACDC cardiac and 4D Lung datasets reveal that our approach\nsimulates 4D videos along the intrinsic motion trajectory, rivaling other\ncompetitive methods on perceptual similarity and temporal consistency. Codes\nwill be available soon.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u56fe\u50cf\u5230\u89c6\u9891\u7684\u6846\u67b6\u6765\u6a21\u62df\u547c\u5438\u5f15\u8d77\u7684\u89c4\u5f8b\u6027\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5efa\u6a21\u4e0a\u7684\u7f3a\u9677\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u65e0\u6cd5\u5728\u6ca1\u6709\u9ad8\u5242\u91cf\u626b\u63cf\u7684\u60c5\u51b5\u4e0b\u6a21\u62df\u65f6\u95f4\u8fd0\u52a8\uff0c\u800c\u60a3\u8005\u8f7b\u5fae\u7684\u79fb\u52a8\u53ef\u80fd\u5bfc\u81f4\u52a8\u6001\u80cc\u666f\uff0c\u4ece\u800c\u5f71\u54cd\u65f6\u95f4\u5efa\u6a21\u3002", "method": "\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u5408\u6210\u6846\u67b6\uff1b\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\uff1b\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u5fae\u5206\u573a\u548c\u76f8\u90bb\u5e27\u4e4b\u95f4\u7684\u76f8\u5bf9\u5fae\u5206\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5728ACDC\u5fc3\u810f\u548c4D\u80ba\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6309\u7167\u5185\u5728\u8fd0\u52a8\u8f68\u8ff9\u6a21\u62df4D\u89c6\u9891\uff0c\u5728\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u4e0e\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u89c6\u9891\u7684\u5408\u6210\u6846\u67b6\u6765\u6a21\u62df\u547c\u5438\u8fc7\u7a0b\u4e2d\u7684\u89c4\u5f8b\u6027\u8fd0\u52a8\u662f\u53ef\u884c\u7684\u3002\u5176\u65b9\u6cd5\u5728\u4fdd\u6301\u56fe\u50cf\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.17063", "pdf": "https://arxiv.org/pdf/2505.17063", "abs": "https://arxiv.org/abs/2505.17063", "authors": ["Yiduo Guo", "Zhen Guo", "Chuanwei Huang", "Zi-Ang Wang", "Zekai Zhang", "Haofei Yu", "Huishuai Zhang", "Yikang Shen"], "title": "Synthetic Data RL: Task Definition Is All You Need", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u4efb\u52a1\u5b9a\u4e49\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884cRL\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u6210\u529f\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u6570\u636e\u6ce8\u91ca\u7684\u4f9d\u8d56\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eRL\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u91c7\u7528\uff0c\u56e0\u6b64\u63d0\u51faSynthetic Data RL\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4efb\u52a1\u5b9a\u4e49\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884cRL\u5fae\u8c03\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4ece\u4efb\u52a1\u5b9a\u4e49\u548c\u68c0\u7d22\u6587\u6863\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u7136\u540e\u6839\u636e\u6a21\u578b\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u8c03\u6574\u95ee\u9898\u96be\u5ea6\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u5728\u6837\u672c\u4e2d\u5e73\u5747\u901a\u8fc7\u7387\u9009\u62e9\u95ee\u9898\u8fdb\u884cRL\u8bad\u7ec3\u3002", "result": "\u5728\u5404\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSynthetic Data RL\u65b9\u6cd5\u5728\u6709\u9650\u7684\u4eba\u7c7b\u6570\u636e\u9884\u7b97\u4e0b\u8d85\u8fc7\u4e86\u76d1\u7763\u5fae\u8c03\uff0c\u51e0\u4e4e\u5339\u914d\u4f7f\u7528\u5168\u4eba\u7c7b\u6570\u636e\u7684RL\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "Synthetic Data RL\u901a\u8fc7\u5408\u6210\u6570\u636e\u8fdb\u884cRL\u5fae\u8c03\uff0c\u5728\u6709\u9650\u7684\u4eba\u7c7b\u6570\u636e\u9884\u7b97\u4e0b\u63d0\u4f9b\u4e86\u4e0e\u4eba\u5de5\u6570\u636e\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.17338", "pdf": "https://arxiv.org/pdf/2505.17338", "abs": "https://arxiv.org/abs/2505.17338", "authors": ["Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Anwesa Choudhuri", "Terrence Chen", "Ziyan Wu"], "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Volumetric rendering of Computed Tomography (CT) scans is crucial for\nvisualizing complex 3D anatomical structures in medical imaging. Current\nhigh-fidelity approaches, especially neural rendering techniques, require\ntime-consuming per-scene optimization, limiting clinical applicability due to\ncomputational demands and poor generalizability. We propose Render-FM, a novel\nfoundation model for direct, real-time volumetric rendering of CT scans.\nRender-FM employs an encoder-decoder architecture that directly regresses 6D\nGaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan\noptimization through large-scale pre-training on diverse medical data. By\nintegrating robust feature extraction with the expressive power of 6DGS, our\napproach efficiently generates high-quality, real-time interactive 3D\nvisualizations across diverse clinical CT data. Experiments demonstrate that\nRender-FM achieves visual fidelity comparable or superior to specialized\nper-scan methods while drastically reducing preparation time from nearly an\nhour to seconds for a single inference step. This advancement enables seamless\nintegration into real-time surgical planning and diagnostic workflows. The\nproject page is: https://gaozhongpai.github.io/renderfm/.", "AI": {"tldr": "Render-FM is a new model for real-time CT scan rendering, offering high visual fidelity and rapid processing to aid clinical workflows.", "motivation": "Current high-fidelity neural rendering techniques require time-consuming optimization per scene, hindering their clinical applicability. There's a need for a method that is both efficient and generalizable.", "method": "Render-FM uses an encoder-decoder architecture to regress 6D Gaussian Splatting (6DGS) parameters directly from CT volumes, leveraging large-scale pre-training on diverse medical data.", "result": "Render-FM provides high-quality, real-time 3D visualizations with significantly reduced preparation time, from nearly an hour to seconds, facilitating real-time applications in medical imaging.", "conclusion": "Render-FM achieves comparable or superior visual fidelity to specialized methods with drastically reduced preparation time, facilitating real-time integration into clinical workflows."}}
{"id": "2505.17065", "pdf": "https://arxiv.org/pdf/2505.17065", "abs": "https://arxiv.org/abs/2505.17065", "authors": ["Valentina Carbonari", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in artificial intelligence, particularly large language\nmodels LLMs, have shown promising capabilities in transforming rare disease\nresearch. This survey paper explores the integration of LLMs in the analysis of\nrare diseases, highlighting significant strides and pivotal studies that\nleverage textual data to uncover insights and patterns critical for diagnosis,\ntreatment, and patient care. While current research predominantly employs\ntextual data, the potential for multimodal data integration combining genetic,\nimaging, and electronic health records stands as a promising frontier. We\nreview foundational papers that demonstrate the application of LLMs in\nidentifying and extracting relevant medical information, simulating intelligent\nconversational agents for patient interaction, and enabling the formulation of\naccurate and timely diagnoses. Furthermore, this paper discusses the challenges\nand ethical considerations inherent in deploying LLMs, including data privacy,\nmodel transparency, and the need for robust, inclusive data sets. As part of\nthis exploration, we present a section on experimentation that utilizes\nmultiple LLMs alongside structured questionnaires, specifically designed for\ndiagnostic purposes in the context of different diseases. We conclude with\nfuture perspectives on the evolution of LLMs towards truly multimodal\nplatforms, which would integrate diverse data types to provide a more\ncomprehensive understanding of rare diseases, ultimately fostering better\noutcomes in clinical settings.", "AI": {"tldr": "The paper surveys the integration of LLMs in rare disease research, emphasizing current applications and future multimodal potentials, while addressing ethical concerns.", "motivation": "To explore the integration of LLMs in rare disease research and their potential in diagnostics, treatment, and patient care.", "method": "Exploration of LLMs integration in rare disease analysis, review of foundational papers, and experimentation using multiple LLMs with structured questionnaires.", "result": "Demonstrated application of LLMs in identifying medical information, simulating patient interaction, and formulating diagnoses, with discussion on challenges and ethical considerations.", "conclusion": "LLMs are poised to evolve into truly multimodal platforms that integrate various data types, fostering better outcomes in rare disease research and clinical settings."}}
{"id": "2505.17343", "pdf": "https://arxiv.org/pdf/2505.17343", "abs": "https://arxiv.org/abs/2505.17343", "authors": ["Dillon Lohr", "Michael J. Proulx", "Mehedi Hasan Raju", "Oleg V. Komogortsev"], "title": "Ocular Authentication: Fusion of Gaze and Periocular Modalities", "categories": ["cs.CV", "cs.HC"], "comment": "Supplementary material is available", "summary": "This paper investigates the feasibility of fusing two eye-centric\nauthentication modalities-eye movements and periocular images-within a\ncalibration-free authentication system. While each modality has independently\nshown promise for user authentication, their combination within a unified\ngaze-estimation pipeline has not been thoroughly explored at scale. In this\nreport, we propose a multimodal authentication system and evaluate it using a\nlarge-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)\nsignal quality equivalent to a consumer-facing virtual reality (VR) device. Our\nresults show that the multimodal approach consistently outperforms both\nunimodal systems across all scenarios, surpassing the FIDO benchmark. The\nintegration of a state-of-the-art machine learning architecture contributed\nsignificantly to the overall authentication performance at scale, driven by the\nmodel's ability to capture authentication representations and the complementary\ndiscriminative characteristics of the fused modalities.", "AI": {"tldr": "\u7814\u7a76\u773c\u52a8\u548c\u773c\u5468\u56fe\u50cf\u7684\u878d\u5408\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u5355\u72ec\u7684\u773c\u52a8\u548c\u773c\u5468\u56fe\u50cf\u5728\u7528\u6237\u8eab\u4efd\u9a8c\u8bc1\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u7ec4\u5408\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u6ce8\u89c6\u4f30\u8ba1\u7b97\u6cd5\u4e2d\u5c1a\u672a\u8fdb\u884c\u89c4\u6a21\u5316\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u67b6\u6784\u6765\u6355\u6349\u8eab\u4efd\u9a8c\u8bc1\u8868\u793a\uff0c\u5e76\u878d\u5408\u773c\u52a8\u548c\u773c\u5468\u56fe\u50cf\u4e24\u79cd\u6a21\u6001\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u660e\u663e\u4f18\u4e8e\u5355\u6a21\u6001\u7cfb\u7edf\uff0c\u8d85\u8d8aFIDO\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89c4\u6a21\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6bd4\u5355\u6a21\u6001\u7cfb\u7edf\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86FIDO\u57fa\u51c6\u3002"}}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067", "abs": "https://arxiv.org/abs/2505.17067", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u4ee5\u53ca\u4e13\u5bb6\u4e58\u79ef\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00\u548c\u591a\u56fe\u7247\u8bbe\u7f6e\u4e0b\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u68c0\u6d4b\u4ece\u56fe\u7247\u63cf\u8ff0\u4e2d\u83b7\u53d6\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u5728\u591a\u8bed\u8a00\u548c\u591a\u56fe\u7247\u8bbe\u7f6e\u4e2d\u662f\u81f3\u5173\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u3002TAUKDIAL-2024\u6311\u6218\u8d5b\u901a\u8fc7\u5f15\u5165\u591a\u8bed\u8a00\u53d1\u8a00\u8005\u548c\u591a\u5f20\u56fe\u7247\u6765\u6269\u5c55\u8fd9\u4e00\u8303\u56f4\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u6790\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a(1) \u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u5224\u522b\u8868\u793a\u5b66\u4e60\uff1b(2) \u5f15\u5165\u56fe\u50cf\u6a21\u6001\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u8bed\u97f3\u548c\u6587\u672c\u6a21\u6001\uff1b(3) \u5e94\u7528\u4e13\u5bb6\u4e58\u79ef(PoE)\u7b56\u7565\u6765\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u548c\u8fc7\u62df\u5408\u3002", "result": "\u57fa\u4e8e\u6587\u672c\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u6846\u67b6\u7684MCI\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u672a\u52a0\u6743\u5e73\u5747\u53ec\u56de\u7387\uff08UAR\uff09\u63d0\u9ad8\u4e867.1%\uff08\u4ece68.1%\u523075.2%\uff09\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e862.9%\uff08\u4ece80.6%\u523083.5%\uff09\u3002\u7279\u522b\u662f\uff0c\u5bf9\u6bd4\u5b66\u4e60\u7ec4\u4ef6\u5728\u6587\u672c\u6a21\u6001\u4e0a\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u591a\u8bed\u8a00\u548c\u591a\u56fe\u50cf\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u6a21\u6001\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002"}}
{"id": "2505.17352", "pdf": "https://arxiv.org/pdf/2505.17352", "abs": "https://arxiv.org/abs/2505.17352", "authors": ["Preeti Lamba", "Kiran Ravish", "Ankita Kushwaha", "Pawan Kumar"], "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have emerged as leading generative models for images and\nother modalities, but aligning their outputs with human preferences and safety\nconstraints remains a critical challenge. This thesis proposal investigates\nmethods to align diffusion models using reinforcement learning (RL) and reward\nmodeling. We survey recent advances in fine-tuning text-to-image diffusion\nmodels with human feedback, including reinforcement learning from human and AI\nfeedback, direct preference optimization, and differentiable reward approaches.\nWe classify these methods based on the type of feedback (human, automated,\nbinary or ranked preferences), the fine-tuning technique (policy gradient,\nreward-weighted likelihood, direct backpropagation, etc.), and their efficiency\nand safety outcomes. We compare key algorithms and frameworks, highlighting how\nthey improve alignment with user intent or safety standards, and discuss\ninter-relationships such as how newer methods build on or diverge from earlier\nones. Based on the survey, we identify five promising research directions for\nthe next two years: (1) multi-objective alignment with combined rewards, (2)\nefficient human feedback usage and active learning, (3) robust safety alignment\nagainst adversarial inputs, (4) continual and online alignment of diffusion\nmodels, and (5) interpretable and trustworthy reward modeling for generative\nimages. Each direction is elaborated with its problem statement, challenges,\nrelated work, and a proposed research plan. The proposal is organized as a\ncomprehensive document with literature review, comparative tables of methods,\nand detailed research plans, aiming to contribute new insights and techniques\nfor safer and value-aligned diffusion-based generative AI.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5956\u52b1\u5efa\u6a21\u6765\u5bf9\u9f50\u6269\u6563\u6a21\u578b\uff0c\u5e76\u8bc6\u522b\u672a\u6765\u7684\u4e94\u4e2a\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u548c\u5b89\u5168\u7ea6\u675f\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u8c03\u67e5\u6700\u8fd1\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u4ece\u4eba\u7c7b\u548cAI\u53cd\u9988\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u53ef\u5fae\u5206\u5956\u52b1\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e94\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u6bcf\u4e2a\u65b9\u5411\u7684\u7814\u7a76\u8ba1\u5212\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u5956\u52b1\u5efa\u6a21\u6765\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u548c\u5b89\u5168\u6807\u51c6\u7684\u5bf9\u9f50\u3002"}}
{"id": "2505.17068", "pdf": "https://arxiv.org/pdf/2505.17068", "abs": "https://arxiv.org/abs/2505.17068", "authors": ["Jorge Paz-Ruza", "Amparo Alonso-Betanzos", "Bertha Guijarro-Berdi\u00f1as", "Carlos Eiras-Franco"], "title": "Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": "IJCNN 2025", "summary": "In health-related topics, user toxicity in online discussions frequently\nbecomes a source of social conflict or promotion of dangerous, unscientific\nbehaviour; common approaches for battling it include different forms of\ndetection, flagging and/or removal of existing toxic comments, which is often\ncounterproductive for platforms and users alike. In this work, we propose the\nalternative of combatting user toxicity predictively, anticipating where a user\ncould interact toxically in health-related online discussions. Applying a\nCollaborative Filtering-based Machine Learning methodology, we predict the\ntoxicity in COVID-related conversations between any user and subcommunity of\nReddit, surpassing 80% predictive performance in relevant metrics, and allowing\nus to prevent the pairing of conflicting users and subcommunities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u9884\u6d4b\u7528\u6237\u5728\u5065\u5eb7\u76f8\u5173\u8bdd\u9898\u4e2d\u7684\u6709\u5bb3\u4e92\u52a8\uff0c\u83b7\u5f97\u8d85\u8fc780%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u4ece\u800c\u53ef\u4ee5\u9884\u9632\u51b2\u7a81\u7528\u6237\u4e0e\u5b50\u793e\u533a\u7684\u914d\u5bf9\u3002", "motivation": "\u76ee\u524d\u5e38\u7528\u7684\u68c0\u6d4b\u3001\u6807\u8bb0\u6216\u79fb\u9664\u6709\u5bb3\u8bc4\u8bba\u7684\u65b9\u6cd5\u901a\u5e38\u5bf9\u5e73\u53f0\u548c\u7528\u6237\u90fd\u9002\u5f97\u5176\u53cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u5e76\u51cf\u5c11\u7ebf\u4e0a\u8ba8\u8bba\u4e2d\u7684\u7528\u6237\u6709\u5bb3\u884c\u4e3a\u3002", "method": "\u5e94\u7528\u57fa\u4e8e\u534f\u540c\u8fc7\u6ee4\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9884\u6d4b\u7528\u6237\u5728\u5065\u5eb7\u76f8\u5173\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u6709\u5bb3\u4e92\u52a8\u884c\u4e3a\uff0c\u7279\u522b\u662fCOVID\u76f8\u5173\u8ba8\u8bba\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9884\u6d4b\u7528\u6237\u4e0eReddit\u5b50\u793e\u533a\u4e4b\u95f4\u7684COVID\u76f8\u5173\u8ba8\u8bba\u7684\u6709\u5bb3\u884c\u4e3a\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u8d85\u8fc780%\u3002", "conclusion": "\u901a\u8fc7\u63d0\u524d\u9884\u6d4b\u7528\u6237\u4e4b\u95f4\u53ef\u80fd\u53d1\u751f\u7684\u6709\u5bb3\u4e92\u52a8\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u51cf\u5c11\u6709\u5bb3\u8bc4\u8bba\u7684\u4ea7\u751f\uff0c\u800c\u4e0d\u662f\u4e8b\u540e\u8fdb\u884c\u68c0\u6d4b\u548c\u79fb\u9664\u3002"}}
{"id": "2505.17353", "pdf": "https://arxiv.org/pdf/2505.17353", "abs": "https://arxiv.org/abs/2505.17353", "authors": ["Minseo Kim", "Axel Levy", "Gordon Wetzstein"], "title": "Dual Ascent Diffusion for Inverse Problems", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "23 pages, 15 figures, 5 tables", "summary": "Ill-posed inverse problems are fundamental in many domains, ranging from\nastrophysics to medical imaging. Emerging diffusion models provide a powerful\nprior for solving these problems. Existing maximum-a-posteriori (MAP) or\nposterior sampling approaches, however, rely on different computational\napproximations, leading to inaccurate or suboptimal samples. To address this\nissue, we introduce a new approach to solving MAP problems with diffusion model\npriors using a dual ascent optimization framework. Our framework achieves\nbetter image quality as measured by various metrics for image restoration\nproblems, it is more robust to high levels of measurement noise, it is faster,\nand it estimates solutions that represent the observations more faithfully than\nthe state of the art.", "AI": {"tldr": "\u65b0\u65b9\u6cd5\u5229\u7528\u53cc\u4e0a\u5347\u4f18\u5316\u6846\u67b6\u4e0e\u6269\u6563\u6a21\u578b\u89e3\u51b3MAP\u95ee\u9898\uff0c\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684MAP\u6216\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u7531\u4e8e\u4e0d\u540c\u7684\u8ba1\u7b97\u8fd1\u4f3c\u800c\u5bfc\u81f4\u6837\u672c\u4e0d\u51c6\u786e\u6216\u6b21\u4f18\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u65b9\u6cd5\u63d0\u9ad8\u56fe\u50cf\u6062\u590d\u95ee\u9898\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u53cc\u4e0a\u5347\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\u5148\u9a8c\u4ee5\u63d0\u5347\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u8be5\u6846\u67b6\u5728\u56fe\u50cf\u6062\u590d\u95ee\u9898\u4e2d\u7684\u5404\u9879\u8d28\u91cf\u5ea6\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u566a\u58f0\u9c81\u68d2\u6027\u548c\u66f4\u5feb\u7684\u8ba1\u7b97\u901f\u5ea6\uff0c\u5e76\u80fd\u66f4\u5fe0\u5b9e\u5730\u4f30\u8ba1\u89c2\u5bdf\u5230\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u4e0a\u5347\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5148\u9a8c\u7684MAP\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u8f83\u73b0\u6709\u6280\u672f\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u566a\u58f0\u9c81\u68d2\u6027\u3001\u901f\u5ea6\u548c\u89e3\u51b3\u65b9\u6848\u5fe0\u5b9e\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.17070", "pdf": "https://arxiv.org/pdf/2505.17070", "abs": "https://arxiv.org/abs/2505.17070", "authors": ["Anandh C", "Karthik Pandia Durai", "Jeena Prakash", "Manickavela Arumugam", "Kadri Hacioglu", "S. Pavankumar Dubagunta", "Andreas Stolcke", "Shankar Venkatesan", "Aravind Ganapathiraju"], "title": "Improving endpoint detection in end-to-end streaming ASR for conversational speech", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2024", "summary": "ASR endpointing (EP) plays a major role in delivering a good user experience\nin products supporting human or artificial agents in human-human/machine\nconversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR\nmodelling technique preferred for streaming. A major limitation of T-ASR is\ndelayed emission of ASR outputs, which could lead to errors or delays in EP.\nInaccurate EP will cut the user off while speaking, returning incomplete\ntranscript while delays in EP will increase the perceived latency, degrading\nthe user experience. We propose methods to improve EP by addressing delayed\nemission along with EP mistakes. To address the delayed emission problem, we\nintroduce an end-of-word token at the end of each word, along with a delay\npenalty. The EP delay is addressed by obtaining a reliable frame-level speech\nactivity detection using an auxiliary network. We apply the proposed methods on\nSwitchboard conversational speech corpus and evaluate it against a delay\npenalty method.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684 ASR \u7aef\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6d41\u5f0f\u4f20\u8f93\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "T-ASR \u6a21\u578b\u666e\u904d\u5b58\u5728\u8f93\u51fa\u5ef6\u8fdf\uff0c\u5bfc\u81f4\u7aef\u70b9\u68c0\u6d4b\u4e2d\u51fa\u73b0\u9519\u8bef\u6216\u5ef6\u8fdf\uff0c\u8fd9\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u7aef\u70b9\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5728\u6bcf\u4e2a\u5355\u8bcd\u672b\u5c3e\u5f15\u5165\u8bcd\u5c3e\u6807\u8bb0\u548c\u5ef6\u8fdf\u60e9\u7f5a\uff0c\u5e76\u4f7f\u7528\u8f85\u52a9\u7f51\u7edc\u8fdb\u884c\u5e27\u7ea7\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\uff0c\u4ee5\u89e3\u51b3\u7aef\u70b9\u68c0\u6d4b\u4e2d\u7684\u5ef6\u8fdf\u548c\u9519\u8bef\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u5ef6\u8fdf\u60e9\u7f5a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u7aef\u70b9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u6d41\u5f0f\u4f20\u8f93\u4e2d\u7684\u7aef\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2505.17358", "pdf": "https://arxiv.org/pdf/2505.17358", "abs": "https://arxiv.org/abs/2505.17358", "authors": ["Chinmay Talegaonkar", "Nikhil Gandudi Suresh", "Zachary Novack", "Yash Belhe", "Priyanka Nagasamudra", "Nicholas Antipa"], "title": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues", "categories": ["cs.CV"], "comment": null, "summary": "Recent monocular metric depth estimation (MMDE) methods have made notable\nprogress towards zero-shot generalization. However, they still exhibit a\nsignificant performance drop on out-of-distribution datasets. We address this\nlimitation by injecting defocus blur cues at inference time into Marigold, a\n\\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular\ndepth estimation (MDE). Our method effectively turns Marigold into a metric\ndepth predictor in a training-free manner. To incorporate defocus cues, we\ncapture two images with a small and a large aperture from the same viewpoint.\nTo recover metric depth, we then optimize the metric depth scaling parameters\nand the noise latents of Marigold at inference time using gradients from a loss\nfunction based on the defocus-blur image formation model. We compare our method\nagainst existing state-of-the-art zero-shot MMDE methods on a self-collected\nreal dataset, showing quantitative and qualitative improvements.", "AI": {"tldr": "\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u6563\u7126\u6a21\u7cca\u7ebf\u7d22\uff0c\u63d0\u5347\u4e86Marigold\u5728\u96f6\u6837\u672c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6cdb\u5316\u4e0a\u5df2\u7ecf\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\u3002\u672c\u7814\u7a76\u9488\u5bf9\u8fd9\u4e00\u5c40\u9650\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u6563\u7126\u6a21\u7cca\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u6355\u6349\u4e0d\u540c\u5149\u5708\u4e0b\u7684\u56fe\u50cf\u89c6\u89d2\uff0c\u4f18\u5316Marigold\u7684\u5c3a\u5ea6\u53c2\u6570\u548c\u566a\u58f0\u6f5c\u53d8\u91cf\uff0c\u4ece\u800c\u6062\u590d\u5ea6\u91cf\u6df1\u5ea6\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u96f6\u6837\u672c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u81ea\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMarigold\u7684\u96f6\u6837\u672c\u3001\u5c3a\u5ea6\u4e0d\u53d8\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u6563\u7126\u6a21\u7cca\u7ebf\u7d22\u6765\u5b9e\u73b0\u8be5\u65b9\u6cd5\u3002"}}
{"id": "2505.17071", "pdf": "https://arxiv.org/pdf/2505.17071", "abs": "https://arxiv.org/abs/2505.17071", "authors": ["Rapha\u00ebl Sarfati", "Haley Moller", "Toni J. B. Liu", "Nicolas Boull\u00e9", "Christopher Earls"], "title": "What's in a prompt? Language models encode literary style in prompt embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Large language models use high-dimensional latent spaces to encode and\nprocess textual information. Much work has investigated how the conceptual\ncontent of words translates into geometrical relationships between their vector\nrepresentations. Fewer studies analyze how the cumulative information of an\nentire prompt becomes condensed into individual embeddings under the action of\ntransformer layers. We use literary pieces to show that information about\nintangible, rather than factual, aspects of the prompt are contained in deep\nrepresentations. We observe that short excerpts (10 - 100 tokens) from\ndifferent novels separate in the latent space independently from what\nnext-token prediction they converge towards. Ensembles from books from the same\nauthors are much more entangled than across authors, suggesting that embeddings\nencode stylistic features. This geometry of style may have applications for\nauthorship attribution and literary analysis, but most importantly reveals the\nsophistication of information processing and compression accomplished by\nlanguage models.", "AI": {"tldr": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u5d4c\u5165\u7269\u7684\u98ce\u683c\u51e0\u4f55\u7ed3\u6784\uff0c\u53ef\u7528\u4e8e\u4f5c\u8005\u5f52\u5c5e\u548c\u6587\u5b66\u5206\u6790\u3002", "motivation": "\u63a2\u8ba8\u6574\u4e2a\u63d0\u793a\u7684\u4fe1\u606f\u5982\u4f55\u5728\u53d8\u538b\u5668\u5c42\u7684\u4f5c\u7528\u4e0b\u6d53\u7f29\u4e3a\u5355\u4e00\u5d4c\u5165\u3002", "method": "\u4f7f\u7528\u6587\u5b66\u4f5c\u54c1\u6765\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u8868\u793a\u5982\u4f55\u5305\u542b\u63d0\u793a\u4e2d\u65e0\u5f62\u800c\u975e\u4e8b\u5b9e\u7684\u4fe1\u606f\u3002", "result": "\u4e0d\u540c\u5c0f\u8bf4\u7684\u7b80\u77ed\u6458\u5f55\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u72ec\u7acb\u5206\u79bb\uff0c\u800c\u6765\u81ea\u540c\u4e00\u4f5c\u8005\u7684\u4e66\u7c4d\u96c6\u5408\u6bd4\u8de8\u4f5c\u8005\u7684\u66f4\u7ea0\u7f20\uff0c\u8868\u660e\u5d4c\u5165\u7269\u7f16\u7801\u98ce\u683c\u7279\u5f81\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u7f16\u7801\u5305\u542b\u98ce\u683c\u7279\u5f81\uff0c\u5e76\u5728\u98ce\u683c\u7684\u51e0\u4f55\u5f62\u6001\u4e0a\u663e\u793a\u51fa\u590d\u6742\u7684\u4fe1\u606f\u5904\u7406\u548c\u538b\u7f29\u80fd\u529b\u3002"}}
{"id": "2505.17363", "pdf": "https://arxiv.org/pdf/2505.17363", "abs": "https://arxiv.org/abs/2505.17363", "authors": ["Hassan Wasswa", "Hussein Abbass", "Timothy Lynar"], "title": "Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches", "categories": ["cs.CV"], "comment": null, "summary": "Due to the exponential rise in IoT-based botnet attacks, researchers have\nexplored various advanced techniques for both dimensionality reduction and\nattack detection to enhance IoT security. Among these, Variational Autoencoders\n(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including\nGraph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have\ngarnered significant research attention in the domain of attack detection. This\nstudy evaluates the effectiveness of four state-of-the-art deep learning\narchitectures for IoT botnet detection: a VAE encoder with a Multi-Layer\nPerceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT\nencoder with an MLP. The evaluation is conducted on a widely studied IoT\nbenchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.\nFor the binary classification task, all models achieved over 99.93% in\naccuracy, recall, precision, and F1-score, with no notable differences in\nperformance. In contrast, for the multiclass classification task, GNN-based\nmodels showed significantly lower performance compared to VAE-MLP and ViT-MLP,\nwith accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,\nVAE-MLP, and ViT-MLP, respectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728IoT botnet\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5176\u4e2dVAE-MLP\u548cViT-MLP\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u6240\u6709\u6a21\u578b\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u9ad8\u7cbe\u51c6\u5ea6\u3002", "motivation": "\u7531\u4e8eIoT botnet\u653b\u51fb\u7684\u6307\u6570\u589e\u957f\uff0c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u4e86\u591a\u79cd\u5148\u8fdb\u6280\u672f\u4ee5\u589e\u5f3aIoT\u5b89\u5168\uff0c\u5305\u62ec\u964d\u7ef4\u548c\u653b\u51fb\u68c0\u6d4b\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff1aVAE\u7f16\u7801\u5668\u4e0eMLP\u3001VAE\u7f16\u7801\u5668\u4e0eGCN\u3001VAE\u7f16\u7801\u5668\u4e0eGAT\u3001ViT\u7f16\u7801\u5668\u4e0eMLP\uff0c\u4f7f\u7528IoT\u57fa\u51c6\u6570\u636e\u96c6N-BaIoT\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u8d85\u8fc799.93%\u3002\u800c\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGNN\u6a21\u578b\u7684\u6027\u80fd\u663e\u8457\u4f4e\u4e8eVAE-MLP\u548cViT-MLP\uff0c\u7cbe\u786e\u5ea6\u5206\u522b\u4e3a86.42%\u300189.46%\u300199.72%\u548c98.38%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728IoT botnet\u68c0\u6d4b\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGNN\u6a21\u578b\u7684\u6027\u80fd\u4e0d\u5982VAE-MLP\u548cViT-MLP\u3002"}}
{"id": "2505.17073", "pdf": "https://arxiv.org/pdf/2505.17073", "abs": "https://arxiv.org/abs/2505.17073", "authors": ["Anurag Mishra"], "title": "Mechanistic Interpretability of GPT-like Models on Summarization Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages (6 content + 2 references/appendix), 6 figures, 2 tables;\n  under review for the ACL 2025 Student Research Workshop", "summary": "Mechanistic interpretability research seeks to reveal the inner workings of\nlarge language models, yet most work focuses on classification or generative\ntasks rather than summarization. This paper presents an interpretability\nframework for analyzing how GPT-like models adapt to summarization tasks. We\nconduct differential analysis between pre-trained and fine-tuned models,\nquantifying changes in attention patterns and internal activations. By\nidentifying specific layers and attention heads that undergo significant\ntransformation, we locate the \"summarization circuit\" within the model\narchitecture. Our findings reveal that middle layers (particularly 2, 3, and 5)\nexhibit the most dramatic changes, with 62% of attention heads showing\ndecreased entropy, indicating a shift toward focused information selection. We\ndemonstrate that targeted LoRA adaptation of these identified circuits achieves\nsignificant performance improvement over standard LoRA fine-tuning while\nrequiring fewer training epochs. This work bridges the gap between black-box\nevaluation and mechanistic understanding, providing insights into how neural\nnetworks perform information selection and compression during summarization.", "AI": {"tldr": "\u7814\u7a76\u89e3\u6790\u4e86GPT\u6a21\u578b\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u673a\u5236\uff0c\u53d1\u73b0\u4e2d\u95f4\u5c42\u7684\"\u6458\u8981\u7535\u8def\"\uff0c\u4f7f\u7528\u5b9a\u5411LoRA\u9002\u914d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u591a\u6570\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u96c6\u4e2d\u5728\u5206\u7c7b\u6216\u751f\u6210\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u6458\u8981\u4efb\u52a1\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u8bd5\u56fe\u63ed\u793aGPT\u6837\u6a21\u578b\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u673a\u5236\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\u7684\u5dee\u5f02\u5206\u6790\u6765\u91cf\u5316\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u5185\u90e8\u6fc0\u6d3b\u7684\u53d8\u5316\uff0c\u8bc6\u522b\u6a21\u578b\u7ed3\u6784\u4e2d\u663e\u8457\u8f6c\u53d8\u7684\u5c42\u548c\u6ce8\u610f\u529b\u5934\u3002", "result": "\u4e2d\u95f4\u5c42\uff08\u7279\u522b\u662f\u7b2c2\u30013\u548c5\u5c42\uff09\u8868\u73b0\u51fa\u6700\u663e\u8457\u7684\u53d8\u5316\uff0c62%\u7684\u6ce8\u610f\u529b\u5934\u51cf\u5c11\u4e86\u71b5\uff0c\u8868\u660e\u4fe1\u606f\u9009\u62e9\u7684\u96c6\u4e2d\uff1b\u5b9a\u5411LoRA\u9002\u914d\u5728\u6027\u80fd\u63d0\u5347\u4e0a\u4f18\u4e8e\u6807\u51c6LoRA\u5fae\u8c03\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86GPT\u6a21\u578b\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u5b58\u5728\u7279\u5b9a\u7684\"\u6458\u8981\u7535\u8def\"\uff0c\u5e76\u53ef\u4ee5\u901a\u8fc7\u5b9a\u5411LoRA\u9002\u914d\u6765\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.17364", "pdf": "https://arxiv.org/pdf/2505.17364", "abs": "https://arxiv.org/abs/2505.17364", "authors": ["Apar Pokhrel", "Gia Dao"], "title": "Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Parking space occupancy detection is a critical component in the development\nof intelligent parking management systems. Traditional object detection\napproaches, such as YOLOv8, provide fast and accurate vehicle detection across\nparking lots but can struggle with borderline cases, such as partially visible\nvehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In\nthis work, we perform a comprehensive comparative analysis of customized\nbackbone architectures integrated with YOLOv8. Specifically, we evaluate\nvarious backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot\ndataset in terms of detection accuracy and computational efficiency.\nExperimental results highlight each architecture's strengths and trade-offs,\nproviding insight into selecting suitable models for parking occupancy.", "AI": {"tldr": "\u6b64\u7814\u7a76\u6bd4\u8f83\u4e86\u51e0\u79cd\u96c6\u6210\u4e8eYOLOv8\u7684\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\uff0c\u4ee5\u4f18\u5316\u505c\u8f66\u4f4d\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u505c\u8f66\u4f4d\u5360\u7528\u68c0\u6d4b\u662f\u667a\u80fd\u505c\u8f66\u7ba1\u7406\u7cfb\u7edf\u53d1\u5c55\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u5bf9\u96c6\u6210\u4e86YOLOv8\u7684\u5b9a\u5236\u9aa8\u5e72\u67b6\u6784\u8fdb\u884c\u5168\u9762\u7684\u6bd4\u8f83\u5206\u6790\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728PKLot\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86ResNet-18\u3001VGG16\u3001EfficientNetV2\u3001Ghost\u7b49\u4e0d\u540c\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u6bd4\u8f83\u5176\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u51f8\u663e\u4e86\u6bcf\u79cd\u67b6\u6784\u7684\u4f18\u70b9\u548c\u6743\u8861\uff0c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u505c\u8f66\u5360\u7528\u6a21\u578b\u63d0\u4f9b\u4e86\u6d1e\u5bdf\u3002", "conclusion": "\u4f20\u7edf\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5982YOLOv8\u80fd\u5feb\u901f\u4e14\u51c6\u786e\u5730\u68c0\u6d4b\u505c\u8f66\u573a\u4e2d\u7684\u8f66\u8f86\uff0c\u4f46\u5728\u8f66\u8f86\u90e8\u5206\u53ef\u89c1\u3001\u5c0f\u578b\u8f66\u8f86\u4ee5\u53ca\u5149\u7167\u6761\u4ef6\u5dee\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u901a\u8fc7\u7efc\u5408\u6bd4\u8f83\u5206\u6790\uff0c\u6211\u4eec\u627e\u5230\u4e86\u5404\u4e2a\u67b6\u6784\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2505.17074", "pdf": "https://arxiv.org/pdf/2505.17074", "abs": "https://arxiv.org/abs/2505.17074", "authors": ["Ruixiao Li", "Fahao Chen", "Peng Li"], "title": "Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nemploying a small speculative model (SSM) to generate multiple candidate tokens\nand verify them using the LLM in parallel. This technique has been widely\nintegrated into LLM inference serving systems. However, inference requests\ntypically exhibit uncertain execution time, which poses a significant challenge\nof efficiently scheduling requests in these systems. Existing work estimates\nexecution time based solely on predicted output length, which could be\ninaccurate because execution time depends on both output length and token\nacceptance rate of verification by the LLM. In this paper, we propose a\nsemi-clairvoyant request scheduling algorithm called\nLeast-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a\nnumber of inference requests, LAPS-SD can effectively minimize average\ninference latency by adaptively scheduling requests according to their features\nduring decoding. When the token acceptance rate is dynamic and execution time\nis difficult to estimate, LAPS-SD maintains multiple priority queues and allows\nrequest execution preemption across different queues. Once the token acceptance\nrate becomes stable, LAPS-SD can accurately estimate the execution time and\nschedule requests accordingly. Extensive experiments show that LAPS-SD reduces\ninference latency by approximately 39\\% compared to state-of-the-art scheduling\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LAPS-SD\u7b97\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u8c03\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u76ee\u524d\u63a8\u7406\u8bf7\u6c42\u5b58\u5728\u4e0d\u786e\u5b9a\u6267\u884c\u65f6\u95f4\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u6709\u6548\u8c03\u5ea6\u7cfb\u7edf\u4e2d\u7684\u8bf7\u6c42\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u9884\u77e5\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5LAPS-SD\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u8bf7\u6c42\u6765\u4f18\u5316\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "LAPS-SD\u51cf\u5c11\u4e86\u7ea639%\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "LAPS-SD\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u6267\u884c\u65f6\u95f4\u60c5\u51b5\u4e0b\u6709\u6548\u5730\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2505.17367", "pdf": "https://arxiv.org/pdf/2505.17367", "abs": "https://arxiv.org/abs/2505.17367", "authors": ["Zichuan Yang"], "title": "EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 4 figures", "summary": "Medical image classification is critical for clinical decision-making, yet\ndemands for accuracy, interpretability, and generalizability remain\nchallenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba\narchitecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for\nmulti-organ medical image classification. EVM-Fusion leverages a multipath\ndesign, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)\nmodules, operate in parallel with a traditional feature pathway. These diverse\nfeatures are dynamically integrated via a two-stage fusion process: cross-modal\nattention followed by the iterative NAF block, which learns an adaptive fusion\nalgorithm. Intrinsic explainability is embedded through path-specific spatial\nattention, Vim {\\Delta}-value maps, traditional feature SE-attention, and\ncross-modal attention weights. Experiments on a diverse 9-class multi-organ\nmedical image dataset demonstrate EVM-Fusion's strong classification\nperformance, achieving 99.75% test accuracy and provide multi-faceted insights\ninto its decision-making process, highlighting its potential for trustworthy AI\nin medical diagnostics.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEVM-Fusion\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u591a\u5668\u5b98\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u3002\u901a\u8fc7\u591a\u8def\u5f84\u8bbe\u8ba1\u548c\u65b0\u9896\u7684\u795e\u7ecf\u7b97\u6cd5\u878d\u5408\u673a\u5236\uff0c\u6b64\u67b6\u6784\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523099.75%\u3002\u8be5\u6280\u672f\u5c55\u793a\u4e86\u5176\u5728\u53ef\u4fe1\u8d56\u7684\u533b\u5b66\u8bca\u65ad\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5bf9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u666e\u904d\u6027\u7684\u8981\u6c42\u4ecd\u7136\u662f\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u548c\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u548c\u4fe1\u8d56\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEVM-Fusion\u7684\u89e3\u91ca\u6027Vision Mamba\u67b6\u6784\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7b97\u6cd5\u878d\u5408\uff08NAF\uff09\u673a\u5236\u7528\u4e8e\u591a\u5668\u5b98\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u3002EVM-Fusion\u901a\u8fc7\u591a\u8def\u5f84\u8bbe\u8ba1\uff0c\u5229\u7528DenseNet\u548cU-Net\u4e3a\u57fa\u7840\u7684\u8def\u5f84\uff0c\u5e76\u7ed3\u5408Vision Mamba\uff08Vim\uff09\u6a21\u5757\uff0c\u4e0e\u4f20\u7edf\u7684\u7279\u5f81\u8def\u5f84\u5e76\u884c\u8fd0\u884c\u3002\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u7279\u5f81\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u878d\u5408\u8fc7\u7a0b\u52a8\u6001\u6574\u5408\uff1a\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u540e\uff0c\u8fdb\u884c\u8fed\u4ee3NAF\u5757\uff0c\u4ece\u800c\u5b66\u4e60\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u878d\u5408\u7b97\u6cd5\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u8def\u5f84\u7279\u5b9a\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u3001Vim \u0394 \u503c\u56fe\u3001\u4f20\u7edf\u7279\u5f81\u7684SE\u6ce8\u610f\u529b\u4ee5\u53ca\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5185\u7f6e\u4e86\u5185\u5728\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u901a\u8fc7\u5728\u4e00\u4e2a\u591a\u6837\u76849\u7c7b\u591a\u5668\u5b98\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cEVM-Fusion\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8fbe\u5230\u4e8699.75%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u591a\u65b9\u9762\u6d1e\u5bdf\uff0c\u7a81\u663e\u4e86\u5176\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u4f5c\u4e3a\u503c\u5f97\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86EVM-Fusion\u5728\u591a\u5668\u5b98\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002\u5e76\u4e14\uff0c\u901a\u8fc7\u5176\u5f3a\u5927\u7684\u5206\u7c7b\u6027\u80fd\u548c\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u7684\u6df1\u5165\u7406\u89e3\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.17075", "pdf": "https://arxiv.org/pdf/2505.17075", "abs": "https://arxiv.org/abs/2505.17075", "authors": ["Fuma Kurata", "Mao Saeki", "Masaki Eguchi", "Shungo Suzuki", "Hiroaki Takatsu", "Yoichi Matsuyama"], "title": "Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study aimed to develop and validate two scales of engagement and rapport\nto evaluate the user experience quality with multimodal dialogue systems in the\ncontext of foreign language learning. The scales were designed based on\ntheories of engagement in educational psychology, social psychology, and second\nlanguage acquisition.Seventy-four Japanese learners of English completed\nroleplay and discussion tasks with trained human tutors and a dialog agent.\nAfter each dialogic task was completed, they responded to the scales of\nengagement and rapport. The validity and reliability of the scales were\ninvestigated through two analyses. We first conducted analysis of Cronbach's\nalpha coefficient and a series of confirmatory factor analyses to test the\nstructural validity of the scales and the reliability of our designed items. We\nthen compared the scores of engagement and rapport between the dialogue with\nhuman tutors and the one with a dialogue agent. The results revealed that our\nscales succeeded in capturing the difference in the dialogue experience quality\nbetween the human interlocutors and the dialogue agent from multiple\nperspectives.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u53c2\u4e0e\u5ea6\u548c\u878d\u6d3d\u5ea6\u91cf\u8868\uff0c\u6210\u529f\u6355\u6349\u4e86\u4eba\u7c7b\u4e0e\u5bf9\u8bdd\u4ee3\u7406\u5728\u5bf9\u8bdd\u4f53\u9a8c\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5916\u8bed\u5b66\u4e60\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u548c\u9a8c\u8bc1\u53c2\u4e0e\u5ea6\u548c\u878d\u6d3d\u5ea6\u91cf\u8868\u3002", "method": "\u57fa\u4e8e\u6559\u80b2\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u5fc3\u7406\u5b66\u548c\u4e8c\u8bed\u4e60\u5f97\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u53c2\u4e0e\u5ea6\u548c\u878d\u6d3d\u5ea6\u91cf\u8868\u3002\u7814\u7a76\u901a\u8fc774\u540d\u65e5\u672c\u82f1\u8bed\u5b66\u4e60\u8005\u4e0e\u53d7\u8fc7\u8bad\u7ec3\u7684\u4eba\u7c7b\u5bfc\u5e08\u548c\u5bf9\u8bdd\u4ee3\u7406\u8fdb\u884c\u89d2\u8272\u626e\u6f14\u548c\u8ba8\u8bba\u4efb\u52a1\uff0c\u5e76\u5728\u6bcf\u6b21\u5bf9\u8bdd\u4efb\u52a1\u540e\u56de\u7b54\u53c2\u4e0e\u5ea6\u548c\u878d\u6d3d\u5ea6\u91cf\u8868\u3002\u91c7\u7528Cronbach's \u03b1\u7cfb\u6570\u5206\u6790\u548c\u4e00\u7cfb\u5217\u9a8c\u8bc1\u6027\u56e0\u7d20\u5206\u6790\u7814\u7a76\u91cf\u8868\u7684\u7ed3\u6784\u6548\u5ea6\u548c\u8bbe\u8ba1\u9879\u76ee\u7684\u53ef\u9760\u6027\uff0c\u7136\u540e\u6bd4\u8f83\u4e0e\u4eba\u7c7b\u5bfc\u5e08\u548c\u5bf9\u8bdd\u4ee3\u7406\u7684\u5bf9\u8bdd\u4e2d\u7684\u53c2\u4e0e\u5ea6\u548c\u878d\u6d3d\u5ea6\u5206\u6570\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bbe\u8ba1\u7684\u91cf\u8868\u6210\u529f\u6355\u6349\u5230\u4e86\u4eba\u7c7b\u5bf9\u8bdd\u8005\u4e0e\u5bf9\u8bdd\u4ee3\u7406\u4e4b\u95f4\u7684\u5bf9\u8bdd\u4f53\u9a8c\u8d28\u91cf\u7684\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5730\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u53c2\u4e0e\u5ea6\u548c\u878d\u6d3d\u5ea6\u91cf\u8868\uff0c\u8fd9\u4e9b\u91cf\u8868\u80fd\u591f\u8bc4\u4f30\u4f7f\u7528\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u8fdb\u884c\u5916\u8bed\u5b66\u4e60\u65f6\u7684\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u3002\u5728\u591a\u89c6\u89d2\u4e0b\uff0c\u8fd9\u4e9b\u91cf\u8868\u80fd\u591f\u533a\u5206\u4eba\u7c7b\u5bf9\u8bdd\u8005\u548c\u5bf9\u8bdd\u4ee3\u7406\u4e4b\u95f4\u7684\u5bf9\u8bdd\u4f53\u9a8c\u8d28\u91cf\u5dee\u5f02\u3002"}}
{"id": "2505.17392", "pdf": "https://arxiv.org/pdf/2505.17392", "abs": "https://arxiv.org/abs/2505.17392", "authors": ["Leon C. C. K", "Zeng Hui"], "title": "Dual-sensing driving detection model", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.4.8; J.7"], "comment": "19 pages", "summary": "In this paper, a novel dual-sensing driver fatigue detection method combining\ncomputer vision and physiological signal analysis is proposed. The system\nexploits the complementary advantages of the two sensing modalities and breaks\nthrough the limitations of existing single-modality methods. We introduce an\ninnovative architecture that combines real-time facial feature analysis with\nphysiological signal processing, combined with advanced fusion strategies, for\nrobust fatigue detection. The system is designed to run efficiently on existing\nhardware while maintaining high accuracy and reliability. Through comprehensive\nexperiments, we demonstrate that our method outperforms traditional methods in\nboth controlled environments and real-world conditions, while maintaining high\naccuracy. The practical applicability of the system has been verified through\nextensive tests in various driving scenarios and shows great potential in\nreducing fatigue-related accidents. This study contributes to the field by\nproviding a more reliable, cost-effective, and humane solution for driver\nfatigue detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u751f\u7406\u4fe1\u53f7\u5206\u6790\u7684\u65b0\u578b\u53cc\u6a21\u6001\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7a81\u7834\u5355\u6a21\u6001\u9650\u5236\uff0c\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5e76\u5df2\u9a8c\u8bc1\u5176\u5b9e\u9645\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6a21\u6001\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u4e24\u79cd\u611f\u77e5\u6a21\u5f0f\u7684\u4e92\u8865\u4f18\u52bf\u7a81\u7834\u8fd9\u4e9b\u5c40\u9650\u3002", "method": "\u672c\u6587\u91c7\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u751f\u7406\u4fe1\u53f7\u5206\u6790\u76f8\u7ed3\u5408\u7684\u65b0\u578b\u53cc\u6a21\u6001\u611f\u77e5\u65b9\u6cd5\u3002\u521b\u65b0\u67b6\u6784\u7ed3\u5408\u5b9e\u65f6\u9762\u90e8\u7279\u5f81\u5206\u6790\u4e0e\u751f\u7406\u4fe1\u53f7\u5904\u7406\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u878d\u5408\u7b56\u7565\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u75b2\u52b3\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u53d7\u63a7\u73af\u5883\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u8fd0\u884c\u4e8e\u73b0\u6709\u786c\u4ef6\uff0c\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u9ad8\u3002\u7cfb\u7edf\u7ecf\u8fc7\u5927\u91cf\u9a7e\u9a76\u60c5\u5883\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u5177\u6709\u51cf\u5c11\u75b2\u52b3\u76f8\u5173\u4e8b\u6545\u7684\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u3001\u6210\u672c\u8f83\u4f4e\u3001\u66f4\u52a0\u4eba\u6027\u5316\u7684\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076", "abs": "https://arxiv.org/abs/2505.17076", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Eng Siong Chng"], "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "5 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.", "AI": {"tldr": "\u7814\u7a76\u5e27\u7387\u5bf9\u8bed\u97f3\u6807\u8bb0\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5bf9\u4e0d\u540c\u8bed\u8a00\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7b49\u5e94\u7528\u4e2d\u5e27\u7387\u9009\u62e9\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u5e27\u7387\u5982\u4f55\u5f71\u54cd\u8bed\u97f3\u6807\u8bb0\u5316\uff0c\u4ee5\u4f18\u5316\u8bed\u97f3\u6807\u8bb0\u5668\u7684\u5e27\u7387\u9009\u62e9\u3002", "method": "\u7f16\u7801\u4e0d\u540c\u5e27\u7387\u7684\u8bed\u97f3\u5e76\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u751f\u6210\u7684\u8bed\u4e49\u6807\u8bb0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5e27\u7387\u53d8\u5316\u5bf9\u6bcf\u79cd\u8bed\u8a00\u7684\u8bed\u97f3\u6807\u8bb0\u5316\u6709\u4e0d\u540c\u5f71\u54cd\u3002", "conclusion": "\u5e27\u7387\u5bf9\u8bed\u97f3\u6807\u8bb0\u7684\u5f71\u54cd\u56e0\u8bed\u8a00\u800c\u5f02\uff0c\u5f3a\u8c03\u4e86\u5e27\u7387\u3001\u97f3\u7d20\u5bc6\u5ea6\u548c\u8bed\u8a00\u7279\u5b9a\u58f0\u5b66\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2505.17395", "pdf": "https://arxiv.org/pdf/2505.17395", "abs": "https://arxiv.org/abs/2505.17395", "authors": ["Gowtham Raj Vuppari", "Navarun Gupta", "Ahmed El-Sayed", "Xingguo Xiong"], "title": "Wildfire Detection Using Vision Transformer with the Wildfire Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Published at ASEE NE 2025", "summary": "The critical need for sophisticated detection techniques has been highlighted\nby the rising frequency and intensity of wildfires in the US, especially in\nCalifornia. In 2023, wildfires caused 130 deaths nationwide, the highest since\n1990. In January 2025, Los Angeles wildfires which included the Palisades and\nEaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused\nloss of human lives. The devastation underscores the urgent need for effective\ndetection and prevention strategies. Deep learning models, such as Vision\nTransformers (ViTs), can enhance early detection by processing complex image\ndata with high accuracy. However, wildfire detection faces challenges,\nincluding the availability of high-quality, real-time data. Wildfires often\noccur in remote areas with limited sensor coverage, and environmental factors\nlike smoke and cloud cover can hinder detection. Additionally, training deep\nlearning models is computationally expensive, and issues like false\npositives/negatives and scaling remain concerns. Integrating detection systems\nwith real-time alert mechanisms also poses difficulties. In this work, we used\nthe wildfire dataset consisting of 10.74 GB high-resolution images categorized\ninto 'fire' and 'nofire' classes is used for training the ViT model. To prepare\nthe data, images are resized to 224 x 224 pixels, converted into tensor format,\nand normalized using ImageNet statistics.", "AI": {"tldr": "This paper explores using Vision Transformers (ViTs) to improve wildfire detection in the US despite challenges in data acquisition and computational costs.", "motivation": "The increasing frequency and intensity of wildfires in the US, especially in areas like California, highlight the critical need for advanced detection techniques to mitigate damage and save lives.", "method": "The paper utilizes Vision Transformers (ViTs) for early wildfire detection, employing a dataset of high-resolution images categorized into 'fire' and 'nofire'. Images are resized to 224 x 224 pixels, converted into tensor format for processing, and normalized using ImageNet statistics to train the model.", "result": "The study presents a methodology for using Vision Transformers (ViTs) to enhance early wildfire detection through processing complex image data. However, challenges such as limited real-time data availability and computational expenses remain significant.", "conclusion": "Deep learning models, particularly Vision Transformers (ViTs), have the potential to significantly enhance the early detection of wildfires by processing complex image data with high accuracy. However, challenges such as obtaining high-quality, real-time data and computational costs in training these models need to be addressed for effectiveness."}}
{"id": "2505.17078", "pdf": "https://arxiv.org/pdf/2505.17078", "abs": "https://arxiv.org/abs/2505.17078", "authors": ["Zenghao Duan", "Zhiyi Yin", "Zhichao Shi", "Liang Pang", "Shaoling Jing", "Jiayi Wu", "Yu Yan", "Huawei Shen", "Xueqi Cheng"], "title": "GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the underlying mechanisms of toxicity generation in\nLarge Language Models (LLMs) and proposes an effective detoxification approach.\nPrior work typically considers the Feed-Forward Network (FFN) as the main\nsource of toxicity, representing toxic regions as a set of toxic vectors or\nlayer-wise subspaces. However, our in-depth analysis reveals that the global\ntoxic subspace offers a more effective and comprehensive representation of\ntoxic region within the model. Building on this insight, we propose GloSS\n(Global Toxic Subspace Suppression), a lightweight, four-stage method that\nmitigates toxicity by identifying and removing the global toxic subspace from\nthe parameters of FFN. Experiments across a range of LLMs show that GloSS\nachieves state-of-the-art detoxification performance while preserving the\nmodels general capabilities, without requiring large-scale data or model\nretraining.", "AI": {"tldr": "The paper proposes a new detoxification method for LLMs, GloSS, which surpasses previous methods by focusing on global toxic subspaces instead of layer-wise ones.", "motivation": "To investigate the underlying mechanisms of toxicity generation in Large Language Models and propose an effective detoxification approach.", "method": "This paper proposes GloSS (Global Toxic Subspace Suppression), a four-stage method that identifies and removes the global toxic subspace from the parameters of FFN.", "result": "Experiments across a range of LLMs show that GloSS achieves state-of-the-art detoxification performance without requiring large-scale data or model retraining.", "conclusion": "GloSS achieves state-of-the-art detoxification performance while preserving the models' general capabilities."}}
{"id": "2505.17412", "pdf": "https://arxiv.org/pdf/2505.17412", "abs": "https://arxiv.org/abs/2505.17412", "authors": ["Shuang Wu", "Youtian Lin", "Feihu Zhang", "Yifei Zeng", "Yikang Yang", "Yajie Bao", "Jiachen Qian", "Siyu Zhu", "Philip Torr", "Xun Cao", "Yao Yao"], "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention", "categories": ["cs.CV"], "comment": "Project page: https://nju3dv.github.io/projects/Direct3D-S2/", "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDirect3D S2\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7a00\u758f\u6ce8\u610f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u4f53\u79ef\u6570\u636e\u7684\u5904\u7406\u6548\u7387\uff0c\u5b9e\u73b0\u4e863D\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u7684\u63d0\u5347\uff0c\u5e76\u53ef\u4ee5\u7528\u8f83\u5c11\u7684\u8d44\u6e90\u8fdb\u884c\u5927\u89c4\u6a21\u8bad\u7ec3\u3002", "motivation": "\u751f\u6210\u9ad8\u5206\u8fa8\u73873D\u5f62\u72b6\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4fc3\u8fdb\u66f4\u9ad8\u6548\u76843D\u751f\u6210\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u7a7a\u95f4\u7a00\u758f\u6ce8\u610f\u673a\u5236\uff0c\u5b83\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u7a00\u758f\u4f53\u79ef\u6570\u636e\u4e0a\u7684\u6269\u6563\u53d8\u538b\u5668\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5728\u8f93\u5165\u3001\u6f5c\u5728\u548c\u8f93\u51fa\u9636\u6bb5\u4fdd\u6301\u4e00\u81f4\u7684\u7a00\u758f\u4f53\u79ef\u683c\u5f0f\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u7a7a\u95f4\u7a00\u758f\u6ce8\u610f\u673a\u5236\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u4e0a\u5b9e\u73b0\u4e863.9\u500d\u548c9.6\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u540c\u65f6\u80fd\u591f\u4f7f\u75288\u4e2aGPU\u8fdb\u884c1024\u5206\u8fa8\u7387\u7684\u8bad\u7ec3\uff0c\u76f8\u8f83\u4f20\u7edf\u9700\u8981\u81f3\u5c1132\u4e2aGPU\u8fdb\u884c256\u5206\u8fa8\u7387\u7684\u8bad\u7ec3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4f53\u79ef\u7684\u53ef\u6269\u5c553D\u751f\u6210\u6846\u67b6Direct3D S2\uff0c\u53ef\u4ee5\u4ee5\u663e\u8457\u964d\u4f4e\u7684\u8bad\u7ec3\u6210\u672c\u5b9e\u73b0\u5353\u8d8a\u7684\u8f93\u51fa\u8d28\u91cf\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0cDirect3D S2\u4e0d\u4ec5\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u800c\u4e14\u5b9e\u73b0\u4e86\u4f7f\u7528\u53ea\u67098\u4e2aGPU\u8fdb\u884c1024\u5206\u8fa8\u7387\u8bad\u7ec3\u7684\u80fd\u529b\u3002"}}
{"id": "2505.17080", "pdf": "https://arxiv.org/pdf/2505.17080", "abs": "https://arxiv.org/abs/2505.17080", "authors": ["Davide Picca"], "title": "Not Minds, but Signs: Reframing LLMs through Semiotics", "categories": ["cs.CL"], "comment": null, "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0d\u5e94\u88ab\u89c6\u4f5c\u8ba4\u77e5\u7cfb\u7edf\uff0c\u800c\u5e94\u4ece\u7b26\u53f7\u5b66\u89d2\u5ea6\u7406\u89e3\u5176\u5728\u6587\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u5f3a\u8c03\u5176\u751f\u6210\u6587\u672c\u7684\u80fd\u529b\u800c\u975e\u601d\u8003\uff0c\u91cd\u65b0\u5ba1\u89c6\u8bed\u8a00\u4e0e\u4eba\u5de5\u7cfb\u7edf\u5728\u77e5\u8bc6\u751f\u4ea7\u4e2d\u7684\u89d2\u8272\u3002", "motivation": "\u6311\u6218\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u8ba4\u77e5\u7cfb\u7edf\u7684\u8d8b\u52bf\uff0c\u65e8\u5728\u901a\u8fc7\u7b26\u53f7\u5b66\u6846\u67b6\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7406\u89e3\uff0c\u5e76\u907f\u514d\u62df\u4eba\u5316\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53ca\u5b9e\u9645\u6848\u4f8b\u5e94\u7528\uff0c\u5c55\u793aLLMs\u5982\u4f55\u4f5c\u4e3a\u7b26\u53f7\u4ee3\u7406\uff0c\u5176\u8f93\u51fa\u53ef\u89c6\u4e3a\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u5f00\u653e\u7ed9\u4e0a\u4e0b\u6587\u534f\u5546\u53ca\u6279\u5224\u6027\u53cd\u601d\u3002", "result": "\u5c55\u793aLLMs\u5728\u7b26\u53f7\u73af\u5883\u4e2d\u7684\u884c\u52a8\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u751f\u6210\u6587\u672c\u800c\u975e\u7406\u89e3\u8bed\u8a00\u7684\u5de5\u5177\uff0c\u63d0\u4f9b\u66f4\u5177\u4f26\u7406\u610f\u8bc6\u7684\u6846\u67b6\u6765\u7814\u7a76\u4e0e\u5e94\u7528LLMs\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u91c7\u7528\u7b26\u53f7\u5b66\u89c6\u89d2\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u5316\u8fc7\u7a0b\u4e2d\u6240\u626e\u6f14\u7684\u89d2\u8272\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u751f\u6210\u6587\u672c\u7684\u5de5\u5177\uff0c\u800c\u975e\u5177\u6709\u8ba4\u77e5\u529f\u80fd\u7684\u7cfb\u7edf\u3002"}}
{"id": "2505.17423", "pdf": "https://arxiv.org/pdf/2505.17423", "abs": "https://arxiv.org/abs/2505.17423", "authors": ["Shenghui Chen", "Po-han Li", "Sandeep Chichali", "Ufuk Topcu"], "title": "VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR", "categories": ["cs.CV", "cs.HC", "cs.IT", "math.IT"], "comment": null, "summary": "Many decision-making tasks, where both accuracy and efficiency matter, still\nrequire human supervision. For example, tasks like traffic officers reviewing\nhour-long dashcam footage or researchers screening conference videos can\nbenefit from concise summaries that reduce cognitive load and save time. Yet\ncurrent vision-language models (VLMs) often produce verbose, redundant outputs\nthat hinder task performance. Existing video caption evaluation depends on\ncostly human annotations and overlooks the summaries' utility in downstream\ntasks. We address these gaps with Video-to-text Information Bottleneck\nEvaluation (VIBE), an annotation-free method that scores VLM outputs using two\nmetrics: grounding (how well the summary aligns with visual content) and\nutility (how informative it is for the task). VIBE selects from randomly\nsampled VLM outputs by ranking them according to the two scores to support\neffective human decision-making. Human studies on LearningPaper24,\nSUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE\nconsistently improve performance-boosting task accuracy by up to 61.23% and\nreducing response time by 75.77% compared to naive VLM summaries or raw video.", "AI": {"tldr": "VIBE\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u80fd\u6709\u6548\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6458\u8981\u7684\u5b9e\u7528\u6027\u548c\u5bf9\u9f50\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u751f\u6210\u7684\u8f93\u51fa\u5f80\u5f80\u5197\u957f\u4e14\u91cd\u590d\uff0c\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7b80\u6d01\u6458\u8981\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u4eba\u7c7b\u76d1\u7763\u4efb\u52a1\u4e2d\u7684\u8ba4\u77e5\u8d1f\u62c5\u548c\u8282\u7701\u65f6\u95f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u9891\u5230\u6587\u672c\u4fe1\u606f\u74f6\u9888\u8bc4\u4f30\u65b9\u6cd5\uff08VIBE\uff09\uff0c\u901a\u8fc7\u4e24\u4e2a\u6307\u6807\uff08\u4e0e\u89c6\u89c9\u5185\u5bb9\u7684\u5bf9\u9f50\u5ea6\u548c\u4fe1\u606f\u5b9e\u7528\u6027\uff09\u5bf9VLM\u8f93\u51fa\u8fdb\u884c\u65e0\u6807\u6ce8\u8bc4\u5206\u3002", "result": "\u901a\u8fc7\u5bf9LearningPaper24\u3001SUTD-TrafficQA\u548cLongVideoBench\u8fdb\u884c\u4eba\u7c7b\u7814\u7a76\uff0cVIBE\u6458\u8981\u5728\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u548c\u51cf\u5c11\u54cd\u5e94\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e0e\u5929\u771fVLM\u6458\u8981\u6216\u539f\u59cb\u89c6\u9891\u76f8\u6bd4\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e8661.23%\uff0c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c11\u4e8675.77%\u3002", "conclusion": "VIBE\u9009\u51fa\u7684\u6458\u8981\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u548c\u51cf\u5c11\u4e86\u54cd\u5e94\u65f6\u95f4\uff0c\u8868\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.17082", "pdf": "https://arxiv.org/pdf/2505.17082", "abs": "https://arxiv.org/abs/2505.17082", "authors": ["Abderrahman Skiredj", "Ferdaous Azhari", "Houdaifa Atou", "Nouamane Tazi", "Ismail Berrada"], "title": "GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-source large language models (LLMs) still marginalise Moroccan Arabic\n(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters\nor to sacrifice the very reasoning skills that make LLMs useful. We show that a\nrigorously quality-over-quantity alignment strategy can surface fluent Darija\nwhile safeguarding the backbone s cross-lingual reasoning at a sliver of the\nusual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6\nK and TULU 50 K into Darija, preserve 20 of the English originals, and add\nmathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on\n5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the\nreasoning-dense TULU portion pushes it to 47.5 with no English regression.\nScaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which\nmatches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,\nscoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc\nretains Gemma-27B s strong maths and general-reasoning ability, showing only\nminimal movement on GSM8K and English benchmarks. The entire model is trained\nin just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable\nlanguage technology. We release code, data and checkpoints to spur\nDarija-centric applications in education, public services and everyday digital\ninteraction.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5bf9\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fbe\u91cc\u4e9a\u8bed\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5176\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u7eff\u8272AI\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u8bed\uff08\u8fbe\u91cc\u4e9a\u8bed\uff09\u652f\u6301\u4e0d\u8db3\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u8fbe\u91cc\u4e9a\u8bed\u7684\u6d41\u5229\u5ea6\uff0c\u540c\u65f6\u4fdd\u62a4\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06\u4e09\u79cd\u7d27\u51d1\u578b\u6307\u4ee4\u5957\u4ef6\u7ffb\u8bd1\u4e3a\u8fbe\u91cc\u4e9a\u8bed\uff0c\u5e76\u4fdd\u7559\u90e8\u5206\u539f\u59cb\u82f1\u8bed\u6307\u4ee4\uff0c\u540c\u65f6\u6dfb\u52a0\u6570\u5b66\u3001\u7f16\u7801\u548c\u79d1\u5b66\u63d0\u793a\uff0c\u7136\u540e\u901a\u8fc7LoRA\u8c03\u4f18Gemma\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7LoRA\u8c03\u4f18\uff0c\u8fbe\u91cc\u4e9aMMLU\u4ece32.8\u63d0\u5347\u81f342.7\uff0c\u5e76\u4e14\u6dfb\u52a0\u63a8\u7406\u5bc6\u96c6\u7684\u90e8\u5206\u8fdb\u4e00\u6b65\u63d0\u9ad8\u523047.5\uff1bGemMaroc-27B\u6a21\u578b\u5728\u8fbe\u91cc\u4e9a\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u4fdd\u7559\u4e86\u5f3a\u5927\u7684\u6570\u5b66\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u91c7\u7528\u8d28\u91cf\u91cd\u4e8e\u6570\u91cf\u7684\u5bf9\u9f50\u7b56\u7565\u53ef\u4ee5\u5728\u4fdd\u6301\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u6d41\u7545\u7684\u8fbe\u91cc\u4e9a\u8bed\u5904\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u8fbe\u91cc\u4e9a\u8bed\u4efb\u52a1\u8868\u73b0\uff0c\u8fd8\u5c55\u793a\u4e86\u7eff\u8272\u4eba\u5de5\u667a\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.17425", "pdf": "https://arxiv.org/pdf/2505.17425", "abs": "https://arxiv.org/abs/2505.17425", "authors": ["Wei Jie Yeo", "Rui Mao", "Moloud Abdar", "Erik Cambria", "Ranjan Satapathy"], "title": "Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads", "categories": ["cs.CV", "cs.CL"], "comment": "Under review", "summary": "Multimodal models like CLIP have gained significant attention due to their\nremarkable zero-shot performance across various tasks. However, studies have\nrevealed that CLIP can inadvertently learn spurious associations between target\nvariables and confounding factors. To address this, we introduce\n\\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies\nspurious attention heads in Vision Transformers via mechanistic insights and\nmitigates them through targeted ablation. Furthermore, LTC identifies salient,\ntask-relevant attention heads, enabling the integration of discriminative\nfeatures through orthogonal projection to improve classification performance.\nWe evaluate LTC on benchmarks with inherent background and gender biases,\nachieving over a $>50\\%$ gain in worst-group accuracy compared to non-training\npost-hoc baselines. Additionally, we visualize the representation of selected\nheads and find that the presented interpretation corroborates our contrastive\nmechanism for identifying both spurious and salient attention heads. Code\navailable at https://github.com/wj210/CLIP_LTC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLTC\u7684\u5bf9\u6bd4\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u5207\u9664\u865a\u5047\u6ce8\u610f\u529b\u5934\u6765\u7f13\u89e3CLIP\u4e2d\u7684\u9519\u8bef\u5173\u8054\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u504f\u89c1\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u65e0\u76d1\u7763\u60c5\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6613\u4e8e\u5b66\u4e60\u76ee\u6807\u53d8\u91cf\u4e0e\u6df7\u6dc6\u56e0\u7d20\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7ea0\u6b63\u8fd9\u4e9b\u865a\u5047\u5173\u8054\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u6846\u67b6LTC\uff0c\u901a\u8fc7\u673a\u5236\u6d1e\u5bdf\u8bc6\u522bVision Transformers\u4e2d\u7684\u865a\u5047\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5207\u9664\u52a0\u4ee5\u7f13\u89e3\u3002", "result": "LTC\u6210\u529f\u8bc6\u522b\u5e76\u6821\u6b63\u4e86\u865a\u5047\u6ce8\u610f\u529b\u5934\uff0c\u540c\u65f6\u5229\u7528\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u5f15\u5165\u5224\u522b\u7279\u5f81\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "LTC\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5b58\u5728\u80cc\u666f\u548c\u6027\u522b\u504f\u5dee\u7684\u57fa\u51c6\u4e0a\u7684\u5206\u7c7b\u8868\u73b0\uff0c\u7279\u522b\u662f\u6700\u5dee\u7ec4\u7684\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc750%\u3002"}}
{"id": "2505.17083", "pdf": "https://arxiv.org/pdf/2505.17083", "abs": "https://arxiv.org/abs/2505.17083", "authors": ["Ben Anson", "Xi Wang", "Laurence Aitchison"], "title": "Scale-invariant Attention", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Preprint", "summary": "One persistent challenge in LLM research is the development of attention\nmechanisms that are able to generalise from training on shorter contexts to\ninference on longer contexts. We propose two conditions that we expect all\neffective long context attention mechanisms to have: scale-invariant total\nattention, and scale-invariant attention sparsity. Under a Gaussian assumption,\nwe show that a simple position-dependent transformation of the attention logits\nis sufficient for these conditions to hold. Experimentally we find that the\nresulting scale-invariant attention scheme gives considerable benefits in terms\nof validation loss when zero-shot generalising from training on short contexts\nto validation on longer contexts, and is effective at long-context retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u5177\u6709\u5c3a\u5ea6\u4e0d\u53d8\u7279\u6027\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u4f7f\u7528\uff0c\u5728\u9a8c\u8bc1\u635f\u5931\u53ca\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u4ece\u77ed\u4e0a\u4e0b\u6587\u8bad\u7ec3\u63a8\u5e7f\u5230\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u6709\u6548\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "method": "\u5728\u9ad8\u65af\u5047\u8bbe\u4e0b\uff0c\u6211\u4eec\u8fdb\u884c\u6ce8\u610f\u529blogits\u7684\u7b80\u5355\u4f4d\u7f6e\u4f9d\u8d56\u53d8\u6362\uff0c\u4ee5\u6ee1\u8db3\u63d0\u51fa\u7684\u5c3a\u5ea6\u4e0d\u53d8\u603b\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u7a00\u758f\u6027\u6761\u4ef6\u3002", "result": "\u9a8c\u8bc1\u635f\u5931\u65b9\u9762\u53d6\u5f97\u660e\u663e\u4f18\u52bf\uff0c\u6709\u6548\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5177\u6709\u5c3a\u5ea6\u4e0d\u53d8\u603b\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u7a00\u758f\u6027\u7684\u6ce8\u610f\u529b\u673a\u5236\u5728\u4ece\u77ed\u4e0a\u4e0b\u6587\u8bad\u7ec3\u5230\u957f\u4e0a\u4e0b\u6587\u9a8c\u8bc1\u4e2d\uff0c\u5728\u9a8c\u8bc1\u635f\u5931\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e2d\u6548\u679c\u826f\u597d\u3002"}}
{"id": "2505.17437", "pdf": "https://arxiv.org/pdf/2505.17437", "abs": "https://arxiv.org/abs/2505.17437", "authors": ["Yuanshao Zhu", "James Jianqiao Yu", "Xiangyu Zhao", "Xiao Han", "Qidong Liu", "Xuetao Wei", "Yuxuan Liang"], "title": "Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as a full paper by KDD'25 - Research Track", "summary": "The widespread adoption of mobile devices and data collection technologies\nhas led to an exponential increase in trajectory data, presenting significant\nchallenges in spatio-temporal data mining, particularly for efficient and\naccurate trajectory retrieval. However, existing methods for trajectory\nretrieval face notable limitations, including inefficiencies in large-scale\ndata, lack of support for condition-based queries, and reliance on trajectory\nsimilarity measures. To address the above challenges, we propose OmniTraj, a\ngeneralized and flexible omni-semantic trajectory retrieval framework that\nintegrates four complementary modalities or semantics -- raw trajectories,\ntopology, road segments, and regions -- into a unified system. Unlike\ntraditional approaches that are limited to computing and processing\ntrajectories as a single modality, OmniTraj designs dedicated encoders for each\nmodality, which are embedded and fused into a shared representation space. This\ndesign enables OmniTraj to support accurate and flexible queries based on any\nindividual modality or combination thereof, overcoming the rigidity of\ntraditional similarity-based methods. Extensive experiments on two real-world\ndatasets demonstrate the effectiveness of OmniTraj in handling large-scale\ndata, providing flexible, multi-modality queries, and supporting downstream\ntasks and applications.", "AI": {"tldr": "OmniTraj\u662f\u4e00\u4e2a\u5c06\u591a\u79cd\u6a21\u6001\u6574\u5408\u5230\u7edf\u4e00\u7cfb\u7edf\u4e2d\u7684\u8f68\u8ff9\u68c0\u7d22\u6846\u67b6\uff0c\u652f\u6301\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u67e5\u8be2\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u6548\u679c\u663e\u8457\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u8bbe\u5907\u548c\u6570\u636e\u6536\u96c6\u6280\u672f\u7684\u666e\u53ca\uff0c\u8f68\u8ff9\u6570\u636e\u5448\u6307\u6570\u589e\u957f\uff0c\u5bf9\u65f6\u7a7a\u6570\u636e\u6316\u6398\u5c24\u5176\u662f\u9ad8\u6548\u51c6\u786e\u7684\u8f68\u8ff9\u68c0\u7d22\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u76ee\u524d\u7684\u8f68\u8ff9\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u8bf8\u591a\u9650\u5236\uff0c\u5982\u7f3a\u4e4f\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u7684\u6548\u7387\u3001\u6761\u4ef6\u67e5\u8be2\u652f\u6301\u4e0d\u8db3\u4ee5\u53ca\u4f9d\u8d56\u8f68\u8ff9\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "OmniTraj\u6846\u67b6\u8bbe\u8ba1\u4e86\u4e13\u7528\u7684\u7f16\u7801\u5668\u7528\u4e8e\u6bcf\u79cd\u6a21\u6001\uff0c\u5c06\u5176\u5d4c\u5165\u5e76\u878d\u5408\u5230\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\uff0c\u652f\u6301\u57fa\u4e8e\u4efb\u4f55\u5355\u4e00\u6a21\u6001\u6216\u5176\u7ec4\u5408\u7684\u67e5\u8be2\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOmniTraj\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3001\u63d0\u4f9b\u7075\u6d3b\u7684\u591a\u6a21\u6001\u67e5\u8be2\u4ee5\u53ca\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u548c\u5e94\u7528\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "OmniTraj\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5e76\u652f\u6301\u7075\u6d3b\u7684\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u540c\u65f6\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u548c\u5e94\u7528\u3002"}}
{"id": "2505.17086", "pdf": "https://arxiv.org/pdf/2505.17086", "abs": "https://arxiv.org/abs/2505.17086", "authors": ["Yihong Wu", "Liheng Ma", "Muzhi Li", "Jiaming Zhou", "Jianye Hao", "Ho-fung Leung", "Irwin King", "Yingxue Zhang", "Jian-Yun Nie"], "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination.\n  While Retrieval-Augmented Generation mitigates these issues by integrating\nexternal knowledge, existing approaches rely heavily on in-context learning,\nwhose performance is constrained by the fundamental reasoning capabilities of\nLLMs.\n  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex\nQuestion Answering, comprising a planner that decomposes questions into a\ndirected acyclic graph of subquestions and a worker that resolves questions via\nretrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy\nGradient Optimization), a novel reinforcement learning method that replaces\ntraditional policy gradient updates with Maximum Likelihood Estimation (MLE) by\nsampling trajectories from an asymptotically optimal policy. MyGO eliminates\nthe need for gradient rescaling and reference models, ensuring stable and\nefficient training.\n  Empirical results across multiple datasets demonstrate the effectiveness of\nMujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a\nscalable and resource-efficient solution for complex QA tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faMujica-MyGO\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u7f3a\u4e4f\u4e8b\u5b9e\u77e5\u8bc6\u800c\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u5c3d\u7ba1\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5e76\u53d7\u9650\u4e8eLLM\u7684\u57fa\u672c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Mujica\uff0c\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u95ee\u7b54\u7684\u591a\u8df3\u8054\u5408\u667a\u80fd\u7cfb\u7edf\uff0c\u5305\u62ec\u4e00\u4e2a\u7528\u4e8e\u95ee\u9898\u5206\u89e3\u7684\u89c4\u5212\u5668\u548c\u4e00\u4e2a\u901a\u8fc7\u68c0\u7d22\u548c\u63a8\u7406\u89e3\u51b3\u95ee\u9898\u7684\u5de5\u4f5c\u5668\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86MyGO\uff0c\u4e00\u79cd\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u66ff\u4ee3\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u5c55\u793a\u4e86Mujica-MyGO\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u7684\u6709\u6548\u6027\u3002", "conclusion": "Mujica-MyGO\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17440", "pdf": "https://arxiv.org/pdf/2505.17440", "abs": "https://arxiv.org/abs/2505.17440", "authors": ["Hefei Mei", "Zirui Wang", "Shen You", "Minjing Dong", "Chang Xu"], "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u7684\u89c6\u89c9\u7f16\u7801\u5668\u653b\u51fb(VEAttack)\uff0c\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u6807\u8bb0\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u663e\u8457\u524a\u5f31LVLM\u7684\u591a\u79cd\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86LVLM\u653b\u51fb\u7684\u82e5\u5e72\u5173\u952e\u89c2\u5bdf\u3002", "motivation": "\u73b0\u6709\u7684\u6709\u6548\u653b\u51fb\u96c6\u4e2d\u5728\u4efb\u52a1\u7279\u5b9a\u7684\u767d\u76d2\u8bbe\u7f6e\u4e2d\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728LVLMs\u80cc\u666f\u4e0b\u53d7\u9650\uff0c\u56e0\u4e3aLVLMs\u65e8\u5728\u5904\u7406\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u5168\u6a21\u578b\u68af\u5ea6\u8ba1\u7b97\u3002\u672c\u6587\u901a\u8fc7\u653b\u51fb\u89c6\u89c9\u7f16\u7801\u5668\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "VEAttack\u901a\u8fc7\u6700\u5c0f\u5316\u5e72\u51c0\u548c\u6270\u52a8\u89c6\u89c9\u7279\u5f81\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u6765\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u56fe\u50cf\u6807\u8bb0\u800c\u975e\u5206\u7c7b\u6807\u8bb0\u7684\u65b9\u5f0f\u8fdb\u884c\u56fe\u50cf\u6270\u52a8\u3002", "result": "\u5728\u56fe\u50cf\u5b57\u5e55\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d94.5%\u548c\u5728\u89c6\u89c9\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d75.7%\u3002\u6b64\u5916\uff0c\u63ed\u793a\u4e86\u4e00\u4e9b\u4e0eLVLM\u653b\u51fb/\u9632\u5fa1\u76f8\u5173\u7684\u5173\u952e\u89c2\u5bdf\uff1a1\uff09LLM\u7684\u9690\u85cf\u5c42\u53d8\u5316\uff0c2\uff09\u6807\u8bb0\u6ce8\u610f\u529b\u5dee\u5f02\uff0c3\uff09\u4f20\u9012\u653b\u51fb\u4e2d\u7684\u83ab\u6bd4\u4e4c\u65af\u5e26\u73b0\u8c61\uff0c4\uff09\u5bf9\u653b\u51fb\u6b65\u9aa4\u4f4e\u654f\u611f\u6027\u3002", "conclusion": "VEAttack\u5bf9LVLM\u7684\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u653b\u51fb\uff0c\u4e0d\u9700\u8981\u8bbf\u95ee\u540e\u7eed\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4efb\u52a1\u4fe1\u606f\u548c\u6807\u7b7e\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u6d88\u9664\u4f20\u7edf\u767d\u76d2\u653b\u51fb\u5728LVLM\u4e2d\u7684\u4efb\u52a1\u548c\u6807\u7b7e\u4f9d\u8d56\u6027\u3002"}}
{"id": "2505.17087", "pdf": "https://arxiv.org/pdf/2505.17087", "abs": "https://arxiv.org/abs/2505.17087", "authors": ["Gordana Ispirova", "Michael Sebek", "Giulia Menichetti"], "title": "Informatics for Food Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB", "cs.LG"], "comment": null, "summary": "This chapter explores the evolution, classification, and health implications\nof food processing, while emphasizing the transformative role of machine\nlearning, artificial intelligence (AI), and data science in advancing food\ninformatics. It begins with a historical overview and a critical review of\ntraditional classification frameworks such as NOVA, Nutri-Score, and SIGA,\nhighlighting their strengths and limitations, particularly the subjectivity and\nreproducibility challenges that hinder epidemiological research and public\npolicy. To address these issues, the chapter presents novel computational\napproaches, including FoodProX, a random forest model trained on nutrient\ncomposition data to infer processing levels and generate a continuous FPro\nscore. It also explores how large language models like BERT and BioBERT can\nsemantically embed food descriptions and ingredient lists for predictive tasks,\neven in the presence of missing data. A key contribution of the chapter is a\nnovel case study using the Open Food Facts database, showcasing how multimodal\nAI models can integrate structured and unstructured data to classify foods at\nscale, offering a new paradigm for food processing assessment in public health\nand research.", "AI": {"tldr": "\u672c\u7ae0\u8282\u63a2\u8ba8\u4e86\u98df\u54c1\u52a0\u5de5\u7684\u6f14\u53d8\u53ca\u5206\u7c7b\uff0c\u5206\u6790\u673a\u5668\u5b66\u4e60\u53caAI\u5728\u98df\u54c1\u4fe1\u606f\u5b66\u4e2d\u7684\u4f5c\u7528\uff0c\u5f15\u5165\u4e86FoodProX\u6a21\u578b\u53ca\u591a\u6a21\u6001AI\u4ee5\u6539\u5584\u98df\u54c1\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u7c7b\u6846\u67b6\u5982NOVA\u3001Nutri-Score\u548cSIGA\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u590d\u5236\u6027\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u6d41\u884c\u75c5\u5b66\u7814\u7a76\u548c\u516c\u5171\u653f\u7b56\u5f62\u6210\u969c\u788d\uff0c\u56e0\u800c\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u6539\u5584\u98df\u54c1\u5206\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u7684FoodProX\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u8425\u517b\u6210\u5206\u6570\u636e\u4ee5\u63a8\u65ad\u52a0\u5de5\u6c34\u5e73\uff0c\u5e76\u751f\u6210\u8fde\u7eedFPro\u8bc4\u5206\uff0c\u8fd8\u5229\u7528\u4e86BERT\u548cBioBERT\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u5d4c\u5165\u6765\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u4f7f\u7528Open Food Facts\u6570\u636e\u5e93\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001AI\u6a21\u578b\u5982\u4f55\u6574\u5408\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u6570\u636e\u5b9e\u73b0\u98df\u54c1\u5206\u7c7b\uff0c\u5e76\u4e3a\u516c\u4f17\u5065\u5eb7\u63d0\u4f9b\u65b0\u7684\u98df\u54c1\u52a0\u5de5\u8bc4\u4f30\u9014\u5f84\u3002", "conclusion": "\u8be5\u7ae0\u8282\u8ba4\u4e3a\u591a\u6a21\u6001AI\u6a21\u578b\u80fd\u591f\u6574\u5408\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u98df\u54c1\u5206\u7c7b\uff0c\u4e3a\u516c\u5171\u5065\u5eb7\u548c\u7814\u7a76\u4e2d\u7684\u98df\u54c1\u52a0\u5de5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.17442", "pdf": "https://arxiv.org/pdf/2505.17442", "abs": "https://arxiv.org/abs/2505.17442", "authors": ["Hao Jing", "Anhong Wang", "Yifan Zhang", "Donghan Bu", "Junhui Hou"], "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u9884\u6d4b\u77e5\u8bc6\u84b8\u998f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u5c04\u7387\u91cd\u5efa\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u9ad8\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u70b9\u4e91\u68c0\u6d4b\u7684\u51c6\u786e\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u538b\u7f29\u4f20\u8f93\u7cfb\u7edf\u5728\u53cd\u5c04\u7387\u7f16\u7801\u4e0a\u7684\u8d1f\u62c5\u548c\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u9884\u6d4b\u77e5\u8bc6\u84b8\u998f(RPKD)\u76843D\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u538b\u7f29\u70b9\u4e91\u5750\u6807\uff0c\u820d\u5f03\u53cd\u5c04\u7387\uff0c\u5229\u7528\u51e0\u4f55\u53cd\u5c04\u9884\u6d4b\u6a21\u5757\u91cd\u5efa\u53cd\u5c04\u7387\uff0c\u5e76\u901a\u8fc7\u6559\u5e08\u68c0\u6d4b\u5668\u8fdb\u884c\u53cd\u5c04\u7387\u77e5\u8bc6\u84b8\u998f\u548c\u68c0\u6d4b\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u9ad8\u5b66\u751f\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728KITTI\u6570\u636e\u96c6\u4f4e\u7801\u7387\uff082.146 Bpp\uff09\u4e0b\uff0cRPKD-PV\u65b9\u6cd5\u7684mAP\u8fbe\u523073.6\uff0c\u8d85\u8fc7\u73b0\u6709\u7684PV-RCNN\u57fa\u7ebf\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u9884\u6d4b\u77e5\u8bc6\u84b8\u998f(RPKD)\u76843D\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u5c04\u9884\u6d4b\u6a21\u5757\u91cd\u5efa\u4e22\u5f03\u7684\u53cd\u5c04\u7387\uff0c\u63d0\u9ad8\u4e86\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u70b9\u4e91\u7684\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5b66\u751f\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u5728\u4e0d\u540c\u7801\u7387\u4e0b\u538b\u7f29\u70b9\u4e91\u7684\u68c0\u6d4b\u7cbe\u5ea6\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2505.17089", "pdf": "https://arxiv.org/pdf/2505.17089", "abs": "https://arxiv.org/abs/2505.17089", "authors": ["Md Rafi Ur Rashid", "Vishnu Asutosh Dasu", "Ye Wang", "Gang Tan", "Shagufta Mehnaz"], "title": "Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models", "categories": ["cs.CL"], "comment": "26 pages, 2 figures", "summary": "Large Language Models (LLMs) exhibit impressive capabilities, but remain\nsusceptible to a growing spectrum of safety risks, including jailbreaks, toxic\ncontent, hallucinations, and bias. Existing defenses often address only a\nsingle threat type or resort to rigid outright rejection, sacrificing user\nexperience and failing to generalize across diverse and novel attacks. This\npaper introduces Adversarial Scenario Extrapolation (ASE), a novel\ninference-time computation framework that leverages Chain-of-Thought (CoT)\nreasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides\nthe LLM through a self-generative process of contemplating potential\nadversarial scenarios and formulating defensive strategies before generating a\nresponse to the user query. Comprehensive evaluation on four adversarial\nbenchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak\nattack success rates and minimal toxicity, while slashing outright rejections\nto <4%. ASE outperforms six state-of-the-art defenses in\nrobustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and\n4-10x lower bias scores. By transforming adversarial perception into an\nintrinsic cognitive process, ASE sets a new paradigm for secure and natural\nhuman-AI interaction.", "AI": {"tldr": "The paper introduces ASE, a novel framework enhancing LLM robustness against diverse attacks by simulating adversarial scenarios. It shows considerable improvement in security and natural interaction metrics.", "motivation": "LLMs are prone to various safety risks like jailbreaks, toxic content, hallucinations, and bias, while existing defenses either address a single threat or resort to rigid rejection, which hampers user experience and does not generalize well across attacks.", "method": "The paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that uses Chain-of-Thought (CoT) reasoning to guide LLMs through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before responding to user queries.", "result": "Evaluation on four benchmarks with the latest LLMs shows ASE achieves near-zero jailbreak attack success rates, minimal toxicity, reduces rejections to less than 4%, and outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and 4-10x lower bias scores.", "conclusion": "ASE sets a new paradigm for secure and natural human-AI interaction by transforming adversarial perception into an intrinsic cognitive process."}}
{"id": "2505.17445", "pdf": "https://arxiv.org/pdf/2505.17445", "abs": "https://arxiv.org/abs/2505.17445", "authors": ["Inpyo Song", "Hyemin Hwang", "Jangwon Lee"], "title": "PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "In the United States, as of 2023, pet ownership has reached 66% of households\nand continues to rise annually. This trend underscores the critical need for\neffective pet identification and monitoring methods, particularly as nearly 10\nmillion cats and dogs are reported stolen or lost each year. However,\ntraditional methods for finding lost animals like GPS tags or ID photos have\nlimitations-they can be removed, face signal issues, and depend on someone\nfinding and reporting the pet. To address these limitations, we introduce\nPawPrint and PawPrint+, the first publicly available datasets focused on\nindividual-level footprint identification for dogs and cats. Through\ncomprehensive benchmarking of both modern deep neural networks (e.g., CNN,\nTransformers) and classical local features, we observe varying advantages and\ndrawbacks depending on substrate complexity and data availability. These\ninsights suggest future directions for combining learned global representations\nwith local descriptors to enhance reliability across diverse, real-world\nconditions. As this approach provides a non-invasive alternative to traditional\nID tags, we anticipate promising applications in ethical pet management and\nwildlife conservation efforts.", "AI": {"tldr": "\u5f15\u5165PawPrint\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7ecf\u5178\u7279\u5f81\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5ba0\u7269\u8db3\u8ff9\u8bc6\u522b\u65b9\u6cd5\uff0c\u6539\u5584\u4f20\u7edf\u8bc6\u522b\u624b\u6bb5\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5ba0\u7269\u62e5\u6709\u7387\u7684\u4e0a\u5347\u4ee5\u53ca\u5ba0\u7269\u8d70\u5931\u548c\u88ab\u5077\u4e8b\u4ef6\u7684\u9891\u53d1\uff0c\u73b0\u6709\u7684\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u8bf8\u591a\u9650\u5236\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5ba0\u7269\u8bc6\u522b\u4e0e\u8ffd\u8e2a\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5982CNN\uff0cTransformers\uff09\u548c\u7ecf\u5178\u5c40\u90e8\u7279\u5f81\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc6\u522b\u72d7\u548c\u732b\u7684\u8db3\u8ff9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u65b9\u6cd5\u5728\u57fa\u8d28\u590d\u6742\u6027\u548c\u6570\u636e\u53ef\u83b7\u5f97\u6027\u65b9\u9762\u5404\u6709\u4f18\u52a3\u3002\u7ed3\u5408\u5b66\u4e60\u7684\u5168\u5c40\u8868\u793a\u548c\u5c40\u90e8\u63cf\u8ff0\u7b26\u7684\u65b9\u6cd5\u6709\u671b\u5728\u4e0d\u540c\u7684\u5b9e\u9645\u6761\u4ef6\u4e0b\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165PawPrint\u548cPawPrint+\u6570\u636e\u96c6\u4e3a\u5ba0\u7269\u548c\u91ce\u751f\u52a8\u7269\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u7684\u8bc6\u522b\u548c\u76d1\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\uff0c\u8fd8\u4e3a\u4eca\u540e\u7684\u76f8\u5173\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.17091", "pdf": "https://arxiv.org/pdf/2505.17091", "abs": "https://arxiv.org/abs/2505.17091", "authors": ["Prateek Verma", "Mert Pilanci"], "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "6 pages, 3 figures, 4 tables. Under Review WASPAA 2025", "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.", "AI": {"tldr": "\u8bad\u7ec3\u6587\u672cLLM\u53ef\u4ee5\u83b7\u5f97\u7406\u89e3\u56fe\u50cf\u548c\u97f3\u9891\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u5e94\u7528\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u7814\u7a76\u6587\u672c\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u9605\u8bfb\u5f00\u53d1\u51fa\u7406\u89e3\u56fe\u50cf\u548c\u97f3\u9891\u7684\u80fd\u529b\uff0c\u63a8\u8fdb\u5bf9\u6587\u672cLLM\u5b66\u4e60\u5185\u90e8\u7535\u8def\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u81ea\u56de\u5f52LLM\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u4ee4\u724c\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8f93\u5165\u56fe\u50cf\u8865\u4e01\u3001\u97f3\u9891\u6ce2\u5f62\u6216\u4ee4\u724c\u4f5c\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u5206\u7c7b\u7ba1\u9053\u4e2d\u7684\u5d4c\u5165\u6216\u7c7b\u522b\u6807\u7b7e\u3002", "result": "\u5c55\u793a\u6587\u672c\u6743\u91cd\u6709\u52a9\u4e8e\u97f3\u9891\u5206\u7c7b\uff08FSD-50K\u548cGTZAN\u6570\u636e\u96c6\uff09\u4ee5\u53ca\u56fe\u50cf\u5206\u7c7b\uff08CIFAR-10\u548cFashion-MNIST\u6570\u636e\u96c6\uff09\u548c\u56fe\u50cf\u8865\u4e01\u3002", "conclusion": "\u901a\u8fc7\u6fc0\u6d3b\u6587\u672c\u6a21\u578b\u5185\u90e8\u5f3a\u5927\u7684\u8fde\u63a5\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5404\u79cd\u5e94\u7528\uff0c\u800c\u4e0d\u9700\u8981\u6bcf\u6b21\u90fd\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2505.17449", "pdf": "https://arxiv.org/pdf/2505.17449", "abs": "https://arxiv.org/abs/2505.17449", "authors": ["Inpyo Song", "Jangwon Lee"], "title": "Real-time Traffic Accident Anticipation with Feature Reuse", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "This paper addresses the problem of anticipating traffic accidents, which\naims to forecast potential accidents before they happen. Real-time anticipation\nis crucial for safe autonomous driving, yet most methods rely on\ncomputationally heavy modules like optical flow and intermediate feature\nextractors, making real-world deployment challenging. In this paper, we thus\nintroduce RARE (Real-time Accident anticipation with Reused Embeddings), a\nlightweight framework that capitalizes on intermediate features from a single\npre-trained object detector. By eliminating additional feature-extraction\npipelines, RARE significantly reduces latency. Furthermore, we introduce a\nnovel Attention Score Ranking Loss, which prioritizes higher attention on\naccident-related objects over non-relevant ones. This loss enhances both\naccuracy and interpretability. RARE demonstrates a 4-8 times speedup over\nexisting approaches on the DAD and CCD benchmarks, achieving a latency of\n13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced\ncomplexity, it attains state-of-the-art Average Precision and reliably\nanticipates imminent collisions in real time. These results highlight RARE's\npotential for safety-critical applications where timely and explainable\nanticipation is essential.", "AI": {"tldr": "RARE\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6846\u67b6\u51cf\u5c11\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4e8b\u6545\u9884\u6d4b\uff0c\u9002\u5408\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "motivation": "\u9884\u6d4b\u6f5c\u5728\u4ea4\u901a\u4e8b\u6545\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u3002", "method": "\u5f15\u5165RARE\u6846\u67b6\uff0c\u5229\u7528\u5355\u4e2a\u9884\u8bad\u7ec3\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u5f97\u5206\u6392\u5e8f\u635f\u5931\u4f18\u5148\u5173\u6ce8\u76f8\u5173\u5bf9\u8c61\uff0c\u51cf\u5c11\u989d\u5916\u7279\u5f81\u63d0\u53d6\u6b65\u9aa4\u3002", "result": "RARE\u5728DAD\u548cCCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e864-8\u500d\u52a0\u901f\uff0c\u8fbe\u5230\u6bcf\u5e2713.6\u6beb\u79d2\uff0873.3 FPS\uff09\u7684\u5ef6\u8fdf\uff0c\u5e76\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u5747\u7cbe\u5ea6\u3002", "conclusion": "RARE\u63d0\u4f9b\u7684\u5b9e\u65f6\u4e8b\u6545\u9884\u6d4b\u6846\u67b6\u5728\u51cf\u5c11\u5ef6\u8fdf\u548c\u63d0\u9ad8\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7a81\u51fa\u5176\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.17095", "pdf": "https://arxiv.org/pdf/2505.17095", "abs": "https://arxiv.org/abs/2505.17095", "authors": ["Kristine Ann M. Carandang", "Jasper Meynard P. Ara\u00f1a", "Ethan Robert A. Casin", "Christopher P. Monterola", "Daniel Stanley Y. Tan", "Jesus Felix B. Valenzuela", "Christian M. Alis"], "title": "Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation", "categories": ["cs.CL"], "comment": null, "summary": "Due to the legal and ethical responsibilities of healthcare providers (HCPs)\nfor accurate documentation and protection of patient data privacy, the natural\nvariability in the responses of large language models (LLMs) presents\nchallenges for incorporating clinical note generation (CNG) systems, driven by\nLLMs, into real-world clinical processes. The complexity is further amplified\nby the detailed nature of texts in CNG. To enhance the confidence of HCPs in\ntools powered by LLMs, this study evaluates the reliability of 12 open-weight\nand proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms\nof their ability to generate notes that are string equivalent (consistency\nrate), have the same meaning (semantic consistency) and are correct (semantic\nsimilarity), across several iterations using the same prompt. The results show\nthat (1) LLMs from all model families are stable, such that their responses are\nsemantically consistent despite being written in various ways, and (2) most of\nthe LLMs generated notes close to the corresponding notes made by experts.\nOverall, Meta's Llama 70B was the most reliable, followed by Mistral's Small\nmodel. With these findings, we recommend the local deployment of these\nrelatively smaller open-weight models for CNG to ensure compliance with data\nprivacy regulations, as well as to improve the efficiency of HCPs in clinical\ndocumentation.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8612\u4e2a\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u8bb0\u5f55\u751f\u6210\u4e2d\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6b63\u786e\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0Meta\u7684Llama 70B\u6700\u53ef\u9760\uff0c\u5efa\u8bae\u672c\u5730\u90e8\u7f72\u4ee5\u6539\u5584\u6570\u636e\u9690\u79c1\u5408\u89c4\u548c\u6587\u6863\u5199\u4f5c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u533b\u7597\u63d0\u4f9b\u8005\u5728\u51c6\u786e\u8bb0\u5f55\u548c\u4fdd\u62a4\u60a3\u8005\u6570\u636e\u9690\u79c1\u65b9\u9762\u7684\u6cd5\u5f8b\u548c\u4f26\u7406\u8d23\u4efb\uff0c\u9700\u8981\u8bc4\u4f30\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4e34\u5e8a\u8bb0\u5f55\u751f\u6210\u7cfb\u7edf\u5728\u771f\u5b9e\u4e34\u5e8a\u6d41\u7a0b\u4e2d\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002", "method": "\u8bc4\u4f3012\u4e2a\u6765\u81eaAnthropic\u3001Meta\u3001Mistral\u548cOpenAI\u7684\u5f00\u6e90\u53ca\u5546\u4e1a\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u8bb0\u5f55\u751f\u6210\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5305\u62ec\u4ed6\u4eec\u5728\u5b57\u7b26\u4e32\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6b63\u786e\u6027\uff08\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u6a21\u578b\u7ec4\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u7a33\u5b9a\uff0c\u5927\u90e8\u5206\u6a21\u578b\u751f\u6210\u7684\u8bb0\u5f55\u63a5\u8fd1\u4e13\u5bb6\u7f16\u5199\u7684\u5bf9\u5e94\u8bb0\u5f55\u3002", "conclusion": "Meta\u7684Llama 70B\u5728\u751f\u6210\u4e34\u5e8a\u8bb0\u5f55\u65b9\u9762\u6700\u53ef\u9760\uff0c\u5176\u6b21\u662fMistral\u7684\u5c0f\u6a21\u578b\u3002\u5efa\u8bae\u672c\u5730\u90e8\u7f72\u8fd9\u4e9b\u76f8\u5bf9\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u6570\u636e\u9690\u79c1\u5408\u89c4\u5e76\u63d0\u9ad8\u533b\u7597\u63d0\u4f9b\u8005\u5728\u4e34\u5e8a\u6587\u6863\u7f16\u5199\u4e2d\u7684\u6548\u7387\u3002"}}
{"id": "2505.17457", "pdf": "https://arxiv.org/pdf/2505.17457", "abs": "https://arxiv.org/abs/2505.17457", "authors": ["Jiaxuan Lu", "Junyan Shi", "Yuhui Lin", "Fang Yan", "Yue Gao", "Shaoting Zhang", "Xiaosong Wang"], "title": "Graph Mamba for Efficient Whole Slide Image Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Whole Slide Images (WSIs) in histopathology present a significant challenge\nfor large-scale medical image analysis due to their high resolution, large\nsize, and complex tile relationships. Existing Multiple Instance Learning (MIL)\nmethods, such as Graph Neural Networks (GNNs) and Transformer-based models,\nface limitations in scalability and computational cost. To bridge this gap, we\npropose the WSI-GMamba framework, which synergistically combines the relational\nmodeling strengths of GNNs with the efficiency of Mamba, the State Space Model\ndesigned for sequence learning. The proposed GMamba block integrates Message\nPassing, Graph Scanning & Flattening, and feature aggregation via a\nBidirectional State Space Model (Bi-SSM), achieving Transformer-level\nperformance with 7* fewer FLOPs. By leveraging the complementary strengths of\nlightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable\nsolution for large-scale WSI analysis, offering both high accuracy and\ncomputational efficiency for slide-level classification.", "AI": {"tldr": "WSI-GMamba\u6846\u67b6\u7ed3\u5408GNNs\u548cMamba\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u4e86WSI\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u5927\u89c4\u6a21\u5206\u6790\u3002", "motivation": "\u7531\u4e8eWSIs\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u5206\u8fa8\u7387\u9ad8\u3001\u4f53\u79ef\u5927\u4ee5\u53ca\u590d\u6742\u7684\u7ad6\u6392\u5173\u7cfb\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u65b9\u6cd5\uff0c\u6bd4\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u7684\u9650\u5236\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86WSI-GMamba\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "WSI-GMamba\u6846\u67b6\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7GNNs\u7684\u5173\u7cfb\u6a21\u578b\u548cMamba\u7684\u8ba1\u7b97\u6548\u7387\u3002GMamba\u6a21\u5757\u901a\u8fc7\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08Bi-SSM\uff09\u96c6\u6210\u6d88\u606f\u4f20\u9012\u3001\u56fe\u626b\u63cf\u4e0e\u538b\u7f29\uff0c\u4ee5\u53ca\u7279\u5f81\u805a\u5408\uff0c\u8fbe\u5230Transformer\u7ea7\u522b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c117\u500d\u7684FLOPs\u3002", "result": "WSI-GMamba\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7GNNs\u548cMamba\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u4e8e\u5927\u89c4\u6a21WSI\u5206\u6790\u3002\u8be5\u6846\u67b6\u5728\u5e7b\u706f\u7247\u7ea7\u5206\u7c7b\u4e2d\u63d0\u4f9b\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "WSI-GMamba\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5927\u89c4\u6a21WSI\u5206\u6790\uff0c\u5728\u63d0\u4f9b\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u5e7b\u706f\u7247\u7ea7\u5206\u7c7b\u3002"}}
{"id": "2505.17098", "pdf": "https://arxiv.org/pdf/2505.17098", "abs": "https://arxiv.org/abs/2505.17098", "authors": ["Yanshu Li", "Tian Yun", "Jianjiang Yang", "Pinyuan Feng", "Jinfa Huang", "Ruixiang Tang"], "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration", "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 11 figures, 19 tables. arXiv admin note: substantial text\n  overlap with arXiv:2503.04839", "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input in-context\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures in-context sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a valuable perspective for\ninterpreting and improving multimodal ICL.", "AI": {"tldr": "The paper presents TACO, a model that leverages task mapping for better multimodal ICL, showing improved performance across various tasks and datasets.", "motivation": "Address limitations in understanding how LVLMs exploit in-context sequences during inference and to improve multimodal ICL.", "method": "Present TACO, a lightweight transformer-based model equipped with task-aware attention that dynamically configures in-context sequences, and inject task-mapping signals into the autoregressive decoding process.", "result": "Experiments on five LVLMs and nine datasets show TACO's consistent superiority over baselines across diverse ICL tasks.", "conclusion": "TACO consistently surpasses baselines across diverse ICL tasks, indicating that task mapping is a valuable perspective for interpreting and improving multimodal ICL."}}
{"id": "2505.17461", "pdf": "https://arxiv.org/pdf/2505.17461", "abs": "https://arxiv.org/abs/2505.17461", "authors": ["Kazuki Hayashi", "Shintaro Ozaki", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large-scale Vision Language Models (LVLMs) are increasingly being applied to\na wide range of real-world multimodal applications, involving complex visual\nand linguistic reasoning. As these models become more integrated into practical\nuse, they are expected to handle complex aspects of human interaction. Among\nthese, color perception is a fundamental yet highly variable aspect of visual\nunderstanding. It differs across individuals due to biological factors such as\nColor Vision Deficiencies (CVDs), as well as differences in culture and\nlanguage. Despite its importance, perceptual diversity has received limited\nattention. In our study, we evaluate LVLMs' ability to account for individual\nlevel perceptual variation using the Ishihara Test, a widely used method for\ndetecting CVDs. Our results show that LVLMs can explain CVDs in natural\nlanguage, but they cannot simulate how people with CVDs perceive color in image\nbased tasks. These findings highlight the need for multimodal systems that can\naccount for color perceptual diversity and support broader discussions on\nperceptual inclusiveness and fairness in multimodal AI.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cLVLMs\u80fd\u89e3\u91ca\u4f46\u65e0\u6cd5\u6a21\u62df\u8272\u89c9\u969c\u788d\u60a3\u8005\u7684\u989c\u8272\u611f\u77e5\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u5305\u5bb9\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u3002", "motivation": "\u63a2\u8ba8LVLMs\u5728\u5904\u7406\u4eba\u7c7b\u89c6\u89c9\u548c\u8bed\u8a00\u611f\u77e5\u591a\u6837\u6027\u65f6\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u989c\u8272\u611f\u77e5\u65b9\u9762\u7684\u4e2a\u4f53\u5dee\u5f02\u3002", "method": "\u4f7f\u7528Ishihara Test\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30LVLMs\u662f\u5426\u80fd\u591f\u89e3\u91ca\u548c\u6a21\u62df\u8272\u89c9\u969c\u788d\u60a3\u8005\u7684\u89c6\u89c9\u4f53\u9a8c\u3002", "result": "LVLMs\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u8272\u89c9\u5f02\u5e38\uff0c\u4f46\u65e0\u6cd5\u6a21\u62df\u8fd9\u4e9b\u4eba\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u989c\u8272\u611f\u77e5\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8003\u8651\u989c\u8272\u611f\u77e5\u591a\u6837\u6027\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u4ee5\u4fc3\u8fdb\u5173\u4e8e\u611f\u77e5\u5305\u5bb9\u6027\u548c\u516c\u5e73\u6027\u7684\u66f4\u5e7f\u6cdb\u8ba8\u8bba\u3002"}}
{"id": "2505.17099", "pdf": "https://arxiv.org/pdf/2505.17099", "abs": "https://arxiv.org/abs/2505.17099", "authors": ["Xiaozhao Liu", "Dinggang Shen", "Xihui Liu"], "title": "Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation", "categories": ["cs.CL"], "comment": "Code, checkpoint and text samples available at\n  https://github.com/justin-xzliu/GLIM", "summary": "Pretrained generative models have opened new frontiers in brain decoding by\nenabling the synthesis of realistic texts and images from non-invasive brain\nrecordings. However, the reliability of such outputs remains\nquestionable--whether they truly reflect semantic activation in the brain, or\nare merely hallucinated by the powerful generative models. In this paper, we\nfocus on EEG-to-text decoding and address its hallucination issue through the\nlens of posterior collapse. Acknowledging the underlying mismatch in\ninformation capacity between EEG and text, we reframe the decoding task as\nsemantic summarization of core meanings rather than previously verbatim\nreconstruction of stimulus texts. To this end, we propose the Generative\nLanguage Inspection Model (GLIM), which emphasizes learning informative and\ninterpretable EEG representations to improve semantic grounding under\nheterogeneous and small-scale data conditions. Experiments on the public ZuCo\ndataset demonstrate that GLIM consistently generates fluent, EEG-grounded\nsentences without teacher forcing. Moreover, it supports more robust evaluation\nbeyond text similarity, through EEG-text retrieval and zero-shot semantic\nclassification across sentiment categories, relation types, and corpus topics.\nTogether, our architecture and evaluation protocols lay the foundation for\nreliable and scalable benchmarking in generative brain decoding.", "AI": {"tldr": "\u63d0\u51faGLIM\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u603b\u7ed3\u548c\u6539\u8fdb\u8bc4\u4f30\u6807\u51c6\uff0c\u6210\u529f\u89e3\u51b3EEG\u5230\u6587\u672c\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u8bed\u4e49\u89e3\u7801\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u6587\u7ae0\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5f53\u524dEEG\u5230\u6587\u672c\u89e3\u7801\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u6587\u672c\u662f\u5426\u771f\u6b63\u53cd\u6620\u4e86\u5927\u8111\u4e2d\u7684\u8bed\u4e49\u6fc0\u6d3b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u751f\u6210\u6a21\u578b\u7684\u5e7b\u89c9\u3002\u4f5c\u8005\u8ba4\u8bc6\u5230EEG\u548c\u6587\u672c\u4e4b\u95f4\u5728\u4fe1\u606f\u5bb9\u91cf\u4e0a\u5b58\u5728\u7684\u6839\u672c\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5c06\u89e3\u7801\u4efb\u52a1\u91cd\u65b0\u6846\u5b9a\u4e3a\u6838\u5fc3\u610f\u4e49\u7684\u8bed\u4e49\u603b\u7ed3\uff0c\u800c\u4e0d\u662f\u4ee5\u524d\u7684\u9010\u5b57\u6587\u672c\u91cd\u5efa\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u8bed\u8a00\u68c0\u67e5\u6a21\u578b\uff08GLIM\uff09\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u4fe1\u606f\u91cf\u5927\u4e14\u53ef\u89e3\u91ca\u7684EEG\u8868\u793a\uff0c\u6539\u8fdb\u4e86\u5728\u5f02\u8d28\u548c\u5c0f\u89c4\u6a21\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u57fa\u7840\u3002\u8fd9\u4e9b\u6539\u8fdb\u5305\u62ec\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5f3a\u5236\u7684\u81ea\u7136\u6d41\u7545\u7684EEG\u57fa\u7840\u53e5\u5b50\u751f\u6210\uff0c\u5e76\u652f\u6301\u8d85\u8d8a\u6587\u672c\u76f8\u4f3c\u6027\u7684\u5065\u58ee\u8bc4\u4f30\uff0c\u901a\u8fc7EEG-\u6587\u672c\u68c0\u7d22\u548c\u96f6\u6837\u672c\u8bed\u4e49\u5206\u7c7b\u8fdb\u884c\u8de8\u60c5\u611f\u7c7b\u522b\u3001\u5173\u7cfb\u7c7b\u578b\u548c\u8bed\u6599\u5e93\u4e3b\u9898\u7684\u8bc4\u4f30\u3002", "result": "\u5728ZuCo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGLIM\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u6559\u5e08\u5f3a\u5236\u60c5\u51b5\u4e0b\uff0c\u4e00\u81f4\u6027\u751f\u6210\u6d41\u7545\u4e14\u57fa\u4e8eEEG\u7684\u53e5\u5b50\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8fdb\u884cEEG-\u6587\u672c\u68c0\u7d22\u548c\u96f6\u6837\u672c\u8bed\u4e49\u5206\u7c7b\uff0c\u5728\u60c5\u611f\u7c7b\u522b\u3001\u5173\u7cfb\u7c7b\u578b\u548c\u8bed\u6599\u5e93\u4e3b\u9898\u4e0a\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684GLIM\u6a21\u578b\u4e3aEEG\u5230\u6587\u672c\u7684\u89e3\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u603b\u7ed3\u548c\u6539\u8fdb\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u4e3a\u5927\u8111\u89e3\u7801\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.17473", "pdf": "https://arxiv.org/pdf/2505.17473", "abs": "https://arxiv.org/abs/2505.17473", "authors": ["Jiangning Zhu", "Yuxing Zhou", "Zheng Wang", "Juntao Yao", "Yima Gu", "Yuhui Yuan", "Shixia Liu"], "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nOrionBench, a benchmark designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 26,250 real\nand 78,750 synthetic infographics, with over 6.9 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of OrionBench through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection.", "AI": {"tldr": "OrionBench\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u7cbe\u786e\u68c0\u6d4b\u6a21\u578b\u7684\u57fa\u51c6\u5de5\u5177\uff0c\u652f\u6301\u56fe\u8868\u548c\u4eba\u7c7b\u53ef\u8bc6\u522b\u5bf9\u8c61\u7684\u68c0\u6d4b\uff0c\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u8868\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u56fe\u8868\u5728\u79d1\u5b66\u3001\u5546\u4e1a\u3001\u548c\u4ea4\u6d41\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u56fe\u8868\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5728\u4fe1\u606f\u56fe\u5143\u7d20\u7684\u89c6\u89c9\u5b9a\u4f4d\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u5faa\u73af\u548c\u7f16\u7a0b\u65b9\u6cd5\u6765\u751f\u6210\u6807\u6ce8\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6807\u6ce8\u6765\u5f00\u53d1\u68c0\u6d4b\u6a21\u578b\u3002", "result": "OrionBench\u5305\u542b\u5927\u91cf\u771f\u5b9e\u548c\u5408\u6210\u7684\u4fe1\u606f\u56fe\u53ca\u6807\u6ce8\uff0c\u901a\u8fc7\u6784\u5efa\u601d\u7ef4\u76d2\u5b50\u65b9\u6848\u3001\u6bd4\u8f83\u73b0\u6709\u6a21\u578b\u548c\u5e94\u7528\u4e8e\u6587\u6863\u5e03\u5c40\u4e0eUI\u5143\u7d20\u68c0\u6d4b\u4e09\u4e2a\u65b9\u9762\u8bc1\u660e\u5176\u6548\u7528\u3002", "conclusion": "OrionBench\u63d0\u5347\u4e86\u56fe\u8868\u548c\u4eba\u7c7b\u53ef\u8bc6\u522b\u5bf9\u8c61\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u8868\u7406\u89e3\u6027\u80fd\u3002"}}
{"id": "2505.17100", "pdf": "https://arxiv.org/pdf/2505.17100", "abs": "https://arxiv.org/abs/2505.17100", "authors": ["Haoyan Yang", "Runxue Bao", "Cao Xiao", "Jun Ma", "Parminder Bhatia", "Shangqian Gao", "Taha Kass-Hout"], "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a promising tool for automatically evaluating\ngenerated outputs, but its reliability is often undermined by potential biases\nin judgment. Existing efforts to mitigate these biases face key limitations:\nin-context learning-based methods fail to address rooted biases due to the\nevaluator's limited capacity for self-reflection, whereas fine-tuning is not\napplicable to all evaluator types, especially closed-source models. To address\nthis challenge, we introduce the Reasoning-based Bias Detector (RBD), which is\na plug-in module that identifies biased evaluations and generates structured\nreasoning to guide evaluator self-correction. Rather than modifying the\nevaluator itself, RBD operates externally and engages in an iterative process\nof bias detection and feedback-driven revision. To support its development, we\ndesign a complete pipeline consisting of biased dataset construction,\nsupervision collection, distilled reasoning-based fine-tuning of RBD, and\nintegration with LLM evaluators. We fine-tune four sizes of RBD models, ranging\nfrom 1.5B to 14B, and observe consistent performance improvements across all\nscales. Experimental results on 4 bias types--verbosity, position, bandwagon,\nand sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong\neffectiveness. For example, the RBD-8B model improves evaluation accuracy by an\naverage of 18.5% and consistency by 10.9%, and surpasses prompting-based\nbaselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results\nhighlight RBD's effectiveness and scalability. Additional experiments further\ndemonstrate its strong generalization across biases and domains, as well as its\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u5dee\u68c0\u6d4b\u673a\u5236RBD\uff0c\u4e0d\u9700\u8981\u4fee\u6539LLM\u8bc4\u4f30\u5668\u672c\u8eab\uff0c\u5373\u53ef\u63d0\u9ad8\u5176\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u8bc4\u4f30\u5668\u7531\u4e8e\u6f5c\u5728\u504f\u89c1\u5bfc\u81f4\u53ef\u9760\u6027\u53d7\u635f\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Reasoning-based Bias Detector (RBD)\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\uff0c\u901a\u8fc7\u504f\u5dee\u68c0\u6d4b\u4e0e\u53cd\u9988\u9a71\u52a8\u4fee\u6b63\u7684\u8fed\u4ee3\u8fc7\u7a0b\u6765\u64cd\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u5b8c\u6574\u7684\u7ba1\u9053\u652f\u6301\u5176\u53d1\u5c55\u3002", "result": "RBD\u5728\u591a\u79cd\u504f\u89c1\u7c7b\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6709\u6548\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cRBD-8B\u6a21\u578b\u5c06\u8bc4\u4f30\u51c6\u786e\u6027\u5e73\u5747\u63d0\u9ad8\u4e8618.5%\uff0c\u4e00\u81f4\u6027\u63d0\u9ad8\u4e8610.9%\uff0c\u8d85\u8d8a\u4e86\u63d0\u793a\u57fa\u51c6\u548c\u5fae\u8c03\u8bc4\u5224\u800512.8%\u548c17.2%\u3002\u6b64\u5916\uff0c\u5176\u8de8\u9886\u57df\u7684\u504f\u89c1\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u826f\u597d\u3002", "conclusion": "RBD\u5177\u6709\u663e\u8457\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5176\u80fd\u591f\u5728\u591a\u4e2aLLM\u8bc4\u4f30\u5668\u4e0a\u63d0\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.17475", "pdf": "https://arxiv.org/pdf/2505.17475", "abs": "https://arxiv.org/abs/2505.17475", "authors": ["Uyoung Jeong", "Jonathan Freer", "Seungryul Baek", "Hyung Jin Chang", "Kwang In Kim"], "title": "PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025", "summary": "We study multi-dataset training (MDT) for pose estimation, where skeletal\nheterogeneity presents a unique challenge that existing methods have yet to\naddress. In traditional domains, \\eg regression and classification, MDT\ntypically relies on dataset merging or multi-head supervision. However, the\ndiversity of skeleton types and limited cross-dataset supervision complicate\nintegration in pose estimation. To address these challenges, we introduce\nPoseBH, a new MDT framework that tackles keypoint heterogeneity and limited\nsupervision through two key techniques. First, we propose nonparametric\nkeypoint prototypes that learn within a unified embedding space, enabling\nseamless integration across skeleton types. Second, we develop a cross-type\nself-supervision mechanism that aligns keypoint predictions with keypoint\nembedding prototypes, providing supervision without relying on teacher-student\nmodels or additional augmentations. PoseBH substantially improves\ngeneralization across whole-body and animal pose datasets, including\nCOCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard\nhuman pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint\nembeddings transfer effectively to hand shape estimation (InterHand2.6M) and\nhuman body shape estimation (3DPW). The code for PoseBH is available at:\nhttps://github.com/uyoung-jeong/PoseBH.", "AI": {"tldr": "\u63a8\u51fa\u4e86PoseBH\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5316\u5173\u952e\u70b9\u539f\u578b\u548c\u81ea\u76d1\u7763\u673a\u5236\u89e3\u51b3\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6210\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u59ff\u6001\u4f30\u8ba1\u4e2d\u9aa8\u67b6\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u96c6\u5408\u5e76\u548c\u591a\u5934\u76d1\u7763\u65b9\u9762\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u975e\u53c2\u6570\u5316\u5173\u952e\u70b9\u539f\u578b\u548c\u8de8\u7c7b\u578b\u81ea\u76d1\u7763\u673a\u5236\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u5173\u952e\u70b9\uff0c\u4ece\u800c\u5b9e\u73b0\u5404\u7c7b\u9aa8\u67b6\u7c7b\u578b\u7684\u65e0\u7f1d\u6574\u5408\u3002", "result": "PoseBH\u517c\u5177\u63d0\u9ad8\u80fd\u529b\u548c\u5bf9\u591a\u79cd\u6570\u636e\u96c6\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4ee3\u7801\u5728GitHub\u4e0a\u53ef\u7528\u3002", "conclusion": "PoseBH\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u4eba\u4f53\u548c\u52a8\u7269\u59ff\u6001\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5728\u6807\u51c6\u4eba\u7c7b\u59ff\u6001\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e86\u8868\u73b0\u3002"}}
{"id": "2505.17101", "pdf": "https://arxiv.org/pdf/2505.17101", "abs": "https://arxiv.org/abs/2505.17101", "authors": ["Santiago Acevedo", "Andrea Mascaretti", "Riccardo Rende", "Mat\u00e9o Mahaut", "Marco Baroni", "Alessandro Laio"], "title": "An approach to identify the most semantically informative deep representations of text and images", "categories": ["cs.CL", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information is spread across many tokens\nand it is characterized by long-distance correlations between tokens and by a\ncausal left-to-right (i.e., past-future) asymmetry. We also identify layers\nencoding semantic information within visual transformers. We show that caption\nrepresentations in the semantic layers of LLMs predict visual representations\nof the corresponding images. We observe significant and model-dependent\ninformation asymmetries between image and text representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u76f8\u4f3c\u8868\u793a\u3002\u53d1\u73b0\u8bed\u4e49\u4fe1\u606f\u5728\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8f6c\u6362\u5668\u5185\u90e8\u5c42\u7ea7\u548ctoken\u4e4b\u95f4\u7684\u5206\u5e03\u89c4\u5f8b\u53ca\u5176\u5bfc\u81f4\u7684\u56e0\u679c\u4e0d\u5bf9\u79f0\u6027\u3002\u8f83\u5927\u6a21\u578b\u53ef\u63d0\u53d6\u66f4\u591a\u4fe1\u606f\uff0c\u5b57\u5e55\u8868\u793a\u53ef\u9884\u6d4b\u89c6\u89c9\u8868\u793a\u5e76\u5b58\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u4e8e\u8bed\u4e49\u76f8\u5173\u6570\u636e\u4f1a\u53d1\u5c55\u51fa\u76f8\u4f3c\u7684\u8868\u793a\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9a\u91cf\u5206\u6790,\u6df1\u5165\u63a2\u8ba8\u8bed\u4e49\u4fe1\u606f\u5728\u8de8\u9886\u57df\u6570\u636e\uff08\u5982\u56fe\u50cf\u4e0e\u6587\u672c\uff09\u4e2d\u7684\u8868\u5f81\u5f62\u5f0f\u53ca\u5176\u7f16\u7801\u8fc7\u7a0b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u6d4b\u91cf\u8bed\u4e49\u76f8\u5173\u6570\u636e\u8868\u793a\u7684\u76f8\u5bf9\u4fe1\u606f\u542b\u91cf\u6765\u5b9a\u91cf\u8c03\u67e5\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u63a2\u7a76\u5982\u4f55\u5c06\u5176\u7f16\u7801\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8f6c\u6362\u5668\u7684\u591a\u4e2atoken\u4e2d\u3002\u901a\u8fc7\u5904\u7406\u7ffb\u8bd1\u53e5\u5bf9\uff0c\u6211\u4eec\u8fa8\u8bc6\u51fa\u5185\u5728\u7684\u201c\u8bed\u4e49\u201d\u5c42\uff0c\u4ee5\u53ca\u66f4\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982DeepSeek-V3\uff09\u6bd4\u8f83\u5c0f\u7684\u6a21\u578b\uff08\u5982Llama3.1-8B\uff09\u63d0\u53d6\u66f4\u591a\u7684\u901a\u7528\u4fe1\u606f\u3002", "result": "\u5728LLMs\u7684\u8bed\u4e49\u5c42\u4e2d\uff0c\u8f83\u5927\u7684\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u66f4\u591a\u7684\u901a\u7528\u4fe1\u606f\uff1b\u8bed\u4e49\u4fe1\u606f\u4e0d\u4ec5\u6563\u5e03\u4e8e\u591a\u4e2atoken\u95f4\uff0c\u8fd8\u5c55\u73b0\u4e86\u957f\u8ddd\u79bb\u4ee5\u53ca\u8fc7\u53bb\u5230\u672a\u6765\u7684\u56e0\u679c\u4e0d\u5bf9\u79f0\u6027\u3002\u8bed\u4e49\u5c42\u5185\u7684\u5b57\u5e55\u8868\u793a\u80fd\u591f\u9884\u6d4b\u5bf9\u5e94\u56fe\u50cf\u7684\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u5c55\u793a\u663e\u8457\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6027\u3002", "conclusion": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u8bed\u4e49\u76f8\u5173\u7684\u6570\u636e\u65f6\u80fd\u591f\u53d1\u5c55\u51fa\u76f8\u4f3c\u7684\u8868\u793a\uff0c\u5373\u4fbf\u8fd9\u4e9b\u6570\u636e\u6765\u81ea\u4e0d\u540c\u7684\u9886\u57df\u3002\u8bed\u4e49\u4fe1\u606f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u89c9\u8f6c\u6362\u5668\u4e2d\u6563\u5e03\u4e8e\u591a\u4e2atoken\u4e4b\u95f4\uff0c\u5e76\u5c55\u793a\u51fa\u957f\u8ddd\u79bb\u7684\u76f8\u5173\u6027\u4ee5\u53ca\u8fc7\u53bb\u5230\u672a\u6765\u7684\u56e0\u679c\u4e0d\u5bf9\u79f0\u6027\u3002\u8bed\u4e49\u5c42\u4e0d\u4ec5\u80fd\u591f\u9884\u6d4b\u89c6\u89c9\u56fe\u50cf\u7684\u8868\u793a\uff0c\u8fd8\u5c55\u793a\u4e86\u56fe\u50cf\u4e0e\u6587\u672c\u8868\u793a\u4e4b\u95f4\u663e\u8457\u7684\u3001\u4e0e\u6a21\u578b\u76f8\u5173\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6027\u3002"}}
{"id": "2505.17476", "pdf": "https://arxiv.org/pdf/2505.17476", "abs": "https://arxiv.org/abs/2505.17476", "authors": ["Yuchen Zhang", "Yaxiong Wang", "Yujiao Wu", "Lianwei Wu", "Li Zhu"], "title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts", "categories": ["cs.CV"], "comment": null, "summary": "The detection and grounding of multimedia manipulation has emerged as a\ncritical challenge in combating AI-generated disinformation. While existing\nmethods have made progress in recent years, we identify two fundamental\nlimitations in current approaches: (1) Underestimation of MLLM-driven deception\nrisk: prevailing techniques primarily address rule-based text manipulations,\nyet fail to account for sophisticated misinformation synthesized by multimodal\nlarge language models (MLLMs) that can dynamically generate semantically\ncoherent, contextually plausible yet deceptive narratives conditioned on\nmanipulated images; (2) Unrealistic misalignment artifacts: currently focused\nscenarios rely on artificially misaligned content that lacks semantic\ncoherence, rendering them easily detectable. To address these gaps\nholistically, we propose a new adversarial pipeline that leverages MLLMs to\ngenerate high-risk disinformation. Our approach begins with constructing the\nMLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered\nusing state-of-the-art editing techniques and then paired with MLLM-generated\ndeceptive texts that maintain semantic consistency with the visual\nmanipulations. Building upon this foundation, we present the Artifact-aware\nManipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:\nArtifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,\nto tame MLLMs for the MDSM problem. Comprehensive experiments validate our\nframework's superior generalization capabilities as a unified architecture for\ndetecting MLLM-powered multimodal deceptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e94\u5bf9MLLM\u9a71\u52a8\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u5bf9\u6297\u6027\u7ba1\u9053\uff0c\u901a\u8fc7\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u548c\u65b0\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u4e2d\u4f4e\u4f30\u6b3a\u9a97\u98ce\u9669\u548c\u4e0d\u771f\u5b9e\u5931\u8c03\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u5c40\u9650\u6027\uff1a\uff081\uff09\u4f4e\u4f30\u4e86MLLM\u9a71\u52a8\u7684\u6b3a\u9a97\u98ce\u9669\uff1b\uff082\uff09\u4f9d\u8d56\u4e8e\u4e0d\u5177\u6709\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u4eba\u5de5\u5931\u8c03\u5185\u5bb9\u3002\u4e3a\u4e86\u5168\u9762\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5bf9\u6297\u6027\u7ba1\u9053\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u7ba1\u9053\u548c\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efaMLLM\u9a71\u52a8\u7684\u5408\u6210\u591a\u6a21\u6001\uff08MDSM\uff09\u6570\u636e\u96c6\uff0c\u7136\u540e\u5229\u7528\u4eba\u5de5\u5236\u54c1\u611f\u77e5\u7f16\u7801\u7b56\u7565\u548c\u64cd\u4f5c\u5bfc\u5411\u63a8\u7406\u6846\u67b6\uff08AMD\uff09\uff0c\u9488\u5bf9MLLM\u751f\u6210\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4f5c\u4e3a\u4e00\u79cd\u68c0\u6d4bMLLM\u9a71\u52a8\u7684\u591a\u6a21\u6001\u6b3a\u9a97\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u7ba1\u9053\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6765\u751f\u6210\u9ad8\u98ce\u9669\u7684\u865a\u5047\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u6784\u5efaMLLM\u9a71\u52a8\u7684\u5408\u6210\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08MDSM\uff09\u53ca\u4eba\u5de5\u5236\u54c1\u611f\u77e5\u7f16\u7801\u7b56\u7565\u548c\u64cd\u4f5c\u5bfc\u5411\u63a8\u7406\u6846\u67b6\uff08AMD\uff09\u6765\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\uff0c\u8fd9\u4e00\u65b9\u6848\u9a8c\u8bc1\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.17102", "pdf": "https://arxiv.org/pdf/2505.17102", "abs": "https://arxiv.org/abs/2505.17102", "authors": ["Pramit Bhattacharyya", "Arnab Bhattacharya"], "title": "BanglaByT5: Byte-Level Modelling for Bangla", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing tasks. However, most LLM models use traditional\ntokenizers like BPE and SentencePiece, which fail to capture the finer nuances\nof a morphologically rich language like Bangla (Bengali). In this work, we\nintroduce BanglaByT5, the first byte-level encoder-decoder model explicitly\ntailored for Bangla. Built upon a small variant of Googles ByT5 architecture,\nBanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality\nliterary and newspaper articles. Through zeroshot and supervised evaluations\nacross generative and classification tasks, BanglaByT5 demonstrates competitive\nperformance, surpassing several multilingual and larger models. Our findings\nhighlight the efficacy of byte-level modelling for morphologically rich\nlanguages and highlight BanglaByT5 potential as a lightweight yet powerful tool\nfor Bangla NLP, particularly in both resource-constrained and scalable\nenvironments.", "AI": {"tldr": "BanglaByT5, a byte-level language model tailored for Bangla, shows superior performance in processing Bangla compared to traditional tokenizers and larger models.", "motivation": "Traditional tokenizers fail to adequately capture the nuances of morphologically rich languages such as Bangla. A byte-level approach is introduced to address this gap and enhance language processing capabilities.", "method": "Introduction of BanglaByT5, a byte-level encoder-decoder model, built on Google's ByT5 architecture and pre-trained using a 14GB corpus of Bangla literature and news articles.", "result": "BanglaByT5 shows competitive performance in both zero-shot and supervised tasks, outperforming several larger multilingual models, proving effective for Bangla NLP.", "conclusion": "Byte-level modeling, as demonstrated by BanglaByT5, is effective for morphologically rich languages like Bangla, providing a powerful tool for language processing tasks in diverse environments."}}
{"id": "2505.17493", "pdf": "https://arxiv.org/pdf/2505.17493", "abs": "https://arxiv.org/abs/2505.17493", "authors": ["Jingde Huang", "Zhangyu Huang", "Chenyu Li", "Jiantong Liu"], "title": "Research on Defect Detection Method of Motor Control Board Based on Image Processing", "categories": ["cs.CV"], "comment": null, "summary": "The motor control board has various defects such as inconsistent color\ndifferences, incorrect plug-in positions, solder short circuits, and more.\nThese defects directly affect the performance and stability of the motor\ncontrol board, thereby having a negative impact on product quality. Therefore,\nstudying the defect detection technology of the motor control board is an\nimportant means to improve the quality control level of the motor control\nboard. Firstly, the processing methods of digital images about the motor\ncontrol board were studied, and the noise suppression methods that affect image\nfeature extraction were analyzed. Secondly, a specific model for defect feature\nextraction and color difference recognition of the tested motor control board\nwas established, and qualified or defective products were determined based on\nfeature thresholds. Thirdly, the search algorithm for defective images was\noptimized. Finally, comparative experiments were conducted on the typical motor\ncontrol board, and the experimental results demonstrate that the accuracy of\nthe motor control board defect detection model-based on image processing\nestablished in this paper reached over 99%. It is suitable for timely image\nprocessing of large quantities of motor control boards on the production line,\nand achieved efficient defect detection. The defect detection method can not\nonly be used for online detection of the motor control board defects, but also\nprovide solutions for the integrated circuit board defect processing for the\nindustry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7535\u673a\u63a7\u5236\u677f\u7684\u7f3a\u9677\u68c0\u6d4b\u6280\u672f\uff0c\u4f7f\u7528\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u51c6\u786e\u7387\u8fbe\u523099%\u4ee5\u4e0a\uff0c\u53ef\u7528\u4e8e\u751f\u4ea7\u7ebf\u4e0a\u7684\u5728\u7ebf\u68c0\u6d4b\u3002", "motivation": "\u7814\u7a76\u7535\u673a\u63a7\u5236\u677f\u7684\u7f3a\u9677\u68c0\u6d4b\u6280\u672f\u662f\u63d0\u9ad8\u7535\u673a\u63a7\u5236\u677f\u8d28\u91cf\u63a7\u5236\u6c34\u5e73\u7684\u91cd\u8981\u624b\u6bb5\u3002", "method": "\u7814\u7a76\u4e86\u7535\u673a\u63a7\u5236\u677f\u6570\u5b57\u56fe\u50cf\u7684\u5904\u7406\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5f71\u54cd\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u7684\u566a\u58f0\u6291\u5236\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u68c0\u6d4b\u7535\u673a\u63a7\u5236\u677f\u7f3a\u9677\u7279\u5f81\u63d0\u53d6\u53ca\u8272\u5dee\u8bc6\u522b\u7684\u7279\u5b9a\u6a21\u578b\uff0c\u4f18\u5316\u4e86\u7f3a\u9677\u56fe\u50cf\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u7535\u673a\u63a7\u5236\u677f\u7f3a\u9677\u68c0\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u80fd\u591f\u5b9e\u73b0\u751f\u4ea7\u7ebf\u4e0a\u5927\u91cf\u7535\u673a\u63a7\u5236\u677f\u7684\u53ca\u65f6\u56fe\u50cf\u5904\u7406\uff0c\u8fbe\u5230\u9ad8\u6548\u7f3a\u9677\u68c0\u6d4b\u3002", "conclusion": "\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u7528\u4e8e\u7535\u673a\u63a7\u5236\u677f\u7f3a\u9677\u7684\u5728\u7ebf\u68c0\u6d4b\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u6210\u7535\u8def\u677f\u7f3a\u9677\u5904\u7406\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17103", "pdf": "https://arxiv.org/pdf/2505.17103", "abs": "https://arxiv.org/abs/2505.17103", "authors": ["C\u00e9cile Rousseau", "Tobia Boschi", "Giandomenico Cornacchia", "Dhaval Salwala", "Alessandra Pascale", "Juan Bernabe Moreno"], "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. SDForger\nsource code will be open-sourced soon.", "AI": {"tldr": "SDForger\u662f\u4e00\u4e2a\u5229\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5c11\u91cf\u6837\u672c\u4e2d\u9ad8\u6548\u751f\u6210\u5408\u6210\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u5728\u5404\u79cd\u8bc4\u4f30\u548c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u5728\u4fdd\u969c\u6570\u636e\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7075\u6d3b\u9ad8\u6548\u7684\u6846\u67b6\u751f\u6210\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u5408\u6210\u6570\u636e\u751f\u6210\u9700\u6c42\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8868\u683c\u5d4c\u5165\uff0c\u7136\u540e\u7f16\u7801\u4e3a\u6587\u672c\uff0c\u7528\u4e8e\u5fae\u8c03\u81ea\u56de\u5f52LLM\u3002\u968f\u540e\uff0c\u901a\u8fc7\u91c7\u6837\u65b0\u7684\u6587\u672c\u5d4c\u5165\u5e76\u89e3\u7801\u4e3a\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u3002", "result": "SDForger\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5373\u5c06\u5f00\u6e90\u3002", "conclusion": "SDForger\u5728\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5728\u76f8\u4f3c\u6027\u8bc4\u4f30\u548c\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.17501", "pdf": "https://arxiv.org/pdf/2505.17501", "abs": "https://arxiv.org/abs/2505.17501", "authors": ["Yuehan Jin", "Xiaoqing Liu", "Yiyuan Yang", "Zhiwen Yu", "Tong Zhang", "Kaixiang Yang"], "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoHyDR\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u751f\u6210\u5668\u548c\u5bf9\u6297\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u9488\u5bf9\u73b0\u5b9e\u4e2d\u566a\u58f0\u6216\u4f20\u611f\u5668\u6545\u969c\u5bfc\u81f4\u7684\u6570\u636e\u7f3a\u5931\u6216\u635f\u574f\uff0c\u5bf9\u5e94\u7684\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b(IMER)\u7684\u6311\u6218\uff0c\u56e0\u6b64\u5f15\u5165\u4e00\u4e2a\u5904\u7406\u7f3a\u5931\u6a21\u6001\u7684\u6062\u590d\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6RoHyDR\uff0c\u91c7\u7528\u6269\u6563\u751f\u6210\u5668\u548c\u5bf9\u6297\u5b66\u4e60\u8fdb\u884c\u65e0\u6a21\u6001\u6062\u590d\uff0c\u5e76\u7ed3\u5408\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u6765\u589e\u5f3a\u7a33\u5b9a\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "RoHyDR\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e24\u5927\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684IMER\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\uff0cRoHyDR\u663e\u793a\u51fa\u5728\u5904\u7406\u4e0d\u540c\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18IMER\u65b9\u6cd5\uff0c\u5728\u9c81\u68d2\u6027\u8bc6\u522b\u8868\u73b0\u4e0a\u90fd\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2505.17104", "pdf": "https://arxiv.org/pdf/2505.17104", "abs": "https://arxiv.org/abs/2505.17104", "authors": ["Tao Sun", "Enhao Pan", "Zhengkai Yang", "Kaixin Sui", "Jiajun Shi", "Xianfu Cheng", "Tongliang Li", "Wenhao Huang", "Ge Zhang", "Jian Yang", "Zhoujun Li"], "title": "P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Academic posters are vital for scholarly communication, yet their manual\ncreation is time-consuming. However, automated academic poster generation faces\nsignificant challenges in preserving intricate scientific details and achieving\neffective visual-textual integration. Existing approaches often struggle with\nsemantic richness and structural nuances, and lack standardized benchmarks for\nevaluating generated academic posters comprehensively. To address these\nlimitations, we introduce P2P, the first flexible, LLM-based multi-agent\nframework that generates high-quality, HTML-rendered academic posters directly\nfrom research papers, demonstrating strong potential for practical\napplications. P2P employs three specialized agents-for visual element\nprocessing, content generation, and final poster assembly-each integrated with\ndedicated checker modules to enable iterative refinement and ensure output\nquality. To foster advancements and rigorous evaluation in this domain, we\nconstruct and release P2PInstruct, the first large-scale instruction dataset\ncomprising over 30,000 high-quality examples tailored for the academic\npaper-to-poster generation task. Furthermore, we establish P2PEval, a\ncomprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation\nmethodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and\ndetailed, human-annotated checklists. Our contributions aim to streamline\nresearch dissemination and provide the community with robust tools for\ndeveloping and evaluating next-generation poster generation systems.", "AI": {"tldr": "Introduction of P2P, a multi-agent framework, to automate academic poster creation from papers, accompanied by new datasets and benchmarks for improved evaluation.", "motivation": "The motivation is to automate the creation of academic posters, overcoming challenges like semantic richness and structural nuances, and provide standardized benchmarks for evaluation.", "method": "P2P uses a multi-agent framework with three specialized agents for visual element processing, content generation, and final poster assembly, alongside checker modules. It also involves the creation of P2PInstruct, a large-scale instruction dataset, and P2PEval, a benchmark for evaluation.", "result": "P2P generates high-quality, HTML-rendered academic posters and demonstrates strong potential for practical applications. Additionally, the newly established datasets and benchmarks aim to advance the field.", "conclusion": "The P2P framework and its accompanying datasets/tools aim to streamline the process of generating and evaluating academic posters from papers, providing robust tools and benchmarks for future advancements."}}
{"id": "2505.17509", "pdf": "https://arxiv.org/pdf/2505.17509", "abs": "https://arxiv.org/abs/2505.17509", "authors": ["Shiji Zhao", "Qihui Zhu", "Shukun Xiong", "Shouwei Ruan", "Yize Fan", "Ranjie Duan", "Qing Guo", "Xingxing Wei"], "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings.", "AI": {"tldr": "\u4e3a\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5bf9\u6297\u6837\u672c\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51faAdversarial Mixture Prompt Tuning (AMPT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u5b66\u4e60\u63d0\u793a\u63d0\u9ad8\u6cdb\u5316\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u589e\u5f3a\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u6297\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5bf9\u5bf9\u6297\u6837\u672c\u9ad8\u5ea6\u654f\u611f\uff0c\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdversarial Mixture Prompt Tuning (AMPT)\u7684\u5bf9\u6297\u6027\u8c03\u6574\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u5b66\u4e60\u63d0\u793a\u7684\u6570\u91cf\u800c\u4e0d\u662f\u5ef6\u957f\u63d0\u793a\u957f\u5ea6\uff0c\u63d0\u9ad8\u5bf9\u6297\u6027\u591a\u6837\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u6743\u91cd\u8def\u7531\u5668\u6839\u636e\u8f93\u5165\u5bf9\u6297\u6027\u56fe\u50cf\u9884\u6d4b\u591a\u4e2a\u5b66\u4e60\u63d0\u793a\u7684\u6df7\u5408\u6743\u91cd\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u572811\u4e2a\u6570\u636e\u96c6\u7684\u4e0d\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u597d\u7684\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7AMPT\u65b9\u6cd5\uff0cVLMs\u5728\u9762\u5bf9\u4e0d\u540c\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u5f97\u5230\u4e86\u589e\u5f3a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.17106", "pdf": "https://arxiv.org/pdf/2505.17106", "abs": "https://arxiv.org/abs/2505.17106", "authors": ["Yifei Liu", "Yu Cui", "Haibin Zhang"], "title": "RRTL: Red Teaming Reasoning Large Language Models in Tool Learning", "categories": ["cs.CL"], "comment": null, "summary": "While tool learning significantly enhances the capabilities of large language\nmodels (LLMs), it also introduces substantial security risks. Prior research\nhas revealed various vulnerabilities in traditional LLMs during tool learning.\nHowever, the safety of newly emerging reasoning LLMs (RLLMs), such as\nDeepSeek-R1, in the context of tool learning remains underexplored. To bridge\nthis gap, we propose RRTL, a red teaming approach specifically designed to\nevaluate RLLMs in tool learning. It integrates two novel strategies: (1) the\nidentification of deceptive threats, which evaluates the model's behavior in\nconcealing the usage of unsafe tools and their potential risks; and (2) the use\nof Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also\nincludes a benchmark for traditional LLMs. We conduct a comprehensive\nevaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs\ngenerally achieve stronger safety performance than traditional LLMs, yet\nsubstantial safety disparities persist across models; (2) RLLMs can pose\nserious deceptive risks by frequently failing to disclose tool usage and to\nwarn users of potential tool output risks; (3) CoT prompting reveals\nmulti-lingual safety vulnerabilities in RLLMs. Our work provides important\ninsights into enhancing the security of RLLMs in tool learning.", "AI": {"tldr": "\u63d0\u51faRRTL\u65b9\u6cd5\u8bc4\u4f30RLLMs\u5de5\u5177\u5b66\u4e60\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5176\u5b89\u5168\u6027\u4f18\u4e8e\u4f20\u7edfLLMs\uff0c\u4f46\u5b58\u5728\u6b3a\u9a97\u6027\u98ce\u9669\u548c\u591a\u8bed\u8a00\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9RLLMs\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u5b89\u5168\u6027\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8bbe\u8ba1\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u80fd\u3002", "method": "\u91c7\u7528RRTL\u7ea2\u961f\u65b9\u6cd5\u6765\u8bc4\u4f30RLLMs\u7684\u5de5\u5177\u5b66\u4e60\u5b89\u5168\u6027\uff0c\u5305\u62ec\u8bc6\u522b\u6b3a\u9a97\u6027\u5a01\u80c1\u548c\u4f7f\u7528\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u8feb\u4f7f\u5de5\u5177\u8c03\u7528\u3002\u8fd8\u5efa\u7acb\u4e86\u4f20\u7edfLLMs\u7684\u57fa\u51c6\u3002", "result": "\u4e03\u4e2a\u4e3b\u6d41RLLMs\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5c3d\u7ba1\u5176\u603b\u4f53\u5b89\u5168\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfLLMs\uff0c\u4f46\u5b83\u4eec\u5728\u5de5\u5177\u4f7f\u7528\u62ab\u9732\u548c\u98ce\u9669\u8b66\u793a\u65b9\u9762\u5b58\u5728\u6b3a\u9a97\u6027\u98ce\u9669\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u5b89\u5168\u6027\u4e0a\u4ecd\u6709\u6f0f\u6d1e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RRTL\u65b9\u6cd5\u63ed\u793a\u4e86RLLMs\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u5176\u5b89\u5168\u6027\u7684\u5efa\u8bae\u3002RLLMs\u6bd4\u4f20\u7edfLLMs\u5177\u6709\u66f4\u597d\u7684\u5b89\u5168\u6027\u80fd\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u5dee\u5f02\u3002"}}
{"id": "2505.17529", "pdf": "https://arxiv.org/pdf/2505.17529", "abs": "https://arxiv.org/abs/2505.17529", "authors": ["Yeongjae Cho", "Keonwoo Kim", "Taebaek Hwang", "Sungzoon Cho"], "title": "Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly expanded their utility in tasks like image captioning and visual\nquestion answering. However, they still struggle with object hallucination,\nwhere models generate descriptions that inaccurately reflect the visual content\nby including nonexistent objects or misrepresenting existing ones. While\nprevious methods, such as data augmentation and training-free approaches,\nstrive to tackle this issue, they still encounter scalability challenges and\noften depend on additional external modules. In this work, we propose Ensemble\nDecoding (ED), a novel strategy that splits the input image into sub-images and\ncombines logit distributions by assigning weights through the attention map.\nFurthermore, we introduce ED adaptive plausibility constraint to calibrate\nlogit distribution and FastED, a variant designed for speed-critical\napplications. Extensive experiments across hallucination benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance, validating the\neffectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86Ensemble Decoding\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3LVLMs\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u679c\u663e\u8457\u3002", "motivation": "LVLMs\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4ecd\u5b58\u5728\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\uff0c\u4ee5\u81f3\u4e8e\u751f\u6210\u7684\u63cf\u8ff0\u9519\u8bef\u5730\u53cd\u6620\u89c6\u89c9\u5185\u5bb9\u3002\u73b0\u6709\u65b9\u6cd5\u9047\u5230\u53ef\u6269\u5c55\u6027\u6311\u6218\u5e76\u4f9d\u8d56\u989d\u5916\u6a21\u5757\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aEnsemble Decoding\uff08ED\uff09\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u8f93\u5165\u56fe\u50cf\u5206\u89e3\u4e3a\u5b50\u56fe\u50cf\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u5206\u914d\u6743\u91cd\u6765\u7ec4\u5408logit\u5206\u5e03\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86ED\u81ea\u9002\u5e94\u5408\u7406\u6027\u7ea6\u675f\u6765\u6821\u51c6logit\u5206\u5e03\uff0c\u5e76\u5f00\u53d1\u4e86FastED\u53d8\u4f53\u4ee5\u6ee1\u8db3\u901f\u5ea6\u5173\u952e\u7684\u5e94\u7528\u9700\u6c42\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u65f6\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684Ensemble Decoding\uff08ED\uff09\u65b9\u6cd5\u5728\u89e3\u51b3LVLMs\u7684\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\u4e0a\u6709\u6548\uff0c\u5e76\u5728\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u6027\u80fd\u6c34\u5e73\u3002"}}
{"id": "2505.17110", "pdf": "https://arxiv.org/pdf/2505.17110", "abs": "https://arxiv.org/abs/2505.17110", "authors": ["Junlin Li", "Guodong DU", "Jing Li", "Sim Kuan Goh", "Wenya Wang", "Yequan Wang", "Fangming Liu", "Ho-Kin Tang", "Saleh Alharbi", "Daojing He", "Min Zhang"], "title": "Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) with multimodal encoders on\nmodality-specific data expands the modalities that LLMs can handle, leading to\nthe formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies\non resource-intensive and inflexible fine-tuning from scratch with new\nmultimodal data. In this paper, we propose MMER (Multi-modality Expansion and\nRetention), a training-free approach that integrates existing MLLMs for\neffective multimodal expansion while retaining their original performance.\nSpecifically, MMER reuses MLLMs' multimodal encoders while merging their LLM\nparameters. By comparing original and merged LLM parameters, MMER generates\nbinary masks to approximately separate LLM parameters for each modality. These\ndecoupled parameters can independently process modality-specific inputs,\nreducing parameter conflicts and preserving original MLLMs' fidelity. MMER can\nalso mitigate catastrophic forgetting by applying a similar process to MLLMs\nfine-tuned on new tasks. Extensive experiments show significant improvements\nover baselines, proving that MMER effectively expands LLMs' multimodal\ncapabilities while retaining 99% of the original performance, and also markedly\nmitigates catastrophic forgetting.", "AI": {"tldr": "MMER\u901a\u8fc7\u65e0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u91cd\u7528\u548c\u5408\u5e76\u5df2\u5b58\u5728\u7684MLLMs\u4ee5\u6269\u5c55\u5176\u591a\u6a21\u6001\u80fd\u529b\u5e76\u4fdd\u7559\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u7ebf\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u591a\u6a21\u6001LLM\u8bad\u7ec3\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u7f3a\u70b9\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848MMER\uff0c\u5e0c\u671b\u80fd\u5b9e\u73b0\u591a\u6a21\u6001\u6269\u5c55\u548c\u4fdd\u7559\u539f\u59cb\u6027\u80fd\u3002", "method": "MMER\u901a\u8fc7\u91cd\u7528MLLM\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\u5e76\u5408\u5e76\u5176LLM\u53c2\u6570\uff0c\u540c\u65f6\u6bd4\u8f83\u539f\u59cb\u548c\u5408\u5e76\u7684LLM\u53c2\u6570\u6765\u751f\u6210\u4e8c\u8fdb\u5236\u63a9\u7801\u3002\u8fd9\u4e9b\u89e3\u8026\u7684\u53c2\u6570\u53ef\u4ee5\u72ec\u7acb\u5904\u7406\u7279\u5b9a\u6a21\u6001\u7684\u8f93\u5165\uff0c\u51cf\u5c11\u53c2\u6570\u51b2\u7a81\u5e76\u4fdd\u7559\u539f\u59cbMLLM\u7684\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u57fa\u51c6\u7ebf\u76f8\u6bd4\uff0cMMER\u5728\u591a\u6a21\u6001\u80fd\u529b\u4e0a\u6709\u663e\u8457\u63d0\u9ad8\uff0c\u540c\u65f6\u4fdd\u755999%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u5e76\u663e\u8457\u51cf\u8f7b\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "MMER\u6709\u6548\u5730\u6269\u5c55\u4e86LLMs\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u4e8699%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
