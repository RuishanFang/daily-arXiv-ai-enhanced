{"id": "2505.17037", "pdf": "https://arxiv.org/pdf/2505.17037", "abs": "https://arxiv.org/abs/2505.17037", "authors": ["Dimitri Schreiter"], "title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has emerged as a critical component in optimizing large\nlanguage models (LLMs) for domain-specific tasks. However, the role of prompt\nspecificity, especially in domains like STEM (physics, chemistry, biology,\ncomputer science and mathematics), medicine, and law, remains underexplored.\nThis thesis addresses the problem of whether increasing the specificity of\nvocabulary in prompts improves LLM performance in domain-specific\nquestion-answering and reasoning tasks. We developed a synonymization framework\nto systematically substitute nouns, verbs, and adjectives with varying\nspecificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,\nGranite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in\nSTEM, law, and medicine. Our results reveal that while generally increasing the\nspecificity of prompts does not have a significant impact, there appears to be\na specificity range, across all considered models, where the LLM performs the\nbest. Identifying this optimal specificity range offers a key insight for\nprompt design, suggesting that manipulating prompts within this range could\nmaximize LLM performance and lead to more efficient applications in specialized\ndomains.", "AI": {"tldr": "\u589e\u52a0\u63d0\u793a\u8bcd\u5177\u4f53\u6027\u5bf9LLM\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u8303\u56f4\uff0c\u53ef\u4f18\u5316LLM\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u5728STEM\u3001\u533b\u5b66\u548c\u6cd5\u5f8b\u7b49\u9886\u57df\u4e2d\uff0c\u589e\u52a0\u63d0\u793a\u8bcd\u6c47\u7684\u5177\u4f53\u6027\u662f\u5426\u80fd\u591f\u63d0\u9ad8LLM\u5728\u7279\u5b9a\u9886\u57df\u7684\u95ee\u9898\u89e3\u7b54\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540c\u4e49\u8bcd\u66ff\u6362\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u66ff\u6362\u540d\u8bcd\u3001\u52a8\u8bcd\u548c\u5f62\u5bb9\u8bcd\uff0c\u5e76\u6d4b\u91cf\u5176\u5bf9\u56db\u4e2aLLM\uff08Llama-3.1-70B-Instruct\u3001Granite-13B-Instruct-V2\u3001Flan-T5-XL\u548cMistral-Large 2\uff09\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u4e00\u822c\u6765\u8bf4\u63d0\u5347\u63d0\u793a\u5177\u4f53\u6027\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u6709\u4e00\u4e2a\u5177\u4f53\u6027\u8303\u56f4\u53ef\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u8bc6\u522b\u6700\u4f18\u5177\u4f53\u6027\u8303\u56f4\u5bf9\u63d0\u793a\u8bbe\u8ba1\u6709\u91cd\u8981\u610f\u4e49\uff0c\u80fd\u4f18\u5316LLM\u6027\u80fd\u5e76\u63d0\u5347\u5728\u7279\u5b9a\u9886\u57df\u7684\u5e94\u7528\u6548\u7387\u3002", "conclusion": "\u589e\u52a0\u63d0\u793a\u8bcd\u6c47\u7684\u5177\u4f53\u6027\u5bf9LLM\u6027\u80fd\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6700\u4f73\u5177\u4f53\u6027\u8303\u56f4\u3002\u5728\u8fd9\u4e00\u8303\u56f4\u5185\uff0cLLM\u8868\u73b0\u6700\u597d\u3002\u8bc6\u522b\u8fd9\u4e00\u8303\u56f4\u5bf9\u63d0\u793a\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u4ee5\u4f18\u5316\u7279\u5b9a\u9886\u57df\u7684LLM\u5e94\u7528\u3002"}}
{"id": "2505.17038", "pdf": "https://arxiv.org/pdf/2505.17038", "abs": "https://arxiv.org/abs/2505.17038", "authors": ["Xian Gong", "Paul X. McCarthy", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Massive and diverse web data are increasingly vital for government disaster\nresponse, as demonstrated by the 2022 floods in New South Wales (NSW),\nAustralia. This study examines how X (formerly Twitter) and public inquiry\nsubmissions provide insights into public behaviour during crises. We analyse\nmore than 55,000 flood-related tweets and 1,450 submissions to identify\nbehavioural patterns during extreme weather events. While social media posts\nare short and fragmented, inquiry submissions are detailed, multi-page\ndocuments offering structured insights. Our methodology integrates Latent\nDirichlet Allocation (LDA) for topic modelling with Large Language Models\n(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and\ngeographic patterns, while LLMs improve filtering by identifying flood-relevant\ntweets using public submissions as a reference. This Relevance Index method\nreduces noise and prioritizes actionable content, improving situational\nawareness for emergency responders. By combining these complementary data\nstreams, our approach introduces a novel AI-driven method to refine\ncrisis-related social media content, improve real-time disaster response, and\ninform long-term resilience planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u548c\u8c03\u67e5\u63d0\u4ea4\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6539\u8fdb\u707e\u96be\u54cd\u5e94\u3002", "motivation": "\u653f\u5e9c\u5728\u707e\u96be\u54cd\u5e94\u4e2d\u6108\u53d1\u4f9d\u8d56\u4e8e\u5e9e\u5927\u4e14\u591a\u6837\u5316\u7684\u7f51\u7edc\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u671f\u95f4\u7684\u516c\u4f17\u884c\u4e3a\u5206\u6790\uff0c\u8fd9\u57282022\u5e74\u6fb3\u5927\u5229\u4e9a\u65b0\u5357\u5a01\u5c14\u58eb\u5dde\uff08NSW\uff09\u7684\u6d2a\u6c34\u4e2d\u5c24\u4e3a\u4f53\u73b0\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e3b\u9898\u6a21\u578b\u7684\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u5e03\uff08LDA\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u3002\u4f7f\u7528\u516c\u5171\u8c03\u67e5\u63d0\u4ea4\u4f5c\u4e3a\u53c2\u8003\u6765\u8fc7\u6ee4\u63a8\u6587\uff0c\u63d0\u9ad8\u76f8\u5173\u6027\u6307\u6570\uff0c\u901a\u8fc7\u51cf\u5c11\u566a\u97f3\u548c\u4f18\u5148\u663e\u793a\u53ef\u64cd\u4f5c\u5185\u5bb9\u6765\u6539\u5584\u5e94\u6025\u54cd\u5e94\u7684\u60c5\u5883\u611f\u77e5\u3002", "result": "LDA\u63ed\u793a\u4e86\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u53cd\u6620\u7684\u4e0d\u540c\u610f\u89c1\u548c\u5730\u7406\u6a21\u5f0f\uff0c\u800cLLMs\u5219\u901a\u8fc7\u8bc6\u522b\u4e0e\u6d2a\u6c34\u76f8\u5173\u7684\u63a8\u6587\uff0c\u6539\u8fdb\u4e86\u8fc7\u6ee4\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5e94\u6025\u54cd\u5e94\u7684\u60c5\u5883\u611f\u77e5\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u65b9\u6cd5\uff0c\u7ed3\u5408\u793e\u4ea4\u5a92\u4f53\u548c\u516c\u5171\u8c03\u67e5\u6570\u636e\u6d41\uff0c\u80fd\u591f\u6539\u8fdb\u5371\u673a\u76f8\u5173\u5185\u5bb9\u7684\u7cbe\u70bc\uff0c\u63d0\u9ad8\u5b9e\u65f6\u707e\u5bb3\u54cd\u5e94\uff0c\u5e76\u4e3a\u957f\u671f\u97e7\u6027\u89c4\u5212\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2505.17039", "pdf": "https://arxiv.org/pdf/2505.17039", "abs": "https://arxiv.org/abs/2505.17039", "authors": ["Diego Bonatto"], "title": "A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes", "categories": ["cs.CL"], "comment": "46 pages, 8 figures, 1 table", "summary": "A data-driven quantitative approach was used to develop a novel\nclassification system for beer categories and styles. Sixty-two thousand one\nhundred twenty-one beer recipes were mined and analyzed, considering ingredient\nprofiles, fermentation parameters, and recipe vital statistics. Statistical\nanalyses combined with self-organizing maps (SOMs) identified four major\nsuperclusters that showed distinctive malt and hop usage patterns, style\ncharacteristics, and historical brewing traditions. Cold fermented styles\nshowed a conservative grain and hop composition, whereas hot fermented beers\nexhibited high heterogeneity, reflecting regional preferences and innovation.\nThis new taxonomy offers a reproducible and objective framework beyond\ntraditional sensory-based classifications, providing brewers, researchers, and\neducators with a scalable tool for recipe analysis and beer development. The\nfindings in this work provide an understanding of beer diversity and open\navenues for linking ingredient usage with fermentation profiles and flavor\noutcomes.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u67906\u4e07\u591a\u4efd\u5564\u9152\u914d\u65b9\uff0c\u4f7f\u7528\u81ea\u7ec4\u7ec7\u6620\u5c04\u6cd5\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u98ce\u683c\u95f4\u7684\u539f\u6599\u4f7f\u7528\u6a21\u5f0f\u548c\u5386\u53f2\u4f20\u7edf\uff0c\u4e3a\u5564\u9152\u5206\u6790\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u5de5\u5177\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u4ee5\u8d85\u8d8a\u4f20\u7edf\u7684\u57fa\u4e8e\u611f\u5b98\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5564\u9152\u914d\u65b9\u5206\u6790\u548c\u5f00\u53d1\u63d0\u4f9b\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u5206\u67906\u4e07\u591a\u4efd\u5564\u9152\u914d\u65b9\uff0c\u7ed3\u5408\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOMs\uff09\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u8d85\u7ea7\u7c07\u7fa4\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7684\u9ea6\u82bd\u548c\u5564\u9152\u82b1\u4f7f\u7528\u6a21\u5f0f\u3001\u98ce\u683c\u7279\u5f81\u548c\u5386\u53f2\u917f\u9020\u4f20\u7edf\u3002\u51b7\u53d1\u9175\u98ce\u683c\u5c55\u73b0\u4e86\u4fdd\u5b88\u7684\u7cae\u98df\u548c\u5564\u9152\u82b1\u7ec4\u6210\uff0c\u800c\u70ed\u53d1\u9175\u5564\u9152\u8868\u73b0\u51fa\u9ad8\u5ea6\u5f02\u8d28\u6027\uff0c\u53cd\u6620\u4e86\u5730\u533a\u7684\u504f\u597d\u548c\u521b\u65b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u53ef\u4ee5\u91cd\u73b0\u5e76\u5ba2\u89c2\u5730\u4e3a\u5564\u9152\u914d\u65b9\u5206\u6790\u548c\u5f00\u53d1\u63d0\u4f9b\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042", "abs": "https://arxiv.org/abs/2505.17042", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6848\u4e3b\u8981\u662f\u5355\u6a21\u6001\u7684\uff0c\u53ea\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\uff0c\u672a\u5229\u7528\u5f71\u50cf\u8d44\u6599\uff0c\u5e76\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u957f\u683c\u5f0f\u7684\u653e\u5c04\u5b66\u6570\u636e\u3002\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u653e\u5c04\u5b66\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u7efc\u5408\u5229\u7528\u6587\u672c\u548c\u5f71\u50cf\u6570\u636e\u3002", "result": "\u6211\u4eec\u7684\u591a\u6a21\u6001VLM\u6846\u67b6\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u751f\u6210\u4e86\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ee5\u5f80\u7684\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17064", "pdf": "https://arxiv.org/pdf/2505.17064", "abs": "https://arxiv.org/abs/2505.17064", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. In this work, we present a\nsystematic and reproducible methodology for evaluating how TTI systems depict\ndifferent historical periods. For this purpose, we introduce the HistVis\ndataset, a curated collection of 30,000 synthetic images generated by three\nstate-of-the-art diffusion models using carefully designed prompts depicting\nuniversal human activities across different historical periods. We evaluate\ngenerated imagery across three key aspects: (1) Implicit Stylistic\nAssociations: examining default visual styles associated with specific eras;\n(2) Historical Consistency: identifying anachronisms such as modern artifacts\nin pre-modern contexts; and (3) Demographic Representation: comparing generated\nracial and gender distributions against historically plausible baselines. Our\nfindings reveal systematic inaccuracies in historically themed generated\nimagery, as TTI models frequently stereotype past eras by incorporating\nunstated stylistic cues, introduce anachronisms, and fail to reflect plausible\ndemographic patterns. By offering a scalable methodology and benchmark for\nassessing historical representation in generated imagery, this work provides an\ninitial step toward building more historically accurate and culturally aligned\nTTI models.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7HistVis\u6570\u636e\u96c6\u8bc4\u4f30\u4e86TTI\u7cfb\u7edf\u5728\u63cf\u7ed8\u5386\u53f2\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u98ce\u683c\u3001\u65f6\u4ee3\u548c\u4eba\u53e3\u7edf\u8ba1\u4e0a\u5b58\u5728\u4e0d\u51c6\u786e\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5386\u53f2\u518d\u73b0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6587\u672c\u751f\u6210\u56fe\u50cf(TTI)\u6269\u6563\u6a21\u578b\u5728\u5185\u5bb9\u521b\u4f5c\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u6709\u5f71\u54cd\u529b\uff0c\u4eba\u4eec\u5bf9\u5176\u793e\u4f1a\u548c\u6587\u5316\u5f71\u54cd\u7684\u5173\u6ce8\u4e5f\u5728\u589e\u52a0\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u6b64\u524d\u7684\u7814\u7a76\u4e3b\u8981\u8003\u5bdf\u4e86\u4eba\u53e3\u548c\u6587\u5316\u504f\u89c1\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u51c6\u786e\u518d\u73b0\u5386\u53f2\u80cc\u666f\u65b9\u9762\u7684\u80fd\u529b\u5374\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u4e14\u53ef\u91cd\u73b0\u7684\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30TTI\u7cfb\u7edf\u5982\u4f55\u63cf\u7ed8\u4e0d\u540c\u5386\u53f2\u65f6\u671f\u3002\u4e3a\u6b64\u76ee\u7684\uff0c\u5f15\u5165\u4e86HistVis\u6570\u636e\u96c6\u2014\u2014\u4e00\u4e2a\u5305\u542b\u4e09\u79cd\u5148\u8fdb\u6269\u6563\u6a21\u578b\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u751f\u6210\u768430000\u5e45\u5408\u6210\u56fe\u50cf\u7684\u7cbe\u9009\u96c6\u5408\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u751f\u6210\u7684\u56fe\u50cf\u5728\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u7684\u8868\u73b0\uff1a\u9690\u542b\u98ce\u683c\u5173\u8054\u3001\u5386\u53f2\u4e00\u81f4\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u4ee3\u8868\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5386\u53f2\u4e3b\u9898\u7684\u751f\u6210\u56fe\u50cf\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u51c6\u786e\u6027\uff0cTTI\u6a21\u578b\u5e38\u901a\u8fc7\u672a\u7ecf\u58f0\u660e\u7684\u98ce\u683c\u7ebf\u7d22\u523b\u677f\u5316\u8fc7\u53bb\u7684\u65f6\u4ee3\uff0c\u5f15\u5165\u4e86\u73b0\u4ee3\u80cc\u666f\u4e0b\u4e0d\u5e94\u51fa\u73b0\u7684\u7269\u4f53\uff0c\u5e76\u672a\u80fd\u53cd\u6620\u51fa\u5408\u7406\u7684\u79cd\u65cf\u548c\u6027\u522b\u5206\u5e03\u3002", "conclusion": "\u901a\u8fc7\u8fd9\u9879\u5de5\u4f5c\uff0c\u7814\u7a76\u8005\u63ed\u793a\u51fa\u5f53\u524dTTI\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5386\u53f2\u4e3b\u9898\u56fe\u50cf\u65f6\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u4e0d\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u6a21\u578b\u5e38\u5e38\u4ee5\u672a\u7ecf\u58f0\u660e\u7684\u98ce\u683c\u7ebf\u7d22\u523b\u677f\u5316\u8fc7\u53bb\u7684\u65f6\u4ee3\uff0c\u5f15\u5165\u65f6\u4ee3\u9519\u7f6e\u73b0\u8c61\uff0c\u5e76\u672a\u80fd\u53cd\u6620\u51fa\u5408\u7406\u7684\u4eba\u53e3\u7edf\u8ba1\u6a21\u5f0f\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u5386\u53f2\u518d\u73b0\uff0c\u65e8\u5728\u63a8\u52a8\u6784\u5efa\u66f4\u5177\u5386\u53f2\u51c6\u786e\u6027\u548c\u6587\u5316\u5bf9\u9f50\u7684TTI\u6a21\u578b\u3002"}}
{"id": "2505.17043", "pdf": "https://arxiv.org/pdf/2505.17043", "abs": "https://arxiv.org/abs/2505.17043", "authors": ["Anya Belz"], "title": "QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reproduction studies reported in NLP provide individual data points which in\ncombination indicate worryingly low levels of reproducibility in the field.\nBecause each reproduction study reports quantitative conclusions based on its\nown, often not explicitly stated, criteria for reproduction success/failure,\nthe conclusions drawn are hard to interpret, compare, and learn from. In this\npaper, we present QRA++, a quantitative approach to reproducibility assessment\nthat (i) produces continuous-valued degree of reproducibility assessments at\nthree levels of granularity; (ii) utilises reproducibility measures that are\ndirectly comparable across different studies; and (iii) grounds expectations\nabout degree of reproducibility in degree of similarity between experiments.\nQRA++ enables more informative reproducibility assessments to be conducted, and\nconclusions to be drawn about what causes reproducibility to be better/poorer.\nWe illustrate this by applying QRA++ to three example sets of comparable\nexperiments, revealing clear evidence that degree of reproducibility depends on\nsimilarity of experiment properties, but also system type and evaluation\nmethod.", "AI": {"tldr": "QRA++ \u662f\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30 NLP \u53ef\u91cd\u590d\u6027\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u4f7f\u8bc4\u4f30\u7ed3\u679c\u66f4\u5177\u53ef\u6bd4\u6027\u548c\u4fe1\u606f\u6027\u3002", "motivation": "\u7531\u4e8e\u4e2a\u522b\u518d\u73b0\u7814\u7a76\u96be\u4ee5\u63d0\u4f9b\u660e\u786e\u7684\u7ed3\u8bba\uff0c\u672c\u7814\u7a76\u63d0\u51fa QRA++\uff0c\u4ee5\u4fbf\u80fd\u591f\u8fdb\u884c\u53ef\u6bd4\u8f83\u548c\u53ef\u89e3\u91ca\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a QRA++ \u7684\u5b9a\u91cf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e09\u4e2a\u7c92\u5ea6\u6c34\u5e73\u7684\u8fde\u7eed\u503c\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5e94\u7528 QRA++\uff0c\u63ed\u793a\u4e86\u53ef\u91cd\u590d\u6027\u7a0b\u5ea6\u4e0d\u4ec5\u4e0e\u5b9e\u9a8c\u5c5e\u6027\u7684\u76f8\u4f3c\u6027\u6709\u5173\uff0c\u8fd8\u4e0e\u7cfb\u7edf\u7c7b\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u6709\u5173\u3002", "conclusion": "QRA++ \u80fd\u591f\u5b9e\u73b0\u66f4\u4fe1\u606f\u5316\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\uff0c\u5e76\u5e2e\u52a9\u786e\u5b9a\u5f71\u54cd\u53ef\u91cd\u590d\u6027\u7684\u56e0\u7d20\u3002"}}
{"id": "2505.17090", "pdf": "https://arxiv.org/pdf/2505.17090", "abs": "https://arxiv.org/abs/2505.17090", "authors": ["Phoebe Chua", "Cathy Mengying Fang", "Takehiko Ohkawa", "Raja Kushalnagar", "Suranga Nanayakkara", "Pattie Maes"], "title": "EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language", "categories": ["cs.CV"], "comment": null, "summary": "Unlike spoken languages where the use of prosodic features to convey emotion\nis well studied, indicators of emotion in sign language remain poorly\nunderstood, creating communication barriers in critical settings. Sign\nlanguages present unique challenges as facial expressions and hand movements\nsimultaneously serve both grammatical and emotional functions. To address this\ngap, we introduce EmoSign, the first sign video dataset containing sentiment\nand emotion labels for 200 American Sign Language (ASL) videos. We also collect\nopen-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL\nsigners with professional interpretation experience. Alongside the annotations,\nwe include baseline models for sentiment and emotion classification. This\ndataset not only addresses a critical gap in existing sign language research\nbut also establishes a new benchmark for understanding model capabilities in\nmultimodal emotion recognition for sign languages. The dataset is made\navailable at https://huggingface.co/datasets/catfang/emosign.", "AI": {"tldr": "EmoSign \u6570\u636e\u96c6\u89e3\u51b3\u4e86\u624b\u8bed\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u7531\u4e8e\u624b\u8bed\u4e2d\u7684\u60c5\u611f\u6307\u6807\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u9020\u6210\u4e86\u6279\u5224\u6027\u73af\u5883\u4e2d\u7684\u6c9f\u901a\u969c\u788d\uff0c\u6545\u9700\u5efa\u7acb\u8fd9\u4e00 Emosign \u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5305\u542b\u60c5\u611f\u548c\u60c5\u7eea\u6807\u7b7e\u7684\u7f8e\u56fd\u624b\u8bed\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u7531\u4e13\u4e1a\u89e3\u8bd1\u7ecf\u9a8c\u7684\u4e09\u4f4d\u804b\u4eba\u624b\u8bed\u7528\u6237\u8fdb\u884c\u7684\u5f00\u653e\u5f0f\u60c5\u611f\u63d0\u793a\u63cf\u8ff0\u6ce8\u91ca\uff1b\u5e76\u63d0\u4f9b\u60c5\u611f\u5206\u7c7b\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u4e3a\u7f8e\u56fd\u624b\u8bed\u89c6\u9891\u63d0\u4f9b\u60c5\u611f\u548c\u60c5\u7eea\u6807\u7b7e\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5e76\u5305\u542b\u57fa\u7ebf\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u3002", "conclusion": "EmoSign \u6570\u636e\u96c6\u586b\u8865\u4e86\u624b\u8bed\u60c5\u611f\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u80fd\u529b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2505.17045", "pdf": "https://arxiv.org/pdf/2505.17045", "abs": "https://arxiv.org/abs/2505.17045", "authors": ["Afifah Kashif", "Heer Patel"], "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have separately highlighted significant biases within\nfoundational large language models (LLMs) against certain nationalities and\nstigmatized social groups. This research investigates the ethical implications\nof these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.\nThrough structured prompt series, we evaluate model responses to several\nscenarios involving American and North Korean nationalities with various mental\ndisabilities. Findings reveal significant discrepancies in empathy levels with\nNorth Koreans facing greater negative bias, particularly when mental disability\nis also a factor. This underscores the need for improvements in LLMs designed\nwith a nuanced understanding of intersectional identity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86GPT-3.5/4/4o\u6a21\u578b\u5728\u5904\u7406\u7279\u5b9a\u56fd\u7c4d\u548c\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\u7684\u504f\u89c1\uff0c\u7279\u522b\u662f\u5bf9\u5317\u671d\u9c9c\u4eba\u7684\u8d1f\u9762\u504f\u89c1\uff0c\u5f3a\u8c03\u4e86\u6539\u5584\u6a21\u578b\u516c\u5e73\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u663e\u793a\uff0c\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u5bf9\u67d0\u4e9b\u56fd\u7c4d\u548c\u793e\u4f1a\u7fa4\u4f53\u7684\u504f\u89c1\u3002\u672c\u6587\u7814\u7a76\u8fd9\u4e9b\u504f\u89c1\u5728GPT-3.5/4/4o\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u4f26\u7406\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u63d0\u793a\u7cfb\u5217\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u6d89\u53ca\u7f8e\u56fd\u548c\u671d\u9c9c\u56fd\u7c4d\u4ee5\u53ca\u591a\u79cd\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u573a\u666f\u4e2d\u7684\u54cd\u5e94\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7f8e\u56fd\u4eba\u76f8\u6bd4\uff0c\u5317\u671d\u9c9c\u4eba\u5c24\u5176\u5728\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u60c5\u5883\u4e0b\u906d\u9047\u4e86\u66f4\u5927\u7684\u6d88\u6781\u504f\u89c1\u3002\u8fd9\u53cd\u6620\u4e86\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540c\u7fa4\u4f53\u65f6\u7684\u5171\u60c5\u80fd\u529b\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\uff0cLLMs\u5728\u5904\u7406\u591a\u91cd\u8eab\u4efd\u4ea4\u53c9\u95ee\u9898\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5230\u7279\u5b9a\u56fd\u7c4d\u548c\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\u3002\u8fd9\u9700\u8981\u5728\u6a21\u578b\u8bbe\u8ba1\u4e2d\u5f15\u5165\u66f4\u7ec6\u81f4\u7684\u8eab\u4efd\u6954\u5165\u673a\u5236\uff0c\u786e\u4fdd\u540c\u60c5\u5fc3\u7684\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2505.17097", "pdf": "https://arxiv.org/pdf/2505.17097", "abs": "https://arxiv.org/abs/2505.17097", "authors": ["Yanshu Li", "JianJiang Yang", "Bozheng Li", "Ruixiang Tang"], "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 6 tables", "summary": "Multimodal in-context learning (ICL) enables large vision-language models\n(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of\nreal-world applications. However, multimodal ICL remains unstable, and current\nresearch largely focuses on optimizing sequence configuration while overlooking\nthe internal mechanisms of LVLMs. In this work, we first provide a theoretical\nanalysis of attentional dynamics in multimodal ICL and identify three core\nlimitations of standard attention that ICL impair performance. To address these\nchallenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet\neffective plug-and-play method for directly calibrating LVLM attention logits.\nCAMA is training-free and can be seamlessly applied to various open-source\nLVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its\neffectiveness and generality. CAMA opens new opportunities for deeper\nexploration and targeted utilization of LVLM attention dynamics to advance\nmultimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5CAMA\uff0c\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u5185\u5728\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u7684ICL\u652f\u6301\u5927\u91cf\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5176\u4ecd\u7136\u4e0d\u7a33\u5b9a\uff0c\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f18\u5316\u5e8f\u5217\u914d\u7f6e\uff0c\u800c\u5ffd\u7565\u4e86LVLM\u7684\u5185\u5728\u673a\u5236\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53d1\u73b0\u6807\u51c6\u6ce8\u610f\u529b\u5f71\u54cdICL\u8868\u73b0\u7684\u4e09\u4e2a\u6838\u5fc3\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aContext-Aware Modulated Attention (CAMA)\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u6821\u51c6LVLM\u7684\u6ce8\u610f\u529b\u63a7\u5236\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2aLVLM\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u901a\u8fc7\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86CAMA\u7684\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684CAMA\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u5e94\u7528\u4e8e\u5404\u79cd\u5f00\u6e90LVLM\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2505.17047", "pdf": "https://arxiv.org/pdf/2505.17047", "abs": "https://arxiv.org/abs/2505.17047", "authors": ["Erin Palm", "Astrit Manikantan", "Mark E. Pepin", "Herprit Mahal", "Srikanth Subramanya Belwadi"], "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025", "summary": "In medical practices across the United States, physicians have begun\nimplementing generative artificial intelligence (AI) tools to perform the\nfunction of scribes in order to reduce the burden of documenting clinical\nencounters. Despite their widespread use, no established methods exist to gauge\nthe quality of AI scribes. To address this gap, we developed a blinded study\ncomparing the relative performance of large language model (LLM) generated\nclinical notes with those from field experts based on audio-recorded clinical\nencounters. Quantitative metrics from the Physician Documentation Quality\nInstrument (PDQI9) provided a framework to measure note quality, which we\nadapted to assess relative performance of AI generated notes. Clinical experts\nspanning 5 medical specialties used the PDQI9 tool to evaluate\nspecialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators\nfrom each specialty scored notes drafted from a total of 97 patient visits. We\nfound uniformly high inter rater agreement (RWG greater than 0.7) between\nevaluators in general medicine, orthopedics, and obstetrics and gynecology, and\nmoderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and\ncardiology. We found a modest yet significant difference in the overall note\nquality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes\nscored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9\ninstrument as a practical method to gauge the quality of LLM authored notes, as\ncompared to human-authored notes.", "AI": {"tldr": "The study developed a method to compare AI-generated clinical notes with human experts' notes, finding slightly better quality in human notes but supporting the use of AI in documentation.", "motivation": "To address the lack of established methods for evaluating the quality of AI-generated clinical notes used as scribes, given their increasing use in medical practices.", "method": "A blinded study comparing large language model (LLM) generated clinical notes with those drafted by field experts, using the PDQI9 assessment tool, involving clinical experts from five medical specialties. Notes from 97 patient visits were evaluated.", "result": "There is a modest yet significant difference in quality between AI-generated notes and human-created notes, with human notes slightly scoring higher on average. High inter-rater agreement was found among evaluators in most specialties, supporting the method's reliability.", "conclusion": "The study supports the use of the PDQI9 instrument as a viable method to evaluate the quality of notes generated by LLMs compared to human-written notes."}}
{"id": "2505.17127", "pdf": "https://arxiv.org/pdf/2505.17127", "abs": "https://arxiv.org/abs/2505.17127", "authors": ["Michal Golovanevsky", "William Rudman", "Michael Lepori", "Amir Bar", "Ritambhara Singh", "Carsten Eickhoff"], "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165Visual CounterFact\u6570\u636e\u96c6\u6765\u6d4b\u8bd5\u591a\u6a21\u6001\u6a21\u578b\u5728\u76f4\u89c9\u77e5\u8bc6\u4e0e\u89c6\u89c9\u8bc1\u636e\u51b2\u7a81\u4e2d\u7684\u51b3\u7b56\uff0c\u5e76\u63d0\u51faPvP\u673a\u5236\u4ee5\u63a7\u5236\u8f93\u51fa\uff0c\u53d1\u73b0\u89c6\u89c9\u8bc1\u636e\u5f80\u5f80\u6700\u7ec8\u5360\u636e\u4e3b\u5bfc\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u662f\u66f4\u591a\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u7684\u4e16\u754c\u77e5\u8bc6\u8fd8\u662f\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u5f15\u5165Visual CounterFact\u6570\u636e\u96c6\uff0c\u5305\u542b\u89c6\u89c9\u771f\u5b9e\u7684\u53cd\u4e8b\u5b9e\uff0c\u4ee5\u68c0\u9a8c\u6a21\u578b\u5728\u76f4\u89c9\u77e5\u8bc6\u7279\u6027\u4e0e\u89c6\u89c9\u4fe1\u606f\u7684\u51b2\u7a81\u4e2d\u5982\u4f55\u505a\u51fa\u51b3\u7b56\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u6fc0\u6d3b\u5c42\u6b21\u7684\u5e72\u9884\u6280\u672f\u2014\u2014PvP\u5bfc\u5411\u5411\u91cf\uff0c\u4ee5\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6a21\u578b\u9884\u6d4b\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u9996\u5148\u53cd\u6620\u51fa\u8bb0\u5fc6\u7684\u76f4\u89c9\u77e5\u8bc6\uff0c\u4f46\u5728\u4e2d\u540e\u671f\u5c42\u6b21\u5219\u9010\u6e10\u503e\u5411\u4e8e\u89c6\u89c9\u8bc1\u636e\u3002PvP\u673a\u5236\u80fd\u5c0692.5%\u7684\u989c\u8272\u9884\u6d4b\u548c74.6%\u7684\u5c3a\u5bf8\u9884\u6d4b\u4ece\u76f4\u89c9\u8f6c\u5411\u53cd\u4e8b\u5b9e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u5c42\u6b21\u7684\u5e72\u9884\uff0c\u53ef\u4ee5\u5728\u8f93\u51fa\u7ed3\u679c\u4e2d\u66f4\u503e\u5411\u4e8e\u4e16\u754c\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\u3002PvP\u673a\u5236\u80fd\u591f\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6210\u529f\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2505.17048", "pdf": "https://arxiv.org/pdf/2505.17048", "abs": "https://arxiv.org/abs/2505.17048", "authors": ["Agam Shah", "Siddhant Sukhani", "Huzaifa Pardawala", "Saketh Budideti", "Riya Bhadani", "Rudra Gopal", "Siddhartha Somani", "Michael Galarnyk", "Soungmin Lee", "Arnav Hiray", "Akshar Ravichandran", "Eric Kim", "Pranav Aluru", "Joshua Zhang", "Sebastian Jaskowski", "Veer Guda", "Meghaj Tarte", "Liqin Ye", "Spencer Gosden", "Rutwik Routu", "Rachel Yuh", "Sloka Chava", "Sahasra Chava", "Dylan Patrick Kelly", "Aiden Chiang", "Harsit Mittal", "Sudheer Chava"], "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "Central banks around the world play a crucial role in maintaining economic\nstability. Deciphering policy implications in their communications is\nessential, especially as misinterpretations can disproportionately impact\nvulnerable populations. To address this, we introduce the World Central Banks\n(WCB) dataset, the most comprehensive monetary policy corpus to date,\ncomprising over 380k sentences from 25 central banks across diverse geographic\nregions, spanning 28 years of historical data. After uniformly sampling 1k\nsentences per bank (25k total) across all available years, we annotate and\nreview each sentence using dual annotators, disagreement resolutions, and\nsecondary expert reviews. We define three tasks: Stance Detection, Temporal\nClassification, and Uncertainty Estimation, with each sentence annotated for\nall three. We benchmark seven Pretrained Language Models (PLMs) and nine Large\nLanguage Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on\nthese tasks, running 15,075 benchmarking experiments. We find that a model\ntrained on aggregated data across banks significantly surpasses a model trained\non an individual bank's data, confirming the principle \"the whole is greater\nthan the sum of its parts.\" Additionally, rigorous human evaluations, error\nanalyses, and predictive tasks validate our framework's economic utility. Our\nartifacts are accessible through the HuggingFace and GitHub under the\nCC-BY-NC-SA 4.0 license.", "AI": {"tldr": "\u5f15\u5165WCB\u6570\u636e\u96c6\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6574\u5408\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u5355\u72ec\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7ecf\u6d4e\u6548\u7528\u3002", "motivation": "\u89e3\u6790\u4e2d\u592e\u94f6\u884c\u901a\u4fe1\u4e2d\u7684\u653f\u7b56\u542b\u4e49\uff0c\u4ee5\u907f\u514d\u9519\u8bef\u89e3\u8bfb\u5bf9\u8106\u5f31\u4eba\u7fa4\u7684\u4e0d\u5229\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aWCB\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b380k\u53e5\u5b50\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u53cc\u4eba\u6807\u6ce8\u548c\u4e13\u5bb6\u5ba1\u6838\u3002\u5b9a\u4e49\u4e86\u7acb\u573a\u68c0\u6d4b\u3001\u65f6\u95f4\u5206\u7c7b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e09\u4e2a\u4efb\u52a1\u3002\u4f7f\u7528\u591a\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u8868\u660e\uff0c\u5728\u96c6\u6210\u591a\u4e2a\u94f6\u884c\u7684\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u679c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u94f6\u884c\u6570\u636e\u6a21\u578b\uff0c\u5e76\u7ecf\u8fc7\u4e25\u683c\u7684\u4eba\u7c7b\u8bc4\u4f30\u3001\u9519\u8bef\u5206\u6790\u53ca\u9884\u6d4b\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7ecf\u6d4e\u5b9e\u7528\u6027\u3002", "conclusion": "\u6a21\u578b\u5728\u6574\u5408\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u5355\u72ec\u94f6\u884c\u6570\u636e\uff0c\u8bc1\u5b9e\u4e86\u6574\u4f53\u4f18\u4e8e\u90e8\u5206\u4e4b\u548c\u7684\u539f\u7406\u3002"}}
{"id": "2505.17132", "pdf": "https://arxiv.org/pdf/2505.17132", "abs": "https://arxiv.org/abs/2505.17132", "authors": ["Tanqiu Jiang", "Jiacheng Liang", "Rongyi Zhu", "Jiawei Zhou", "Fenglong Ma", "Ting Wang"], "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86DTR\uff0c\u4e00\u79cd\u901a\u8fc7\u4f18\u5316KV\u7f13\u5b58\u51cf\u8f7b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u653b\u51fb\u7684\u63a8\u7406\u65f6\u95f4\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5229\u7528\u89c6\u89c9\u6587\u672c\u4ea4\u4e92\u7684\u8d8a\u72f1\u653b\u51fb\u7684\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9632\u5fa1\u673a\u5236\u6765\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u6a21\u6001\u5f15\u53d1\u7684\u5b89\u5168\u76f8\u5173\u5206\u5e03\u53d8\u5316\u516c\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u6807\u8bb0\u6743\u91cd\uff0c\u6700\u5c0f\u5316\u5bf9\u6297\u6027\u89c6\u89c9\u8f93\u5165\u7684\u5f71\u54cd\u3002", "result": "DTR\u5728\u653b\u51fb\u9c81\u68d2\u6027\u548c\u5bf9\u826f\u6027\u4efb\u52a1\u7684\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\uff0c\u6210\u529f\u5e94\u7528KV\u7f13\u5b58\u4f18\u5316\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "conclusion": "DTR\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u7684\u5173\u952e\u503c(KV)\u7f13\u5b58\u5b9e\u73b0\u4e86\u63a8\u7406\u65f6\u95f4\u9632\u5fa1\uff0c\u4ee5\u51cf\u8f7b\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049", "abs": "https://arxiv.org/abs/2505.17049", "authors": ["David Rozado"], "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\u00e9sum\u00e9 Evaluations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u804c\u4e1a\u5019\u9009\u4eba\u9009\u62e9\u4e2d\u5c55\u73b0\u51fa\u5bf9\u5973\u6027\u540d\u5b57\u7684\u504f\u597d\uff0c\u5c3d\u7ba1\u5177\u5907\u76f8\u540c\u8d44\u5386\uff0c\u800c\u52a0\u5165\u6027\u522b\u5b57\u6bb5\u53ca\u6027\u522b\u4ee3\u8bcd\u540e\uff0c\u8fd9\u79cd\u504f\u597d\u66f4\u5f3a\u3002\u6b64\u5916\uff0c\u4f4d\u7f6e\u504f\u5dee\u663e\u7740\uff0c\u63d0\u793a\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u4f7f\u7528\u9700\u8981\u8c28\u614e\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6839\u636e\u7b80\u5386\u8bc4\u4f30\u804c\u4e1a\u5019\u9009\u4eba\u65f6\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5206\u6790\u6027\u522b\u504f\u89c1\u7684\u5b58\u5728\u3002", "method": "\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6d89\u53ca22\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6bcf\u4e2a\u6a21\u578b\u88ab\u7cfb\u7edf\u5730\u7ed9\u51fa\u4e00\u4e2a\u804c\u4f4d\u63cf\u8ff0\u548c\u4e00\u5bf9\u804c\u4e1a\u5339\u914d\u7684\u7b80\u5386\uff08\u4e00\u4e2a\u7537\u6027\u540d\u5b57\uff0c\u4e00\u4e2a\u5973\u6027\u540d\u5b57\uff09\uff0c\u8981\u6c42\u9009\u62e9\u66f4\u5408\u9002\u7684\u5019\u9009\u4eba\u3002", "result": "\u6240\u6709LLMs\u572870\u4e2a\u4e0d\u540c\u804c\u4e1a\u4e2d\u4e00\u81f4\u504f\u597d\u9009\u53d6\u5973\u6027\u540d\u5b57\u7684\u5019\u9009\u4eba\uff0c\u5c3d\u7ba1\u5019\u9009\u4eba\u5177\u5907\u76f8\u540c\u7684\u4e13\u4e1a\u8d44\u5386\u3002\u5728\u52a0\u5165\u6027\u522b\u5b57\u6bb5\u540e\uff0c\u8fd9\u79cd\u5bf9\u5973\u6027\u5019\u9009\u4eba\u7684\u504f\u597d\u8fdb\u4e00\u6b65\u589e\u52a0\u3002\u4f7f\u7528\u6027\u522b\u4e2d\u7acb\u6807\u8bc6\u7b26\u65f6\uff0c\u90e8\u5206\u6a21\u578b\u503e\u5411\u9009\u62e9\u201c\u5019\u9009\u4ebaA\u201d\u3002\u4e0d\u540c\u6027\u522b\u8d4b\u4e88\u6027\u522b\u4e2d\u7acb\u6807\u8bc6\u7b26\u53ef\u8fbe\u5230\u6027\u522b\u5e73\u8861\u3002\u72ec\u7acb\u8bc4\u5206\u65f6\uff0c\u5973\u6027\u7b80\u5386\u5f97\u5206\u7565\u9ad8\u4f46\u5f71\u54cd\u8f83\u5c0f\u3002\u52a0\u5165\u6027\u522b\u4ee3\u8bcd\u540e\uff0c\u5019\u9009\u4eba\u88ab\u9009\u4e2d\u7684\u51e0\u7387\u7565\u5fae\u589e\u52a0\u3002\u5927\u591a\u6570\u6a21\u578b\u663e\u793a\u51fa\u9009\u53d6\u9996\u5148\u5217\u51fa\u5019\u9009\u4eba\u7684\u4f4d\u7f6e\u504f\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0cLLMs\u5728\u9009\u62e9\u804c\u4e1a\u5019\u9009\u4eba\u65f6\u503e\u5411\u4e8e\u9009\u62e9\u5177\u6709\u5973\u6027\u540d\u5b57\u7684\u5019\u9009\u4eba\uff0c\u8fd9\u5bf9\u5728\u9ad8\u98ce\u9669\u7684\u81ea\u52a8\u5316\u51b3\u7b56\u73af\u5883\u4e2d\u4f7f\u7528LLMs\u63d0\u51fa\u4e86\u8b66\u793a\u3002"}}
{"id": "2505.17201", "pdf": "https://arxiv.org/pdf/2505.17201", "abs": "https://arxiv.org/abs/2505.17201", "authors": ["Chaim Chai Elchik", "Fatemeh Karimi Nejadasl", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking (MOT) in computer vision has made significant\nadvancements, yet tracking small fish in underwater environments presents\nunique challenges due to complex 3D motions and data noise. Traditional\nsingle-view MOT models often fall short in these settings. This thesis\naddresses these challenges by adapting state-of-the-art single-view MOT models,\nFairMOT and YOLOv8, for underwater fish detecting and tracking in ecological\nstudies. The core contribution of this research is the development of a\nmulti-view framework that utilizes stereo video inputs to enhance tracking\naccuracy and fish behavior pattern recognition. By integrating and evaluating\nthese models on underwater fish video datasets, the study aims to demonstrate\nsignificant improvements in precision and reliability compared to single-view\napproaches. The proposed framework detects fish entities with a relative\naccuracy of 47% and employs stereo-matching techniques to produce a novel 3D\noutput, providing a more comprehensive understanding of fish movements and\ninteractions", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u89c6\u56fe\u6846\u67b6\uff0c\u5229\u7528\u7acb\u4f53\u89c6\u9891\u63d0\u9ad8\u6c34\u4e0b\u9c7c\u7c7b\u7684\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u663e\u793a\u51fa\u6bd4\u5355\u89c6\u56fe\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5355\u89c6\u56feMOT\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u9c7c\u7c7b\u7531\u4e8e\u590d\u67423D\u8fd0\u52a8\u548c\u6570\u636e\u566a\u58f0\u5e26\u6765\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6539\u8fdb\u7684\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9002\u5e94\u6700\u5148\u8fdb\u7684\u5355\u89c6\u56feMOT\u6a21\u578bFairMOT\u548cYOLOv8\uff0c\u4ee5\u53ca\u5f00\u53d1\u591a\u89c6\u56fe\u6846\u67b6\u6765\u8fdb\u884c\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u548c\u8ddf\u8e2a\u3002\u5229\u7528\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u589e\u5f3a\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e76\u4f7f\u7528\u7acb\u4f53\u5339\u914d\u6280\u672f\u751f\u62103D\u8f93\u51fa\u3002", "result": "\u5f00\u53d1\u7684\u6846\u67b6\u5728\u68c0\u6d4b\u9c7c\u7c7b\u5b9e\u4f53\u65b9\u9762\u5177\u670947%\u7684\u76f8\u5bf9\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u76843D\u8f93\u51fa\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u9c7c\u7c7b\u8fd0\u52a8\u548c\u4e92\u52a8\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u63d0\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c7c\u7c7b\u884c\u4e3a\u6a21\u5f0f\u8bc6\u522b\u3002\u96c6\u6210\u548c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u6c34\u4e0b\u9c7c\u7c7b\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u663e\u793a\u51fa\u4e0e\u5355\u89c6\u56fe\u65b9\u6cd5\u76f8\u6bd4\u5728\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050", "abs": "https://arxiv.org/abs/2505.17050", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "AI": {"tldr": "\u5f15\u5165\u4e86PBLBench\u5bf9\u8c61\u57fa\u51c6\u4ee5\u6d4b\u8bd5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u9762\u4e34\u8f83\u5927\u6311\u6218\u3002", "motivation": "\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528AHP\u8fdb\u884c\u4e13\u5bb6\u9a71\u52a8\u7684\u6210\u5bf9\u6bd4\u8f83\u6765\u5efa\u7acb\u7ed3\u6784\u5316\u548c\u52a0\u6743\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728PBLBench\u4e2d\u4ec5\u8868\u73b0\u51fa59%\u7684\u6392\u540d\u51c6\u786e\u6027\uff0c\u8868\u660e\u8be5\u57fa\u51c6\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "PBLBench\u6311\u6218\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u76ee\u6807\u662f\u5728\u6559\u80b2\u73af\u5883\u4e2d\u63d0\u5347\u5176\u8868\u73b0\u4ee5\u51cf\u8f7b\u6559\u5e08\u8d1f\u62c5\u3002"}}
{"id": "2505.17223", "pdf": "https://arxiv.org/pdf/2505.17223", "abs": "https://arxiv.org/abs/2505.17223", "authors": ["Siyang Song", "Micol Spitale", "Xiangyu Kong", "Hengde Zhu", "Cheng Luo", "Cristina Palmero", "German Barquero", "Sergio Escalera", "Michel Valstar", "Mohamed Daoudi", "Tobias Baur", "Fabien Ringeval", "Andrew Howes", "Elisabeth Andre", "Hatice Gunes"], "title": "REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge", "categories": ["cs.CV", "68T40"], "comment": null, "summary": "In dyadic interactions, a broad spectrum of human facial reactions might be\nappropriate for responding to each human speaker behaviour. Following the\nsuccessful organisation of the REACT 2023 and REACT 2024 challenges, we are\nproposing the REACT 2025 challenge encouraging the development and benchmarking\nof Machine Learning (ML) models that can be used to generate multiple\nappropriate, diverse, realistic and synchronised human-style facial reactions\nexpressed by human listeners in response to an input stimulus (i.e.,\naudio-visual behaviours expressed by their corresponding speakers). As a key of\nthe challenge, we provide challenge participants with the first natural and\nlarge-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human\ndyadic interactions containing a total of 2856 interaction sessions covering\nfive different topics. In addition, this paper also presents the challenge\nguidelines and the performance of our baselines on the two proposed\nsub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge\nbaseline code is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2025", "AI": {"tldr": "REACT 2025\u6311\u6218\u65e8\u5728\u5f00\u53d1ML\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u7a7f\u6234\u5f0f\u9762\u90e8\u53cd\u5e94\uff0c\u63d0\u4f9b\u4e86MARS\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7531\u8bb2\u8bdd\u8005\u884c\u4e3a\u5f15\u8d77\u7684\u591a\u6837\u5316\u9762\u90e8\u53cd\u5e94\uff0c\u63d0\u51faREACT 2025\u6311\u6218\uff0c\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u751f\u6210\u591a\u6837\u6027\u548c\u73b0\u5b9e\u6027\u9762\u90e8\u53cd\u5e94\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86REACT 2025\u6311\u6218\uff0c\u9f13\u52b1\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u5927\u89c4\u6a21\u7684MARS\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u4e24\u4e2a\u5b50\u6311\u6218\uff1a\u79bb\u7ebfMAFRG\u548c\u5728\u7ebfMAFRG\uff0c\u5e76\u62a5\u544a\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5176\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u672c\u8bba\u6587\u901a\u8fc7\u63d0\u51faREACT 2025\u6311\u6218\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ee5\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u542c\u8005\u5728\u53cc\u4eba\u4e92\u52a8\u4e2d\u5bf9\u523a\u6fc0\u505a\u51fa\u53cd\u5e94\u65f6\u7684\u9002\u5f53\u9762\u90e8\u8868\u60c5\u3002"}}
{"id": "2505.17051", "pdf": "https://arxiv.org/pdf/2505.17051", "abs": "https://arxiv.org/abs/2505.17051", "authors": ["Bernd Huber", "Ghazal Fazelnia", "Andreas Damianou", "Sebastian Peleato", "Max Lefarov", "Praveen Ravichandran", "Marco De Nadai", "Mounia Lalmas-Roellke", "Paul N. Bennett"], "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating contextually relevant\ncontent. However, tailoring these outputs to individual users for effective\npersonalization is a significant challenge. While rich user-specific\ninformation often exists as pre-existing user representations, such as\nembeddings learned from preferences or behaviors, current methods to leverage\nthese for LLM personalization typically require costly fine-tuning or\ntoken-heavy prompting. We propose Embedding-to-Prefix (E2P), a\nparameter-efficient method that injects pre-computed context embeddings into an\nLLM's hidden representation space through a learned projection to a single soft\ntoken prefix. This enables effective personalization while keeping the backbone\nmodel frozen and avoiding expensive adaptation techniques. We evaluate E2P\nacross two public datasets and in a production setting: dialogue\npersonalization on Persona-Chat, contextual headline generation on PENS, and\nlarge-scale personalization for music and podcast consumption. Results show\nthat E2P preserves contextual signals and achieves strong performance with\nminimal computational overhead, offering a scalable, efficient solution for\ncontextualizing generative AI systems.", "AI": {"tldr": "Embedding-to-Prefix (E2P)\u65b9\u6cd5\u901a\u8fc7\u6ce8\u5165\u4e0a\u4e0b\u6587\u5d4c\u5165\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e2a\u6027\u5316\uff0c\u800c\u4e0d\u9700\u8981\u6602\u8d35\u7684\u9002\u914d\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u5229\u7528LLM\u8fdb\u884c\u4e2a\u6027\u5316\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u6216\u5927\u91cf\u7684\u63d0\u793a\uff0c\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEmbedding-to-Prefix (E2P)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6295\u5c04\u5c06\u9884\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6ce8\u5165\u5230LLM\u7684\u9690\u85cf\u8868\u793a\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u3002", "result": "E2P\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u4fe1\u53f7\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9884\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6ce8\u5165\u5230LLM\u7684\u9690\u85cf\u8868\u793a\u7a7a\u95f4\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5728\u4fdd\u6301\u9aa8\u5e72\u6a21\u578b\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6709\u6548\u7684\u4e2a\u6027\u5316\u3002"}}
{"id": "2505.17235", "pdf": "https://arxiv.org/pdf/2505.17235", "abs": "https://arxiv.org/abs/2505.17235", "authors": ["Omar Moured", "Yufan Chen", "Ruiping Liu", "Simon Rei\u00df", "Philip Torr", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "CHAOS: Chart Analysis with Outlier Samples", "categories": ["cs.CV", "cs.CL"], "comment": "Data and code are publicly available at:\n  http://huggingface.co/datasets/omoured/CHAOS", "summary": "Charts play a critical role in data analysis and visualization, yet\nreal-world applications often present charts with challenging or noisy\nfeatures. However, \"outlier charts\" pose a substantial challenge even for\nMultimodal Large Language Models (MLLMs), which can struggle to interpret\nperturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier\nSamples), a robustness benchmark to systematically evaluate MLLMs against chart\nperturbations. CHAOS encompasses five types of textual and ten types of visual\nperturbations, each presented at three levels of severity (easy, mid, hard)\ninspired by the study result of human evaluation. The benchmark includes 13\nstate-of-the-art MLLMs divided into three groups (i.e., general-, document-,\nand chart-specific models) according to the training scope and data.\nComprehensive analysis involves two downstream tasks (ChartQA and\nChart-to-Text). Extensive experiments and case studies highlight critical\ninsights into robustness of models across chart perturbations, aiming to guide\nfuture research in chart understanding domain. Data and code are publicly\navailable at: http://huggingface.co/datasets/omoured/CHAOS.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CHAOS\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u8fdb\u884c\u56fe\u8868\u5206\u6790\u548c\u53ef\u89c6\u5316\uff0c\u73b0\u5b9e\u5e94\u7528\u4e2d\u56fe\u8868\u5e38\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\u6216\u566a\u58f0\u7279\u5f81\uff0c\u5bf9\u2018\u5f02\u5e38\u56fe\u8868\u2019\u7684\u89e3\u8bfb\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "CHAOS\u57fa\u51c6\u6d4b\u8bd5\u5305\u62ec\u4e94\u79cd\u6587\u672c\u548c\u5341\u79cd\u89c6\u89c9\u6270\u52a8\uff0c\u6bcf\u79cd\u6270\u52a8\u5206\u4e3a\u4e09\u4e2a\u7ea7\u522b\uff08\u7b80\u5355\uff0c\u4e2d\u7b49\uff0c\u56f0\u96be\uff09\uff0c\u8fd9\u4e9b\u7ea7\u522b\u7075\u611f\u6765\u81ea\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u7684\u7814\u7a76\u7ed3\u679c\u3002\u6d4b\u8bd5\u5305\u62ec13\u79cd\u6700\u5148\u8fdb\u7684MLLMs\u5206\u4e3a\u4e09\u7ec4\uff08\u4e00\u822c\u6a21\u578b\u3001\u6587\u6863\u6a21\u578b\u548c\u4e13\u95e8\u9488\u5bf9\u56fe\u8868\u7684\u6a21\u578b\uff09\uff0c\u6839\u636e\u5176\u8bad\u7ec3\u8303\u56f4\u548c\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u548c\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u65b9\u9762\u7684\u9c81\u68d2\u6027\u7684\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u65e8\u5728\u6307\u5bfc\u672a\u6765\u56fe\u8868\u7406\u89e3\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "\u6211\u4eec\u5f15\u5165\u4e86CHAOS\u4f5c\u4e3a\u4e00\u4e2a\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30MLLMs\u5728\u9762\u5bf9\u56fe\u8868\u6270\u52a8\u65f6\u7684\u8868\u73b0\u3002"}}
{"id": "2505.17052", "pdf": "https://arxiv.org/pdf/2505.17052", "abs": "https://arxiv.org/abs/2505.17052", "authors": ["Jinwoo Park", "Seunggeun Cho", "Dongsu Han"], "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) power many modern applications, but serving them\nat scale remains costly and resource-intensive. Current server-centric systems\noverlook consumer-grade GPUs at the edge. We introduce SpecEdge, an\nedge-assisted inference framework that splits LLM workloads between edge and\nserver GPUs using a speculative decoding scheme, exchanging only token outputs\nover the network. SpecEdge employs proactive edge drafting to overlap edge\ntoken creation with server verification and pipeline-aware scheduling that\ninterleaves multiple user requests to increase server-side throughput.\nExperiments show SpecEdge enhances overall cost efficiency by 1.91x through\nachieving 2.22x server throughput, and reduces inter token latency by 11.24%\ncompared to a server-only baseline, introducing a scalable, cost-effective\nparadigm for LLM serving.", "AI": {"tldr": "SpecEdge\u901a\u8fc7\u8fb9\u7f18\u548c\u670d\u52a1\u5668\u534f\u4f5c\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u63a8\u7406\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u670d\u52a1\u6210\u672c\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u4ee5\u670d\u52a1\u5668\u4e3a\u4e2d\u5fc3\u7684\u7cfb\u7edf\u5ffd\u89c6\u4e86\u8fb9\u7f18\u7684\u6d88\u8d39\u7ea7GPU\u8d44\u6e90\uff0c\u800c\u5728\u8fb9\u7f18\u53c2\u4e0e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u66f4\u7ecf\u6d4e\u7684\u65b9\u5f0f\u8fdb\u884c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u662f\u503c\u5f97\u63a2\u7d22\u7684\u65b9\u5411\u3002", "method": "SpecEdge\u5229\u7528\u4e86\u4e00\u79cd\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\uff0c\u5c06\u8fb9\u7f18\u548c\u670d\u52a1\u5668GPUs\u8fdb\u884c\u8d1f\u8f7d\u5206\u62c5\uff0c\u5e76\u4f7f\u7528\u7ba1\u9053\u611f\u77e5\u8c03\u5ea6\u6765\u4ea4\u9519\u5904\u7406\u591a\u4e2a\u7528\u6237\u8bf7\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpecEdge\u901a\u8fc7\u5b9e\u73b02.22\u500d\u7684\u670d\u52a1\u5668\u541e\u5410\u91cf\uff0c\u63d0\u9ad8\u4e861.91\u500d\u7684\u6574\u4f53\u6210\u672c\u6548\u7387\uff0c\u5e76\u5c06\u6bcf\u4e2atoken\u7684\u5ef6\u8fdf\u964d\u4f4e\u4e8611.24%\u3002", "conclusion": "SpecEdge\u5728\u6574\u4f53\u6210\u672c\u6548\u7387\u548c\u5ef6\u8fdf\u65b9\u9762\u90fd\u4f18\u4e8e\u4ec5\u670d\u52a1\u5668\u7684\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u4e3aLLM\u670d\u52a1\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2505.17245", "pdf": "https://arxiv.org/pdf/2505.17245", "abs": "https://arxiv.org/abs/2505.17245", "authors": ["Ryota Yagi"], "title": "Extending Dataset Pruning to Object Detection: A Variance-based Approach", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset pruning -- selecting a small yet informative subset of training data\n-- has emerged as a promising strategy for efficient machine learning, offering\nsignificant reductions in computational cost and storage compared to\nalternatives like dataset distillation. While pruning methods have shown strong\nperformance in image classification, their extension to more complex computer\nvision tasks, particularly object detection, remains relatively underexplored.\nIn this paper, we present the first principled extension of classification\npruning techniques to the object detection domain, to the best of our\nknowledge. We identify and address three key challenges that hinder this\ntransition: the Object-Level Attribution Problem, the Scoring Strategy Problem,\nand the Image-Level Aggregation Problem. To overcome these, we propose tailored\nsolutions, including a novel scoring method called Variance-based Prediction\nScore (VPS). VPS leverages both Intersection over Union (IoU) and confidence\nscores to effectively identify informative training samples specific to\ndetection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate\nthat our approach consistently outperforms prior dataset pruning methods in\nterms of mean Average Precision (mAP). We also show that annotation count and\nclass distribution shift can influence detection performance, but selecting\ninformative examples is a more critical factor than dataset size or balance.\nOur work bridges dataset pruning and object detection, paving the way for\ndataset pruning in complex vision tasks.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u9488\u5bf9\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4fee\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u8bc4\u5206\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u66f4\u590d\u6742\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5e94\u7528\u6570\u636e\u96c6\u4fee\u526a\u6280\u672f\uff0c\u5982\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5--\u57fa\u4e8e\u65b9\u5dee\u7684\u9884\u6d4b\u8bc4\u5206 (VPS)\uff0c\u7ed3\u5408\u4e86IoU\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u8bc6\u522b\u7279\u5b9a\u4e8e\u68c0\u6d4b\u4efb\u52a1\u7684\u4fe1\u606f\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728PASCAL VOC\u548cMS COCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u5e73\u5747\u7cbe\u5ea6(mAP)\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u6570\u636e\u96c6\u4fee\u526a\u65b9\u6cd5\u3002\u8fd8\u53d1\u73b0\uff0c\u6ce8\u91ca\u6570\u91cf\u548c\u7c7b\u5206\u5e03\u53d8\u5316\u4f1a\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\uff0c\u4f46\u9009\u62e9\u4fe1\u606f\u793a\u4f8b\u6bd4\u6570\u636e\u96c6\u5927\u5c0f\u6216\u5e73\u8861\u6027\u66f4\u4e3a\u5173\u952e\u3002", "conclusion": "\u6210\u529f\u5c06\u6570\u636e\u96c6\u4fee\u526a\u6280\u672f\u5e94\u7528\u5230\u4e86\u5bf9\u8c61\u68c0\u6d4b\u9886\u57df\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u96c6\u4fee\u526a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.17053", "pdf": "https://arxiv.org/pdf/2505.17053", "abs": "https://arxiv.org/abs/2505.17053", "authors": ["Ou Jiamin", "Eikmans Emile", "Buskens Vincent", "Pankowska Paulina", "Shan Yuli"], "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures, 2 tables", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin understanding human languages, this study explores how they translate this\nunderstanding into social exchange contexts that capture certain essences of\nreal world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were\nplaced in economic trust games where players balance self-interest with trust\nand reciprocity, making decisions that reveal their social preferences and\ninteractive reasoning abilities. Our study shows that LLMs deviate from pure\nself-interest and exhibit trust and reciprocity even without being prompted to\nadopt a specific persona. In the simplest one-shot interaction, LLMs emulated\nhow human players place trust at the beginning of such a game. Larger\nhuman-machine divergences emerged in scenarios involving trust repayment or\nmulti-round interactions, where decisions were influenced by both social\npreferences and interactive reasoning. LLMs responses varied significantly when\nprompted to adopt personas like selfish or unselfish players, with the impact\noutweighing differences between models or game types. Response of ChatGPT-4, in\nan unselfish or neutral persona, resembled the highest trust and reciprocity,\nsurpassing humans, Claude, and Bard. Claude and Bard displayed trust and\nreciprocity levels that sometimes exceeded and sometimes fell below human\nchoices. When given selfish personas, all LLMs showed lower trust and\nreciprocity than humans. Interactive reasoning to the actions of counterparts\nor changing game mechanics appeared to be random rather than stable,\nreproducible characteristics in the response of LLMs, though some improvements\nwere observed when ChatGPT-4 responded in selfish or unselfish personas.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u6a21\u62df\u4eba\u7c7b\u7684\u4e92\u52a8\u884c\u4e3a\uff0c\u53d1\u73b0LLMs\u5728\u6e38\u620f\u4e2d\u80fd\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\u3002\u8fd9\u79cd\u80fd\u529b\u5728\u6ca1\u6709\u63d0\u793a\u7279\u5b9a\u4eba\u8bbe\u65f6\u5df2\u7ecf\u663e\u73b0\uff0c\u4e14\u5728\u88ab\u63d0\u793a\u540e\u5dee\u5f02\u66f4\u5927\u3002", "motivation": "\u8fd9\u9879\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u5176\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u8f6c\u5316\u4e3a\u793e\u4f1a\u4ea4\u4e92\u8bed\u5883\uff0c\u7279\u522b\u662f\u6a21\u62df\u4eba\u7c7b\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u7684\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793aLLMs\u5728\u5904\u7406\u4fe1\u4efb\u548c\u4e92\u60e0\u65f6\u7684\u793e\u4f1a\u504f\u597d\u548c\u4e92\u52a8\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u2014\u2014ChatGPT-4\u3001Claude\u548cBard\u2014\u2014\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002\u6e38\u620f\u4e2d\uff0c\u73a9\u5bb6\u9700\u8981\u5728\u81ea\u6211\u5229\u76ca\u4e0e\u4fe1\u4efb\u548c\u4e92\u60e0\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u505a\u51fa\u80fd\u591f\u63ed\u793a\u5176\u793e\u4f1a\u504f\u597d\u548c\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u7684\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u6e38\u620f\u4e2d\u504f\u79bb\u7eaf\u7cb9\u7684\u81ea\u6211\u5229\u76ca\uff0c\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\u3002\u5728\u7b80\u5355\u7684\u5355\u6b21\u4ea4\u4e92\u4e2d\uff0cLLMs\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u73a9\u5bb6\u5728\u6e38\u620f\u5f00\u59cb\u65f6\u7684\u4fe1\u4efb\u884c\u4e3a\u3002\u5728\u6d89\u53ca\u4fe1\u4efb\u56de\u62a5\u6216\u591a\u8f6e\u4ea4\u4e92\u7684\u573a\u666f\u4e2d\uff0cLLMs\u7684\u51b3\u7b56\u53d7\u793e\u4f1a\u504f\u597d\u548c\u4e92\u52a8\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u66f4\u5927\u7684\u4e0e\u4eba\u7c7b\u7684\u5dee\u5f02\u3002\u5f53\u88ab\u63d0\u793a\u91c7\u7528\u81ea\u79c1\u6216\u65e0\u79c1\u4eba\u8bbe\u65f6\uff0cChatGPT-4\u5728\u65e0\u79c1\u6216\u4e2d\u7acb\u4eba\u8bbe\u4e0b\u7684\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\u6700\u9ad8\uff0c\u8d85\u8fc7\u4e86\u4eba\u7c7b\u548c\u5176\u4ed6\u6a21\u578b\u3002\u800c\u5728\u88ab\u8d4b\u4e88\u81ea\u79c1\u4eba\u8bbe\u65f6\uff0c\u6240\u6709LLMs\u7684\u4fe1\u4efb\u548c\u4e92\u60e0\u6c34\u5e73\u5747\u4f4e\u4e8e\u4eba\u7c7b\u3002\u5728\u5e94\u5bf9\u5bf9\u624b\u884c\u52a8\u6216\u6e38\u620f\u673a\u5236\u53d8\u5316\u65f6\uff0cLLMs\u7684\u4e92\u52a8\u63a8\u7406\u8868\u73b0\u51fa\u968f\u673a\u800c\u975e\u7a33\u5b9a\u518d\u73b0\u7684\u7279\u5f81\uff0c\u5c3d\u7ba1ChatGPT-4\u5728\u81ea\u79c1\u6216\u65e0\u79c1\u4eba\u8bbe\u4e0b\u6709\u4e00\u4e9b\u6539\u5584\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u51fa\u4e8e\u81ea\u8eab\u5229\u76ca\u7684\u8003\u8651\u3002\u5373\u4f7f\u5728\u672a\u88ab\u63d0\u793a\u91c7\u7528\u7279\u5b9a\u4eba\u8bbe\u65f6\uff0cLLMs\u4e5f\u80fd\u6a21\u62df\u4eba\u7c7b\u5728\u4fe1\u4efb\u535a\u5f08\u4e2d\u7684\u884c\u4e3a\uff0c\u4f46\u5728\u4fe1\u4efb\u56de\u62a5\u6216\u591a\u8f6e\u4ea4\u4e92\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u4eba\u7c7b\u7684\u884c\u4e3a\u5dee\u5f02\u66f4\u5927\u3002\u6b64\u5916\uff0c\u5f53\u88ab\u63d0\u793a\u91c7\u7528\u7279\u5b9a\u4eba\u8bbe\u65f6\uff0cLLMs\u7684\u53cd\u5e94\u5dee\u5f02\u663e\u8457\uff0c\u8d85\u8fc7\u4e86\u4e0d\u540c\u6a21\u578b\u6216\u6e38\u620f\u7c7b\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002"}}
{"id": "2505.17256", "pdf": "https://arxiv.org/pdf/2505.17256", "abs": "https://arxiv.org/abs/2505.17256", "authors": ["Liang Shi", "Yun Fu"], "title": "ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved text-to-face\ngeneration, but achieving fine-grained control over facial features remains a\nchallenge. Existing methods often require training additional modules to handle\nspecific controls such as identity, attributes, or age, making them inflexible\nand resource-intensive. We propose ExpertGen, a training-free framework that\nleverages pre-trained expert models such as face recognition, facial attribute\nrecognition, and age estimation networks to guide generation with fine control.\nOur approach uses a latent consistency model to ensure realistic and\nin-distribution predictions at each diffusion step, enabling accurate guidance\nsignals to effectively steer the diffusion process. We show qualitatively and\nquantitatively that expert models can guide the generation process with high\nprecision, and multiple experts can collaborate to enable simultaneous control\nover diverse facial aspects. By allowing direct integration of off-the-shelf\nexpert models, our method transforms any such model into a plug-and-play\ncomponent for controllable face generation.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6ExpertGen\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u591a\u4e13\u5bb6\u534f\u4f5c\u63a7\u5236\u7684\u6587\u672c\u5230\u9762\u90e8\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u7684\u6a21\u5757\u8bad\u7ec3\u4ee5\u5904\u7406\u7279\u5b9a\u63a7\u5236\uff0c\u5982\u8eab\u4efd\u6216\u5e74\u9f84\uff0c\u4f7f\u5176\u4e0d\u7075\u6d3b\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u6211\u4eec\u63d0\u51faExpertGen\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u5f15\u5bfc\u751f\u6210\uff0c\u901a\u8fc7\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u786e\u4fdd\u6bcf\u4e2a\u6269\u6563\u6b65\u9aa4\u7684\u9884\u6d4b\u771f\u5b9e\u4e14\u7b26\u5408\u5206\u5e03\u3002", "result": "\u4e13\u5bb6\u6a21\u578b\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u591a\u4e2a\u4e13\u5bb6\u53ef\u4ee5\u534f\u4f5c\u5b9e\u73b0\u5bf9\u4e0d\u540c\u9762\u90e8\u7279\u5f81\u7684\u540c\u65f6\u63a7\u5236\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5141\u8bb8\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u76f4\u63a5\u96c6\u6210\uff0c\u5e76\u80fd\u591f\u4f5c\u4e3a\u53ef\u63a7\u9762\u90e8\u751f\u6210\u7684\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\u3002"}}
{"id": "2505.17054", "pdf": "https://arxiv.org/pdf/2505.17054", "abs": "https://arxiv.org/abs/2505.17054", "authors": ["Linglong Qian", "Zina Ibrahim"], "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.", "AI": {"tldr": "The paper introduces \\METHOD~, a novel transformer architecture that outperforms existing models in clinical sequence modelling and maintains clinical relevance and computational efficiency.", "motivation": "The unique challenges in healthcare domains such as irregular sampling, variable temporal dependencies, and complex contextual relationships in patient timelines necessitate a specialized transformer architecture for clinical sequence modelling in electronic health records.", "method": "Introduces \\METHOD~ (Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture with three key innovations: a patient-aware attention mechanism, an adaptive sliding window attention scheme, and a U-Net inspired architecture with dynamic skip connections.", "result": "\\METHOD~ consistently outperforms the state-of-the-art \\ETHOS~ model, especially in predicting high-severity cases requiring urgent clinical intervention. It shows stable performance across varying inference lengths and better preserves clinical hierarchies and relationships between medical concepts.", "conclusion": "\\METHOD~ represents a significant advancement in transformer architectures optimized for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency."}}
{"id": "2505.17280", "pdf": "https://arxiv.org/pdf/2505.17280", "abs": "https://arxiv.org/abs/2505.17280", "authors": ["Pushkar Shukla", "Aditya Chinchure", "Emily Diana", "Alexander Tolbert", "Kartik Hosanagar", "Vineeth N Balasubramanian", "Leonid Sigal", "Matthew Turk"], "title": "Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "The biases exhibited by text-to-image (TTI) models are often treated as\nindependent, though in reality, they may be deeply interrelated. Addressing\nbias along one dimension - such as ethnicity or age - can inadvertently affect\nanother, like gender, either mitigating or exacerbating existing disparities.\nUnderstanding these interdependencies is crucial for designing fairer\ngenerative models, yet measuring such effects quantitatively remains a\nchallenge. To address this, we introduce BiasConnect, a novel tool for\nanalyzing and quantifying bias interactions in TTI models. BiasConnect uses\ncounterfactual interventions along different bias axes to reveal the underlying\nstructure of these interactions and estimates the effect of mitigating one bias\naxis on another. These estimates show strong correlation (+0.65) with observed\npost-mitigation outcomes. Building on BiasConnect, we propose InterMit, an\nintersectional bias mitigation algorithm guided by user-defined target\ndistributions and priority weights. InterMit achieves lower bias (0.33 vs.\n0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields\nsuperior image quality compared to traditional techniques. Although our\nimplementation is training-free, InterMit is modular and can be integrated with\nmany existing debiasing approaches for TTI models, making it a flexible and\nextensible solution.", "AI": {"tldr": "BiasConnect\u901a\u8fc7\u91cf\u5316\u504f\u89c1\u4e4b\u95f4\u7684\u4e92\u52a8\u5173\u7cfb\u5e2e\u52a9TTI\u6a21\u578b\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u751f\u6210\uff1bInterMit\u5728\u8f83\u5c11\u6b65\u9aa4\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u7684\u504f\u89c1\u4e14\u56fe\u50cf\u8d28\u91cf\u66f4\u4f18\u3002", "motivation": "\u7406\u89e3\u504f\u89c1\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u662f\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\uff0c\u800c\u91cf\u5316\u8fd9\u4e9b\u5f71\u54cd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "BiasConnect\u4f7f\u7528\u53cd\u4e8b\u5b9e\u5e72\u9884\u5206\u6790\u4e0d\u540c\u504f\u89c1\u8f74\u4e0a\u7684\u504f\u89c1\u4e92\u52a8\u7ed3\u6784\uff0c\u5e76\u4f30\u8ba1\u8c03\u8282\u4e00\u4e2a\u504f\u89c1\u8f74\u5bf9\u53e6\u4e00\u4e2a\u4ea7\u751f\u7684\u5f71\u54cd\uff1bInterMit\u5219\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u76ee\u6807\u5206\u5e03\u53ca\u4f18\u5148\u6743\u91cd\u6765\u5f15\u5bfc\u504f\u89c1\u7f13\u89e3\u3002", "result": "BiasConnect\u7684\u4f30\u7b97\u4e0e\u89c2\u5bdf\u5230\u7684\u504f\u89c1\u7f13\u89e3\u540e\u7684\u7ed3\u679c\u5448\u73b0\u51fa\u5f3a\u76f8\u5173\u6027\uff08+0.65\uff09\uff0c\u800cInterMit\u5728\u964d\u4f4e\u504f\u89c1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff080.33\u5bf9\u6bd40.52\uff09\uff0c\u9700\u8981\u7684\u7f13\u89e3\u6b65\u9aa4\u66f4\u5c11\uff08\u5e73\u5747\u6b65\u9aa42.38\u5bf9\u6bd43.15\uff09\uff0c\u4e14\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002", "conclusion": "BiasConnect\u548cInterMit\u4e3aTTI\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u504f\u89c1\u91cf\u5316\u548c\u8c03\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u8f83\u4f4e\u7684\u504f\u89c1\u6c34\u5e73\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055", "abs": "https://arxiv.org/abs/2505.17055", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a97.59%\u51c6\u786e\u7387\u7684\u5df4\u52d2\u65af\u5766\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u4ee5\u4fc3\u8fdb\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\uff0c\u63d0\u9ad8\u4ed6\u4eec\u7684\u5b66\u4e60\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u7531\u4e8e\u5df4\u52d2\u65af\u5766\u624b\u8bed\u7684\u6570\u5b57\u8d44\u6e90\u7f3a\u4e4f\uff0c\u7814\u7a76\u7684\u52a8\u673a\u662f\u901a\u8fc7\u5148\u8fdb\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6765\u589e\u5f3a\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u4f7f\u7528Vision Transformer ViTModel\u8fdb\u884c\u624b\u52bf\u5206\u7c7b\uff0c\u8be5\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u4ee5\u63d0\u9ad8\u5176\u5bf9\u6570\u5b66\u624b\u52bf\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e8697.59%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bc6\u522b\u6570\u5b66\u624b\u52bf\u65b9\u9762\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u51c6\u786e\u7684\u5df4\u52d2\u65af\u5766\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6765\u589e\u5f3a\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\u3002\u8be5\u7cfb\u7edf\u53d6\u5f97\u9ad8\u8fbe97.59%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u9760\u6027\u548c\u7cbe\u786e\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u6559\u80b2\u5de5\u5177\u53ef\u4ee5\u901a\u8fc7\u4e92\u52a8\u89e3\u51b3\u65b9\u6848\u6765\u7f29\u5c0f\u542c\u969c\u5b66\u751f\u7684\u5b66\u4e60\u5dee\u8ddd\u3002"}}
{"id": "2505.17311", "pdf": "https://arxiv.org/pdf/2505.17311", "abs": "https://arxiv.org/abs/2505.17311", "authors": ["Harim Kim", "Yuhan Wang", "Minkyu Ahn", "Heeyoul Choi", "Yuyin Zhou", "Charmgil Hong"], "title": "Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI 2025 early accept", "summary": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for\nidentifying pathological abnormalities without requiring extensive labeled\ndata. However, existing diffusion-based UAD models rely solely on imaging\nfeatures, limiting their ability to distinguish between normal anatomical\nvariations and pathological anomalies. To address this, we propose Diff3M, a\nmulti-modal diffusion-based framework that integrates chest X-rays and\nstructured Electronic Health Records (EHRs) for enhanced anomaly detection.\nSpecifically, we introduce a novel image-EHR cross-attention module to\nincorporate structured clinical context into the image generation process,\nimproving the model's ability to differentiate normal from abnormal features.\nAdditionally, we develop a static masking strategy to enhance the\nreconstruction of normal-like images from anomalies. Extensive evaluations on\nCheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art\nperformance, outperforming existing UAD methods in medical imaging. Our code is\navailable at this http URL https://github.com/nth221/Diff3M", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6269\u6563\u6846\u67b6Diff3M\uff0c\u901a\u8fc7\u6574\u5408X\u5149\u7247\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684UAD\u6a21\u578b\u4ec5\u4f9d\u8d56\u4e8e\u5f71\u50cf\u7279\u5f81\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6b63\u5e38\u7684\u89e3\u5256\u53d8\u5f02\u548c\u75c5\u7406\u5f02\u5e38\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf-EHR\u8de8\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5c06\u7ed3\u6784\u5316\u4e34\u5e8a\u80cc\u666f\u7eb3\u5165\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u9759\u6001\u63a9\u7801\u7b56\u7565\u6765\u589e\u5f3a\u4ece\u5f02\u5e38\u5230\u6b63\u5e38\u6837\u8c8c\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "result": "Diff3M\u5728CheXpert\u548cMIMIC-CXR/IV\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684UAD\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Diff3M\u6846\u67b6\u901a\u8fc7\u6574\u5408\u80f8\u90e8X\u5149\u7247\u548c\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728CheXpert\u548cMIMIC-CXR/IV\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u8868\u73b0\u3002"}}
{"id": "2505.17056", "pdf": "https://arxiv.org/pdf/2505.17056", "abs": "https://arxiv.org/abs/2505.17056", "authors": ["Luoxi Tang", "Tharunya Sundar", "Shuai Yang", "Ankita Patra", "Manohar Chippada", "Giqi Zhao", "Yi Li", "Riteng Zhang", "Tunan Zhao", "Ting Yang", "Yuqiao Meng", "Weicheng Ma", "Zhaohan Xi"], "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u82f1\u8bed\u6807\u51c6\u5316\u8003\u8bd5\u51c6\u5907\u4e2d\u7684\u6f5c\u529b\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6ESTBOOK\u7528\u4e8e\u8bc4\u4f30\u5176\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u5206\u89e3\u5206\u6790\u6846\u67b6\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u8c03\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u652f\u6301\u82f1\u8bed\u6807\u51c6\u5316\u8003\u8bd5\u51c6\u5907\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u5176\u5728\u751f\u6210\u51c6\u786e\u548c\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aESTBOOK\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u82f1\u8bed\u6807\u51c6\u5316\u8003\u8bd5\uff08EST\uff09\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u3002ESTBOOK\u6c47\u96c6\u4e86\u4e94\u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u7684\u6d4b\u8bd5\uff0c\u6db5\u76d629\u79cd\u95ee\u9898\u7c7b\u578b\u548c\u8d85\u8fc710576\u4e2a\u95ee\u9898\uff0c\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u8868\u683c\u548c\u6570\u5b66\u7b26\u53f7\u7b49\u591a\u79cd\u5f62\u5f0f\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u5206\u6790\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684EST\u95ee\u9898\u62c6\u5206\u4e3a\u7279\u5b9a\u4efb\u52a1\u7684\u89e3\u51b3\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u4f7f\u7528ESTBOOK\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u63a8\u65ad\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u5206\u6790\u6846\u67b6\u8bc4\u4f30\u5176\u5728\u5404\u4e2a\u63a8\u7406\u73af\u8282\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u6f5c\u5728\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u9700\u8981\u91c7\u53d6\u9488\u5bf9\u6027\u7684\u7b56\u7565\u6765\u63d0\u9ad8\u5176\u4f5c\u4e3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models.", "AI": {"tldr": "\u63d0\u51fa\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u4ee5\u589e\u5f3a\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6539\u5584\u89c6\u89c9\u5d4c\u5165\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u9996\u5148\u8c03\u67e5\u4e86\u6295\u5f71\u5668\u5728\u538b\u7f29\u89c6\u89c9\u5d4c\u5165\u548c\u4e0e\u8bcd\u5d4c\u5165\u5bf9\u9f50\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u4ee5\u589e\u5f3a\u8865\u4e01\u7ea7\u5bf9\u9f50\u3002", "result": "\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u63d0\u9ad8\u4e86\u8865\u4e01\u7ea7\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u538b\u7f29\u80fd\u529b\u4ee5\u53ca\u66f4\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u5185\u5bb9\u751f\u6210\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u8865\u4e01\u5bf9\u9f50\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u89c6\u89c9\u8865\u4e01\u4e0e\u8bed\u4e49\u8bcd\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.17058", "pdf": "https://arxiv.org/pdf/2505.17058", "abs": "https://arxiv.org/abs/2505.17058", "authors": ["David Osei Opoku", "Ming Sheng", "Yong Zhang"], "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 5 figures;", "summary": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.", "AI": {"tldr": "DO-RAG, a scalable hybrid QA system, enhances factual accuracy by integrating knowledge graphs and vector retrieval, achieving high recall and relevancy.", "motivation": "Existing RAG frameworks struggle to integrate heterogeneous data while maintaining consistency in reasoning. There is a need for systems with high factual accuracy grounded in structured knowledge.", "method": "Proposes a hybrid QA framework called DO-RAG that integrates multi-level knowledge graph construction with semantic vector retrieval, employing an agentic chain-of-thought architecture for extracting structured relationships and constructing dynamic knowledge graphs.", "result": "Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%.", "conclusion": "DO-RAG represents a significant improvement in the field of domain-specific QA systems by enhancing recall and answer relevancy through the integration of knowledge graphs and semantic vector retrieval, outperforming existing frameworks."}}
{"id": "2505.17317", "pdf": "https://arxiv.org/pdf/2505.17317", "abs": "https://arxiv.org/abs/2505.17317", "authors": ["Alyson East", "Elizabeth G. Campolongo", "Luke Meyers", "S M Rayeed", "Samuel Stevens", "Iuliia Zarubiieva", "Isadora E. Fluck", "Jennifer C. Gir\u00f3n", "Maximiliane Jousse", "Scott Lowe", "Kayla I Perry", "Isabelle Betancourt", "Noah Charney", "Evan Donoso", "Nathan Fox", "Kim J. Landsbergen", "Ekaterina Nepovinnykh", "Michelle Ramirez", "Parkash Singh", "Khum Thapa-Magar", "Matthew Thompson", "Evan Waite", "Tanya Berger-Wolf", "Hilmar Lapp", "Paula Mabee", "Graham Taylor", "Sydne Record"], "title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Biological collections house millions of specimens documenting Earth's\nbiodiversity, with digital images increasingly available through open-access\nplatforms. Most imaging protocols were developed for human visual\ninterpretation without considering computational analysis requirements. This\npaper aims to bridge the gap between current imaging practices and the\npotential for automated analysis by presenting key considerations for creating\nbiological specimen images optimized for computer vision applications. We\nprovide conceptual computer vision topics for context, addressing fundamental\nconcerns including model generalization, data leakage, and comprehensive\nmetadata documentation, and outline practical guidance on specimen imagine, and\ndata storage. These recommendations were synthesized through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists. Through this synthesis, we have identified ten\ninterconnected considerations that form a framework for successfully\nintegrating biological specimen images into computer vision pipelines. The key\nelements include: (1) comprehensive metadata documentation, (2) standardized\nspecimen positioning, (3) consistent size and color calibration, (4) protocols\nfor handling multiple specimens in one image, (5) uniform background selection,\n(6) controlled lighting, (7) appropriate resolution and magnification, (8)\noptimal file formats, (9) robust data archiving strategies, and (10) accessible\ndata sharing practices. By implementing these recommendations, collection\nmanagers, taxonomists, and biodiversity informaticians can generate images that\nsupport automated trait extraction, species identification, and novel\necological and evolutionary analyses at unprecedented scales. Successful\nimplementation lies in thorough documentation of methodological choices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5341\u4e2a\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u4ee5\u4f18\u5316\u751f\u7269\u6807\u672c\u56fe\u50cf\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\uff0c\u5e76\u5f3a\u8c03\u8be6\u7ec6\u8bb0\u5f55\u65b9\u6cd5\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f25\u5408\u73b0\u6709\u6210\u50cf\u5b9e\u8df5\u4e0e\u81ea\u52a8\u5316\u5206\u6790\u6f5c\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f18\u5316\u751f\u7269\u6807\u672c\u56fe\u50cf\u4ee5\u9002\u5e94\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8de8\u5b66\u79d1\u7684\u5408\u4f5c\uff0c\u5236\u5b9a\u4e86\u5341\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8003\u8651\u56e0\u7d20\uff0c\u4e3a\u751f\u7269\u6807\u672c\u56fe\u50cf\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u4e2d\u7684\u6210\u529f\u6574\u5408\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "result": "\u751f\u6210\u652f\u6301\u81ea\u52a8\u6027\u72b6\u63d0\u53d6\u3001\u7269\u79cd\u8bc6\u522b\u53ca\u65b0\u9896\u751f\u6001\u548c\u8fdb\u5316\u5206\u6790\u7684\u56fe\u50cf\u3002", "conclusion": "\u6210\u529f\u7684\u751f\u7269\u6807\u672c\u56fe\u50cf\u521b\u5efa\u9700\u8981\u8be6\u7ec6\u8bb0\u5f55\u65b9\u6cd5\u9009\u62e9\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u7684\u81ea\u52a8\u6027\u72b6\u63d0\u53d6\u3001\u7269\u79cd\u8bc6\u522b\u53ca\u65b0\u9896\u7684\u751f\u6001\u548c\u8fdb\u5316\u5206\u6790\u3002"}}
{"id": "2505.17059", "pdf": "https://arxiv.org/pdf/2505.17059", "abs": "https://arxiv.org/abs/2505.17059", "authors": ["Van-Tinh Nguyen", "Hoang-Duong Pham", "Thanh-Hai To", "Cong-Tuan Hung Do", "Thi-Thu-Trang Dong", "Vu-Trung Duong Le", "Van-Phuc Hoang"], "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary\n  version posted for early dissemination and feedback", "summary": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.", "AI": {"tldr": "Medalyze, an application using FLAN-T5-Large models, excels in summarizing medical texts and improves healthcare information access.", "motivation": "To address the challenges in understanding complex medical texts due to terminology and context-specific language.", "method": "This paper utilizes three specialized FLAN-T5-Large models, fine-tuned for different tasks related to medical texts. The system is deployed on web and mobile platforms using a scalable API and YugabyteDB for real-time inference.", "result": "The system shows superior performance in summarization tasks over GPT-4, supported by BLEU, ROUGE-L, BERTScore, and SpaCy Similarity metrics.", "conclusion": "Medalyze provides an effective solution for accessing healthcare information, with superior performance over GPT-4 in specific tasks."}}
{"id": "2505.17328", "pdf": "https://arxiv.org/pdf/2505.17328", "abs": "https://arxiv.org/abs/2505.17328", "authors": ["Dylan Kline"], "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning", "categories": ["cs.CV"], "comment": null, "summary": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames.", "AI": {"tldr": "\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\uff0c\u6210\u529f\u5b66\u4e60\u4e86\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u6e38\u620f\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u6e38\u620f\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6e38\u620f\u56fe\u50cf\u7f16\u7801\u5668\u5f80\u5f80\u8fc7\u62df\u5408\u4e8e\u7279\u5b9a\u6e38\u620f\u7684\u89c6\u89c9\u98ce\u683c\uff0c\u9650\u5236\u4e86\u5728\u65b0\u6e38\u620f\u4e2d\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5b66\u4e60\u80fd\u591f\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u901a\u7528\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u6539\u5584\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\uff0c\u901a\u8fc7\u4e00\u4e2a\u5bf9\u6297\u6027\u7684\u57df\u5206\u7c7b\u5668\u540c\u65f6\u9f13\u52b1\u76f8\u4f3c\u5185\u5bb9\u805a\u7c7b\u5e76\u6291\u5236\u6e38\u620f\u7279\u5b9a\u7684\u89c6\u89c9\u7ebf\u7d22\u3002\u8fd9\u79cd\u65b9\u6cd5\u4ea7\u751f\u7684\u5d4c\u5165\u5728\u4e0d\u540c\u7684\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5728Bingsu\u6e38\u620f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u77ed\u65f6\u95f4\u7684\u8bad\u7ec3\u540e\uff0c\u7279\u5f81\u4e0d\u518d\u6309\u6e38\u620f\u805a\u7c7b\uff0c\u8bf4\u660e\u5b9e\u73b0\u4e86\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u53ef\u4ee5\u63d0\u5347\u8de8\u6e38\u620f\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u8de8\u6e38\u620f\u7684\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u663e\u793a\u51fa\u6539\u8fdb\u8de8\u6e38\u620f\u8fc1\u79fb\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u5728\u5c3d\u53ef\u80fd\u5c11\u7684\u5fae\u8c03\u4e0b\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u3002"}}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060", "abs": "https://arxiv.org/abs/2505.17060", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "AI": {"tldr": "Introduces SALMONN-omni, a full-duplex speech LLM improving conversational AI efficiency, achieving notable performance gains without extensive training data.", "motivation": "To address error accumulation, context-dependent barge-in, and echo cancellation challenges in modular conversational systems by creating a more efficient single-model solution.", "method": "Introduces a standalone full-duplex speech LLM with a novel dynamic thinking mechanism, avoiding audio codecs in the token space and improving through reinforcement learning.", "result": "SALMONN-omni achieves at least 30% relative performance improvement over existing models in benchmarks and excels in complex conversational scenarios.", "conclusion": "SALMONN-omni significantly improves full-duplex conversational systems by integrating a dynamic thinking mechanism, outperforming existing models with less training data."}}
{"id": "2505.17330", "pdf": "https://arxiv.org/pdf/2505.17330", "abs": "https://arxiv.org/abs/2505.17330", "authors": ["Amit Agarwal", "Srikant Panda", "Kulbhushan Pachauri"], "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; I.7"], "comment": "Published in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Industry Track, pages 100-114", "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag", "AI": {"tldr": "FS-DAG\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5728\u5c11\u6837\u672c\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u5ea6\u7684\u6027\u80fd\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u5c11\u6837\u672c\u73af\u5883\u4e2d\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9OCR\u9519\u8bef\u3001\u62fc\u5199\u9519\u8bef\u548c\u9886\u57df\u53d8\u5316\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "FS-DAG\u4f7f\u7528\u6a21\u5757\u5316\u6846\u67b6\u5c06\u9886\u57df\u7279\u5b9a\u548c\u8bed\u8a00/\u89c6\u89c9\u7279\u5b9a\u7684\u9aa8\u5e72\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u6587\u6863\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u8fdb\u884c\u9002\u5e94\u3002", "result": "FS-DAG\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u5feb\u901f\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u63d0\u5347\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u6027\u80fd\u663e\u8457\u3002", "conclusion": "FS-DAG\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061", "abs": "https://arxiv.org/abs/2505.17061", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565MoD\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u89e3\u7801\u7b56\u7565\u6765\u8bc4\u4f30\u56fe\u50cftoken\u7684\u6b63\u786e\u6027\uff0c\u4ece\u800c\u663e\u8457\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136LVLM\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u65b9\u6cd5\u3002", "method": "Mixture of Decoding\uff08MoD\uff09\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u89e3\u7801\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5bf9\u56fe\u50cftoken\u6ce8\u610f\u529b\u7684\u6b63\u786e\u6027\u6765\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u4e0a\uff0cMoD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u89e3\u7801\u65b9\u6cd5\u5e76\u6709\u6548\u7f13\u89e3LVLM\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "MoD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u51cf\u8f7bLVLMs\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2505.17333", "pdf": "https://arxiv.org/pdf/2505.17333", "abs": "https://arxiv.org/abs/2505.17333", "authors": ["Xin You", "Minghui Zhang", "Hanxiao Zhang", "Jie Yang", "Nassir Navab"], "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis", "categories": ["cs.CV"], "comment": "early accepted by MICCAI", "summary": "Temporal modeling on regular respiration-induced motions is crucial to\nimage-guided clinical applications. Existing methods cannot simulate temporal\nmotions unless high-dose imaging scans including starting and ending frames\nexist simultaneously. However, in the preoperative data acquisition stage, the\nslight movement of patients may result in dynamic backgrounds between the first\nand last frames in a respiratory period. This additional deviation can hardly\nbe removed by image registration, thus affecting the temporal modeling. To\naddress that limitation, we pioneeringly simulate the regular motion process\nvia the image-to-video (I2V) synthesis framework, which animates with the first\nframe to forecast future frames of a given length. Besides, to promote the\ntemporal consistency of animated videos, we devise the Temporal Differential\nDiffusion Model to generate temporal differential fields, which measure the\nrelative differential representations between adjacent frames. The prompt\nattention layer is devised for fine-grained differential fields, and the field\naugmented layer is adopted to better interact these fields with the I2V\nframework, promoting more accurate temporal variation of synthesized videos.\nExtensive results on ACDC cardiac and 4D Lung datasets reveal that our approach\nsimulates 4D videos along the intrinsic motion trajectory, rivaling other\ncompetitive methods on perceptual similarity and temporal consistency. Codes\nwill be available soon.", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\u6765\u6539\u5584\u65f6\u95f4\u8fd0\u52a8\u7684\u5efa\u6a21\uff0c\u4ee5\u6a21\u62df4D\u89c6\u9891\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u663e\u793a\u4e86\u4f18\u8d8a\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u65e0\u6cd5\u6a21\u62df\u65f6\u95f4\u8fd0\u52a8\uff0c\u9664\u975e\u540c\u65f6\u5b58\u5728\u9ad8\u5242\u91cf\u6210\u50cf\u626b\u63cf\u7684\u8d77\u59cb\u548c\u7ed3\u675f\u5e27\u3002\u5728\u672f\u524d\u6570\u636e\u91c7\u96c6\u9636\u6bb5\uff0c\u8f7b\u5fae\u7684\u60a3\u8005\u8fd0\u52a8\u53ef\u80fd\u5bfc\u81f4\u547c\u5438\u5468\u671f\u5185\u7684\u52a8\u6001\u80cc\u666f\uff0c\u4ece\u800c\u5f71\u54cd\u65f6\u95f4\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6b64\u9650\u5236\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u5fae\u5206\u573a\uff0c\u91c7\u7528\u63d0\u793a\u6ce8\u610f\u5c42\u548c\u573a\u589e\u5f3a\u5c42\u6765\u4fc3\u8fdb\u5fae\u5206\u573a\u4e0eI2V\u6846\u67b6\u7684\u4ea4\u4e92\uff0c\u63d0\u9ad8\u5408\u6210\u89c6\u9891\u7684\u65f6\u95f4\u53d8\u5316\u51c6\u786e\u6027\u3002", "result": "\u5728ACDC\u5fc3\u810f\u548c4D\u80ba\u90e8\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6cbf\u7740\u5185\u5728\u8fd0\u52a8\u8f68\u8ff9\u6a21\u62df4D\u89c6\u9891\uff0c\u5728\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u4e0e\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u5fae\u5206\u6269\u6563\u6a21\u578b\u6210\u529f\u5730\u6539\u5584\u4e86\u4e34\u65f6\u5dee\u5f02\u573a\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u5728\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u76844D\u89c6\u9891\u6a21\u62df\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u7ade\u4e89\u529b\u3002"}}
