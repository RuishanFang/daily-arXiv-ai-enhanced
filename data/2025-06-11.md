<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.CY](#cs.CY) [Total: 3]
- [quant-ph](#quant-ph) [Total: 5]
- [eess.SP](#eess.SP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.CV](#cs.CV) [Total: 27]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [nlin.CG](#nlin.CG) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Main category: cs.CL

TL;DR: LLMs在关系提取中有保守偏见，导致信息损失。保守偏见发生频率是幻觉的两倍。


<details>
  <summary>Details</summary>
Motivation: 分析大语言模型在关系提取任务中的保守偏见及其导致的信息损失。

Method: 使用SBERT和LLM提示来捕获受约束提示中的保守偏见行为与从半约束和开放式提示中生成标签之间的语义相似性。

Result: 保守偏见比幻觉现象发生得更频繁。通过多种提示、数据集和关系类型系统评估这一权衡，引入霍布森选择的概念，以捕捉模型选择安全但无信息标签的情境。

Conclusion: 大语言模型（LLMs）在关系提取任务中表现出明显的保守偏见，虽然这种行为有助于防止错误的关系分配，但也导致了显著的信息损失。保守偏见比幻觉现象发生得更频繁。

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [2] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Main category: cs.CL

TL;DR: 提出了一种新的方法 QA-LIGN，使语言模型对齐过程更透明和可控，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 为了避免传统奖励方法中将多目标混杂成一个不透明信号的问题，加强语言模型在不同原则（如有帮助性、诚实性和无害性）上的对齐。

Method: 引入了 QA-LIGN，一种自动化符号化奖励分解方法，通过制定特定原则的评估问题并为每个原则导出独立的奖励组件。

Result: 实验表明，QA-LIGN 在保持最终任务性能的同时，提供了对齐过程的更高透明度和适应性。

Conclusion: QA-LIGN 提供了一个更透明和适应性更强的对齐过程，同时实现了与或优于 DPO 基线的性能。

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [3] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Main category: cs.CL

TL;DR: EconWebArena是一个用于评估自主代理经济任务的基准，显示了现有多模态LLMs在现实网页环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在现实网页环境中进行复杂、多模态经济任务的自主代理评估基准，以强调使用权威数据源进行经济推理的准确性。

Method: 通过提示多个大型语言模型（LLMs）生成候选任务，然后进行严格的人类审核以确保任务的清晰性、可行性和数据源的可靠性，最后进行性能评价和故障案例分析。

Result: 测试结果揭示了显著性能差距，并强调了在经济网页智能中进行视觉植入、导航以及多模态理解的持续挑战。

Conclusion: EconWebArena是一个严格的测试平台，用于评估经济网页智能。

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [4] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Main category: cs.CL

TL;DR: CAF-I通过多代理系统解决了现有LLM方法在讽刺检测中的不足，展示了卓越的零样本性能，极大提高了检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM方法在讽刺检测方面存在挑战，包括视角单一、缺乏全面理解以及缺乏可解释性。

Method: 引入讽刺检测协作代理框架（CAF-I），这是一个由LLM驱动的多代理系统。CAF-I利用专门的语境、语义和修辞代理进行多维度分析，并进行互动协作优化。决策代理整合这些视角，并由精炼评估代理提供优化的条件反馈。

Result: CAF-I在基准数据集上的实验显示其在零样本学习中实现了最先进的性能，达到了绝大多数指标的最先进水平。CAF-I的平均Macro-F1达到76.31，比之前最强的基线提高了4.98点。

Conclusion: CAF-I通过有效模拟人类的多视角分析，提升了检测的准确性和可解释性，成为讽刺检测领域的新主流方法。

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [5] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本研究介绍了一个用于多语言仇恨言论检测的三语言数据集，并利用注意力层和大型语言模型，如GPT-3.5 Turbo和Qwen 2.5 72B，实现了显著的性能改进。


<details>
  <summary>Details</summary>
Motivation: 在英语和西班牙语等语言中，仇恨言论检测已被广泛研究，而乌尔都语的研究仍然不足，尤其是在使用翻译方法时。为了弥补这一空白，我们引入了英语、乌尔都语和西班牙语的三个语言的数据集。

Method: 我们的方法利用注意力层作为转换器模型和大语言模型（LLMs）的前置步骤，增强了多语言仇恨言论检测的特征提取。对于非转换器模型，我们使用TF-IDF进行特征提取。数据集使用包括GPT-3.5 Turbo和Qwen 2.5 72B在内的最先进模型进行基准测试，以及一些传统的机器学习模型如SVM和其他转换器（例如，BERT，RoBERTa）。

Result: 我们的方法与GPT-3.5 Turbo和Qwen 2.5 72B融合，表现出色，其中英语宏F1分数为0.87（GPT-3.5 Turbo），西班牙语为0.85（GPT-3.5 Turbo），乌尔都语为0.81（Qwen 2.5 72B），以及联合多语言模型为0.88（Qwen 2.5 72B）。结果显示，英语相比于SVM基线（0.80）提高了8.75％，西班牙语提高了8.97％（基线0.78），乌尔都语提高了5.19％（基线0.77），而联合多语言模型提高了7.32％（基线0.82）。

Conclusion: 我们的框架为多语言仇恨言论检测提供了强有力的解决方案，促进了全球更安全的数字社区。

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [6] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Main category: cs.CL

TL;DR: ETT-CKGE方法通过任务驱动的tokens实现高效知识转移，显著提高了预测性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在快照之间保留知识的效率和可扩展性上存在困难，主要由于手动设计的节点/关系重要性分数不理想以及计算昂贵的图遍历。

Method: ETT-CKGE引入了一组可学习的任务驱动tokens，这些tokens直接捕获与任务相关的信号，无需显式的节点评分或遍历，通过简单的矩阵操作实现知识转移。

Result: 实验表明ETT-CKGE在六个基准数据集上具备更高的预测性能和训练效率。

Conclusion: ETT-CKGE方法在多个基准数据集上都表现出色，不仅提高了训练效率和可扩展性，还在预测性能方面表现优异或与最新方法媲美。

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [7] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Main category: cs.CL

TL;DR: 本研究推出了GrAImes评估协议，以评估AI生成微小说的文学价值，并通过验证展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型可以生成叙事一致且语言连贯的短篇小说，但对于这些输出的文学价值评估尤其是美学质量方面的研究却极少。本研究旨在通过考虑文本的主题连贯性、文本清晰度、解释深度和美学质量等文学标准来评估AI生成的微小说。

Method: 提出并验证了一个基于文学理论的评估协议GrAImes，该协议可用于评估AI生成的微型小说的文学价值。

Result: 验证结果表明，GrAImes评估协议能够为评估AI生成微小说及其文学价值提供客观框架。

Conclusion: 本研究提出了一种名为GrAImes的评估协议，用于从文学理论的角度评估AI生成的微型小说，并通过文献专家和文学爱好者的回答验证了此评估协议。

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [8] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: LLM-BT利用大语言模型实现术语回译验证与标准化，展示了高效的术语一致性和跨语言稳健性支持。


<details>
  <summary>Details</summary>
Motivation: 应对快速增长的英语技术术语在AI和量子计算等快速发展的领域中对传统专家驱动标准化的挑战。

Method: 利用大语言模型进行跨语言语义对齐的回译框架，通过“Retrieve–Generate–Verify–Optimize”流程，实现术语的一致性验证。

Result: LLM-BT框架在术语一致性验证中表现优异，案例研究显示超过90%的精确或语义匹配，跨语言稳健性强，BLEU > 0.45；葡萄牙语术语准确性达到100%。

Conclusion: LLM-BT框架通过自动化术语验证和标准化，成功支持全球科学技术领域的术语治理。

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [9] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: The paper examines the proactive interference effect on LLMs, finds a working memory bottleneck affecting retrieval accuracy, and suggests the need for better methods to manage interference.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of intra-context interference on retrieval performance in LLMs as longer contexts are often assumed to improve retrieval.

Method: The study adapts the proactive interference (PI) paradigm from cognitive science to evaluate LLMs by sequentially streaming semantically related key-value updates and querying only the final values, observing a decline in retrieval accuracy as interference increases.

Result: LLMs' retrieval accuracy decreases log-linearly to zero as interference accumulates, and prompt engineering to mitigate interference shows limited success, highlighting the need for methods to suppress irrelevant content during retrieval.

Conclusion: LLMs have a fundamental limitation in disentangling interference and manipulating information flexibly, suggesting a working memory bottleneck beyond mere context access.

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [10] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Main category: cs.CL

TL;DR: 研究发现，利用写作过程数据可以提升大语言模型提供的反馈质量，使其更加贴合学习者的思维。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在支持学生写作方面越来越常见，但其反馈仅基于最终作文缺乏关于文本写作过程的重要上下文。本文探索使用通过击键记录和定期快照收集的写作过程数据是否可以帮助LLMs提供更好反馈。

Method: 我们构建了一个数字写作工具，记录学生输入以及文章随时间的演变过程，二十名学生使用该工具撰写限时文章，文章随后通过两种方式进行评估：使用最终文章和完整写作轨迹生成的LLM反馈，以及任务结束后学生对反馈的有用性和关联性进行调查。

Result: 早期结果显示，学习者更喜欢过程感知型LLMs反馈，觉得这些反馈更符合他们的思维。同时发现某些类型的编辑，如添加新内容或重新组织段落，与在连贯性和阐述等领域获得更高分数密切相关。

Conclusion: 使LLMs更加了解写作过程可以提升其反馈的意义、个性化和支持性。

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [11] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 复合AI系统的设计和优化面临新挑战，本文系统回顾了优化技术，指出了开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着复杂AI工作流程的设计和优化发生范式转变，复合AI系统变得越来越能够执行复杂任务。优化这些系统中的各个组件及其交互的新挑战因此产生，尤其是在非可微分系统的优化上，提出自然语言反馈的新方法。

Method: 本文采用了系统综述的方法，涵盖了数值和基于语言的技术。

Result: 定义了复合AI系统优化的概念，并根据几个关键维度对现有方法进行了分类。

Conclusion: 该论文系统地回顾了复合AI系统优化的最新进展，并在这快速发展的领域中指出了开放的研究挑战和未来的发展方向。

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [12] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: CLAIM-BENCH为评估LLMs在科学理解方面的能力提供了新的标准，闭源模型表现优于开源，并提出改进方法以提升科学论证的理解。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在科学文献中理解和处理复杂关系，特别是主张和证据之间逻辑联系的能力。

Method: 研究提出了CLAIM-BENCH评估基准和系统比较三种基于分治策略的方法，评估六种不同的大型语言模型在科学信息处理能力的优缺点。

Result: 研究结果表明，闭源模型（如GPT-4和Claude）在精确度和召回率方面优于开源模型。此外，使用精心设计的三遍法和逐一提示方法可显著提高LLMs正确链接分散证据与主张的能力，但计算成本也随之增加。

Conclusion: 尽管大型语言模型（LLMs）被广泛应用于复杂的研究任务，但它们在处理复杂科学文献中的理解和推理能力上仍存在显著局限。CLAIM-BENCH为评估这些模型在科学信息提取和验证方面的能力提供了全面的基准。

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [13] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: 本研究开发了一种推理分类方法，用于分析和生成阅读理解题，通过GPT-4o实现自动题目生成，结果显示高质量但推理类型匹配率有待提高。


<details>
  <summary>Details</summary>
Motivation: 推理能力是阅读理解中至关重要但复杂的一项技能，诊断性阅读理解题可以帮助教育者为学龄学生提供更有效的阅读指导和干预。

Method: 使用一种新的推理分类方法分析诊断性阅读理解题库中的项目分布，并通过少样例提示用GPT-4o生成桥接推理阅读理解项目，比较有无链式思考提示的情况下的表现。

Result: GPT-4o生成了93.8%的高质量问题，适合在3至12年级的实际应用中使用，但只有42.6%的生成问题准确匹配了目标推理类型。

Conclusion: 将自动项目生成与人工判断相结合，为大规模、高质量的诊断性阅读理解评估提供了一条有前途的道路。

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [14] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Main category: cs.CL

TL;DR: 研究人员发布了一大规模高质量的历史文本数据集，以填补训练大规模语言模型数据稀缺的问题，强调数据的来源和可持续管理。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型（LLMs）快速发展的背景下，优质的训练数据稀缺且重要。这促使研究人员寻找并公开可用的高质量数据集，并确保其来源明确，以促进可持续数据管理。

Method: 与哈佛图书馆合作，提取、分析以及处理参与谷歌图书项目的公共领域书籍，形成一个详细记录的历史文本数据集。

Result: 发布了Institutional Books 1.0，包含约1,075,899卷书籍的OCR提取文本（原始和后处理）及其元数据，并识别其中983,004卷书籍，约2420亿个字符在公共领域。

Conclusion: 通过详细处理和发布这些公共领域书籍，研究人员提供了一种可持续发展的数据管理方法，为训练大规模语言模型提供了丰富并可过滤的历史文本数据资源。

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [15] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Main category: cs.CL

TL;DR: 论文探讨通过抑制自反映标记如"Wait"和"Hmm"来提高大规模推理模型效率的可行性，提出NoWait方法能在多个推理任务中减少训练长度且不降低模型效用。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型尽管支持复杂的逐步推理，却常引入冗长及冗余的结果，影响效率。研究的目的在于考察抑制显式自我反思对推理的必要性，期望在不损害模型效用的基础上提高推理效率。

Method: 提出NoWait方法，通过在推理过程中抑制特定标记从而取消显式的自反映，进行实验验证其在文本、视觉和视频推理任务中的效果。

Result: 实验证明，NoWait方法能在多个推理任务中将推理链长度缩短27%-51%，同时不影响模型的效用，表明该方法能够有效且无损地优化多模态推理。

Conclusion: NoWait方法为高效且能保持效用的多模态推理提供了一种可插拔解决方案，证明了抑制显式自反映对增强模型效率的必要性。

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [16] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: 研究提出了一种多认知水平评估框架评估医学领域LLMs，发现随着认知复杂性增加，模型性能显著下降，尤其在更高认知水平模型规模影响更大，指出提升LLMs医学能力的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多个医学基准测试中表现出色，但其在不同认知水平上的能力仍未得到充分探索。受到Bloom's分类法的启发，我们提出了一种多认知水平的评估框架，用于评估医学领域的LLMs能力。

Method: 我们提出了一种新的评估框架，该框架整合现有的医学数据集，并引入针对三个认知水平的任务：初步知识掌握、综合知识应用和基于情景的问题解决。利用该框架，我们系统地评估了六个主要家族的最先进的通用和医学LLMs：Llama、Qwen、Gemma、Phi、GPT和DeepSeek。

Result: 我们的研究发现，随着认知复杂性的增加，所评估模型的性能显著下降，模型规模在更高认知水平的性能中起着更关键的作用。

Conclusion: 需要提高LLMs在更高认知水平下的医学能力，并为开发适合实际医学应用的LLMs提供了见解。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [17] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: 本文呼吁文本嵌入研究应超越表面意义，更加重视隐含语义，强调更具语言学依据的训练和评估方法，以改进模型对真实世界语言的复杂性理解。


<details>
  <summary>Details</summary>
Motivation: 大多数文本嵌入模型集中在表面语义上，而语言学理论表明意义通常是隐含的，由语用学、说话者意图和社会文化背景所塑造。

Method: 进行试点研究，比较现有最先进模型和简单基线模型在隐含语义任务上的表现。

Result: 试验表明，即使是最先进的嵌入模型在处理隐含语义任务时，表现也仅比简单的基线稍好。

Conclusion: 嵌入研究需要聚焦于更具语言学依据的训练数据，以及设计能评估更深层次语义理解的基准，同时明确隐含意义作为核心建模目标。

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [18] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 提出了一种因果归因框架，用于识别LLM中行为相关的注意力头，实现更准确的推理时间调整和优越的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 在推理时调整大型语言模型（LLM）的响应特征而无需修改其参数具有重要意义。然而，现有的模块选择方法往往依赖于浅层线索或临时启发式，从而导致次优或意外的结果。

Method: 提出了一种原则性的因果归因框架，用于识别变行为相关的变换器注意力头。具体来说，为每个头部的注意力激活训练一个向量量化自动编码器（VQ-AE），将潜在空间分为行为相关和行为无关的子空间，并用共享的可学习代码本进行量化。然后通过二分类度量评估行为对齐和行为违反响应的VQ-AE编码的可分性来量化每个头的行为相关性。

Result: 实验表明，该方法能够更准确地进行推理时干预，在真实性引导任务上表现优越。此外，这种方法选择的注意力头在跨领域的真实性引导场景中表现出了强大的零样本泛化能力。

Conclusion: 这项工作为识别变行为相关注意力头提供了一个原则性的方法框架，可用于更精确地进行推理时调整，并在跨领域场景中表现优越。

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [19] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Main category: cs.CL

TL;DR: 介绍了一种新的因果链RAG方法，通过构建因果三元组图，显著提升了语言模型在专门领域的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专门领域理解因果关系面临挑战，尽管检索增强生成能够提升事实准确性，但传统方法缺乏建模真实因果依赖所需的结构。

Method: 提出了一种新的方法，称为因果链RAG（CC-RAG）。该方法结合零样本三重提取和主题感知图链，以构建有结构的多跳推理。具体地，利用特定领域的语料库，CC-RAG构建因果三元组的有向无环图，并使用正向/逆向链以指导结构化答案生成。

Result: 在比特币价格波动和戈谢病两个真实领域的实验中，CC-RAG在链相似性、信息密度和词汇多样性方面优于标准RAG和零样本LLMs。

Conclusion: 结果表明，显式地建模因果结构可以使LLMs能够生成更准确、更可解释的响应，尤其是在传统检索方法失败的专门领域。

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [20] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出了一种无需训练的PCD方法，通过对比注意力提升长文本性能，实验验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法训练成本高且在统计行为和成本效益方法上探讨不足，需寻找无训练成本的方法来解决长文本性能衰减问题。

Method: 引入Positional Contrastive Decoding (PCD)，通过对比长感知注意力和局部感知注意力派生的logits，提升模型的长文本关注能力。

Result: 实验结果表明，PCD在长上下文基准上达到了最新的性能水平，表明其有效性。

Conclusion: PCD显著提升了长上下文基准的性能，证明其有效缓解了注意力得分衰减问题。

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [21] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: 通过引入SpecKV和SpecPC方法，利用草稿模型更准确预测标记和KV对重要性，提高了长上下文大型语言模型推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的近似方法通常依赖于对标记或KV对重要性的粗略预测。为了更准确地预测它们的重要性，我们提出使用小型草稿模型来优化长上下文大型语言模型的推理过程。

Method: 我们提出了一种利用小型草稿模型的框架，以更准确地预测标记和KV对的重要性。具体包括：SpecKV，使用草稿输出准确评估每个KV对的重要性，实现更有效的KV缓存丢弃；SpecPC，通过草稿模型的注意力激活识别并丢弃不重要的提示标记。

Result: 与现有基线相比，我们的方法在长上下文基准测试中的准确率更高，同时保持相同的内存使用、延迟和吞吐量改进。

Conclusion: 我们的技术在保持相同的内存使用、延迟和吞吐量改进的同时，相比现有基线方法能在长上下文基准测试中持续取得更高的准确率。

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [22] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 开发了EIFBENCH基准测试和SegPO算法，以真实评估和提升大语言模型在多任务复杂环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于现有基准测试主要关注单一任务环境，无法全面反映真实世界的复杂性，该研究旨在通过开发新的基准测试EIFBENCH，来更真实和稳健地评估大语言模型的能力。

Method: 提出了Segment Policy Optimization (SegPO)算法，用于提升大语言模型在多任务工作流中的执行能力。

Result: 在EIFBENCH的评估中，现有的大语言模型在面对极其复杂的指令时表现出显著的性能差异。

Conclusion: 该论文强调现有大语言模型在应对极其复杂指令时性能差异显著，显示出需要持续优化以应对复杂挑战。

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [23] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文介绍了mSTEB基准来评估大型语言模型在低资源语言上的表现，结果显示高资源与低资源语言表现差距显著，需要更多投资改善低资源语言的覆盖。


<details>
  <summary>Details</summary>
Motivation: 填补现有评价标准不足之处，为低资源语言提供一个标准化的评价基准。

Method: 引入了mSTEB，一个新的基准，用于评估大型语言模型在包括语言识别、文本分类、问题回答以及翻译任务等多种任务上的表现。

Result: 评估结果显示高资源语言和低资源语言在模型表现上有显著差距，特别是在非洲和美洲/大洋洲的语言方面。

Conclusion: 需要更多的投资来提高低资源语言在大型语言模型中的表现和覆盖。

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [24] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出的TACTIC框架利用认知策略显著提升了机器翻译质量，超过GPT-4.1和DeepSeek-R1。


<details>
  <summary>Details</summary>
Motivation: 现有的多代理翻译系统在翻译质量提升方面有所潜力，但忽略了认知翻译学的基础见解，即人类译者使用不同的认知策略进行翻译，此框架旨在解决这一问题。

Method: 作者提出了一个认知信息驱动的多代理框架，称为TACTIC（Translation Agents with Cognitive-Theoretic Interactive Collaboration），该框架利用六个功能不同的代理来模拟人的翻译认知过程。

Result: 在FLORES-200和WMT24基准测试中，TACTIC框架的表现超过了现有的先进模型，如GPT-4.1和DeepSeek-R1，取得了目前最先进的性能。

Conclusion: TACTIC框架能够显著提升机器翻译质量，尤其是在不同语言对上的翻译表现。

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [25] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 提出了AutoMeco框架和MIRA策略来优化LLM的元认知评估，在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究对LLM的自我评估能力进行了探索，但缺乏对步骤级分析和适应的研究。因此，需要一种更好的方法来分析和提升LLM的元认知能力。

Method: 本研究提出了一个自动化元认知评估框架AutoMeco，并通过使用训练无关的马尔可夫内在奖励调整策略（MIRA）来提升当前的元认知能力。

Result: 在三个数学推理数据集和三个LLM上的实验结果验证了AutoMeco的合理性，并且使用MIRA策略可以更好地评估LLM的元认知能力。

Conclusion: 提出了一种自动化元认知评估框架AutoMeco，并证明其在三个数学推理数据集和三种LLM上的合理性。MIRA策略可以有效提升现有元认知能力的评估效果。

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [26] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: Know-MRI is an open-source tool designed to enhance the interpretability of LLMs by systematically analyzing their knowledge mechanisms using a flexible core module for matching input data with suitable interpretation methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current interpretation methods which vary in input data formats and interpreting outputs, thereby restricting their practical applicability.

Method: Development of an extensible core module that automatically matches different input data with suitable interpretation methods and consolidates interpretation outputs.

Result: Know-MRI allows for a more comprehensive diagnosis of model's internal mechanisms by enabling the use of appropriate interpretation methods based on the input, facilitating analysis from multiple perspectives.

Conclusion: Know-MRI provides a systematic and comprehensive solution to analyze the internal knowledge mechanisms of large language models, enhancing their interpretability across various input data formats.

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [27] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 探索如何通过数值精度和数据并行化策略影响训练速度和模型准确性，以支持低资源环境中的领域适应。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）的代价很高，包括能源、硬件和注释数据，并且通常反映出主导文化和价值观的立场。

Method: 评估不同数值精度和数据并行化策略对训练速度和模型准确性的影响。

Result: 为低资源环境中的领域适应提供了方法，使能效、可访问性和硬件需求成为关键考量。

Conclusion: 领域适应是使模型更好地适应多样文化和价值环境的一个策略，但其计算成本仍然是一个显著的障碍。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [28] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: Olica是一种新的LLM剪枝方法，通过正交分解和线性校准无需训练即可高效压缩模型。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化剪枝方法对大型语言模型（LLMs）需要大量计算和数据资源重新训练，以恢复损坏的相关性，成本过高。

Method: 提出了一种称为正交分解和线性校准（Olica）的剪枝框架，可以无需重新训练地进行剪枝。该框架通过对多头注意力层进行主成分分析（PCA），无需影响准确性或破坏原始结构，从而压缩模型。

Result: 提出的Olica在数据使用、GPU内存和运行时间方面效率很高，并且在多个基准测试中表现优异。

Conclusion: Olica框架提供了一种高效的方法来压缩LLMs，无需重新训练，同时保持模型性能。

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [29] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Main category: cs.CL

TL;DR: 引入U-CoT+框架，通过将视觉模因转化为文本描述，并使用通用大型语言模型进行有害模因检测，解决了现有方法在资源效率和可解释性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有检测有害模因的方法在资源效率、灵活性或可解释性方面存在困难，限制了其在内容审核系统中的实际部署。

Method: 提出了一种新的有害模因检测框架U-CoT+，通过开发一个高保真模因到文本的转化管道，将视觉模因转换为细节保留的文本描述，从而实现了模因解释与分类的解耦。此设计使得使用通用大型语言模型进行资源高效的有害模因检测成为可能。

Result: 在七个基准数据集上的大量实验验证了该框架的有效性，特别是在解释性和低资源的情况下实现了有害模因的检测。

Conclusion: 本研究展示了U-CoT+框架在解释性和资源效率上的潜力，使其能够适应不同平台、地区和时间的有害检测标准。

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [30] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: Adaptive-$k$是一种有效的自适应检索方法，不需要模型微调等复杂操作，在减少代币使用量的情况下仍然提高了QA准确性。


<details>
  <summary>Details</summary>
Motivation: 在开放域问答中，检索增强生成（RAG）和长上下文语言模型（LCLMs）均解决了LLMs的上下文限制，但最佳外部上下文的检索仍未解决。现有自适应方法在处理factoid QA表现良好，但在处理上下文大小未知且可变的aggregation QA时存在困难。

Method: Adaptive-$k$检索，根据查询与候选段落之间相似度分布自适应选择段落数量的单次传递方法。

Result: Adaptive-$k$在五个LCLMs和两个嵌入模型上提升准确性，表明动态调整上下文大小可实现更高效、更准确的QA。

Conclusion: Adaptive-$k$ retrieval在不需要模型微调、额外的LLM推理或更改现有检索-阅读流水线的情况下，在事实性和聚合性QA基准上匹配或超过固定-$k$基线，并在使用代币量减少至全上下文输入的十分之一的情况下，仍然检索到70%的相关段落。

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [31] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 研究分析了图文匹配评估中的不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成模型常常无法准确生成与文本提示匹配的图像。本研究旨在探索可靠评估框架需要满足哪些关键属性，以改善图文对齐评估。

Method: 通过经验研究分析主流评估框架是否满足图文对齐评估的关键属性。

Result: 当前评估框架未能全面满足可靠评估的两个关键属性，作者提出了一些改进建议。

Conclusion: 现有主流评估框架在评估图文对齐时未能全面满足可靠评估的关键属性。

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [32] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: This study audits mid-tier SLMs, showing that fairness and efficiency can coexist; biases vary by architecture, and compression affects bias-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the ethical risks and performance of Small Language Models, especially those in the 'middle tier' between BERT-class encoders and large language models, as they are being rapidly adopted for resource-constrained environments.

Method: The paper conducts a large-scale audit by evaluating nine open-source instruction-tuned SLMs using the BBQ benchmark under zero-shot prompting, analyzing utility and fairness in both ambiguous and disambiguated contexts.

Result: The audit reveals that Phi models can be both efficient and ethical, LLaMA 3.2 models show strong bias, and compression such as 4-bit AWQ quantization can have mixed impacts on performance and bias.

Conclusion: SLMs can achieve high levels of fairness and efficiency without compromising one for the other. However, different model architectures have varying social biases, and model compression introduces trade-offs between utility and bias.

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [33] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 引入EtiCor++语料库来评估LLMs对全球礼仪的理解，并发现其在某些地区存在偏见


<details>
  <summary>Details</summary>
Motivation: 希望提高LLMs对文化礼仪的敏感性，弥补现有资源在评估礼仪理解和偏见方面的不足

Method: 开发和实验EtiCor++语料库，并设计任务和指标来评估和测量LLMs对礼仪的知识和偏见

Result: 实验表明，LLMs对某些地区存在固有偏见

Conclusion: EtiCor++语料库成功揭示了LLMs在地域文化礼仪上的偏见，强调了对特定区域礼仪理解的重要性

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [34] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Main category: cs.CL

TL;DR: 提出了一种一致性驱动的原型提示框架，用于广义意图发现，在无监督数据上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 有监督的方法过于依赖于标注的域内数据，在处理域外意图时表现不佳，限制了其实用性。现有方法主要关注聚类无监督数据，而忽视了领域适应问题。

Method: 我们提出了一种一致性驱动的原型提示框架，用于从整合旧知识和新知识的角度进行广义意图发现。该框架包括一种用于从外部来源转移旧知识的原型提示框架，以及一种用于从目标领域学习新知识的层次一致性约束。

Result: 实验证明，我们的方法显著优于所有基线方法，达到了最先进的水平。

Conclusion: 我们的模型显著优于所有基线方法，取得了最先进的结果，证明了我们方法的有效性和泛化性。

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [35] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: 提出一种新的知识冲突分类法和CONFLICTS基准来改进大型语言模型在检索增强生成中的表现，实验显示模型在解决冲突方面有改善但仍需更多研发。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型在RAG中的表现，尤其是处理信息冲突的能力，同时提供一个基准来跟踪模型在这方面的进展。

Method: 提出了一种用于分类RAG中的知识冲突的新分类法，并创建了一个名为CONFLICTS的基准用于评估模型处理知识冲突的效果。进行了广泛的实验来测试模型在该基准上的表现，尤其是在处理冲突信息时的表现。

Result: 实验表明，大型语言模型在解决信息来源之间冲突时往往表现不足，而通过提示模型明确推理检索文档中的可能冲突可以显著改善其响应的品质和适当性。

Conclusion: 通过在检索增强生成（RAG）中引入一个关于知识冲突类型的新分类法以及一个叫做CONFLICTS的高质量基准，对模型如何处理多种知识冲突进行了追踪和实验，发现当前的大型语言模型在处理信息冲突时仍面临挑战，虽然通过提示模型进行明确的推理有助于提高其响应质量，但仍有很大的提升空间。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [36] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: 引入了CoMuMDR，即一个支持多域、代码混合的多模态语料库，用于会话话语解析。实验表明，现有模型在此语料库上表现欠佳，需开发更先进的模型。


<details>
  <summary>Details</summary>
Motivation: 当前的会话话语解析数据集仅限于单一领域的英语书面对话，因此研究人员希望开发支持多域、代码混合的语料库，以改善自然语言理解应用。

Method: 本研究使用了多种顶尖的基线模型来测试新语料库的表现。

Result: 实验结果表明，目前顶尖的基线模型在处理多域代码混合语料库上性能不佳，这凸显了开发更好的模型以适应现实环境的必要性。

Conclusion: 本研究强调了目前的顶尖模型在多域代码混合语料库上的表现不佳，说明现有模型在处理现实环境中的多域代码混合语料库时存在不足。

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [37] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: 论文提出了一种用于大型语言模型推理的后训练框架，通过对比推理反馈和残差嵌入精炼，大幅提高推理准确率，并在MathQA上取得5%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有隐性推理方法在后训练更新推理嵌入时的有效性挑战，以更精确地指导模型达到更准确的解决方案。

Method: 引入两个策略：对比推理反馈和残差嵌入精炼，分别通过与强基线和弱基线的推理嵌入比较更新嵌入方向，以及逐步整合当前与历史梯度以稳定更新过程。

Result: 在五个推理基准测试中证明了框架的有效性，MathQA准确率提升了5%，且不需额外训练。

Conclusion: 我们提出的轻量级后训练框架有效提升了隐性推理的准确性，对推理嵌入进行精炼，首次在不增加额外训练负担的情况下实现了显著的准确率提升。

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [38] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Törö,Antti Suni,Juraj Šimko*

Main category: cs.CL

TL;DR: 利用机器学习方法分析语言嵌入，结果表明与传统方法相符，并能有效捕捉语言的类型学模式。未来工作将扩展到低资源语言并考虑社会语言变异。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法能够从语音嵌入中直接探索语言变异，开启大规模数据驱动分析的新途径。

Method: 使用线性判别分析（LDA）对106种语言的嵌入进行聚类，并与谱系、词汇和地理距离进行比较。

Result: 嵌入式方法提供了语言变异的可扩展分析潜力，为语言之间的关系提供了新的视角。

Conclusion: 基于语音嵌入的距离与传统测量非常接近，能够有效捕捉全球和局部语言类型模式。

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [39] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: 提出了一种结合调整过的文本转语音模型的LLM狼人杀游戏系统，以增强用户互动体验，无需复杂的提示工程。


<details>
  <summary>Details</summary>
Motivation: 社交推理游戏在商业应用和AI研究中越来越受欢迎，尤其是LLM的推理与说服能力增强后，使人机互动体验更上一个台阶。

Method: 提出基于LLM构建的狼人杀游戏系统，并结合经过调整的文本转语音模型来增强用户参与感。

Result: 与多种LLM模型兼容的文本转语音模型能提高用户的参与度和游戏体验。

Conclusion: 过去的研究需要通过微调或复杂的提示工程来增强人机互动游戏体验，但随着LLMs的推理能力增强，这些额外的组件可能并非必需。

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [40] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Main category: cs.CL

TL;DR: CounselBench评估表明LLMs在心理健康支持中的潜力和安全隐患，强调需要改善LLM在高风险环境中的行为。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在实际辅导场景中的行为表现，并评估是否能够在心理健康支持中超越人类治疗师。

Method: 构建CounselBench-EVAL和CounselBench-Adv两个组件进行大规模基准测试，包括2000次专家评估和120个对抗性问题，检查模特特定的失败模式。

Result: LLMs在单次咨询中的感知质量常常优于在线人类治疗师，但存在安全问题。CounselBench揭示了LLMs评估中的一致性和特定模式失败，并为未来该领域的改进提供了新的方向。

Conclusion: CounselBench为评估和改进大型语言模型在高风险心理健康环境中的行为提供了临床基础框架。尽管LLMs在感知质量上常常超过在线人类治疗师，但也存在未经授权的医疗建议等安全问题。通过对比LLM与人类专家的评估发现，LLM评估员常常高估模型回应并忽视安全问题。

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [41] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 研究发现文本编码器在语义细节识别上存在局限，提出并微调模型以改善表现，尤其在新的CapRetrieval数据集上效果显著。


<details>
  <summary>Details</summary>
Motivation: 研究中观察到文本编码器在识别语义中细粒度的实体或事件时存在局限性，这导致在简单案例中也会出现密集检索失败。

Method: 提出了一种新的中文评估数据集CapRetrieval，并通过数据生成策略对编码器进行微调。

Result: 微调后的编码器在CapRetrieval数据集上表现最佳。还发现了一个粒度困境问题，即编码器在表达细粒度显著性和整体语义对齐时的挑战。

Conclusion: 本研究通过提出数据生成策略并进行微调，解决了编码器在细粒度匹配上的局限性，并在CapRetrieval数据集上取得了最佳性能。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [42] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的课程学习框架，通过难度测量和训练调度来提高语言模型知识蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型知识蒸馏方法无法控制学生模型分布迁移，导致遗忘、模式崩溃和训练推理不一致问题。

Method: 我们提出了一种名为“逐级递增训练过载”(POCL)的课程学习框架，包含难度测量器和训练调度器两个核心组件。

Result: 实验表明，POCL能够在各类知识蒸馏方法和模型类别中一致性地提升蒸馏学生模型的性能。

Conclusion: 我们的研究提供了一种新的课程学习框架，增强了蒸馏小学生模型的稳定性和性能。

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [43] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schütze,Michael Färber*

Main category: cs.CL

TL;DR: 研究性格特征如何影响大语言模型进行仇恨言论分类，发现显著的性格驱动的标注变化，提示需要优化人格提示以确保公平性和价值对齐。


<details>
  <summary>Details</summary>
Motivation: 探讨性格特征对大语言模型中仇恨言论分类标注行为的影响，以改善公正性和与人类价值观的对齐。

Method: 使用MBTI性格提示对四个开源大语言模型进行仇恨言论分类，通过三个仇恨言论数据集评估模型输出，分析性格提示对结果的影响。

Result: 研究发现性格提示导致标注结果明显变化，包括与标准答案不一致、性格间的分歧以及logit层级的偏差。

Conclusion: 本文首次系统研究性格特征在仇恨言论分类中的作用，结果表明性格特征显著影响标注行为，这提示在基于大语言模型的标注工作流中定义人格提示时需谨慎，以确保公平性和与人类价值观的对齐。

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [44] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Main category: cs.CL

TL;DR: RAISE, a retrieval-augmented framework, improves scientific reasoning by better retrieving logically relevant documents through a step-by-step process, outperforming other benchmarks.


<details>
  <summary>Details</summary>
Motivation: Scientific reasoning requires handling long-chain reasoning, domain-specific terminologies, and updates in findings, which poses challenges addressed by RAISE.

Method: RAISE is a step-by-step retrieval-augmented framework consisting of problem decomposition, logical query generation, and logical retrieval.

Result: RAISE retrieves documents that are logically relevant, performing better than other baselines in scientific reasoning tasks.

Conclusion: RAISE consistently outperforms other baselines in scientific reasoning benchmarks by retrieving logically relevant documents.

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [45] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Main category: cs.CL

TL;DR: MEMETRON offers a novel framework for optimizing large language model outputs using a task-agnostic, hybrid metaheuristic approach, improving performance without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing heuristic decoding strategies offer limited control and do not optimize specifically for task objectives in large language models.

Method: MEMETRON uses hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the response space, guided by reward models and contextual operations performed by the LLM itself.

Result: The framework efficiently discovers high-reward responses without the need for model retraining or gradient access and generalizes across diverse tasks.

Conclusion: MEMETRON significantly outperforms standard decoding and reranking methods in aligning large language models with human preferences.

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [46] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: TableDreamer框架通过弱点指导的数据合成提升表格指令调优的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM数据合成方法在生成表格指令调优数据时存在输入空间探索不足和忽视目标LLM表格理解能力弱点的问题。

Method: 提出了一种渐进式的、以弱点为导向的数据合成框架，首先合成多样化的表格及相关指令作为种子数据，然后在新识别到的弱点数据的指导下进行输入空间的迭代探索，最终生成用于微调目标LLM的训练数据。

Result: 实验结果显示，在10个表格基准上，TableDreamer框架将Llama3.1-8B-instruct的平均准确率提升了11.62%，从49.07%提升至60.69%，且超过了使用更多训练数据的现有数据合成基线。

Conclusion: 本文介绍了一种旨在提高表格指令调优的框架，名为TableDreamer，通过引入弱点引导的数据合成策略，有效增强了模型的性能。

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [47] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: 本文提出了一种针对肠道微生物组的生成性关系提取方法，通过大语言模型优化上下文进行关系抽取，尽管取得了一些进展，但BERT方法在性能上仍占优。


<details>
  <summary>Details</summary>
Motivation: 肠道微生物组是一个复杂且资源匮乏的生物医学领域，本研究旨在开发一种针对这种特定领域的生成性关系提取方法，以有效研究其中的相互作用。

Method: 研究采用了一种生成性RE管道，通过大语言模型(LLM)进行摘要，以在提取关系之前优化上下文。并通过指令调优生成进行关系提取。

Result: 初步结果表明，摘要有助于提高生成性RE的性能，通过减少噪音和引导模型方面表现出改善。尽管如此，基于BERT的RE方法在性能上仍然优于生成性模型。

Conclusion: 本文研究了在肠道微生物组中应用生成性关系提取（RE）的潜力，并指出尽管生成性模型在某些方面表现出色，但基于BERT的RE方法在整体性能上仍然优于这些生成性模型。

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [48] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为RuleReasoner的方法，通过精心设计的任务集和动态抽样进行基于规则的推理，在多项任务中优于大型推理模型，并具备更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型具有显著的推理能力并且通过强化学习显著提升了性能，但尚不清楚小型推理模型是否能够在跨任务和跨领域中有效地学习基于规则的推理。

Method: 引入了一种称为RuleReasoner的方法，通过大量精心设计的任务和一种新颖的领域感知动态抽样方法来进行基于规则的推理。具体而言，RuleReasoner通过更新不同领域的抽样权重来重新采样每个训练批次，促进领域增强和灵活的在线学习日程，避免了现有方法中需要的人为混合训练配方。

Result: 在同分布（ID）和分布外（OOD）基准测试中，RuleReasoner超过了最先进的大型推理模型，ID任务平均提高4.1个百分点，OOD任务平均提高10.4个百分点。此外，RuleReasoner在计算效率方面也优于之前的动态抽样方法。

Conclusion: RuleReasoner在ID任务和OOD任务中都优于最先进的大型推理模型，并且在计算效率方面也更高。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [49] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 研究通过提示词优化来减少LLM响应的长度，成功达成最多60%的能源节约，同时维持响应质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理过程中消耗较多能源，因此需要开发节能的推理方法。当前输出压缩研究较少，有必要对其进行探索。

Method: 提出了一些简单而直观的提示工程策略，旨在减少LLM响应的长度并控制信息内容。

Result: 进行实证评估后发现，通过恰当的提示词设计，可以在降低响应长度的同时保持响应质量，实现25-60%的能量优化。

Conclusion: 通过优化提示词，可以有效降低LLM生成的响应长度，从而实现能耗优化。

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [50] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: 引入ClimateViz基准，评估多模态模型在科学图表事实核查上的表现，结果显示当前模型难以达到人类水平的准确性。


<details>
  <summary>Details</summary>
Motivation: 科学事实核查主要关注文本和表格，而科学图表作为展示定量证据和统计推理的关键工具常被忽视。

Method: 引入了ClimateViz，这是一个使用专家策划的科学图表进行科学事实核查的大规模基准。评估了多模态语言模型在零样本和少样本条件下的表现。

Result: 即使是最好的模型如Gemini 2.5和InternVL 2.5，在标签仅设置中也只能达到76.2到77.8%的准确度，远低于人类绩效（89.3和92.7%）。

Conclusion: 当前的多模态语言模型在基于图表的推理上表现出困难，即使是最好的系统，其准确性仍远低于人类表现。

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [51] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO通过识别偏好关键token提升LLM对齐质量，避免过优化，无需额外计算。


<details>
  <summary>Details</summary>
Motivation: 传统DAAs在调整token概率时未考虑其对偏好的相关性，导致对齐质量下降以及过度优化问题。为了解决这些问题，提出了ConfPO。

Method: ConfPO方法通过识别和优化训练策略模型信心中最关键的偏好相关token，实现偏好学习。

Result: 实验结果表明，ConfPO在多种LLMs中均优于传统的统一DAAs，在对齐基准测试中如AlpacaEval 2和Arena-Hard中表现更佳。

Conclusion: ConfPO方法在LLM中实现了比传统DAAs更好的对齐效果，而无需额外的计算开销。

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [52] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: 提出了一种基于NLI的解释性合规检测方法，使用LLM生成担保案例，并证明其在多跳推理任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统需遵循法规，需通过声明-论证-证据框架检验担保案例的有效性，但存在法律和技术文本复杂性、需模型解释、担保案例数据有限的问题。

Method: 提出了一种基于自然语言推理（NLI）的合规检测方法：EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM)。并使用大型语言模型生成担保案例。

Result: 实验表明，从GDPR要求生成的担保案例在多跳推理任务中的有效性。

Conclusion: 基于自然语言推理（NLI）的方法在自动化法规合规过程中具有潜力。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [53] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 本研究提出了一种多语言语音情感识别方法，通过多教师知识蒸馏显著提升了性能，尤其是在悲伤和中性情感的识别上，但对愤怒和快乐识别仍有待提高。


<details>
  <summary>Details</summary>
Motivation: 为了提高人机交互中的语音情感识别，尤其是实现多语言系统的构建，而不仅仅是单语言的系统。

Method: 引入一种新颖的语言感知多教师知识蒸馏方法，使用Wav2Vec2.0作为单语言教师模型的基础，将多个单语言教师模型的知识整合到一个多语言学生模型中。

Result: 学生模型在英语数据集上的加权召回率达到72.9，在芬兰语数据集上的无加权召回率达到63.4，超过了微调和知识蒸馏的基线。该方法在提高对悲伤和中性情感的召回率方面表现优异，但在识别愤怒和快乐情感上仍面临挑战。

Conclusion: 所提出的方法在构建多语言语音情感识别系统上取得了显著效果，特别是在悲伤和中性情感的识别上，但对于愤怒和快乐情感的识别仍需改进。

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [54] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 本文提出了一种改进的批评代理和计算器代理，克服了LLMs在金融文件数值问答中的不足，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在许多自然语言处理任务中表现出色，但在处理包括表格和文本数据的金融文件的数值问答时仍然具有挑战。

Method: 本文提出了一种改进的批评代理，以及计算器代理，并研究了这些代理之间的交互及其对性能的影响。

Result: 改进的批评代理和计算器代理优于之前的最先进方法（思想程序），并且更安全。

Conclusion: 传统的批评代理在没有先验标签的情况下表现较差，改进的批评代理和计算器代理可以显著提高性能。

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [55] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: 研究分析了ArXiv上的AI论文，揭示计算机科学团队在社会化AI研究中的日益重要性，质疑了关于社会性AI驱动因素的常见看法。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中深度嵌入，要求使AI开发与伦理和社会价值观保持一致的呼声越来越高。

Method: 通过分析2014至2024年间在ArXiv上发表的100,000多篇AI相关论文，开发了识别社会内容的分类器，并测量研究论文中这些考虑因素的体现程度。

Result: 发现计算机科学团队越来越多地在研究中解决社会关注，涵盖从公平、安全到医疗和错误信息的广泛领域。跨学科团队仍然更可能产生社会化研究，但计算机科学团队的贡献正在增加。

Conclusion: 计算机科学团队在将社会关注融入AI研究中扮演日益重要的角色，挑战了有关社会性AI的常见假设。

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [56] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: 该论文提出了一种用于核应用的领域特定大型语言模型，并展示了其在捕捉核领域专用词汇方面的潜力，但同时也指出了该模型在领域准确性方面需要进一步的改进。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够处理核领域特定任务的内部大型语言模型，确保数据的安全性和保密性。

Method: 该模型基于紧凑的Transformer架构，在单个GPU上进行训练，旨在保护核操作中固有的敏感数据。

Result: 模型在捕捉核领域专用词汇方面表现出色，但生成的文本有时缺乏句法连贯性，揭示了需要更丰富的数据集、更复杂的预处理和指令微调以提高领域准确性。

Conclusion: 作者展示了一种面向核应用的领域特定大型语言模型的可行性，该模型符合严格的网络安全和数据保密标准。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [57] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Main category: cs.CL

TL;DR: 本研究探讨通过合成数据生成，将核工业中的非结构化文本转化为结构化问答对，帮助开发强大的LLM应用。


<details>
  <summary>Details</summary>
Motivation: 核工业中的非结构化文本数据难以直接用于LLM的高级应用，亟需将其转化为结构化的问答对。

Method: 利用LLM分析文本，提取关键信息，生成相关问题，并评估生成的数据集质量。

Result: 合成数据缓解了数据稀缺和隐私问题，将非结构化文本转化为了可用的问答对。

Conclusion: 合成数据可以解锁核工业中LLM的潜力，改善信息检索、知识共享和决策。

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [58] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: 研究探讨了上下文学习在对话状态跟踪中的应用，采用句子嵌入和k近邻方法进行演示选择，并分析了影响DST性能的因素，取得了关于LLM上下文学习能力的重要发现。


<details>
  <summary>Details</summary>
Motivation: 探讨在上下文学习中影响对话状态跟踪效果的因素，并通过适当选择演示提高其有效性。

Method: 采用基于句子嵌入的k近邻方法，选择合适的演示作为上下文学习的演示，同时使用模板结构化输入到大型语言模型（LLM）。

Result: 通过使用MultiWoZ2.4数据集，重点研究了OLMo-7B-instruct、Mistral-7B-Instruct-v0.3和Llama3.2-3B-Instruct模型，发现了多项有关LLM上下文学习能力及其对DST影响的有价值信息。

Conclusion: 研究结果揭示了通过上下文学习实现有效对话状态跟踪的若干关键因素。

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [59] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: 提出了一种利用函数调用的LLMs来替代直接NL-to-SQL生成方法，以应对核电站数据查询的准确性和安全性挑战。


<details>
  <summary>Details</summary>
Motivation: 在核电站中获取操作数据非常关键，因为这些数据支持的重要决策需要保持高度的准确性和透明度。传统上，采用自然语言到SQL(NL-to-SQL)的方法来查询这些数据，但存在显著风险，因为最终用户难以验证生成的SQL查询，而核电站的遗留数据库通常复杂且结构不良，这使得查询生成更加复杂。

Method: 提出了一种利用大型语言模型(LLMs)进行函数调用的混合方法，以解决上述挑战。定义了一组预先批准的、用途特定的函数来代表常见用例，查询则通过调用这些函数来处理，这些函数封装了经过验证的SQL逻辑。

Result: 通过性能比较，函数调用方法在查询生成的准确性和可维护性上表现出了改善。

Conclusion: 展示了一种新颖的、可操作的框架，以在关键系统中实现稳健的数据检索，同时兼顾用户易用性和操作安全性。

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [60] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Main category: cs.CL

TL;DR: 本文研究了多个大型语言模型在阿拉伯语上的表现，尤其是DeepSeek模型，通过不同的测试策略提升了推理和分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索和提高大语言模型在阿拉伯语数据上的性能，这些数据具有丰富的形态特征、多样的方言和复杂的书写系统。

Method: 我们采用零样本、少样本和微调策略对多种语言模型进行评估，每个任务使用不同的数据集测试语言推理能力。

Result: 研究发现，通过选择合适的上下文示例，分类任务的F1得分平均提升超过13分。此外，DeepSeek架构在零样本设置下超越GPT o4-mini基线12分，而基于LoRA的微调进一步提升至多8分的F1和BLEU分数。

Conclusion: DeepSeek模型在阿拉伯语自然语言处理任务中表现出色，特别是在复杂推理任务中优于GPT o4-mini基线。选择合适的上下文示例和LoRA微调策略能有效提升性能。

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [61] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: New method improves entity extraction accuracy from legal documents. LLaMA models with finetuning and GPT-4 Turbo outperform classic methods.


<details>
  <summary>Details</summary>
Motivation: Quantifying insurance costs from legal documents by accurately extracting relevant entities like disability percentages and compensation amounts is challenging due to the complexity of legal language.

Method: Two-step procedure: document segmentation and entity extraction using LLaMA models and GPT-4 Turbo. Finetuning with LoRA for LLaMA models.

Result: Segment vectorization with LLMs surpasses the classic method with 39.5% accuracy. LLaMA-2 70B finetuned achieves 79.4%, and GPT-4 Turbo reaches 86.1% accuracy.

Conclusion: LLaMA-3 8B base model performs comparably to finetuned LLaMA-2 70B, and GPT-4 Turbo achieves the highest accuracy. Hallucinations are reduced after finetuning.

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [62] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: 本文引入了SRB-300数据集，提升了瑞士德语的语音识别性能，尤其是对自发会话语音。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别模型虽然在控制环境中有效，但在处理自发会话语音时表现不佳，因此需要新的数据集来解决这一问题。

Method: 通过对多个OpenAI Whisper模型在新的SRB-300数据集上进行微调，提升了语音识别性能。

Result: 最佳微调模型large-v3在SRB-300数据集上取得了17.1%的词错误率和74.8的BLEU得分，显著优于之前的零样本性能指标。

Conclusion: 新引入的SRB-300数据集显著提升了对瑞士德语自发会话语音的识别效果。

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [63] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 提出GRACE框架和ALKALI工具来对抗对抗性攻击，有效减少高攻击成功率，并推出潜在对齐失败的评估指标AVQI。


<details>
  <summary>Details</summary>
Motivation: 当前的对抗性威胁增速超过防御措施的适应能力，需解决对抗性提示在潜在几何上的伪装问题。

Method: 提出GRACE框架，通过偏好学习和潜在空间正则化以实现安全与对抗性完成之间的潜在分离，并引入AVQI度量潜在对齐失败。

Result: GRACE框架能显著减少高达39%的攻击成功率（ASR），而AVQI可以揭示潜在的对齐失败。论文还提供了ALKALI工具用于对抗性基准测试。

Conclusion: 引入了ALKALI和GRACE两个工具，用于提升大语言模型在面对对抗性攻击时的安全性。

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [64] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: 植物科学领域缺乏适应工具，PlantBert能从植物应激响应文献中抽取结构化知识，成功进行低资源科学领域的强大领域适应，公开发布以促进跨学科创新。


<details>
  <summary>Details</summary>
Motivation: 尽管变压器模型在生物医学和临床自然语言处理中取得了突破，但植物科学领域明显缺乏这样的适应工具。我们希望通过开发语义高保真的语言模型来弥合农业NLP中的关键差距。

Method: 我们的方法结合了基于Transformer的建模、规则增强的语言后处理和本体论为基础的实体规范化，使PlantBert能够以精确和语义忠实的方式捕捉生物学上有意义的关系。底层语料库采用与作物本体协调的分层架构进行注释，涵盖了植物适应性分子的、生理的、生化的和农艺学的多个层面。

Result: PlantBert表现出良好的实体类型泛化能力，并成功展示了在低资源的科学领域中实现强大的领域适应性。该模型被公开发布，以促进透明性并加速计算植物科学中的跨学科创新。

Conclusion: PlantBert成功证明了在低资源领域实现强大的领域适应性，并为植物基因组学、表型学和农艺知识发现中的智能数据驱动系统铺路。

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [65] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: 研究采用大型语言模型，通过分段、提取和评价流程，将法律文本转化为形式表示，结果显示与专家手工结果较高的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究使用大型语言模型自动分析法律文本的语义，并将其转化为可抗辩的品行逻辑的正式表示，以实现更有效的法律信息处理。

Method: 提出了一种结构化管道，将复杂的规范语言分割成原子片段，提取义务规则，并评估其语法和语义的一致性。研究评估了各种LLM配置，包括提示工程策略、微调模型和多阶段管道。

Result: 实证结果表明，机器生成的形式化与专家手工制作的形式化之间有良好的一致性，尤其是在有效提示的情况下，LLMs能够显著促进法律信息学的可扩展性。

Conclusion: 该研究展示了大型语言模型(LLMs)在法律文本自动语义分析中的有效性，特别是在转化为可抗辩的品行逻辑形式表示方面。结果表明，当LLMs有效提示时，它们在法律信息学中的可扩展性贡献显著。

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [66] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 新方法结合语言模型和语言学规则，提高了希腊方言文本的标准化效果，并发现更多语义信息。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（及其方言）的自然语言理解困难，将方言文本规范化便于标准语言工具处理。

Method: 结合基于规则的语言学转换和大型语言模型（LLMs）进行标准化，并使用目标式少量示例进行提示。

Result: 利用新的标准化方法进行下游实验，发现先前结果依赖于肤浅的语言信息，如正字法差异，新方法能揭示更多语义。

Conclusion: 本文提出了一种无须平行数据的希腊方言标准化方法，成功改善了方言理解的质量，并能挖掘出更多的语义信息。

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [67] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND方法通过修改语言模型的损失梯度，改善了知识在多跳推理上的传播能力，虽然在未见过的实体关系上表现优势缩小，但仍超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型尽管能够逐字地再现注入的知识，但在需通过推理传播这些知识以回答问题时表现不佳。

Method: 使用基于超网络的PropMEND方法，通过元学习修改语言建模损失的梯度，鼓励注入信息的传播，从而增强知识传播能力。

Result: PropMEND在RippleEdit数据集上对涉及多跳问题的问题表现出几乎2倍的准确率。在新的Controlled RippleEdit数据集上，尽管在未见过的实体关系对上性能优势减小，但仍优于现有方法。

Conclusion: PropMEND outperforms existing methods in knowledge propagation tasks, including those involving unseen entity-relation pairs, but suggests room for improvement in broader relational propagation.

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [68] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: 在RTX 3080 Ti上使用强化学习和内存优化技术成功训练了1.5B参数的数学推理模型，表现与更大模型相当，降低了高性能AI研究的准入门槛。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的训练通常需要昂贵的计算资源，该研究旨在证明在资源受限环境中可以使用普通游戏GPU训练优秀的数学推理模型。

Method: 采用强化学习和内存优化技术，以在RTX 3080 Ti (16GB)上训练1.5B参数的数学推理模型。

Result: 在RTX 3080 Ti上训练的1.5B参数模型在数学推理基准测试中表现出与更大模型相当或更好的性能。

Conclusion: 通过整合强化学习和内存优化技术，即便在资源受限的环境中，只需使用单个普通游戏GPU，便可训练出与大型模型性能相当的数学推理模型，从而挑战了最佳数学推理需要庞大基础设施的传统观点。

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [69] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种名为FaithfulRAG的新框架，解决了大规模语言模型在知识冲突情况下的不忠问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的忠实RAG方法通过强制抑制模型的参数知识来实现忠实性，从而削弱模型的内部知识结构并增加误解上下文的风险。因此，需要一种新方法来解决知识冲突。

Method: FaithfulRAG框架通过识别冲突的知识并设计自我思考过程，使语言模型能够在生成响应前对冲突的事实进行推理和整合。

Result: FaithfulRAG在大量实验中表现出超越现有最先进方法的效果。

Conclusion: 本文提出的FaithfulRAG框架能够有效解决大规模语言模型在处理检索系统中出现的知识冲突问题，并且在实验证明中表现优于现有方法。

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [70] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究揭示大语言模型在处理政治领域误导性信息时存在挑战，难以有效抵制和纠正错误信念。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在缺乏知识的情况下如何进行交流基础，尤其是在政治领域，错误信息和交流失败的风险较高时。

Method: 研究大语言模型处理直接知识问题和包含预设误报的问题，并评估这些问题是否促使模型进行主动的交流基础建立和纠正错误用户信念。

Result: 大语言模型在进行交流基础建立以及拒绝错误用户信念方面存在显著挑战。

Conclusion: 大语言模型在政治话语中减轻错误信息传播的角色存在问题。

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [71] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 提出新探测技术解决语言模型算术错误，调整数字嵌入结构以降低出错率。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，分布式学习的嵌入在表示精确数量方面是不可靠的。但我们观察到，以前的探测方法不足以揭示具有正弦模式的学习数字嵌入的结构。

Method: 提出了一种新的探测技术，通过解码输入嵌入中的数字值，在多个开源语言模型中实现了接近完美的准确性。

Result: 新探测方法证明了在仅仅预训练后，语言模型能以极高的精度表示数字。该方法解释了语言模型基础算术中的大部分错误，并通过调整嵌入使其符合探测发现的模式可以减少这些错误。

Conclusion: 在预训练语言模型中，数的表示精确性可以解释其在基础算术中的错误。通过对齐嵌入与探测发现的模式可以减少这些错误。

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [72] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 研究针对移动设备的自主代理在处理复合任务时的局限，提出UI-NEXUS基准和AGENT-NEXUS调度系统以改善性能。


<details>
  <summary>Details</summary>
Motivation: 以往的自主代理机构主要关注原子任务的执行，而忽略了复合任务的泛化能力，这对于现实世界应用至关重要。

Method: 引入UI-NEXUS基准，通过对移动代理进行评估，特别是那些具有代理工作流程或作为模型的代理。设计了三类复合操作，并提供交互式评估平台。提出AGENT-NEXUS调度系统，通过动态分解长时间任务提升代理能力。

Result: AGENT-NEXUS提升了现有移动代理在UI-NEXUS基准上的任务成功率，改善幅度为24%至40%，且没有显著增加推理负担。

Conclusion: UI-NEXUS证明了当前代理在处理复合任务中面临的挑战，AGENT-NEXUS有效提升了代理的复合任务处理能力。

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [73] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister*

Main category: cs.CL

TL;DR: FROST-EMA语料库允许研究母语、第二语言与假外国语音的变异，并进行相关的初步案例分析。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从语音和技术角度研究语言变异的新语料库。

Method: 创建一个包含18名双语者的语料库，他们在母语、第二语言和模仿的外国语音中进行讲话。然后使用这些数据进行两个初步案例研究。

Result: 进行了两项初步案例研究，分别考察了仿真语音对自动说话人验证系统的影响以及一个说话者在不同语言环境下的发音模式。

Conclusion: 引入的新语料库可以深入研究语音在不同语言和技术方面的变化，为未来的语言研究提供了重要的工具。

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [74] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Main category: cs.CL

TL;DR: 研究提出了一种新的自然语言相关fMRI任务，通过机器学习成功检测老年人的早期认知能力下降。


<details>
  <summary>Details</summary>
Motivation: 在老年人口中，神经认知障碍（NCD）是一个常见且显著的健康问题，早期检测对于及时干预以预防和减缓其进展至关重要。

Method: 研究中使用一种新的自然语言相关的功能性磁共振成像（fMRI）任务，并结合机器学习分类模型，利用从fMRI任务中提取出的特征和人口学信息来分类认知状态。

Result: 研究在97名来自香港的非痴呆中国老年人中测试了任务的有效性，基于fMRI特征和人口统计数据的机器学习分类模型在认知状态分类中取得了平均曲线下面积为0.86的效果，特征定位显示，最常被数据驱动方法选择的fMRI特征主要来自与语言处理相关的大脑区域。

Conclusion: 该研究表明，基于自然语言的fMRI任务能够有效检测与年龄相关的认知能力下降和神经认知障碍（NCD），具有早期发现的潜力。

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [75] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: 研究通过新数据集和变压器模型提升了儿童发声分类的准确性和鲁棒性，超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 儿童语音技术在下游任务中面临挑战，主要因为训练语料库规模小以及儿童语音的复杂性。

Method: 使用名为SpeechMaturity的新型数据集进行训练，并采用先进的变压器模型执行分类任务。

Result: 模型能够区分儿童的哭声、笑声、成熟的语音以及不成熟的语音，同时在参数上超越了之前的数据集训练的模型，表现出与人类相媲美的分类准确性，并且在城乡环境中均表现出高鲁棒性。

Conclusion: 我们使用SpeechMaturity数据集与先进的变压器模型相结合，可以更好地处理儿童发声分类任务。在多样化的语言和地域背景下，我们的模型显示出比以前的数据集训练的模型更强的性能和鲁棒性。

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [76] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: SWE-Flow框架通过自动分析单元测试推导开发步骤，生成可验证的TDD任务，并提升了TDD编码的性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前软件工程数据依赖于人工提交问题的不足，并通过TDD中的单元测试自动推断增量开发步骤。

Method: 使用Runtime Dependency Graph (RDG)构建函数交互，以生成有结构的开发进度表，并在每一步生成部分代码库、相应的单元测试和必要的代码修改。

Result: 该方法生成了16,061个训练实例和2,020个测试实例，并且在实验证明其能够显著提升在TDD编码环境下的性能。

Conclusion: SWE-Flow框架通过生成自包含的TDD任务，从而提升了TDD环境下的编码性能。

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [77] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Main category: cs.CL

TL;DR: 研究通过XPOS-UPOS对齐提高L2韩语标签的一致性及解析准确性。


<details>
  <summary>Details</summary>
Motivation: 近年来在L2韩语领域需要改进的标注一致性和解析准确性问题。

Method: 引入一个半自动化框架，通过XPOS序列识别形态句法构造并与对应的UPOS类别对齐，扩展现有的L2韩语语料库并微调模型。

Result: 对齐后的数据集改进了标注层的一致性，并提高了形态句法标记和依赖解析的准确性。

Conclusion: XPOS-UPOS对齐在有限标注数据情况下带来了明显性能提升。

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [78] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Main category: cs.CL

TL;DR: 引入样本集聚合器（SSA），通过优化多个样本集并使用强化学习来提高答案准确性，表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型在测试时的计算能力以获得更高的性能。

Method: 提出了一种训练紧凑型语言模型的方法，称为样本集聚合器（SSA），通过使用强化学习使其能够处理多个样本集并输出最终答案。

Result: 实验表明，SSA在多个推理数据集上性能优于其他测试时缩放方法。

Conclusion: 通过将生成答案的语言模型与分析和聚合样本答案的语言模型分离，该方法能够高效地处理来自顶级黑盒模型的输出。

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [79] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Main category: cs.CL

TL;DR: 研究发现，人类和大语言模型（LLM）的校对均提高了第二语言文本的词汇和句法质量，尤其是三种LLM（ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b）在关键特征上的结果高度一致，表明LLM校对更具生成性。


<details>
  <summary>Details</summary>
Motivation: 改善第二语言写作的整体可理解性，并比较人类校对与多个LLM校对的效果。

Method: 检查人类和大语言模型（LLM）校对对相同第二语言写作的词汇和句法干预效果，并评估三种LLM的结果一致性。

Result: 人类和LLM校对均增强了双词组词汇特征，从而可能改善相邻词语之间的连贯性和上下文关联性。LLM校对表现出更具生成性的方法，大幅改造词汇和句子结构，并在名词短语中增加更多形容词修饰语。

Conclusion: LLM校对在词汇和句法结构上进行更具生成性的修改，而三种LLM在主要词汇和句法特征上的结果高度一致。

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [80] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: Router-R1是一个基于强化学习的多语言模型路由器，能优于现有方法处理复杂任务，同时优化性能和成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM路由器通常仅支持单轮一对一映射，限制了解决需要多个LLM互补优势的复杂任务的能力。

Method: 提出了一种基于强化学习的框架Router-R1，通过将多语言模型路由和聚合视为一个顺序决策过程来实现优化。

Result: 在七个一般和多跳问答基准测试中，Router-R1相较于多种强基线表现卓越，展示出优异的性能、强大的泛化能力和良好的成本管理。

Conclusion: Router-R1在多任务处理上表现优异，并且能在成本管理方面优于其他方法。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [81] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 该研究分析视-语言模型的视觉与文本表现差距，通过在视觉数据中实施后层补丁回前层的措施，显著缩小了二者的表现差距。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在视觉输入上的表现令人印象深刻，但在类比的文本任务上却表现得更好。因此，有必要研究这种表现差距的原因。

Method: 研究人员通过识别和比较不同模态下的任务特定计算子图（circuits）来分析视觉语言模型在不同模态上的表现差异。

Result: 通过将视觉数据的后层表示补丁回到前层，实验结果表明，这一简单的干预措施可以平均缩小三分之一的模态间表现差距。

Conclusion: 通过分析视觉语言模型在视觉和文本上的不同表现，研究人员发现通过将后层的视觉数据表示补丁回前层，可以缩小视觉和文本任务之间的表现差距。

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: 研究引入TIP-Search框架，通过动态选择深度学习模型实现时间可预测的推理调度，在金融市场预测中显著提高准确性和满足截止期限的要求。


<details>
  <summary>Details</summary>
Motivation: 严格的延迟要求和高频金融系统的不确定工作负载促使开发了TIP-Search框架。

Method: TIP-Search是一个动态选择深度学习模型的时间可预测推理调度框架。该方法离线分析延迟和泛化性能，然后在线执行任务感知选择，不依赖显式输入域标签。

Result: 在三个真实世界的限价订单簿数据集上（FI-2010, Binance BTC/USDT, LOBSTER AAPL）测试表明，与静态基线相比，准确性提高了最多8.5%，且100%满足截止期限。

Conclusion: TIP-Search在不确定性条件下的低延迟金融推理中表现出色。

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [83] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma,Hojin Lee,Mohith Suresh,Priyam Shankar Sharma,Rahul Vishwakarma,Sparsh Gupta,Yuvraj Anupam Chauhan*

Main category: cs.AI

TL;DR: 引入Cognitive Weave记忆框架，提升大模型在任务完成率和速度上的表现，并关注长期记忆和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 当前的记忆系统在结构灵活性、时间感知以及从原始交互数据中综合出高级见解方面存在局限性。研究旨在超越传统数据存储的记忆架构，实现连续学习、细致推理和动态适应能力。

Method: 提出了一种名为Cognitive Weave的新型记忆框架，依托多层次的时空共振图（STRG）来管理信息，并通过语义使能器接口（SOI）动态丰富信息颗粒（IP），这些IP通过关系链条连接形成不断演化的知识结构。框架还包括认知精炼过程，用以合成更高层次的知识结构。

Result: Cognitive Weave在长期规划任务、动态问答场景以及多回合对话连贯性方面表现出色，与最先进技术相比，任务完成率提高34%，查询延迟减少42%。

Conclusion: Cognitive Weave significantly enhances memory systems used in large language models by improving task completion rates and reducing query latency, while also addressing ethical considerations and future research opportunities.

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [84] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi,Arghya Datta,Nikhil Vichare,Indranil Bhattacharya,Huzefa Raja,Jing Xu,Shayan Ray,Giuseppe Carenini,Abhi Srivastava,Aaron Chan,Man Ho Woo,Amar Kandola,Brandon Theresa,Francesco Carbone*

Main category: cs.AI

TL;DR: 该研究提供了SOP-Bench，用于测试大语言模型在复杂SOP执行中的表现，发现目前能力不足，并呼吁社区扩展该基准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）虽然在一般推理和问题解决上表现良好，但在执行复杂的、要求严格遵循标准操作程序（SOPs）的长周期工作流程中存在困难。

Method: 该论文通过引入合成数据生成框架以创建真实的、工业级SOPs，以测试LLM基础代理的规划、推理和工具使用能力。

Result: SOP-Bench是一个包含1800多个任务的基准测试，覆盖10个工业领域。评估显示Function-Calling和ReAct代理的平均成功率分别为27%和48%。

Conclusion: 当前LLMs的代理能力与自动化真实世界SOPs的需求之间存在显著差距。

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [85] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei,Samuel Holt,Jing Yang,Markus Wulfmeier,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 机器学习领域中的同行评审面临规模危机，需将AI辅助评审作为优先研究任务，利用大型语言模型（LLMs）作为协作者，以提高评审质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习领域的顶级会议投稿数量呈指数增长，超出了合格评审者的有限能力，导致评审质量、 consistency问题以及评审者疲劳，因而需要发展AI辅助同行评审。

Method: 本文提出了一种全面的AI增强生态系统，利用大型语言模型（LLMs）作为作者、评审者和领域主席（ACs）的复杂协作者。具体而言，我们建议AI在事实验证、指导评审者表现、协助作者提高质量、以及支持ACs决策中发挥作用。此外，研究议程包括开发和验证这些AI助手的实验。

Result: 研究结果尚未给出，但本文勾勒了一个研究议程及一些实验示例，以开发和验证AI助手，并探讨了主要的技术和伦理挑战。

Conclusion: 机器学习领域的科学进步依赖于同行评审，但面临规模危机。本文主张将AI辅助的同行评审作为一个优先的研究和基础设施任务，以维护科学验证的完整性和可扩展性，同时保持高质量的同行评审标准。

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [86] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Main category: cs.AI

TL;DR: 开发了一种计算方法，将定量时间约束应用于ASP，并通过处理差异约束提高扩展性。


<details>
  <summary>Details</summary>
Motivation: 帮助ASP表达定量的时间约束，如持续时间和截止日期，并解决ASP在处理细粒度时间约束时的扩展问题。

Method: 利用带差异约束的ASP扩展，通过外部处理时间相关方面来解决问题。

Result: 成功实现时间精度不影响解决方案的分离。

Conclusion: 我们的计算方法使得ASP的度量与时间粒度解耦，提高了规模化能力。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [87] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong,Rithwik Sudharsan,Yibo Yang,Peter Xiangyuan Ma,Ruihan Yang,Stephan Mandt,Joshua S. Bloom*

Main category: cs.AI

TL;DR: AstroCompress展示了神经压缩在增强天文数据收集上的潜力，对现有无损压缩方法进行比较，指出神经方法的优势。


<details>
  <summary>Details</summary>
Motivation: 在空间和地面的天文台，由于其独特的场地条件（寒冷、黑暗）导致数据传输能力受到限制，因此改进无损数据压缩技术变得非常重要。神经数据压缩有望通过学习压缩算法并利用天文图像的独特结构来超越传统技术。

Method: 本文介绍了一种神经压缩挑战工具AstroCompress，针对四个新数据集和一个遗留数据集进行测试，数据集包括太空、地面、多波长和时间序列成像，实验比较了7种无损压缩方法，其中包括三种神经方法和四种非神经方法。

Result: 实验结果表明，无损神经压缩技术能够增强天文台的数据收集能力，并为在科学应用中采用神经压缩技术提供了指导。

Conclusion: 在天文观测中，使用无损神经数据压缩技术可以增强数据收集的效果，并为科学应用中神经压缩技术的采用提供指导。

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [88] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.AI

TL;DR: LeanTutor是一个基于大语言模型的数学证明辅导系统，通过三个模块提供形式化验证和教学指导，性能在生成错误证明提示方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 构建一种基于大语言模型的数学证明辅导系统，以自然语言与学生互动，正式验证学生编写的数学证明，并提供正确的下一步和适当的教学指导。

Method: LeanTutor由三个模块组成: 自动形式化/证明检查器、下一步生成器和自然语言反馈生成器。第一个模块将学生的证明形式化为Lean并通过代码编译验证证明的准确性。第二个模块使用大语言模型和证明搜索生成下一步的Lean策略。第三个模块利用Lean数据生成针对学生用户的教学动机的自然语言提示。

Result: 自动形式化模块能够正确形式化57%的正确证明策略，并在30%的错误证明中准确识别出错误步骤。

Conclusion: LeanTutor系统在生成数学证明的自然语言提示方面优于基线模型，其准确性和相关性均有所提升。

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [89] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose,Andrew B. Kahng,Sayak Kundu,Zhiang Wang*

Main category: cs.AI

TL;DR: ORFS-agent是一种基于LLM的优化代理，提高设计效率和性能，减少优化次数，提供灵活的多目标优化框架。


<details>
  <summary>Details</summary>
Motivation: 由于小参数变化对集成电路设计具有重大影响，因此需要有效的参数优化方法。

Method: 引入ORFS-agent，这是一种基于大型语言模型的迭代优化代理，用于自动调整开源硬件设计流程中的参数。

Result: ORFS-agent提高了布线长度和有效时钟周期超过13%，并且优化迭代次数减少了40%。

Conclusion: ORFS-agent能够改善布线长度和有效时钟周期，且减少了优化迭代次数。

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [90] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.AI

TL;DR: 该研究提出FloorplanMAE框架，通过自监督学习从不完整的平面图生成完整方案，实验结果表明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在建筑设计过程中，能够从不完整的平面图预测完整方案对设计效率提升和减少修改工作量具有重要意义。

Method: 提出FloorplanMAE框架，通过自监督学习来修复不完整的平面图，并使用基于Masked Autoencoders (MAE)的方法来掩盖部分平面图并训练轻量级视觉转换器（ViT）。

Result: 实验结果表明FloorplanMAE模型可以从不完整的平面图生成高质量的完整方案，并在与现有的方法对比中表现出色。

Conclusion: FloorplanMAE能够有效生成高质量完成的平面图，展示出广泛的应用前景。

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [91] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng,An Zhang,Zijian Wu,Weixiang Zhao,Changshuo Shen,Yi Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 研究了大型推理模型如何自动计划推理力度，发现方向向量为控制推理强度的关键。


<details>
  <summary>Details</summary>
Motivation: 虽然自动推理强度分配现象广泛存在，但其底层机制仍未被深入探讨。因此本文旨在从模型激活角度解释这一现象。

Method: 通过模型激活的分析，使用线性探测器预测推理Token数量，并揭示方向向量在推理激活中的作用。

Result: 通过实验验证LRM在生成之前通过激活预先计划推理力度，且这种推理力度通过方向向量的大小来控制，从而影响推理过程。

Conclusion: 本文对LRM中的自动推理力度分配现象进行了研究，揭示了模型在激活过程中如何预先计划推理强度。通过预先分配方向向量的大小，可以控制推理力度，影响推理过程的长度与性能。

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [92] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Main category: cs.AI

TL;DR: 提出了SafeCoT框架，通过最小监督改善VLMs的拒绝行为，有效减少过度拒绝并增强泛化。


<details>
  <summary>Details</summary>
Motivation: 确保视觉语言模型在高风险或模糊场景中做出安全且适当的响应仍然是一项关键挑战。

Method: 引入SafeCoT框架，利用基于规则的链式思考监督，以最少的监督帮助模型进行安全风险推理，并做出情境感知的拒绝。

Result: 实验表明，SafeCoT在多个基准测试中显著减少了过度拒绝行为，并在有限的训练数据下增强了泛化能力。

Conclusion: 该研究提出了一种名为SafeCoT的轻量级可解释框架，通过基于规则的链式思考（CoT）监督来改善视觉语言模型（VLMs）在高风险或模糊情境下的拒绝行为。

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [93] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li,Di Jin,Xiaobao Wang,Dongxiao He,Bingdao Feng,Zhen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种通过单节点触发生成器实现的图后门攻击方法，有效提高目标项目曝光率且对系统性能影响较小。


<details>
  <summary>Details</summary>
Motivation: 现有的塌缩攻击方法在操纵推荐结果以提高目标项目曝光率时，面临低隐蔽性和高破坏性的问题。该研究旨在解决这一问题，提出能够在隐蔽的情况下增强目标项目曝光率的方法。

Method: 本研究设计了一种单节点触发生成器，通过插入一个虚假用户节点来有效地将多个目标项目暴露给目标用户。同时，引入目标节点和无关节点之间的约束条件，以减小虚假节点对推荐系统性能的影响。

Result: 实验结果显示，99%的目标用户中，目标项目的曝光率达到了不低于50%，而对于推荐系统性能的影响则控制在约5%。

Conclusion: 研究表明，通过单节点触发生成器的新型图后门攻击方法，可以在不显著影响系统性能的情况下，显著增加目标项目的暴露率。

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [94] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku,David Theil,Evelyn Eichelsdoerfer Uehara,Sreyoshi Bhaduri,Junnosuke Kuroda,Toshi Yumoto,Alex Gil,Natalie Perez,Rajesh Cherukuri,Naumaan Nayyar*

Main category: cs.AI

TL;DR: 提出了使用大型语言模型结合专家校准的分类法对齐方法，大大提高了自动对齐效果，F1评分达到0.97。


<details>
  <summary>Details</summary>
Motivation: 传统的手工分类对齐方法需要专家审查概念对，但在大规模应用中变得过于昂贵和耗时，并且主观解释常常导致专家间的分歧。现有的自动化方法虽然有前景，但在处理复杂语义关系和跨领域保持一致性方面存在局限性，且通常缺乏透明的推理过程。

Method: 提出了一种新的框架，结合大型语言模型（LLMs）、专家校准和迭代提示优化，以实现自动化分类法对齐。该方法整合了专家标记的例子、多阶段提示工程和人工验证，引导LLMs生成分类链接和支持理由。

Result: 在概念重要性的领域特定映射任务中，所提方法的F1评分达到了0.97，明显超过了人类基准的0.68。

Conclusion: 结果表明，该方法在扩大分类对齐范围的同时维护高质量的映射，并为模棱两可的情况保留了专家监督。

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [95] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh,Zhiguang Cao,Yining Ma,Jianan Zhou,Mohammad Haroon Dupty,Wee Sun Lee*

Main category: cs.AI

TL;DR: 研究提出了SHIELD模型，通过稀疏性和层次性改进VRP问题的处理，在复杂多任务多分布设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型在处理复杂的真实客户分布时存在不足，因此需要一种能够在更复杂和现实的多任务多分布VRP设置下有效运作的模型。

Method: 采用了更深的解码器架构，并结合了深度混合（MoD）技术以促使稀疏性，以及上下文群集层来提高局部表示。

Result: 实验证明了SHIELD模型在9张真实地图上16种VRP变体方面的表现优于现有方法。

Conclusion: 我们提出了SHIELD模型，该模型通过稀疏性和层次性原则在多任务和多分布VRP问题上实现了优越的性能。

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [96] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: 大型语言模型在数学推理方面取得进展，但在能力和泛化上仍需突破，一些研究方向例如知识增强和形式化推理框架可能提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究大规模语言模型在数学推理中的能力，以增强其推理能力并探索将这些技术应用到其他领域的可能性。

Method: 研究从训练免优化的提示方法到微调方法如监督微调和强化学习，以及扩展CoT和测试时缩放的最新工作。

Result: 尽管大型语言模型在数学推理中取得了显著进展，但仍存在基本挑战。

Conclusion: 尽管在数学推理方面取得了显著进展，大型语言模型在能力、效率和泛化方面仍面临基本挑战。

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [97] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: 文章介绍了一种新型框架CIPHER，用于提高工业控制系统的泛化能力和决策透明度。


<details>
  <summary>Details</summary>
Motivation: 当前的工业过程需要应对不可预测的环境和任务，同时减少操作错误带来的高昂成本。传统的人工智能控制系统通常依赖于监督学习，需要大量标注数据，而这限制了其在数据稀缺的工业环境中的泛化能力。因此，我们需要一种能够结合人类推理能力并进行知识整合的系统。

Method: 提出了一种名为CIPHER的视-语言-动作（VLA）模型框架，它试图复制人类在工业控制中的推理过程。该框架以商用级3D打印机为实例，结合了过程专家和回归模型，实现对系统状态的定量表征。此外，该框架使用检索增强生成技术，以访问外部专家知识，并支持基于物理信息的思维链推理。

Result: 该框架显示出强大的泛化能力，可以处理分布外任务。CIPHER能够解释过程监控中的视觉或文本输入，解释其决策，并自主生成精确的机器指令，而无需显式注释。

Conclusion: CIPHER为开发能够精确行动、具有情境推理能力并能透明沟通决策的自主系统奠定了基础，将支持工业环境中的安全和可信部署。

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [98] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi,M Anwar Hossain*

Main category: cs.AI

TL;DR: RHealthTwin is a framework for responsible AI digital twins in healthcare, using structured inputs to address LLM issues. It achieves top metrics and ethical compliance in health domains.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) offer new opportunities for digital twins in healthcare, but their deployment in health contexts raises issues like hallucination and ethical misuse.

Method: The Responsible Health Twin (RHealthTwin) framework processes multimodal inputs to guide a health-focused LLM, improving response safety and relevance. It includes a Responsible Prompt Engine (RPE) to dynamically structure inputs and adapts over time through feedback.

Result: RHealthTwin evaluated in mental support, symptom triage, nutrition planning, and activity coaching, achieved state-of-the-art metrics (BLEU = 0.41, ROUGE-L = 0.63, BERTScore = 0.89) and over 90% in ethical compliance using LLM-as-judge evaluation.

Conclusion: RHealthTwin serves as a foundational framework for responsible AI-powered digital twins in health, addressing hallucination and ethical concerns effectively.

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [99] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL通过针对长尾类和冲突目标的优化改进，实现了在域泛化任务中的最先进表现。


<details>
  <summary>Details</summary>
Motivation: 目前的平滑损失景观的方法在长尾类分布和冲突优化目标下常常失败。因此，需要一种新方法来有效处理这些问题。

Method: 引入FedTAIL，一个通过尖锐度引导、梯度对齐优化来解决长尾类分布和冲突优化目标挑战的联邦域泛化框架。该方法结合了梯度一致性正则器，以减轻分类和对抗目标之间的冲突，并采用类内尖锐度最小化和曲率感知的动态加权方案来对抗类不平衡。此外，还通过将尖锐感知扰动融入熵正则化来增强条件分布对齐。

Result: FedTAIL在标准域泛化基准测试中的表现验证了其在集中式和联邦环境中的有效性。

Conclusion: FedTAIL在标准领域泛化基准测试中实现了最先进的性能，特别是在存在领域变化和标签不平衡的情况下，证明了其在集中式和联邦环境中的有效性。

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [100] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong,Xiaolin Chang*

Main category: cs.AI

TL;DR: 论文提出了一种结合DRL与LLM的新型无人机轨迹规划框架，显著提升多项指标的性能。


<details>
  <summary>Details</summary>
Motivation: 低空经济的快速增长推动了无人机的广泛采用，但现有研究往往忽略了城市空域限制和经济效率等关键因素。

Method: 提出一种结合深度强化学习（DRL）与大型语言模型（LLM）推理的新型无人机轨迹规划框架。

Result: 实验结果表明，该方法在多个指标上明显优于现有基准，包括数据收集率、避撞、成功着陆、法规合规性和能源效率。

Conclusion: 结果验证了该方法在解决低空经济网络约束下无人机轨迹规划关键挑战方面的有效性。

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [101] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv,Jinlong Lei,Peng Yi*

Main category: cs.AI

TL;DR: 本文提出HGformer框架，通过图Transformer和分层决策模型，提高了两阶段Col. Blotto游戏中的策略生成效率和对抗收益。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以在两阶段Col. Blotto游戏中获得全局最优策略，因为其包含游戏阶段的顺序依赖和图拓扑结构施加的复杂约束。

Method: 提出一种称为HGformer的分层图Transformer框架，通过增强的图Transformer编码器和两代理分层决策模型，实现大规模对抗环境中高效的策略生成。设计了一种逐层反馈强化学习算法，将低层决策的长期回报反馈到高层策略的优化中。

Result: 实验结果表明，与现有的分层决策和图神经网络方法相比，HGformer显著提高了资源配置效率和对抗收益，在复杂动态场景中表现更佳。

Conclusion: HGformer框架成功应对了传统方法在两阶段Col. Blotto游戏中的策略优化问题，显著提升了整体性能。

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [102] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens,Xixi Lu*

Main category: cs.AI

TL;DR: FoldA technique uses Petri net unfoldings to improve conformance checking by reducing state space issues and better representing concurrency, albeit with more computation time.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of state space explosion and the failure to represent concurrent behavior in many real-world processes in existing conformance checking methods.

Method: The paper proposes a new technique called FoldA that computes partial-order alignments on the fly using directed Petri net unfoldings.

Result: The results show that FoldA is generally more effective in reducing the number of queued states and providing accurate representation of concurrency, despite requiring more computation time.

Conclusion: The proposed FoldA technique generally reduces the number of queued states and provides a more accurate representation of concurrency, although it requires more computation time compared to existing Astar- and Dijkstra-alignments.

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [103] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen,Daan Brinks,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 研究了一种用于多机器人控制的模块化循环架构，在未知机器人上显示出增强的性能。


<details>
  <summary>Details</summary>
Motivation: 创建一个能够适用于任何机器人形态的通用控制器，以提高计算和数据效率。探索通过深度强化学习代理的架构来利用机器人的模块化结构和上下文信息。

Method: 实现一个模块化循环体系结构，评估其在一组大型MuJoCo机器人上的泛化性能。

Result: 在四种不同环境中，对具有未知动力学、运动学和拓扑结构的机器人显示出显著的性能提升。

Conclusion: 通过交互推断部分可观察的上下文信息，可以更好地泛化到训练过程中未见过的上下文。

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [104] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出了CoVo，一个内在奖励机制，通过自我奖励增强了大型语言模型的推理能力，与监督RL的性能相当或更优。


<details>
  <summary>Details</summary>
Motivation: 目前有监督的强化学习限制了其广泛应用，因此需要一种自我奖励机制来使大型语言模型在没有外部监督的情况下进行学习和推理。

Method: 提出了一种自我奖励的强化学习框架，通过结合一致性和波动性来提升大型语言模型的推理能力。引入了CoVo，一种内在奖励机制，通过强健的向量空间聚合策略和好奇心奖励来促进多样化探索。

Result: 在各种推理基准测试中，CoVo的表现与监督RL相当，甚至更优。

Conclusion: CoVo显著提升了LLM在推理任务中的效果，与甚至超过了有监督的RL性能。

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [105] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun,Yu Yao,Xinshuai Dong,Zongfang Liu,Tongliang Liu,Yumou Qiu,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了一种无需二值化过程的样本有效CI测试，通过解决过度识别约束问题建立潜变量的独立关系，并证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于测量限制，现实世界中的变量通常表示为离散值。直接应用CI测试会导致错误的结论，而最近的进展通过二值化观察数据来推断潜变量之间的CI关系，但会导致信息损失。因此，提出不依赖二值化过程的样本有效CI测试。

Method: 利用节点回归，通过广义矩量法（GMM）解决过度识别约束问题，以推导出适当的测试统计量并正确建立其渐近分布。

Result: 推导出适当的测试统计量并正确建立其渐近分布，通过理论和实证结果证明了提出方法的优越性和有效性。

Conclusion: 通过理论和实证结果证明，提出的CI测试在各种数据集上的优越性和有效性。

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [106] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结合知识图谱与大型语言模型的新方法，在因果发现的任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有使用大型语言模型的因果发现方法结果不稳定，导致因果推断的可靠性受损。研究者旨在通过结合知识图谱来增强这种因果发现方法的稳定性和有效性。

Method: 提出了一种将知识图谱与大型语言模型相结合的新方法，通过识别知识图谱中的信息丰富的元路径子图，并使用排序学习模型进一步优化这些子图的选择。

Result: 在生物医学和开放领域的数据集上的大量实验表明，与多个基线相比，该方法的F1得分提高了44.4点。

Conclusion: 结合知识图谱与大型语言模型的方法在知识型因果发现中表现出色，与传统依赖观察数据的方法相比有所优势。

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [107] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: 本文调查发现，当前对LLM助手和代理的研究和评价集中于一些特定活动和完全替代人类劳动力，忽视了人机协作和通过任务转化提升自动化的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的兴起，数据科学活动自动化成为可能。本文旨在调查和分析LLM在数据科学中的作用及其评价，找出存在的不足之处。

Method: 本文通过调查和分析LLM助手和代理在数据科学中的评价，找出其在目标导向活动以及人机协作中存在的局限性。

Result: 研究发现，现有评价过于集中于目标导向的活动，而忽略了数据管理和探索性活动；侧重于纯助手或完全自主代理，忽视中间水平的人机协作；主要关注人类替代，而忽视了通过任务转化进一步自动化的可能性。

Conclusion: 当前数据科学领域中的LLM助手和代理的评价存在局限，主要集中于某些特定活动和完全的人工替代，而忽视了中间协作以及通过任务转化提升自动化水平的潜力。

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [108] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna,Eugene Hauptmann,Ye Tong Yuan,Jessica Situ,Xian-Hao Liao,Ashly Vivian Beresnitzky,Iris Braunstein,Pattie Maes*

Main category: cs.AI

TL;DR: 使用LLM辅助写作可能导致认知连接性下降，文章拥有感降低，长期使用或对教育有负面影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM辅助写作对神经和行为的影响，以及评估AI在学习中的角色。

Method: 使用脑电图（EEG）评估写作过程中的认知负荷，分析文章使用自然语言处理（NLP），并结合人类教师和AI裁判对文章进行评分。

Result: 参与者使用LLM时表现出较弱的脑连接性，随着工具使用，认知活动下降。LLM-to-Brain组的参与者在第四场次表现出减少的alpha和beta连接，Brain-to-LLM组表现出更高的记忆回忆和脑区激活。自我报告的文章拥有感最低的是LLM组，最高的是Brain-only组。

Conclusion: LLMs虽然提供了即时便利性，但其使用可能导致认知上的代价，特别是长期教育方面的影响值得关注。

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [109] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan,Jianan Zhou,Yifeng Zhang,Yaoxin Wu,Jinbiao Chen,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: POCCO框架通过适应性选择模型和偏好学习，提升了多目标组合优化的解决方案质量以及推广能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习方法在解决多目标组合优化问题时存在瓶颈，因其将问题均等对待并使用单一模型求解，限制了解决方案空间的有效探索，导致性能不佳。

Method: 我们提出了POCCO，一个新颖的即插即用框架，使得子问题的模型结构可以自适应选择。我们设计了一个条件计算模块，将子问题引导至特定的神经网络架构，并提出了一种偏好驱动的优化算法，学习胜负解之间的成对偏好。

Result: 在实验中，将POCCO应用于两种最先进的神经方法，实验结果表明，在四个经典的多目标组合优化基准上，POCCO表现出了显著的优势和很强的泛化能力。

Conclusion: POCCO通过适应性选择模型结构和偏好驱动的优化算法，成功地改善了多目标组合优化问题的解决方案探索和性能。

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [110] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 文章提出了对交通路口驾驶行为进行数据驱动模拟的方法，以改进目前交通模拟器的表现，使用了一种基于多头自注意力的轨迹预测模型和新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 目前交通模拟器主要采用基于规则的方法，这限制了它们模拟真实驾驶行为的能力。交通路口在道路基础设施中是关键组件，因其涉及的安全风险（28%的致命车祸及58%的非致命车祸发生在路口）以及影响道路走廊的运行效率。这引发了一个重要问题：我们是否能够创建一个数据驱动的模拟器来模仿交通路口的驾驶行为宏观和微观统计？

Method: 提出交通工程相关指标来评估生成轨迹预测模型，并提供了一种“模拟-循环”管道。此外，介绍了一种基于多头自注意力的轨迹预测模型，整合了信号信息，这在评估指标上表现优于之前的模型。

Result: 新提出的多头自注意力模型在评估指标上优于先前的模型。

Conclusion: 研究提出了一种结合深度生成建模和新评估指标的方法来改进交通路口的驾驶行为模拟，从而提高了交通模拟器的实时表现和评估能力。

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [111] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 本文研究深度生成模型在城市交通信号灯交叉口的动态表现，提出了新的评估方法来衡量模型从交通工程角度对安全和效率的影响。


<details>
  <summary>Details</summary>
Motivation: 当前交通动态模型主要通过轨迹重构误差进行评估，但这些指标未能充分考虑交通工程的具体问题，如闯红灯等。因此，需要一种更全面的方法来评价模型性能。

Method: 本文使用一个经校准的真实城市交叉口场景大数据来训练一种先进的多车辆轨迹预测模型，并在微观模拟器中在线评估预测模型在未预见交通条件下的性能。

Result: 尽管模型能够在输入理想轨迹时实现低轨迹重构误差，但生成的轨迹仍表现出破坏交通规则的行为。本文引入新的指标来评估这些不良行为并给出结果。

Conclusion: 深度生成模型在交通信号灯交叉口的动态表现有待进一步改善，新的评估指标有助于更深入地理解其在流量管理中的应用潜力。

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [112] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei,Saiping Guan,Da Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文综述了NKG中的链接预测，分类和分析现有方法及其应用场景，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: N-ary知识图谱旨在有效表示复杂的真实世界事实，因此在NKG中进行链接预测以补足缺失元素，提高下游应用性能显得尤为重要。

Method: 对现有NKG链接预测方法进行分类和性能分析，并概述其应用场景以及未来研究方向。

Result: 本文为NKG链接预测进行了系统综述和分类，分析了现有方法的性能及应用场景，并提出了未来研究方向。

Conclusion: 本文首次全面综述了NKG中的链接预测，提供了关于该领域的概述，系统分类现有方法，并分析其性能和应用场景。还指出了未来研究的有希望方向。

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [113] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. Bell*

Main category: cs.AI

TL;DR: AbstentionBench benchmark reveals that current large language models struggle with knowing when not to answer, an unsolved problem that fine-tuning doesn't fix; abstention is key for real-world reliability.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLMs know when not to answer is crucial for their reliable deployment in real-world scenarios, as current models struggle with uncertainty and abstention.

Method: The authors introduced AbstentionBench, a large-scale benchmark to evaluate LLMs' ability to abstain from answering in various contexts, and evaluated 20 LLMs using this benchmark.

Result: The evaluation revealed that reasoning fine-tuning reduces abstention capabilities, and while system prompts help, they do not address the core issue of reasoning about uncertainty.

Conclusion: Abstention remains a critical yet unsolved challenge for LLMs, and reasoning fine-tuning currently degrades abstention capability.

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [114] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: 引入了VIKI-Bench和VIKI-R，用于提高具身AI系统中多代理视觉驱动合作的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM方法在支持不同类型的具身方面仍有限，为此引入了VIKI-Bench来评估基于视觉输入的推理。

Method: 提出了VIKI-Bench，一个分层基准测试平台，包括代理激活、任务规划和轨迹感知三个结构化层级，以及VIKI-R，一个包含两阶段的框架，即预训练视觉语言模型的微调和多级别奖励信号下的强化学习。

Result: VIKI-R在所有任务层级上显著优于基线方法，加强学习促使异质代理间合作模式的涌现。

Conclusion: VIKI-Bench和VIKI-R为具身人工智能系统中基于视觉的多代理合作提供了统一的测试平台和方法。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [115] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku,Kohki Horie,Yoichi Iwata,Kensho Aoki,Naohiro Takahashi,Takuya Akiba*

Main category: cs.AI

TL;DR: ALE-Bench基准用于评估AI在复杂问题上的表现，发现了AI与人类在一致性和长时间段问题解决能力的差距。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在算法工程中解决困难优化问题的表现。

Method: 我们引入了ALE-Bench，这是一种新的基准，用于评估AI系统在基于得分的算法编程竞赛中的表现。软件框架支持互动代理架构，能利用试运行反馈和可视化。

Result: 前沿的LLMs在特定问题上表现良好，但与人类相比在一致性和长期问题解决能力上存在差距。

Conclusion: 最前沿的LLMs在某些问题上表现出了很高的性能，但和人类相比，在问题的一致性和长时间段问题解决能力方面仍存在显著差距。

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [116] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: MasHost uses RL to autonomously design efficient multi-agent systems, outperforming current methods across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing multi-agent systems, which often rely on manually crafted mechanisms, by proposing a more autonomous and adaptable approach using RL.

Method: MasHost employs Reinforcement Learning (RL) through Hierarchical Relative Policy Optimization (HRPO), treating multi-agent system construction as a graph search problem and sampling agent roles and interactions probabilistically.

Result: MasHost outperforms most existing methods in effectiveness, efficiency, and structure rationality, as demonstrated by extensive experiments on six benchmarks.

Conclusion: MasHost framework effectively enhances the performance of multi-agent systems through its novel approach to autonomous and query-adaptive design.

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [117] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: KVmix是一种用于键值缓存的混合精度量化方法，能够在大规模语言模型中实现显著的内存压缩和推理吞吐量提升，接近无损的推理性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在推理过程中，键值缓存的高内存需求限制了在资源受限平台上的部署。量化可以有效缓解键值缓存造成的内存压力，但现有方法存在静态的、一刀切的精度分配问题，或无法在长上下文任务中动态优先考虑关键键值。这些问题导致了内存、准确性和吞吐量的权衡。

Method: 提出一种新的键值缓存混合精度量化方法，称为KVmix。KVmix利用基于梯度的重要性分析来评估键和值投影矩阵如何影响模型损失，从而实现分层的位宽分配进行混合精度量化。该方法动态优先考虑对重要层级使用更高精度，同时对影响较小的层进行激进的量化，实现准确性和效率之间的可调节平衡。还介绍了一种动态长上下文优化策略，自适应地保留最近枢纽标记的全精度键值对，并压缩较旧的，实现高质量序列生成和低内存使用。

Result: 在像Llama和Mistral这样的LLM上，KVmix在非常低的量化配置（键2.19bit，值2.38bit）下实现了几乎无损的推理性能，同时在内存压缩上达到4.9倍，推理吞吐量提高了5.3倍。

Conclusion: KVmix在保持推理性能的同时，通过极低的量化配置实现显著的内存压缩和推理吞吐量提升，这使得其在资源受限的平台上具有很高的实用价值。

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [118] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: 研究提出了Agentic Neural Network框架，通过动态任务分解和迭代反馈优化多智能体协作，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前使用多个大规模语言模型的方法通常依赖于静态、手动设计的多代理配置，因此提出一种新的框架以克服这些限制，通过动态和自动化的团队组建提高效率和适应性。

Method: 提出了一种名为 Agentic Neural Network (ANN) 的框架，使用两阶段优化策略进行多代理协作：前向阶段将任务动态分解为子任务并逐层构建协作代理团队；后向阶段通过迭代反馈优化全球和局部协作。

Result: 在四个基准数据集上，ANN 在相同配置下超越了领先的多代理基线，展示了显著的性能提升。

Conclusion: 这一研究表明，Agentic Neural Network (ANN) 框架能够有效提升多代理系统的性能，结合大规模语言模型的协作能力与神经网络的效率和灵活性。

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [119] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 通过半监督算法，利用多个数据源，我们将难民统计数据细化至高分辨率网格单元，准确率达到92.9%。


<details>
  <summary>Details</summary>
Motivation: 目前的区域和国家统计数据掩盖了局部的流离失所模式，准确的局部数据对于理解流离失所的驱动因素是重要的。

Method: 我们采用的是半监督算法，从行政边界到0.5度网格单元分离难民数据。我们将UNHCR的ProGres注册数据、Google的开放建筑卫星数据以及OpenStreetMap的地点坐标组合在一起，并使用标签传播算法以高粒度创建空间明确的难民统计数据。

Result: 我们的方法能够以92.9%的平均准确率准确定位难民观察数据至合适的网格单元，并帮助识别此前较大区域和国家统计未能反映的局部流离失所模式。

Conclusion: 我们的方法能够以92.9%的平均准确率将超过1000万难民数据分配到适当的网格单元中。该高分辨率数据集为深入理解流离失所的驱动因素提供了基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [120] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: 提出BUOT模型改进现有加权框架，通过双层传输机制提高部分领域适应问题的解决效果。


<details>
  <summary>Details</summary>
Motivation: 当前的加权框架只能刻画样本级关系，导致不足以探索集群结构，且对于不准确预测敏感，容易在异常类中造成混淆。

Method: 提出双层不平衡最优传输模型（BUOT），通过样本级和类别级传输的合作机制来改进传输过程。

Result: 在基准数据集上进行的大量实验验证了BUOT的竞争性。

Conclusion: 通过在统一的传输框架中同时表征样本级和类别级关系，BUOT模型在PDA问题上表现出竞争性。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [121] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: 提出了利用大语言模型知识转移的流场预测框架，显著降低了计算时间，同时保持高预测准确性，适用于多种工程领域。


<details>
  <summary>Details</summary>
Motivation: 解决传统CFD方法的高计算成本问题及现有深度学习模型的有限跨条件转移能力。

Method: 框架将适当的正交分解（POD）降维方法与预训练的大语言模型微调策略结合在一起，POD用于压缩流场特征表示，微调模型用于在状态空间中学习编码系统动态。还设计了面向流体动力学的文本模板以提高预测性能。

Result: 实验证明，该框架在少样本学习情境下优于传统Transformer模型，并能在不同流入条件和机翼几何形状上表现出色的一般化能力。与传统Navier-Stokes方程求解器的小时计算时间相比，预测时间减少至秒，准确率达到90%以上。

Conclusion: 提出了一种基于大语言模型知识转移的通用流场预测框架，显著降低了传统CFD方法的计算成本，且在不同流入条件和机翼几何形状上表现出色的一般化能力。

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [122] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: MBPO通过改善模态平衡优化提升了大型多模态模型的表现，减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 解决LMM在推理过程中语言偏见明显超过视觉输入导致的模态失衡问题。

Method: 采用了生成困难负样本的离线和经过验证的在线数据结合的策略，并使用了Group Relative Policy Optimization (GRPO)进行训练。

Result: 实验结果表明，MBPO在增强LMM性能和减少幻觉方面表现出色。

Conclusion: Modality-Balancing Preference Optimization (MBPO)能够显著提升LMM在艰难的视觉-语言任务上的表现并有效减少幻觉。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [123] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: NVIDIA的MX格式结合窄浮点和缩放因子提升精度缩放，但需优化舍入模式以避免大规模预训练发散，并在8B模型上成功实现。


<details>
  <summary>Details</summary>
Motivation: 在GPU执行过程中，提高效率而不降低准确性是一个重要的研究方向。通过减少位数来表示模型参数和相关张量的精度缩放已经成为实现这一目标的一种有效技术。NVIDIA最新的Blackwell GPUs中的Microscaling (MX) 格式为该领域的精度缩放带来了重大突破。

Method: 使用NVIDIA最新Blackwell GPUs中的MX格式，这些格式结合了窄浮点数据类型和每块缩放因子，从而提供了一种对张量进行量化的细粒度方法。

Result: 研究表明，在进行大规模语言模型的预训练时，建议的OCP规格中的舍入模式可能导致发散。通过使用改进的舍入模式，即使用向无穷大舍入来计算缩放因子，实现了在MXFP8中成功地预训练8B模型处理15T tokens。

Conclusion: 通过优化舍入模式，实现了在MX格式下大规模语言模型预训练的成功收敛，提高了在精度缩放下的数值稳定性。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [124] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: ST-GraphNet使用图神经网络成功预测自动驾驶车辆事故严重性，实现了97.74%的高测试准确率，优于细粒度方法。


<details>
  <summary>Details</summary>
Motivation: 理解自动驾驶车辆（AV）事故严重的时空动态，对于提升城市移动安全和基础设施规划至关重要。

Method: 引入了一种名为ST-GraphNet的时空图神经网络框架，通过使用细粒度和区域聚合的空间图来建模和预测自动驾驶车辆的事故严重性。利用包含来自德克萨斯州的2352份自动驾驶相关事故报告的数据集，构建了两种图表示，并评估了多种图神经网络架构。

Result: ST-GraphNet框架通过使用DSTGCN在粗粒度的H3图上取得了97.74%的测试准确率，大大优于细粒度模型的64.7%。

Conclusion: ST-GraphNet模型使用粗粒度的H3图表作为DSTGCN骨干，实现了97.74%的测试准确率，显著优于最佳细粒度模型的64.7%准确率。

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [125] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: STAMImputer通过时空注意力机制提高交通数据插补性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的方法无法在块状缺失数据场景中有效提取特征，同时静态图结构限制了模型处理非稳定交通数据分布平移问题的灵活性。

Method: 提出了一种称为STAMImputer的时空注意力专家混合网络。

Result: STAMImputer在交通数据插补任务上表现出显著的性能提升。

Conclusion: STAMImputer在多个交通数据集上的实验表明该模型在缺失数据场景中表现优于现有方法。

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [126] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: 研究探讨如何在不调整模型参数的情况下，通过推理技术（如上下文学习）近似实现大规模语言模型的微调效果，提供了资源高效的部署方法理论。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型在自然语言处理中变革性显著，但监督微调过程计算量大，需寻找更高效的技术路线。

Method: 通过形式化证明和理论分析，展示了使用推理技术（特别是上下文学习）近似微调能力的方法。

Result: 在不改变模型参数的情况下，通过推理技术近似微调效果，在特定假设下是可行的，并且在有限资源条件下对文本生成和线性分类问题的投入具有实际价值。

Conclusion: 提供了大规模语言模型高效部署的理论基础，尤其涉及不改变模型参数条件下的推理技术。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [127] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 本研究提出了首个直接优化非线性福利目标的离线多目标强化学习框架FairDICE，通过分布校正估计实现公平意识的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MORL方法主要使用线性标量化来优化多目标策略，但难以实现公平性目标。因此，如何在离线设置中统一优化非线性福利标准是一个具有挑战性的问题。

Method: 采用分布校正估计方法，通过福利最大化和分布正则化联合优化，实现稳定且样本高效的学习。

Result: FairDICE在多个离线基准测试中表现出色，能够实现无需显式偏好权重或权重搜索的稳定和样本高效学习。

Conclusion: FairDICE在多个离线基准测试中表现出较强的公平意识性能，相比于现有的基线方法，该框架能够在稳定高效的情况下优化非线性福利目标。

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [128] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: Lite-RVFL is a fast and efficient network designed to handle concept drift without the need for drift detection or retraining, using a novel weighting objective function and verified through theoretical and experimental analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high computational costs and unsuitability for real-time applications of existing online learning methods that require drift detection and retraining.

Method: Lite-RVFL employs a random vector functional-link network with a novel objective function that assigns exponentially increasing weights to new samples, facilitating drift adaptation.

Result: Theoretical analysis and experimental results validate that Lite-RVFL efficiently adapts to concept drift and captures temporal patterns, demonstrating its effectiveness on real-world safety assessment tasks.

Conclusion: Lite-RVFL provides an efficient solution to adapt to concept drift without retraining and drift detection, proving its feasibility and effectiveness in online learning scenarios.

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [129] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: 提出了一种新的框架Info-Coevolution，通过在线选择性标注来有效地提高数据集构建效率，节省成本。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习中随着数据量不断增加而导致的数据集构建和训练效率挑战，提出了一种无需偏见的在线选择性标注新框架，以解决是否需要为新的数据进行标注和学习的问题。

Method: 提出了Info-Coevolution框架，利用任务特定模型和开源模型进行选择性标注和整合在线及网络数据，以提高数据集的构建效率。同时研究了基于检索的通过无标签开源数据进行数据集增强的方法。

Result: Info-Coevolution框架能够减少ImageNet-1K数据集的标注和训练成本32%，且不损失性能；通过半监督学习，标注成本可进一步降低至50%。

Conclusion: Info-Coevolution框架能够在不影响性能的情况下显著减少数据标注和训练成本，尤其对于大型现实世界的数据集如ImageNet-1K，通过在线选择性标注实现了32%的成本节约，并且不需要调节比率。同时，在使用半监督学习时标注成本可进一步减少至50%。

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [130] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: 研讨时间序列模型在电力市场的预测效果，发现Chronos-Bolt和Time-MoE与传统方法相当，但MSTL模型表现更为稳定。


<details>
  <summary>Details</summary>
Motivation: 探讨生成性人工智能和预训练大型语言模型在电力价格预测中的应用效果。

Method: 评估几种最先进的预训练模型和传统统计及机器学习方法，以生成日常预测并比较其表现。

Result: Chronos-Bolt和Time-MoE与传统模型表现相当，但双季节性MSTL模型在表现上更为突出。

Conclusion: Chronos-Bolt和Time-MoE是最强的时间序列基础模型，与传统模型表现相当。然而，双季节性MSTL模型在所有国家和评估指标中表现一致，无基础模型能在统计上优于它。

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [131] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo 是一种强化学习框架，旨在提高大语言模型的推理效率，同时保持高准确性。通过改进长度奖励设计，Bingo 在多项推理基准测试中表现突出，平衡了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因输出冗长或冗余而导致的低效率问题，同时在推理的准确性和效率之间寻求平衡。

Method: 引入 Bingo 框架，这是一种强化学习框架，结合显著性感知的长度奖励和动态长度奖励两大机制来提高推理效率。

Result: 在多个推理基准上，Bingo 框架不仅提升了模型的推理效率，还提高了准确性，与其他长度奖励基线相比取得了优异的性能。

Conclusion: Bingo 框架在提升大语言模型推理效率的同时，保持了准确性，呈现出在准确性和效率之间的良好平衡，突显了训练语言模型以提高推理效率的潜力。

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [132] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: 该研究提出了一种新的回归层——近邻注意（NONA），提高了回归性能。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法在SFT模型上的性能较好，直接将其作为神经网络预测层可能进一步改善性能。

Method: 引入了近邻注意（NONA）回归层，使用神经网络注意机制和新颖的学习注意遮蔽方案，将k-NN回归算法转换为可微代理。

Result: 在多个非结构化数据集上，NONA显示出比密集层预测和SFT嵌入上的k-NN更好的回归性能。

Conclusion: NONA回归层可以提高基于SFT的嵌入上的传统预测性能。

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [133] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT解决了AI科学发现中的数据稀缺问题，通过自动管道构建了一个大规模的编码任务数据集，提升了训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助科学发现中的数据稀缺问题，构建一个能够自动收集和合成编码任务和解决方案的数据集。

Method: AutoSDT使用了一条自动化管道，收集高质量编码任务，涉及多学科和Python包，并经过LLM编译，提供生态有效的任务和准确的代码解决方案。

Result: AutoSDT-5K数据集成功收集了5,404个编码任务，被专家评价高效，AutoSDT-Coder模型在基准测试中取得了显著的性能提升。

Conclusion: AutoSDT-Coder在科学发现的挑战性基准测试上表现优异，提升了模型的成功率和假设匹配分数，其性能接近于GPT-4o。

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [134] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了一种高效的公平谱聚类方法，较先前方法提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着算法决策的普及，在群体公平性方面的考虑变得越来越重要。

Method: 提出了一种新的高效公平谱聚类方法，通过差异凸函数（DC）框架中的新颖变量增广策略，运用适应于DC问题的交替方向乘子法。

Result: 实验结果表明，该方法在合成和真实基准测试上表现出显著的计算效率提高，尤其是当问题规模增大时。

Conclusion: 该研究显著推进了公平聚类在实际应用中的采用。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [135] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: 该研究提出了一种新框架，通过神经网络和NODEs来识别异质材料的力学特性，不依赖封闭形式的本构方程，具有鲁棒性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 为了识别异质材料的力学性质，而不需要封闭形式的本构方程。

Method: 使用结合傅里叶特征的神经网络和普通神经微分方程（NODEs）框架。

Result: 数值结果表明，该方法在有噪声的情况下和实验数据下对于异质材料的识别是稳健和通用的。

Conclusion: 提出的方法在识别异质材料的力学特性上是一种与传统逆向方法不同的适用替代方案。

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [136] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文提出了一种双层优化算法BLUR，能够更好地解决遗忘问题并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 为大型语言模型开发一种能够在遵守数据法规和推动生成式AI伦理实践的背景下有效去除在训练中获取的知识和能力的方法。

Method: 提出了一种新的双层优化算法，称为双层遗忘（BLUR），该算法模型了遗忘问题的层次结构，其中遗忘问题的优先级高于保留问题。

Result: BLUR算法在各类遗忘任务、模型和评估指标上表现出色，优于所有现有的遗忘算法。

Conclusion: 通过采用双层优化问题的结构来更好地处理遗忘问题，提高模型性能并具备理论保证。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [137] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL通过两种本地训练正则化策略提高了非IID数据下的联邦学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统的解决方法在应对特征漂移时效率低下，且计算成本高，需要一种能在客户端层面模仿IID训练动态的新方法。

Method: 本论文提出了UniVarFL框架，包括两种本地训练正则化策略：分类器方差正则化和超球面均匀性正则化。

Result: 实验证明UniVarFL在多个基准数据集的准确度上优于现有方法。

Conclusion: UniVarFL显著改善了在非IID数据下的联邦学习性能，特别是在资源有限的环境中，表现出较高的扩展性和效率。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [138] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: 该论文提出在联邦学习中使用随机神经网络作为本地模型，以解决数据噪声问题，并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的本地数据集可能存在潜在噪声，影响模型的性能。

Method: 提出使用随机神经网络作为联邦学习框架中的本地模型，以提高模型对潜在噪声的处理能力。

Result: 数值实验显示，该方法在处理非独立同分布数据时具有良好的性能和效果。

Conclusion: 通过使用随机神经网络作为本地模型，能够有效处理联邦学习中本地数据集中的潜在噪声问题。随机神经网络不仅可以帮助估计数据的真实底层状态，还能够量化潜在噪声。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [139] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出了一种基于遗传算法的方法来构建个性化决策树，用于联邦学习，取得了优于传统方法的实验结果。


<details>
  <summary>Details</summary>
Motivation: 为了应对数据隐私问题，研究者开始关注联邦学习，但现有的联邦学习决策树方法受限于分类树和分类数据，因隐私差分约束无法有效处理数值数据。

Method: 采用遗传算法替代传统贪婪树构建算法，以构建个性化决策树，从而同时处理分类数据和数值数据，适用于分类树和回归树。这个方法不受限于差分隐私的约束。

Result: 实验表明，使用遗传算法构建的决策树，其性能优于仅在本地数据上训练的决策树和现有的基准算法。

Conclusion: 使用遗传算法构建个性化决策树能够更好地处理分类和回归任务，并且性能优于传统本地数据训练的决策树以及基准算法。

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [140] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 本文研究了通过相关噪声机制提升差分隐私在AI模型训练中隐私性和实用性的方法，并已在全球范围内得到应用。


<details>
  <summary>Details</summary>
Motivation: 探讨相关噪声机制在差分隐私中的设计与分析，以及其在私人AI和机器学习模型训练中的应用。

Method: 研究相关噪声机制的应用，尤其是在学习算法中如矩阵机制、分解机制和DP-FTRL中。

Result: 通过在噪声引入步骤中引入反相关性，可以显著改善隐私与实用性之间的平衡，且已在工业部署中得到应用。

Conclusion: 相关噪声机制能够通过在随后的步骤中抵消先前的噪声提升差分隐私机制的效果，在保护隐私的同时保持较高的模型实用性。

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [141] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: 研究提出机器学习模型，通过有限测量数据精确推断梁余应力分布，从而优化结构完整性和寿命，减少实验工作量。


<details>
  <summary>Details</summary>
Motivation: 由于实验上全面表征残余应力分布的难度，该研究希望通过机器学习的方法从有限的测量数据中推断出全场应力分布。

Method: 使用基于U-Net架构的机器学习模型，通过系统的超参数调整进行训练，用于学习残余应力分布的潜在结构。

Result: 模型成功生成模拟应力，并在实际表征数据上的表现验证了其有效性，表明其具有出色的预测准确性和显著的泛化能力。

Conclusion: 该研究提出的基于机器学习的残余应力生成器（RSG）能够在有限测量的情况下有效推断出全场应力分布，大幅减少实验工作量。

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [142] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: 研究表明，集成模型的可解释性受模型数量影响较大，通过计算复杂性视角更好理解其可解释性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对集成模型（如增强树）的可解释性的严格数学理解，本研究希望通过分析基本因素（数量、大小、类型）对其的影响来弥补这一空白。

Method: 通过引入计算复杂性理论的概念，研究生成各种集成模型解释的挑战。

Result: 尽管基于标准复杂性假设，固定大小的基础模型的集成仍然难以解释，但较小数量的决策树集成则易于解释，暗示基础模型的数量是关键因素。

Conclusion: 通过计算复杂性理论分析，解释不同配置的集成模型的复杂性模式，发现模型数目的变化对可解释性有显著影响。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [143] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Mondrian通过域分解策略提高了变压器算子模型在高分辨率、多尺度领域的性能。


<details>
  <summary>Details</summary>
Motivation: 将基于变压器的算子模型扩展到高分辨率、多尺度领域仍然是一个挑战。

Method: Mondrian通过将域分解为非重叠子域，并应用注意力于子域限定函数序列上，解耦了注意力与离散化。

Result: Mondrian在Allen-Cahn和Navier-Stokes PDEs上取得了强劲的性能，实现了分辨率扩展而无需重新训练。

Conclusion: Mondrian展现了领域分解注意力在可扩展和通用型神经算子中的潜力。

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [144] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: 研究发现自动驾驶中Transformer模型性能随训练和推理资源的扩展表现出规律性，优化这些缩放特性能够提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探讨自动驾驶领域中运动预测和规划任务的模型性能如何随计算资源的增加而变化，从而优化模型开发过程。

Method: 研究了编码器-解码器自回归Transformer模型在联合运动预测和规划任务上的经验规模规律，使用了50万小时的驾驶数据进行实验。

Result: 随着计算资源的增长，模型性能呈现出类似语言建模的幂律提升，模型训练损失和评估指标之间具有强相关性；推理时的计算缩放也表明，小模型的输出采样和聚类可以使其在特定点之前与大模型竞争。一旦超过转折点，大模型在推理计算效率方面更具优势。

Conclusion: 随着训练计算预算的增加，最佳扩展需要以1.5倍的速度增加模型大小而不是数据集大小。这表明通过优化训练和推理时的缩放属性可以显著改善自动驾驶情境中的运动预测和规划模型性能。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [145] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 提出了一种综合框架用于评估由大型语言模型提取的临床数据的质量，支持肿瘤学领域的AI驱动证据生成。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中临床数据的提取需要可靠性、准确性和公平性，以支持研究、监管和临床应用。现有的质量保证框架无法完全应对由大型语言模型提取的数据的独特错误模式和复杂性。

Method: 本文提出了一个综合框架来评估由大型语言模型提取的临床数据的质量。该框架整合了变量级的性能基准测试与专家的人工抽象，自动验证检查内部一致性和合理性，以及将LLM提取的数据与人工抽象的数据集或外部标准进行复制分析。

Result: 该框架能够识别最需要改进的变量，系统检测潜在错误，并确认数据集在真实研究中的适用性。此外，通过在不同人口统计子群中分层指标，该框架支持偏差评估。

Conclusion: 该框架提供了一种严格透明的方法用于评估由大型语言模型提取的真实世界数据，提高行业标准，并支持信任度高的AI驱动证据生成在肿瘤学研究和实践中的应用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [146] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: 本文提出了一种通过改善遗忘问题来提高随机数据增强泛化能力的新方法，在各种基准测试上显示了强大的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 数据增强被认为能够提升跨分布的泛化能力。虽然有目标的数据增强效果最好，但其高昂的代价是限制因素；而随机增强则便宜但效果欠佳。本文重新审视随机增强，旨在探讨解决其局限性的方法。

Method: 作者提出了一种简单的方法，通过解决遗忘问题以提高随机增强的泛化效果。

Result: 所提方案在多种单源域泛化(sDG)基准上表现出强大的泛化能力。

Conclusion: 通过解决随机增强中的特征遗忘问题，能够提高其在单源域泛化任务中的效果，使随机增强变得更加有效。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [147] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: 提出了一种基于 STL 的模型来改善 LLMs 数学推理任务中的置信度估计效果。


<details>
  <summary>Details</summary>
Motivation: LLMs 在数学推理任务中表现优异，但往往会产生高度自信却不正确的结果，尤其在教育领域，用户可能缺乏评估推理过程的能力。

Method: 我们提出了一种将逐步置信度建模为时间信号的结构框架，并用信号时态逻辑（STL）来评估。我们定义了基于 STL 的正式约束，并计算稳健性分数作为结构化和可解释的置信度估计。

Result: 实验表明，我们的方法在提高校准指标方面表现优异，并提供比传统置信度聚合和事后校准更可靠的不确定性估计。

Conclusion: 我们的研究表明，使用 STL 评估逐步信号的置信度可以提高 LLMs 在数学推理任务中的可靠性。

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [148] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: 提出了一种无需额外参数的方法，在损失函数中加入新的项实现潜在空间的等变性，实验结果显示性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有的等变方法计算量大、参数多且常与特定架构绑定，作者希望找到一种简单无参数的方法来实现等变性。

Method: 提出了一种简单的零参数方法，通过在损失函数中增加一项来在潜在表示中施加近似的等变性。这使得网络可以在潜在空间中学习一个群的表示，并将其学习为常规表示。

Result: 在三个数据集上进行基准测试，结果显示该方法在许多情况下可以用很少的参数实现与现有等变方法相似或更好的性能。

Conclusion: 通过在损失函数中施加近似等变性，只需较少的参数即可实现与现有等变方法相近或更好的表现。这是一种有效的替代方法。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [149] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: 通过SHIELD模型，解决灾难性遗忘和对抗性攻击漏洞，同时提高了持续学习的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络在持续学习中面临灾难性遗忘和对抗性攻击的脆弱性问题。

Method: 引入了SHIELD（Secure Hypernetworks for Incremental Expansion and Learning Defense），采用超网络结合区间算术的持续学习方法。

Result: SHIELD模型可以动态生成单独的网络并对所有任务进行信息聚合，提供对抗性攻击的严格保障，同时不牺牲网络适应性。

Conclusion: SHIELD解决了深度学习中的灾难性遗忘和对抗性攻击问题，增强了网络的安全性和适应性。

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [150] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: HC-RLHF方法在保证安全的同时最大化有用性，通过双模型学习和两步优化，生成高概率的安全模型，优于之前方法。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型对齐方法常常将安全性视为有用性的对立面，导致在敏感领域中可能出现不可接受的响应。为了确保在此类环境中性能可靠，引入HC-RLHF。

Method: HC-RLHF方法将人类偏好明确分为有用性和无害性（安全性），分别通过训练奖励模型和成本模型来学习。采用两步过程寻找安全的解决方案：首先在成本约束的悲观版本下优化奖励函数，然后对训练好的模型进行安全测试。

Result: 理论分析证明HC-RLHF不会以超过用户指定阈值的概率返回不安全的解决方案。实证分析结果表明，HC-RLHF能生成安全性概率较高的模型，并能在无害性和有用性方面优于先前方法。

Conclusion: HC-RLHF能够在安全性和有用性之间实现有效平衡，并且在确保不超过特定安全概率阈值的前提下，不会返回不安全的解决方案。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [151] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 该研究提出了LIES框架，通过神经网络架构优化符号回归以发现解释性和稀疏的公式。


<details>
  <summary>Details</summary>
Motivation: 符号回归旨在发现能准确描述数据的封闭数学表达式，从而提供比标准黑盒模型更具解释性和分析洞察力的方法。现有的符号回归方法通常依赖于基于群体的搜索或自回归模型，这些方法在可扩展性和符号一致性方面存在困难。

Method: 引入了一种固定的神经网络架构LIES（对数、身份、指数、正弦），具有可解释的原始激活，通过适当的过采样策略和量身定制的损失函数进行训练，以促进稀疏性并防止梯度不稳定。之后采用额外的剪枝策略简化学习到的表达式。

Result: LIES框架在符号回归基准测试中产生稀疏且准确的符号公式，超越所有基线。通过消融研究也展示了每个设计组件的重要性。

Conclusion: LIES框架能够发现稀疏且准确的符号表达式，解决现有方法的可扩展性和符号一致性问题，进一步简化并提供了超越基线的表现。

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [152] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 论文中提出了一种同时优化神经网络架构和权重的方法，通过嵌入架构和参数信息至潜在空间，并利用梯度下降优化实现，实验显示其能发现高效且表现强劲的模型。


<details>
  <summary>Details</summary>
Motivation: 手动设计神经网络费时费力，而现有的架构搜索和权重优化方法往往将二者离散化。因此，提出一种同时优化架构和权重的新方法显得必要。

Method: 框架首先训练了一个通用的多尺度自编码器，将架构和参数信息嵌入到连续的潜在空间中，然后通过随机初始化点并使用梯度下降优化，结合稀疏和紧凑性惩罚，以获得最佳神经网络。

Result: 在合成回归任务上的实验表明，该方法成功识别了稀疏且紧凑的神经网络，性能表现强劲。

Conclusion: 该论文提出了一种同时优化神经网络结构和权重的新方法，能够有效发现具有强大性能的稀疏和紧凑神经网络。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [153] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: This paper demonstrates a UDE framework to model battery dynamics in smart grids, showing effectiveness and stability in predictions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in modeling battery dynamics in smart grids due to solar input stochasticity and household load profile variability, which traditional methods struggle to handle.

Method: Proposed a UDE-based approach integrating neural networks and ODEs to model node-specific battery dynamics, using synthetic solar generation and load demand data for simulation.

Result: The UDE effectively models battery trajectory, showing smooth convergence and stability in forecasts, aligning closely with the ground truth.

Conclusion: UDE-based SciML approaches are viable for battery modeling in decentralized energy networks and have broader implications for real-time control and optimization in renewable-integrated smart grids.

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [154] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: 该研究系统评估了特征缩放对多种机器学习模型和数据集的重要性，发现不同模型对缩放技术的依赖程度不同，为缩放方法的选择提供了指导。


<details>
  <summary>Details</summary>
Motivation: 为了解决特征缩放研究的严重不足，该研究系统地评估了多种缩放技术对机器学习算法的影响。

Method: 系统地评估了12种缩放技术在14种不同的机器学习算法和16个数据集上的影响，针对分类和回归任务进行了分析。

Result: 实验分析结果提供了关于选择最佳特性缩放技术的模型特定指导。

Conclusion: 研究发现集成方法（例如随机森林和梯度提升模型如XGBoost、CatBoost和LightGBM）在很大程度上表现稳健，不受缩放影响，而其他广泛使用的模型（如逻辑回归、SVMs、TabNet和MLPs）则表现出显著的性能差异，严重依赖于所选的缩放器。

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [155] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: ECON框架通过强化学习和贝叶斯纳什均衡实现高效的多代理协调，比现有方法提升11.2%，具有扩展能力，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有的多代理框架在提高LLMs推理能力的同时，存在较高的计算成本及缺乏收敛性保障的问题。

Method: 通过经济学中的贝叶斯纳什均衡（BNE）和强化学习相结合的方法实现多代理的协调，LLMs根据其对其他代理的策略信仰独立选择最优响应。

Result: ECON在六个复杂推理和规划任务基准测试上平均提升11.2%，并且具有伸缩性，可灵活整合额外模型。

Conclusion: ECON框架通过结合分布式推理和集中输出，实现了效率较高的多代理协调，并证明其比非均衡方案具有更紧的遗憾界限。

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [156] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: 提出了一个新基准AR-Bench，用于评估语言模型的主动推理能力，结果显示当前模型在这方面有显著不足，需发展新的方法来提升其主动推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了弥补主动推理方面的研究不足，提出了一个新的基准来系统性地评估大型语言模型的主动推理能力。

Method: 设计了一个名为AR-Bench的新基准，通过模拟真实世界的场景来评估模型在常识、逻辑和符号推理中主动获取和利用信息的能力。包括侦探案件、情境谜题和猜数字三个任务类型。

Result: 通过AR-Bench的经验评估显示，当前的大型语言模型在主动推理能力上存在显著不足。这表明其被动推理和主动推理能力之间存在明显差距。

Conclusion: 现有的大型语言模型在处理主动推理任务上存在显著困难，这与其在被动推理任务上的表现形成鲜明对比。即便采用高级策略如树搜索或后训练方法，改进也不明显，未达到实际应用的标准。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [157] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: 针对异质文本属性图图模型欠缺的现状，提出了H^2GFM框架，通过新的上下文自适应图转换器，模型在各种图类型和学习场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要关注于同质文本属性图（HoTAGs），而未充分研究异质文本属性图（HeTAGs）的潜力。因此，为了提高图基础模型（GFM）的广泛适用性，本文提出了一种新型框架，以应对同质和异质TAGs的多样性和复杂性。

Method: 1. 引入H^2GFM框架：通过统一文本空间投影多样化的图元关系，并采用上下文编码以捕获空间及高阶语义关系。2. 提出上下文自适应图转换器 (CGT)，用于捕获上下文邻居及其关系的信息。3. 使用CGT专家混合模型来捕捉不同图类型中结构模式的异质性。

Result: H^2GFM在多种HoTAGs和HeTAGs以及不同学习场景下的实验中表现出色，验证了模型的有效性。

Conclusion: H^2GFM在处理不同类型图的能力和表现方面有显著提升。

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [158] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: 论文提出了一种新的时间链路预测模型L-STEP，通过可学习的位置编码在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的位置编码方法在处理复杂属性图时存在不足，缺乏对时间演变拓扑和特征信息的考虑，难以在大规模结构化数据上应用。

Method: 开发了一种可学习的空间-时间位置编码（L-STEP），并提出了一种简单的时间链路预测模型。

Result: 实验表明，L-STEP在13个经典数据集和10种算法中都实现了出色的时间链路预测性能，并采用了3种不同的采样策略进行验证。

Conclusion: 提出的L-STEP模型在时间链路预测任务上表现优异，尤其是在大规模TGB基准测试中处于领先地位。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [159] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 提出了新的理论框架分析PE的收敛性，证明其在模拟中的实际应用价值，并扩展到一般的Banach空间和有界域。


<details>
  <summary>Details</summary>
Motivation: 现有对PE在不同领域表现不一致的现象缺乏理论支持，且已有的理论分析依赖于不现实的假设。

Method: 提出新的理论框架，分析PE的收敛性，并将其与现有的DP合成数据生成方法联系起来，以拓展PE的适用范围。

Result: PE在$d$维有界域敏感数据集中生成的合成数据，具有期望的1-Wasserstein距离为$\tilde{O}(d(n\epsilon)^{-1/d})$，展示了算法的收敛性。

Conclusion: 研究证明了PE在一般Banach空间和有界域$d$维数据集上的最坏情况收敛性，并展示其在模拟中的实际应用。

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [160] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: SCUD模型将已知的离散跳跃时间分布结合到扩散模型中，实现了在图像、文本和蛋白质数据上的超越性表现。


<details>
  <summary>Details</summary>
Motivation: 传统的离散扩散模型依赖于逐渐去噪，但掩码扩散模型凭借其特殊的方法在实际性能上表现更好，这促使研究人员探索其背后原因以及如何进一步提升离散扩散模型的性能。

Method: SCUD方法将已知的跳跃时间分布整合到离散扩散模型中，并将其应用于具有感应偏的图像、文本和蛋白质数据的噪声过程。

Result: 研究表明，使用SCUD改进过的模型在图像、文本和蛋白质数据的处理上，均超越了传统的掩码扩散模型。

Conclusion: 本文提出了一种新的扩散模型SCUD，它通过结合已知的跳跃时间分布来改进离散扩散模型的性能。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [161] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: 本文综述了图提示技术及其在图学习模型适应阶段的应用。


<details>
  <summary>Details</summary>
Motivation: 图学习模型因其在从大规模图数据中学习表达性表示方面的优异表现而备受关注，图提示作为一种潜力巨大的适应策略，有望推动图学习模型的进一步应用。

Method: 本文采用系统综述的方法，介绍了图预训练方法和图提示的主流技术，并总结了图提示在不同领域的实际应用。

Result: 文章总结了近期在图提示技术方面的进展，并阐述了该技术在各领域的应用及未来研究方向。

Conclusion: 本文对图提示技术进行了系统综述，探讨了其在各个领域的实际应用及现有研究中的挑战和未来方向。

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [162] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: 该研究通过简化分析，利用Grönwall不等式简化了扩散模型中的误差分析，并验证了离散噪声能够达到与高斯噪声相似的效果，同时优化了采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有的离散化误差分析过于复杂，因此需要简化的理论框架来分析DDPM中的VP-SDEs。

Method: 通过使用Grönwall不等式，分析了方差保持SDE的Euler-Maruyama离散化，从而在满足Lipschitz假设的情况下，得出了收敛率为$\mathcal{O}(1/T^{1/2})$。

Result: 实验证明，该简化理论能够准确预测误差，并且使用离散噪声可以获得与高斯噪声相当的样本质量，而噪声缩放不当则会降低性能。

Conclusion: 该研究通过简化分析和离散噪声替代，增强了扩散生成建模的理论严密性与实际效率之间的桥梁。

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [163] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: 研究了一种优化框架，允许在不依赖传统控制或强化学习的策略下优化策略参数，成功开发了在自主系统层面计算策略梯度的算法。


<details>
  <summary>Details</summary>
Motivation: 在策略参数指定后，通过优化这些参数可以实现系统的自主性，从而不需要进一步引用控制或动作的概念。

Method: 开发一种优化框架，将策略参数视为自主动态系统的一部分，不依赖于动态规划或强化学习的方法。

Result: 所开发的算法能够在自主系统层面计算与策略梯度、自然梯度、邻域方法等相同的量，并且适用于行为克隆、机制设计、系统识别、状态估计等各种任务。

Conclusion: 通过将控制权转移至指定的策略，优化策略参数可以在不依赖控制或动作的情况下进行，使相关算法能够在自主系统层面计算与策略梯度等价的量。

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [164] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: 研究开发了一种改进的DP-SGD方法，用于关系数据学习，提供实体级差分隐私保证，并实现了效用和隐私之间的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域的数据学习中，保护个体隐私至关重要，但将差分隐私随机梯度下降（DP-SGD）直接应用于关系学习存在挑战。作者旨在解决关系学习中实体参与关系多样性带来的高敏感性问题以及多阶段耦合采样过程使得标准隐私放大分析失效的问题。

Method: 作者进行严谨的敏感性分析，并引入了一种自适应梯度剪辑方案，根据实体出现频率调整剪辑阈值。此外，还将隐私放大结果推广到了一个关于样本大小耦合的可解析子类。

Result: 实验表明该方法在微调文本编码器进行网络结构关系数据学习中，取得了出色的效用-隐私权衡。

Conclusion: 本文提出了一个针对关系型数据的DP-SGD框架，能够在保证实体级差分隐私（DP）的前提下，有效进行模型训练，并且实现了良好的实用性-隐私性权衡。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [165] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct is a novel optimization algorithm that adjusts learning rates based on activation variance, offering competitive speed and generalization compared to Adam and SGD on image classification tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the stability of neuron outputs and improve generalization beyond conventional activation regularization methods.

Method: AdaAct adjusts learning rates according to activation variance, applying neuron-wise adaptivity during training.

Result: AdaAct shows competitive performance on standard benchmarks like CIFAR and ImageNet, demonstrating both good convergence speed and generalization.

Conclusion: AdaAct effectively improves convergence speed and generalization capabilities, bridging the gap between Adam and SGD while maintaining competitive execution times.

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [166] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: NysAct是一种结合一阶与二阶优化方法优点的可扩展梯度预处理方法，实验显示其提升测试准确性同时降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 一阶自适应梯度方法计算效率高且收敛迅速，但泛化性较差。相比之下，二阶方法增强了收敛性和泛化性，但通常带来了较高的计算和内存成本。因此需要一种方法在一阶和二阶优化方法之间取得平衡。

Method: 提出了NysAct，这是一种可扩展的一阶梯度预处理方法，利用特征值偏移的Nystrom方法来逼近激活协方差矩阵，作为预处理矩阵。

Result: NysAct显著降低了时间和内存复杂性，同时对测试准确性影响甚微。实验显示其在测试准确性方面优于一阶和二阶方法，并且比现有二阶方法需更少的计算资源。

Conclusion: NysAct可以在测试准确性方面取得比一阶和二阶方法更好的结果，同时需要的计算资源更少。

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [167] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: AFDB数据库在精细结构上有偏差，影响模型训练。研究提出了DeSAE模型，通过去偏提升了模型在反向折叠任务中的表现。


<details>
  <summary>Details</summary>
Motivation: AFDB结构数据库虽然提供了无与伦比的结构覆盖率和接近实验精度的优点，但其在结构特征分布上表现出几何偏差，影响到对细粒度原子几何敏感的深度模型的训练和性能。

Method: 引入了一种名为去偏结构自动编码器（DeSAE）的模型，该模型通过从故意破坏的骨架几何中学习重建原生构象，以捕捉更稳健和自然的结构流形。

Result: 应用DeSAE于AFDB结构，能够生成去偏的结构，显著提升反向折叠在多个基准测试中的表现。

Conclusion: 该研究提出了一种名为DeSAE的去偏结构自动编码器，用于从故意损坏的主链几何中重建原生构象，并显著提高了反向折叠在多个基准上的性能。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [168] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习算法DPSDP，提高了LLM的推理能力，通过直接偏好学习进行答案优化，解决现有方法的反馈和训练协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有的验证改进方法存在反馈空间限制和不同参与方协调训练不足的问题，导致性能不佳。

Method: 通过将多轮优化过程建模为马尔可夫决策过程，并引入DPSDP（动态规划直接策略搜索），以强化学习算法训练演员-评论家LLM系统。

Result: 实验表明，使用DPSDP的不同基础模型在分布内和分布外基准测试上均有改善。在MATH 500基准上，五次优化步骤的多数投票将首次准确率从58.2%提高到63.2%。

Conclusion: DPSDP可以通过直接偏好学习自生成数据迭代优化答案，提高大型语言模型的性能。多代理协作和分布泛化带来了显著提升。

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [169] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: 本文探讨通过机器学习技术解决物联网恶意流量实时检测问题，提出gcForest结合失衡处理技术在IoT环境中的优越表现。


<details>
  <summary>Details</summary>
Motivation: 随着物联网网络的快速扩展，实时检测恶意流量成为一个关键的网络安全挑战。本文的动机是通过对恶意软件检测的机器学习技术进行全面实证分析，以应对这一检测挑战。

Method: 本文通过实现和比较多种机器学习技术，提出了对IoT-23数据集中的恶意软件检测进行全面经验分析的方法，并通过三种重采样策略解决数据集内的显著类别不平衡问题。

Result: 研究结果表明，通过将适当的失衡处理技术与集成方法（尤其是gcForest）相结合，可以实现比传统方法更好的检测性能。

Conclusion: 本文结论是结合适当的失衡处理技术与集成方法，特别是gcForest，与传统方法相比，可以实现更好的检测性能。这有助于在IoT环境中开发更智能、高效的自动化威胁检测系统。

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [170] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: 提出了新的强化学习教师框架（RLTs），通过详细解释增强蒸馏效果，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习的探索挑战以及推理语言模型作为蒸馏新学生导师的应用需求，推动了新框架的开发。目标是避免强化学习探索的困难，并增强蒸馏效率。

Method: RLTs通过将问题和解决方案以提问形式提供，然后用详细解释引导学生来进行训练。训练过程中会通过将每个解释反馈给学生，并测试学生对问题解决方案的理解来获得密集奖励。

Result: 在实际应用中，RLTs的原始输出在竞赛和研究级任务上表现优于现有的蒸馏和冷启动方法，并且在更大规模学生训练及零样本任务中同样表现良好。

Conclusion: 本文提出了一种新的框架来解决强化学习中的探索挑战。新的强化学习教师（RLTs）能够提供更高效的下游蒸馏效果。即便是零样本任务，RLTs依然能够保持其效果，提升效率和可重复使用性。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [171] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本文采用深度学习和数据增强框架来检测飓风快速增强，解决类别不平衡问题。结果显示数据增强提高了检测效果，空间坐标对模型至关重要。


<details>
  <summary>Details</summary>
Motivation: 快速增强是飓风中的极端事件，其发生相对较为罕见，导致数据集中的类别不平衡。

Method: 使用深度学习、集合学习和数据增强框架来检测飓风快速增强。

Result: 数据增强改善了飓风快速增强的检测效果，并指出空间坐标对模型输入特征至关重要。

Conclusion: 数据增强能够改善飓风快速增强的检测结果，空间坐标作为输入特征在模型中起着关键作用。

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [172] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: Paper introduces FUSE for efficient fuzzy set representation, significantly improving taxonomy expansion tasks by 23% over other methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for set representation such as vectors or boxes are limited and not closed under set operations, necessitating an improved approach that can handle uncertainty in concept modeling.

Method: This paper proposes Fuzzy Set Embedding (FUSE) which relies on volume approximation to represent fuzzy sets, providing a sound and efficient way for set representation learning that satisfies all set operations using minimal neural architecture.

Result: FUSE provides a compact and efficient set representation learning that preserves information and demonstrates significant improvements in taxonomy expansion tasks, outperforming existing baselines by up to 23%.

Conclusion: FUSE achieves a 23% improvement over existing baselines in taxonomy expansion, marking a significant advancement in computing embeddings of fuzzy sets.

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [173] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: 机器学习和新的数据增强技术相结合，提升了三相引擎故障诊断的准确性和效率，具有良好的工业应用前景。


<details>
  <summary>Details</summary>
Motivation: 尽管传统的签名分析方法是标准实践，但通过整合先进的机器学习技术可以提升诊断性能和准确性。

Method: 我们提出了一种称为“Signature-Guided Data Augmentation (SGDA)”的无监督框架，在健康电流信号的频域中合成物理上合理的故障，通过电机电流签名分析引导，创造多样且现实的异常。

Result: 研究结果表明，这种混合方法利用了监督学习和无监督签名分析的优势，在引擎诊断的准确性和可靠性上拥有显著的提高。

Conclusion: 本研究展示了一种将机器学习算法与新型无监督异常生成方法相结合的混合方法，在引擎诊断中取得了更高的准确性和可靠性，具备广泛的工业应用潜力。

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [174] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 通过多次SGD和数据重用改进线性回归的测试误差缩放律。


<details>
  <summary>Details</summary>
Motivation: 解决因数据不足导致的不可持续的缩放问题，通过数据重用改善线性回归的缩放律。

Method: 研究通过多次通过随机梯度下降（SGD）和使用草图特征对M维线性模型进行训练，推导出测试误差界限。

Result: 在数据协方差具有某种幂律谱，且真实参数遵循已对齐幂律谱的假设下，结果表明多次通过SGD获得的测试误差优于一次通过SGD。

Conclusion: 多次通过SGD在特定条件下优于一次通过SGD，数据重用可以改进已有的缩放律。

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [175] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: 本文提出Smooth Bellman Operator (SBO)，通过平滑样本外Q值来改善Q值估计，在基准测试中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习方法在面对分布变化时，样本外动作会导致Q值的高估问题，现有方法虽实施了约束但又过于保守，限制了Q函数泛化能力，阻碍了策略改进。因此需要一种方法可以更好地估计Q值，提升样本外区域的Q函数泛化能力。

Method: 本文提出了Smooth Bellman Operator (SBO)方法，提升样本外区域的Q值估计，并提出了用Smooth Q-function OOD Generalization (SQOG)算法来缓解过于保守的问题。

Result: 实验证明，SQOG在缓解过约束问题方面表现良好，实现了近乎准确的Q值估计，并在D4RL基准测试中超越现有的最先进方法。

Conclusion: 本文提出了一种新的方法Smooth Bellman Operator (SBO)，通过与邻近的样本内Q值进行平滑处理来更新样本外Q值，理论上证明在凸包及其邻域内SBO可以逼近真实的Q值。在D4RL基准上，SQOG算法在性能和计算效率上优于现有的最先进方法。

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [176] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: GALA is a framework for adaptive learning rate adjustment, improving optimizer performance without extensive tuning.


<details>
  <summary>Details</summary>
Motivation: Finding an optimal learning rate often requires extensive tuning. GALA aims to automate this process for large-scale deep learning models.

Method: GALA dynamically adjusts the learning rate by analyzing alignment between consecutive gradients and estimates of local curvature.

Result: Normalized SGD with GALA shows data-adaptive convergence rates in smooth, nonconvex settings. Empirical tests indicate robust performance without tuning necessity.

Conclusion: GALA allows optimizers like SGD and Adam to perform robustly across a variety of initial learning rates without extensive tuning.

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [177] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: 提出了一种新框架HASFL，通过自适应控制批量大小和模型分割，改善了边缘设备在分割联邦学习中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的分割联邦学习方法由于边缘设备的能力不均一性而显著受到滞后效应的影响。

Method: 提出一种异质性感知的SFL框架HASFL，可以自适应地控制批量大小和模型分割，以平衡异质边缘网络中的通信-计算延迟和训练收敛性。

Result: 通过对各种数据集的大量实验验证了HASFL的有效性，并展示了其相对于最先进基准的优越性。

Conclusion: HASFL能够通过自适应控制批量大小和模型分割，有效解决资源异质性问题，优化边缘设备上的训练性能。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [178] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 本文提出FedLeak系统，通过部分梯度匹配和梯度正则化技术，证实联邦学习环境中数据重建的可能性，揭示其隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究争论联邦学习中梯度泄露攻击的有效性和隐私风险，本文旨在实证这种攻击在现实环境中对用户数据的重建能力。

Method: 通过开发FedLeak系统引入部分梯度匹配和梯度正则化两种新技术，并制定基于广泛文献及行业实践的现实评估协议进行性能评估。

Result: 在实际FL环境下，FedLeak能够实现高保真度的数据重建，证明FL系统的显著脆弱性。

Conclusion: 联邦学习系统存在重大漏洞，迫切需要更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [179] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: 研究了一种在线学习的广义委托代理模型，开发了第一个样本效率证明的算法，成功实现了$\tilde{O}(\sqrt{T})$遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 研究在线学习中广义委托代理模型，因为代理人可以通过战略性虚报信息来操纵委托方的学习，委托方需要设计一种机制以最小化战略遗憾。

Method: 采用了延迟机制以鼓励近似短视的代理行为，奖励角度估计方法，结合扇区测试和匹配程序来恢复类型依赖的奖励函数，以及悲观-乐观的LinUCB算法，帮助委托方在不违背代理激励约束的情况下有效探索。

Result: 提供了一个证明有效的算法，并确立了近似最优的$\tilde{O}(\sqrt{T}) $遗憾界限，提升了在线学习算法设计的鲁棒性。

Conclusion: 本文通过设计一种组合算法，有效地解决了在线学习中广义委托代理模型面临的挑战。

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [180] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: TAWM, a time-aware model, enhances performance and efficiency by learning dynamic tasks with varied observation rates; outperforms traditional models in trials.


<details>
  <summary>Details</summary>
Motivation: To improve performance and data efficiency in control tasks by explicitly incorporating temporal dynamics and optimal sampling rates based on system dynamics.

Method: Introduce TAWM, a model-based approach that conditions on time-step size and trains over a range of time-step values to learn task dynamics.

Result: Empirical evaluations demonstrate TAWM's superior performance and data efficiency compared to conventional models.

Conclusion: TAWM consistently outperforms conventional models in varying observation rates across diverse control tasks with the same number of training samples and iterations.

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [181] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY是一个模型驱动的脱离动力学离线RL算法，通过模型展现和表示学习实现目标域探索，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的脱离动力学的离线RL方法受限于来自目标域的有限转换，无法超越离线数据集探索目标域。

Method: MOBODY通过模型展开在目标域生成新的合成转换，并通过表示学习发现跨域的状态和转换的共享潜在表示，同时加入行为克隆损失来稳定训练。

Result: MOBODY在MuJoCo基准测试中表现优异，显著超越现有的最先进基线。

Conclusion: MOBODY显著优于现有最先进的基线方法，尤其在具有挑战性的场景中提高显著。

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [182] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: Reinforced RCSL enhances standard RCSL by addressing its limitations, improving performance in decision-making tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of the stitching property in RCSL, which limits its performance based on the quality of the policy generating the offline dataset.

Method: Reinforced RCSL introduces the concept of in-distribution optimal return-to-go to identify the best achievable in-dataset future return based on the current state.

Result: Reinforced RCSL shows significant performance improvements across various benchmarks compared to standard RCSL.

Conclusion: Reinforced RCSL consistently outperforms the standard RCSL approach in sequential decision-making tasks.

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [183] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 提出MAC算法，通过高效近似降低计算负担并集成于transformers的FIM，表现优于KFAC等方法。


<details>
  <summary>Details</summary>
Motivation: 为了降低二阶优化方法如KFAC的计算负担，同时保持优越的收敛性能。

Method: 分析KFAC中层级Fisher信息矩阵的Kronecker因子，基于其特征谱提出高效近似方法，形成MAC算法。

Result: MAC在准确性、训练时间和内存使用方面的表现优于现有的优化方法，是首个应用于transformers中注意力层的FIM并结合注意力得分进行预处理的Kronecker分解算法。

Conclusion: MAC在各类网络架构和数据集上的表现优于KFAC及其他最先进的方法，具有更高的准确度、更短的训练时间以及更少的内存使用。

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [184] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: AsFT方法通过增加正则项在调整过程中约束模型更新，显著提升了语言模型的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在细致调整过程中容易受到来自恶意或无害数据的安全风险威胁，因此需要一种方法来确保调整过程的安全性。

Method: 我们提出了一种名为AsFT（Anchoring Safety in Fine-Tuning）的细致调整方法，它在训练目标中融入一个正则项。此正则项以对齐方向为锚点，抑制朝有害方向的更新，确保调整在狭窄的安全基内进行。

Result: AsFT在多个数据集上的广泛实验证明，其优于Safe LoRA，减少了7.60%的有害行为，提升了3.44%的模型性能，并在不同的实验设置下保持了稳健的表现。

Conclusion: AsFT能够有效地抑制更新到有害方向上，在较小的安全基内实现细致调整，比Safe LoRA方案在减少有害行为和提高模型性能上都有较好的表现。

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [185] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种结合热力学原则的降阶建模方法，实现了大幅加速和计算成本降低，保留热力学行为。


<details>
  <summary>Details</summary>
Motivation: 设计一种有效的降阶建模方法，用于可变非线性动力系统，保持热力学原则。

Method: 将编码器用于降维，结合泛参数GENERIC形式的神经网络（pGFINN），并加入物理驱动的主动学习策略，使用贪婪的残差基误差指示器进行训练数据采样。

Result: 在Burgers'方程和1D/1V Vlasov-Poisson方程上的数值实验表明，该方法实现了最高3,528倍的加速，1-3%的相对误差，训练成本降低50-90%，推理成本降低57-61%。

Conclusion: 所提出的方法不仅实现了显著的计算成本减少，还揭示了系统的潜在热力学行为，为物理空间动力学提供了宝贵见解。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [186] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 提出了一种抽象-细化技术，提升计算神经网络预测证明性解释的效率和层次化解释。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络解释方法多依赖于启发式，缺乏形式上可证明的保证。计算具有形式保证的解释面临可扩展性挑战。

Method: 提出了一种新颖的抽象-细化方法，以有效计算神经网络预测的可证明充分解释。通过构建简化网络并逐步增加其规模以达成收敛，从而加速验证。

Result: 实验表明，该方法在提升获取神经网络预测的证明性解释效率的同时，还可提供各层级的细致可解释性。

Conclusion: 为了加速验证并得到证明充足的解释，该方法使用抽象-细化技术来创建一个简化版网络，该简化网络的解释也适用于原网络。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [187] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: 本文提出了SHAM和DiffGradCAM，以提升CAM在对抗性条件下的鲁棒性，并成功验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准的CAM易受被动欺骗的对抗性操控，需要一种新的方法来提高其鲁棒性。

Method: 引入了Salience-Hoax Activation Maps (SHAM)作为一种在对抗性条件下测试CAM鲁棒性的基准，并提出了一种新的DiffGradCAM方法，以解决被动欺骗的漏洞。

Result: 验证表明，DiffGradCAM在不对抗的情况下与标准CAM方法（如GradCAM）的输出相匹配，并且不易受到被动欺骗。

Conclusion: 将SHAM和DiffGradCAM结合起来，可以建立一个新的框架，以测试和提高基于显著性解释的鲁棒性。

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [188] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: The ML4CFD competition demonstrated ML models can outperform traditional CFD solvers, with top teams offering insights into effective surrogate modeling approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in applying ML to scientific domains and benchmark progress through competition.

Method: Organized ML4CFD competition focusing on surrogate modeling for aerodynamic simulations using a dataset from OpenFOAM, evaluated through a multi-criteria framework.

Result: Top competition entries surpassed the performance of OpenFOAM solver on aggregate metrics, identifying superior designs in ML surrogate models.

Conclusion: ML-based surrogates show promise in outperforming traditional solvers for specific criteria in CFD simulations.

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [189] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: 研究表明在特定学习率下，神经网络训练可实现探索与利用的平衡，能更快地达到测试精度。


<details>
  <summary>Details</summary>
Motivation: 传统的算法通常局限于利用型的松弛动态，如梯度下降法，但在较大的学习率下可能存在探索与利用平衡的情况。

Method: 研究神经网络在训练过程中以不常规的大学习率的动态轨迹，通过分析最大Lyapunov指数来解释初始条件的敏感依赖性。

Result: 在某些学习率区间内，训练时间在达到测试集的可接受精度时达到最小值，这意味着可以通过定位于混沌的出现来加速训练。

Conclusion: 瞬态混沌动态在人工神经网络训练中具有建设性作用，使其不仅限于某一种学习任务，而是适用于多个任务和架构。

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [190] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: 该研究首次将进化多目标网络架构搜索应用于自动驾驶的大规模强化学习中，通过进化算法和并行技术进行优化，结果显示，比手动设计的模型参数更少而性能更优。


<details>
  <summary>Details</summary>
Motivation: 首次将进化多目标网络架构搜索应用于大规模强化学习的神经网络架构优化，尤其是在自动驾驶领域。

Method: 使用进化算法和并行化技术进行网络架构搜索，并结合师生模型确保可扩展优化。

Result: 实验结果表明，EMNAS在减少参数的同时提高了奖励率，优于手动设计的模型。

Conclusion: 研究表明，定制的EMNAS比手动设计的模型性能更优，能够用更少的参数实现更高的回报。

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [191] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm is an advanced reasoning LLM for communication system formulation, using unique training strategies to outperform existing models.


<details>
  <summary>Details</summary>
Motivation: The need to advance 6G and future wireless communication technologies requires specialized models, as existing general LLMs lack the necessary domain knowledge and reasoning capabilities.

Method: DeepForm utilizes a two-stage training strategy: Supervised Fine-Tuning with Chain-of-Thought data, followed by a novel rule-based Reinforcement Learning algorithm, C-ReMax, for advanced modeling.

Result: DeepForm significantly outperforms larger proprietary LLMs in various scenarios, demonstrating superior reasoning and modeling capabilities.

Conclusion: DeepForm achieves state-of-the-art performance in automated communication system formulation, surpassing larger proprietary LLMs.

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [192] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 研究表明，用线性分类器分析LLM激活的几何结构无法在不同任务间转移，复杂方法未能克服这一限制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各种任务上表现出色，但其可靠性仍然存在争议。部分研究尝试通过分析推理时的激活情况来评估模型答案的正确性。

Method: 利用线性分类器分析不同任务中激活情况的几何结构，尝试找出能够准确生成答案的激活与产生错误的激活之间的区别。并使用稀疏性正则化器来测试不同任务间分类器的相似性。

Result: 发现这些几何结构本质上是依赖于特定任务的，无法在任务间进行转移，即不同任务间训练的线性分类器几乎没有相似性，它们的支持空间几乎不交叠。

Conclusion: 复杂的方法无法解决此限制，因为用于分类的激活向量在跨任务时形成了明显分离的聚类。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [193] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: 研究提出SLEEPYLAND框架和SOMNUS集成模型，提高了自动睡眠分期的泛化性能，克服了模型偏见，超过了传统方法和人类评分者。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在自动睡眠分期中的公平模型评估、跨多样数据集的泛化、模型偏见以及人工注释的可变性等挑战。

Method: 提出了开源睡眠分期评估框架SLEEPYLAND，并引入了名为SOMNUS的集成模型，通过软投票合并不同架构和通道设置下的模型，评估模型在标准条件下的性能。

Result: SOMNUS在各个数据集上展现了强大的性能，显著优于个别模型和传统方法，并在多注释数据集上胜过最佳的人类评分者。此外，引入的集成分歧指标能够预测评分者不一致的区域，提供人类不确定性的数据驱动代理。

Conclusion: SOMNUS在24个不同数据集上表现出稳健的性能，宏F1分数介于68.7%到87.2%之间，在94.9%的情况下优于单个模型。在多个OUD多注释数据集上，SOMNUS超过了最佳人类评分者，更好地再现了评分者的一致性。

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [194] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: 该研究提出了一种新方法，通过生成式AI提高下水道系统预测精度，并在真实数据上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 提高在极端天气情况下的下水道系统预测精度。

Method: 开发了一种基于扩散的模型来处理多元时间序列数据，并使用符合推断技术来校准预测。

Result: 实验证明模型在真实下水道系统数据上具有优异的性能，能够在严峻天气条件下保持预测的准确性。

Conclusion: 这个研究提出了一种新的深度学习方法，通过生成式人工智能来提高下水道系统中上下文预测的准确性。

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [195] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: 本文介绍了一种新型的Python库CALT，它简化了通过深度学习进行符号计算任务的流程，推动非专家参与这一领域。


<details>
  <summary>Details</summary>
Motivation: 近年来人工智能的进步显示了通过端到端深度学习进行符号计算的可学习性，研究着力于让Transformer模型模拟计算，这为符号计算领域带来了挑战和新的研究方向。

Method: 该工作提出了一种名为CALT（Computer Algebra with Transformer）的Python库，旨在帮助非深度学习专家训练用于符号计算任务的模型。

Result: 该库为非专家提供了一种友好的工具，使他们能够使用深度学习技术来进行符号计算任务。

Conclusion: CALT库能简化符号计算任务中模型训练的复杂性，使更多人能够参与相关领域的研究。

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [196] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: 物理约束的生成框架（PBFM）比传统方法提高了物理残差和分布精度，无需超参数调整，适用于替代建模和加速仿真。


<details>
  <summary>Details</summary>
Motivation: 现有的生成机器学习方法如扩散模型和流匹配虽然在模拟复杂系统行为和建立高效替代模型方面表现出色，但往往只从数据中隐式学习基础物理规律。

Method: 提出了一种新的生成框架—基于物理的流匹配（PBFM），显式地将物理约束（包括偏微分方程残差和代数关系）嵌入到流匹配目标中，采用时间展开来提高最终样本预测精度，并同时最小化流匹配损失和基于物理的残差损失。

Result: 在三个代表性偏微分方程问题上进行的基准测试中，PBFM方法的物理残差精度比FM方法高8倍，并在分布精度方面明显优于现有算法。

Conclusion: PBFM方法在代表性的PDE问题上进行了广泛的基准测试，证明其物理残差精度比传统FM方法高8倍，并在分布精度上明显优于现有算法，提供了一个有效的框架用于替代建模、不确定性量化和加速仿真。

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [197] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: CASE算法通过选择性探索策略高效选择few-shot示例，相比现有技术提高7倍速度并减少87%的LLM调用。


<details>
  <summary>Details</summary>
Motivation: 寻找一种高效选择few-shot examples的方法，以在有限的上下文长度预算下构建有效的提示。

Method: 提出了一种名为CASE的选择性探索策略，通过维护“挑战者”决策臂候选库，仅拉动候选库或当前最优集合中的一个臂，从而减少样本复杂度和LLM的评估次数。

Result: CASE在运行时间上实现了高达7倍的速度提升，并且所需的LLM调用次数减少了87%，而不影响与最先进示例选择方法相比的性能。

Conclusion: CASE在选择demonstration samples方面表现出色，能够在不降低性能的情况下显著减少所需的LLM调用次数，并大幅提高计算效率。

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [198] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: 引入HSG-12M数据集，重新定义了空间多重图学习，开拓了基于几何的图学习和科学发现的新版图。


<details>
  <summary>Details</summary>
Motivation: 现有的图基准假设非空间的简单边，合并物理上不同的路径。为此，本文引入了嵌入度量空间的空间多重图，在其中保留几何上不同的路径作为独立的边。

Method: 引入HSG-12M数据集，其中包含静态和动态的Hamiltonian光谱图；同时，发布了Poly2Graph开源流水线，将任意一维晶体哈密顿量转化为光谱图。

Result: 在多边几何大规模学习中，现有的GNNs揭示了新的挑战。此外，还展示了光谱图作为多项式、向量和矩阵的通用拓扑指纹，建立了新的代数到图的联系。

Conclusion: HSG-12M数据集为几何感知图学习奠定了基础，并在凝聚态物理及其他领域中提供了数据驱动的科学发现的新机会。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [199] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: 提出TiViT框架，将时间序列转换为图像，利用视觉Transformer，取得了时间序列分类的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 由于公开的时间序列数据集稀缺，对时间序列基础模型的开发有限，因此提出通过转化为图像利用视觉模型的可能性。

Method: 提出Time Vision Transformer (TiViT)框架，将时间序列转换为图像，以利用大型预训练的视觉Transformer模块的表示能力，并使用OpenCLIP模型的隐藏表示进行实验。

Result: TiViT在标准时间序列分类基准上取得了最先进的性能，并通过与TSFM特征的结合，进一步提高了性能。

Conclusion: TiViT框架通过将时间序列转换为图像，并利用大型预训练视觉模型的隐藏表示，达到当前最优的时间序列分类性能，并在分析中发现与TSFM表征空间有强互补性。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [200] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出一种新的半梯度DICE方法，解决现有方法削弱OPE能力的问题，确保准确成本估计，达到离线约束RL领域的最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有增强DICE离线RL性能的方法会削弱其OPE能力，不适用于约束RL，需解决这一问题。

Method: 提出一种新的方法，通过半梯度DICE进行OPE和约束RL，确保准确的成本估计。

Result: 新方法在确保准确成本估计的同时，实现了离线约束RL的最优性能。

Conclusion: 新方法能够在不影响OPE能力的情况下进行离线约束RL，并在DSRL基准上实现了最先进的性能。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [201] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: RP-KrossFuse方法通过结合CLIP等跨模态嵌入与单一模态嵌入，实现了模态特定任务中的高性能，同时保持了跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 单一模态嵌入在各自领域表现优异，但缺乏跨模态对齐能力，跨模态嵌入在单一模态任务上表现不如最先进的单模态嵌入，因此需要一种方法来统一跨模态和单模态嵌入，以在个别模态内达到模态专家嵌入的性能，同时保持跨模态对齐。

Method: RP-KrossFuse方法利用基于随机投影的克罗内克积，将跨模态嵌入与单一模态嵌入融合，并在指定的核空间中有效运行，支持通过随机傅里叶特征实现可扩展的实施。

Result: RP-KrossFuse通过多个数值实验展示了其有效性，将CLIP嵌入与单模态图像和文本嵌入相结合，其数值结果表明RP-KrossFuse在保持跨模态对齐的同时，实现了有竞争力的模态特定性能。

Conclusion: RP-KrossFuse方法成功在保持跨模态对齐的同时，实现了与单一模态嵌入相竞争的性能，缩小了跨模态嵌入与单一模态嵌入之间的差距。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [202] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: JoFormer通过新的注意力机制改进了Transformer对位置信息的处理，在语言建模任务中表现出更低困惑度和更快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在序列建模方面表现出色，但如何有效地结合位置信息仍然是一个具有挑战性和活跃的研究领域。

Method: 我们提出了一种基于行程的Transformer架构，即JoFormer，该架构建立在一个非交换代数的基础上，用于在位置之间进行变换的组合。我们从第一原理推导出JoFormer的注意力机制，并证明它包含了如旋转变换等标准方法作为特例。

Result: JoFormer在Tiny Shakespeare字符级语言建模任务中表现出较低的困惑度和更快的收敛速度，验证了其基于行程的位置信息处理方式的优势。

Conclusion: 我们通过本文的研究展示了JoFormer在语言建模任务中的优越表现，并探讨了其作为一种将位置结构集成到Transformer架构中的方法。JoFormer在性能和收敛速度上优于RoFormer，这证明了它的有效性。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [203] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: 简单的k-NN基准在网络流量分类中表现出色，且比复杂模型更具优势，通过分析12个数据集和15个任务，揭示了冗余样本对模型性能的影响，并提出了重新定义任务和评估的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管复杂的神经网络在网络流量分类中表现出色，但简单的k-NN基准也能达到甚至超过复杂方法的效果，因此需要深入研究这一现象。

Method: 通过比较12个数据集和15个分类任务中简单的k-NN模型的性能，分析其表现的原因。

Result: 分析表明，大多数数据集包含超过50%的冗余样本，这导致模型性能被高估，同时对标准机器学习实践在网络流量分类中的适用性提出质疑。

Conclusion: 提出了一种简单的k-NN基准，可以与复杂的网络流量分类方法相媲美，甚至表现更好。调查发现大多数数据集包含大量冗余样本，这导致模型性能被高估，并提出了解决这些挑战的新方向。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [204] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: The paper introduces ChannelTokenFormer, a Transformer model for accurate multivariate time series forecasting, addressing real-world challenges like asynchronous sampling and missing values.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is to address the challenges in time series forecasting related to channel dependency, sampling asynchrony, and missing data, which are not effectively handled by existing models due to overly simplified assumptions.

Method: The authors propose a Transformer-based model called ChannelTokenFormer, which incorporates a flexible architecture tailored for cross-channel interactions, asynchronous sampling adaptation, and missing value handling.

Result: ChannelTokenFormer demonstrates superior robustness and accuracy in forecasting under real-world conditions through experiments on benchmark and industrial datasets.

Conclusion: ChannelTokenFormer provides a more accurate and robust solution for multivariate time series forecasting in real-world settings compared to traditional methods.

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [205] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: 提出了一种微调训练步骤，使得在图像压缩中更好地处理量化噪声，从而提高编码效率。


<details>
  <summary>Details</summary>
Motivation: 尽管变分自编码器在图像压缩方面取得了进展，但在训练过程中考虑量化仍是一个问题。因此，本文提出了一种额外的微调训练步骤，以提高网络性能。

Method: 本文提出了一种在常规端到端训练后，在推理阶段对量化潜变量进行再训练的方法。这种方法对量化噪声进行更准确的建模，通过细化训练步骤，提高网络的率失真效率。

Result: 通过在正确量化的数据上进行再训练，可以提高编码效率，尤其在熵约束量化方面，不增加推理复杂性。

Conclusion: 本文提出了一种额外的微调训练步骤，可以在不增加推理复杂性的情况下，实现额外的编码增益，特别是在熵约束量化方面。在Kodak测试集上平均节省1%到2%的比特率，在TecNick测试集上节省高达2.2%。

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [206] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 我们通过大语言模型生成的蓝图与提示模板搜索机制，提高了小语言模型的推理能力，提升了其在多项任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型由于容量有限，其推理能力受到限制且对提示变化敏感。我们的框架旨在克服这些挑战。

Method: 我们提出了一种通过大语言模型生成蓝图增强小语言模型推理能力的新框架，并集成了一个提示模板搜索机制。

Result: 该框架显著提高了小语言模型在数学、编码和逻辑推理任务中的性能。

Conclusion: 我们的框架提高了小语言模型在各种任务中的性能，包括数学、编码和逻辑推理，而无需增加模型规模或进行额外训练。

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [207] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: 研究公平聚类中的共识聚类问题，开发逼近算法，并证明问题的NP困难性。


<details>
  <summary>Details</summary>
Motivation: 研究如何将现有聚类最小化修改以满足公平性要求，这是许多需要公平代表性的聚类应用中的一个关键后处理步骤。

Method: 开发了一种用于平等组代表的数据集的最优算法，以及用于不同比例两组大小的一般场景的近线性时间常数因子逼近算法。还证明了对于两个大小不等的组，该问题是NP困难的。

Result: 提供了恒定因素逼近算法来解决公平聚类问题，并证明了对于两个大小不等的组，该问题是NP困难的。

Conclusion: 本文首次解决了公平聚类的共识聚类问题，并实现了恒定因素逼近。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [208] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: IS-DAAs通过重要性采样解决DAAs过度优化问题，且性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: DAAs更容易受到过度优化的影响，模型会偏离参考策略，导致性能下降，因此需要新的方法来解决这个问题。

Method: 提出了一种新的重要性采样方法，称为IS-DAAs，通过乘以一个考虑参考策略分布的重要性比率来缓解离线DAAs的过度优化问题，并通过限制重要性比率的最大值来避免高方差问题。

Result: 实验表明IS-DAAs在缓解过度优化方面效果显著，特别是在低正则化强度下表现优于其他解决该问题的方法。

Conclusion: IS-DAAs方法有效缓解了DAAs的过度优化问题，尤其是在低正则化强度下，表现优于其他方法。

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [209] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: 提出了VAE-LF模型以处理高维不完整电力负载数据，结果明显优于其他模型，特别是在低稀疏率数据上。


<details>
  <summary>Details</summary>
Motivation: 智能电网的发展带来了高维和不完整的数据，这对电力负载预测模型提出了挑战，本文旨在针对这一问题提出解决方案。

Method: 本文提出了一种基于变分自编码器（VAE）的潜在特征化模型VAE-LF，用于高效表示和补充电力负载监测的缺失数据。该方法通过将高维不完整电力负载监测数据分割成向量，并依次输入到VAE-LF模型中，生成补全数据。

Result: 在UK-DALE数据集上的实验表明，VAE-LF模型在5%和10%稀疏测试用例中优于其他基准模型，具有显著更低的均方根误差（RMSE）和平均绝对误差（MAE），特别是在低稀疏率数据上表现更优。

Conclusion: VAE-LF模型在处理高维和不完整的电力负载监测数据方面表现优异，能够更高效地补全缺失数据。

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [210] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: 提出了一种基于基准的非侵入性框架R-ICE，用于估算大语言模型的碳排放，展现了巨大的潜力和良好的验证结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的使用对能源网和环境造成了显著负担，可能阻碍组织的可持续性目标。需要有工具来估算能耗或碳排放。

Method: 提出了一种名为R-ICE的框架，利用现有的最先进的基准来估算提示级别的推断碳排放。

Result: 验证结果显示，该框架在不影响准确性的情况下提供了一种更实用且非侵入性的方式来实现动态LLM路由和碳核算等新兴用例。

Conclusion: 基于基准的建模在推断排放估算中具有巨大潜力，应进一步探索。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [211] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: 随机奖励扰动（RRP）是一种新探索策略，增强了策略多样性和探索范围，提高了强化学习算法的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 提升强化学习中的探索策略，实现更高的样本效率和逃离局部最优解。

Method: 引入随机奖励扰动（RRP），通过理论分析证明在环境奖励中添加零均值噪声可以有效提升训练中的策略多样性，从而扩展探索范围。

Result: RRP显著提升了近端策略优化和软行动-评论家算法的性能，表现出更高的样本效率，并能在稀疏和密集奖励情景中逃离局部最优。

Conclusion: RRP与现有行动扰动探索策略完全兼容，可以为探索效果提供附加提升，是一种通用且轻量化的策略，能够与现有强化学习算法结合，几乎不需要实施努力和计算开销。

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [212] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: 研究提出结合政府评估和众包报告的GNN模型，提高城市事件预测准确性，揭示数据中存在的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前城市时空预测中的GNN应用面临数据稀疏和众包报告行为异质性带来的偏差问题，需要开发一种模型来同时利用这两种类型的数据以提高预测准确性。

Method: 采用多视图、多输出GNN模型，该模型利用无偏的政府评估数据和存在偏差的众包报告数据来预测事故的真实潜在状态。

Result: 实地研究纽约市的城市事件，收集并公开了一个包括9615863条众包报告和1041415条政府评估数据的综合数据集。实验结果表明，所提模型在预测潜在状态上优于单独使用众包报告数据或评估数据的模型。

Conclusion: 本文提出了一种结合政府评估数据和众包报告数据的多视图、多输出的图神经网络 (GNN) 模型，可以更准确地预测城市事件中事故的潜在真实状态，并量化众包报告中的人口统计偏差。

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [213] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: 研究提出了一个用于深度神经网络谱稳定性的普遍定理，拓展了适用于稀疏和非独立同分布权重情形的初始化方案的理论。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络中梯度消失或爆炸问题，并扩展对输入输出Jacobian的谱行为的理解。

Method: 通过随机矩阵理论建立一个普遍稳定性定理，以容纳深度神经网络中的稀疏性和非独立同分布（非i.i.d.）、弱相关权重。

Result: 为结构化和相关随机性的现代神经网络的初始化方案提供了更广泛的理论基础，确保更广网络模型中的谱稳定性。

Conclusion: 本研究扩大了深度神经网络初始化方案的理论基础，允许更复杂的网络结构并保证其谱稳定性。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [214] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Main category: cs.LG

TL;DR: 提出了一套设计模式，用于增强AI代理对提示注入攻击的抵抗力，并通过实际案例研究进行了验证和分析，以确保其在实用性和安全性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）驱动的AI代理在各类任务中变得愈发强大和多样化，保障其安全性成为了一个关键挑战。尤其是提示注入攻击，当代理获得工具访问权限或处理敏感信息时，这种攻击尤为危险。

Method: 提出并系统分析了一套设计模式，这些模式旨在增强AI代理抵抗提示注入攻击的能力。同时，通过一系列案例研究来展示这些设计模式的实际应用。

Result: 通过实施和分析提出的设计模式，本研究演示了可以在增强AI代理的提示注入攻击抵抗力的同时，权衡其实用性和安全性。

Conclusion: 提出了一组设计模式，可以让AI代理在应对提示注入攻击时具有可证明的抵抗力。这些设计模式经过系统分析，并在实际案例中展示了其应用性。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [215] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 本文解决了数据保护限制下社会经济研究中缺失数据插补的挑战，通过IMAGIC-500数据集评估各种插补方法的效果。


<details>
  <summary>Details</summary>
Motivation: 社会经济研究中的缺失数据插补是数据科学和机器学习的一个关键挑战。然而，真实的社会经济数据集通常受到严格的数据保护协议限制，通常禁止公众共享，即使是合成衍生物。这大大限制了在这种环境中基准研究的可重现性和可获取性。此外，公开可用的合成数据集很少。因此对社会经济数据集的插补方法的系统评估量化很有限。

Method: 我们使用世界银行公开的合成数据集，选择了一个包含大约50万人和约10万个家庭、19个社会经济特征的子集IMAGIC-500，设计反映了真实家庭调查的层次结构。本文在IMAGIC-500上引入了一个全面的缺失数据插补基准，在各种缺失机制（MCAR，MAR，MNAR）和缺失比率（10%、20%、30%、40%、50%）下进行评估，考虑了连续和分类变量的插补精度、计算效率以及对下游预测任务的影响。

Result: 评估结果揭示了统计、传统机器学习和深度学习插补技术的优缺点，包括最近的扩散方法。

Conclusion: 结果显示统计方法、传统机器学习和深度学习插补技术的优势和劣势，其中包括最新的扩散方法。IMAGIC-500数据集和基准旨在促进稳健的插补算法发展并推动可重复的社会科学研究。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [216] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: 提出了Agile Reinforcement Learning算法，解决边缘计算中软实时应用的任务调度问题，通过知情探索和动作遮掩方法提高击中率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前的调度器依赖于启发式及元启发式算法，难以适应边缘计算环境中的复杂动态条件。因此，有必要开发新的调度方法来克服此类复杂性及动态性问题。

Method: 提出了Agile Reinforcement Learning (aRL)，通过知情探索和动作遮掩方法减少无关动作的探索，以提高RL代理的适应能力及收敛速度。

Result: aRL在实验中表现出比基线方法更高的击中率和更快的收敛速度。

Conclusion: 通过采用知情探索和动作遮掩方法，增强了强化学习代理的可预测性，从而实现快速适应和收敛，使得Agile Reinforcement Learning成为边缘计算中软实时应用任务调度的合适候选方案。实验表明，与基线方法相比，aRL在击中率和收敛速度上表现更优。

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [217] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: 介绍结构引导的GNN，通过新图结构提升异质性数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在异质性数据上表现不佳的问题，通过链接结构相似节点提高标签同质性。

Method: 提出SG-GNN，通过创建新图结构连接具有相似结构属性的节点，并在新旧图结构间进行自适应学习。

Result: SG-GNN在多个基准数据集上展示了优异或极具竞争力的性能，特别是针对异质性数据集。

Conclusion: 结构引导的GNN (SG-GNN)在异质性数据集上表现出色，能够利用结构信息提升GNN性能。

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [218] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 通过引入自我认知的弱点驱动问题合成框架，提升了大型语言模型在复杂推理任务中的表现，平均性能提升约10%。


<details>
  <summary>Details</summary>
Motivation: 现有用于强化学习的数学问题集缺乏高质量和可验证的答案，限制了现行模型的有效性。并且，大多数问题合成策略未考虑模型能力，导致效率低下。

Method: 提出了自我认知的弱点驱动问题合成框架 (SwS)，通过识别模型在训练中持续失败的问题，从中提取核心概念，并合成新问题加强模型的训练。

Result: 提出的框架在不依赖外部知识蒸馏的情况下，使模型能够自我识别和解决弱点，实现了显著的泛化能力，与传统方法相比，在主流推理基准上，7B 和 32B 模型的平均性能分别提高了10.0%和7.7%。

Conclusion: 引入了一种自我认知的弱点驱动问题合成框架 (SwS)，通过识别模型的缺陷并利用这些缺陷加强模型，从而提升了在复杂推理任务上的表现。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [219] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: 研究比较了多种数据插补技术，发现这些技术能显著提高水消费数据的质量，从而改善水分配网络管理，特别是在漏水检测和预测性维护方面。


<details>
  <summary>Details</summary>
Motivation: 智能水表虽能提供详细数据，但技术问题可能导致数据缺失，从而显著影响操作决策和效率。因此，研究旨在探讨应用数据插补技术，以增强水分配网络的智能监测和管理。

Method: 采用了多种数据插补方法，包括k-最近邻算法、MissForest、Transformer和递归神经网络。这些技术被用来提高基于物联网水网监控部署的智能水表数据的监测和管理。

Result: 通过各种插补方法的比较研究，表明有效的数据插补能显著提升水消费数据的准确性和可靠性，有助于泄漏检测和预测性维护调度。

Conclusion: 通过比较不同的数据插补方法，该研究表明有效的数据插补能显著提升从水消费数据中得出的见解质量，从而改善泄漏检测和预测性维护等应用。

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [220] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 测试时间扩展提高LLM推理能力，但现有模型外插能力不足。通过上下文中的探索训练方法e3，我们实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型（LLM）在难题上的性能，特别是在超越其训练最大token预算时通过扩展思考改进推理能力。

Method: 提出了一种称为e3的方法，包括三个关键要素：在上下文中探索能力，通过负梯度增强探索，以及结合任务难度和训练预算设计课程。

Result: 我们的e3方法创造了最佳的1.7B模型，在AIME'25和HMMT'25评分中表现优秀，并实现了2倍于训练token预算的外插性能。

Conclusion: 我们的方法e3通过在上下文中的探索实现了推理模型的外插。从而在测试时间预算上有效运用通过串联操作来提高性能。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [221] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: 论文提出了一种动态概率典型相关分析框架，称为InfoDPCCA，通过信息论目标提取共享潜在表示，提升了建模两个序列的互信息，同时学习单独的潜在组件，通过实验验证其优异的表现。


<details>
  <summary>Details</summary>
Motivation: 从高维时序数据中提取有意义的潜在表示是机器学习的一项重要挑战，涵盖自然科学和工程领域的应用。

Method: 引入InfoDPCCA，一个动态概率化的典型相关分析（CCA）框架，用于建模两个互相关联的观察序列。

Result: 实验表明，InfoDPCCA在表示学习工具中表现优异。

Conclusion: 我们证明了InfoDPCCA在表示学习中很出色，并提高了解释性和鲁棒性。

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [222] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: SeerAttention-R是一个增强推理模型长解码性能的稀疏注意力工具，易于集成并显示出显著的速度和精度优势。


<details>
  <summary>Details</summary>
Motivation: 为了改善推理模型在长解码任务中的表现，开发了一个专门的稀疏注意力框架。

Method: 引入SeerAttention-R框架，使用自我蒸馏的门控机制学习注意力稀疏性，并通过去除查询池以支持自回归解码。

Result: 实现了在AIME基准测试中，在大稀疏注意力块设置下，保持了几乎无损的推理精度，在90%的稀疏情况下，在H100 GPU上，以TileLang开发的新解码内核实现了理论上最高可达9倍的速度提升。

Conclusion: SeerAttention-R框架易于集成到现有预训练模型中，并在不修改原始参数的情况下，提供了显著的性能提升，特别是在长解码任务中表现出色。

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [223] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本研究提出InFOM模型，通过流匹配和用户意图变量提高强化学习模型的适应性和表现。


<details>
  <summary>Details</summary>
Motivation: 将大规模预训练的框架应用于强化学习，以解决样本效率和稳健性问题。

Method: 构建了一个概率模型，通过流匹配预测代理将在时间上遥远的未来访问哪些状态，并在模型中包括捕捉用户意图的潜在变量。

Result: 相较于其他预训练方法，InFOM在36个基于状态和4个基于图像的基准任务上，回报中位数提升了1.8倍，成功率提高了36%。

Conclusion: 提出了一种新的预训练方法InFOM，提高了在多个基准任务上的回报和成功率。

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [224] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: 该研究提出了两种多实验方程学习方法以提高模型普适性，并降低恢复参数的误差。


<details>
  <summary>Details</summary>
Motivation: 方程学习（EQL）方法可以从代理建模（ABM）数据中推导连续模型，但通常需要针对每个参数集进行大量模拟，普适性存在疑虑。

Method: 提出了两种多实验方程学习（ME-EQL）方法：逐次单参数（OAT ME-EQL）和嵌入结构（ES ME-EQL）。OAT ME-EQL为每个参数集学习单独模型，并通过插值连接它们，而ES ME-EQL在所有参数上构建统一的模型库。

Result: 实验结果表明，两种方法都显著降低了从代理建模模拟中恢复参数的相对误差，其中OAT ME-EQL在参数空间普适性上表现更佳。

Conclusion: 本文提出的多实验方程学习方法（ME-EQL）可以提高从自组织生物系统中学习模型的普适性和可解释性。

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [225] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: 提出LMDI+方法改进局部特征重要性评估，优于现有方法LIME和TreeSHAP，显著提高性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 随机森林等树型集成模型在高风险领域中广泛应用，要求高解释性来确保可信预测；现有局部特征重要性方法依赖近似且受限于模型内部结构，引发对改进方法的需求。

Method: 论文提出了Local MDI+ (LMDI+)框架，通过扩展MDI+方法到样本特定设定，超越现有的局部特征重要性方法如LIME和TreeSHAP。

Result: LMDI+在识别实例特定信号特征方面优于LIME和TreeSHAP，平均提高下游任务性能10%，并展示了在实例级特征重要性排名上的更大稳定性。

Conclusion: LMDI+方法不仅提高了实例特定信号特征识别的性能，还在多个随机森林模型中表现出更大的稳定性，支持更好的解读性应用案例。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [226] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: BioLangFusion通过简单有效的融合技术将预训练的多模态模型结合，显著提升了分子属性预测的表现。


<details>
  <summary>Details</summary>
Motivation: 受到分子生物学中心法则的启发，信息从基因流向转录本再到蛋白质，研究者尝试在生物学上具有意义的密码子水平上对齐每种模式的嵌入，以确保直接的跨模式对应关系。

Method: 利用三种标准融合技术来整合不同模式的信息：（i）密码子级别的嵌入串联，（ii）受多实例学习启发的熵正则化注意力池化，（iii）跨模态多头注意力。

Result: 在五项分子性质预测任务中，BioLangFusion的表现优于强大的单一模态基线，表明即使是简单的预训练模型融合也能以最小的开销捕捉到互补的多组学信息。

Conclusion: BioLangFusion能够简单有效地将预训练的DNA、mRNA和蛋白质模型融合为统一的分子表示，超越强大的单模基线。

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [227] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: KARMA通过动态的时间通道分解和多尺度信息处理，显著提升了时间序列预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 多变量长期高效时间序列预测需求迫切，但传统分解方法和基于Transformer的模型存在局限性，需要新的方法来适应复杂时间序列的动态特性。

Method: KARMA引入了自适应时间通道分解模块（ATCD）和混合频率-时间分解模块（HFTD），结合多尺度的基于Mamba的KarmaBlock，实现全局和局部信息的高效处理。

Result: 通过在八个不同领域的真实世界数据集上进行实验验证，KARMA在预测准确性和计算效率上均明显优于其他主流方法。代码和完整结果可以在指定的GitHub库中查看。

Conclusion: KARMA显著优于主流基线方法，在预测准确性和计算效率上都有显著提升。

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [228] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 本文探讨了DRL在环境状态扰动下的鲁棒性问题，提出了一种攻击方法及其增强防御框架BAT，并证明其有效。


<details>
  <summary>Details</summary>
Motivation: 提高DRL代理的鲁棒性，以应对在广泛使用的威胁模型中少有考虑的环境状态扰动问题。

Method: 提出了一种非定向攻击方法作为校准对手，并提出了名为增强对抗训练（BAT）的防御框架，通过监督学习调整代理以避免灾难性失败，随后通过强化学习对抗性训练代理。

Result: 实验结果证明主流代理在环境状态扰动下的脆弱性以及本文提出的攻击方法的有效性。

Conclusion: BAT框架能够显著增强DRL代理在各种情况下对环境状态扰动的鲁棒性，而现有的鲁棒强化学习算法可能不适用于这种情况。

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [229] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: 研究提出了一种数据增强框架，能够使少样本数据训练的生成奖励模型达到与大规模数据训练模型相似的性能。


<details>
  <summary>Details</summary>
Motivation: 在于提高从人类反馈中强化学习（RLHF）的效率和规模，让在少量数据上训练的奖励模型能达到大规模数据训练模型的表现。

Method: 引入偏好精炼，通过链式思维采样揭示多样化和高质量的偏好关系，并采用基于困惑度的评分机制分配细微的偏好等级。此外，使用多级直接偏好优化（M-DPO）来捕捉样本之间细微的偏好差异。

Result: 实验结果显示，所提方法明显提高了数据效率和模型性能，使得在少样本条件下训练的奖励模型能够达到大规模数据训练的模型表现。

Conclusion: 该研究证明了在少量数据的条件下，增强数据效率的方法能够使奖励模型的表现与大规模数据训练的模型相当。这为低资源强化学习提供了一个稳健的解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [230] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: 研究揭示时间序列特征与模型适应性之间的联系，并通过新型TimeFlex模型和数据集进行模型比较。


<details>
  <summary>Details</summary>
Motivation: 揭示时间序列特征与具体模型结构的关联，以提高模型对多样化时间序列的适应性。

Method: 引入一种新数据集和TimeFlex模型，通过比较不同模型在具有不同时间序列特征数据上的表现，以揭示时间序列特征与模型结构间的关联。

Result: 提出一种利用高斯过程生成的新数据集以及新的模块化架构模型TimeFlex，通过这种模型对比当前最先进模型，在不同时间序列条件下的表现，提供了更深入的理解。

Conclusion: TimeFlex模型在不同时间序列条件下表现优异，提高了对模型在多样化时间序列特征下适应性的理解。

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [231] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: 研究三种神经网络架构在命题逻辑任务中的表现，发现它们在未见模式泛化上仍面临挑战，特别是涉及否定时。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在符号规则学习中的能力，以及它们在推理任务中的失败原因。

Method: 研究三种神经网络架构在命题逻辑任务中的泛化表现，使用经过平衡的现有数据集测试在未见运算符组合上的表现。

Result: 所有模型在分布内表现良好，但对未见模式的泛化仍然具有挑战性，特别是涉及否定的情况下。

Conclusion: 标准架构在学习逻辑运算符的系统化表示上存在明显限制，需要更强的归纳偏置来支持基于规则的推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [232] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: 本文研究微调策略对TabPFNv2的影响，发现全参数微调能够提高其性能，尽管在某些数据集中仍不及传统方法。


<details>
  <summary>Details</summary>
Motivation: 虽然以往研究曾探讨过早期基础模型的微调，但其结果不一致，加之TabPFNv2独特的架构，促使我们对微调策略进行深入研究。

Method: 针对不同数据集系统评估各种微调策略，比较其实际效果和时间效率。同时，研究微调如何改变TabPFNv2的内部机制，揭示其成功的原因。

Result: TabPFNv2在I.I.D.拆分的学术数据集上通过微调实现了最先进的结果，但在具有渐变时间变化和丰富特征集的数据集上，TabPFNv2表现较不稳定，传统方法更具优势。

Conclusion: 我们发现全参数微调是TabPFNv2最实用的解决方案，可以同时提高时间效率和效果。该方法通过梯度适应，使TabPFNv2的查询表示和关键表示之间的点积更加准确地反映目标相似性，从而改善了基于检索的预测逻辑。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [233] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: BranchSBM offers a new way to model complex, branching trajectories in generative modeling, overcoming the limitations of existing unimodal transition methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods are inadequate for modeling multi-modal transitions, prompting the need for a framework that captures branched or divergent trajectories in generative modeling.

Method: BranchSBM parameterizes multiple time-dependent velocity fields and growth processes to represent the divergence of distributions into multiple endpoints.

Result: BranchSBM is demonstrated to be more expressive and critical for tasks such as multi-path surface navigation, cell fate bifurcations, and simulating varied cellular responses.

Conclusion: BranchSBM provides an effective solution for modeling complex transitions between distributions, capturing divergent trajectories that lead to multiple outcomes.

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [234] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种无需完整训练过程即可高效预测样本重要性的新方法，通过k-最近邻和图神经网络进行重要性分数外推，实验结果显示该方法在多种情境下有效。


<details>
  <summary>Details</summary>
Motivation: 现有的数据修剪技术虽然可以减少训练样本，但多数需要完整的初始训练过程，导致计算效率难以提升

Method: 使用k-最近邻和图神经网络作为初始方法，通过从小数据集中学习的模式预测整个数据集的样本重要性

Result: 在两种最新的修剪方法、四个不同的数据集以及三种训练范例中有效验证了该框架的有效性

Conclusion: 引入了一种新的重要性分数外推框架，该框架仅需对一小部分数据进行训练，即可预测整个数据集的样本重要性，实现了数据修剪的高效化

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [235] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: SPEED方法通过选择中等难度提示，提升了大型语言模型的训练效率，加速了收敛，同时不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在使用强化学习（RL）进行训练时能够显著增强其推理能力，但由于不高效的均匀提示采样，计算开销很大。

Method: 引入了一种名为SPEED的自适应在线RL课程，选择中等难度的训练实例以最大化学习效率。

Result: 实现了2倍到6倍的训练速度提升，且不影响准确性，无需手动调整，可无缝集成进标准RL算法。

Conclusion: SPEED方法通过选择中等难度的提示，加速了梯度估计器收敛速度，从而显著提高了训练效率。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [236] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: 提出了一种名为编辑流的非自回归模型，能够通过编辑操作实现灵活的序列生成，且在多项生成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 非自回归模型在生成可变长度序列时往往采用固定的、逐字的结构，难以灵活生成。

Method: 提出了编辑流（Edit Flows），通过编辑操作（插入、删除和替换）在序列上定义离散流，使用连续时间马尔可夫链对序列空间进行建模。

Result: 实验证明，编辑流在图像字幕生成上优于自回归模型和掩码模型，并在文本和代码生成方面显著优于掩码构建。

Conclusion: 编辑流能够更加灵活且高效地进行与序列数据结构更紧密对齐的生成。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [237] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: FZOO是一种快速零阶优化器，显著减少了大规模语言模型微调所需的计算资源，同时在多个任务上取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型的微调面临GPU内存瓶颈，零阶优化器能够避免高内存使用但通常需要更多的步骤才能收敛，作者希望改善这一速度和内存之间的权衡。

Method: FZOO通过批量单侧估计和Rademacher随机向量扰动进行CUDA并行处理，减少所需的前向传递次数并加速每批计算。

Result: FZOO比MeZO在准确率上提高3%，所需前向传递次数减少3倍，对RoBERTa-large模型准确率提高5.6%，前向传递次数减少18倍，并且收敛速度与Adam相当。

Conclusion: FZOO在内存效率和收敛速度方面优于现有的零阶优化方法，并与Adam优化器达到相当的性能。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [238] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 该研究通过引入风险可视化方法，深化对可执行预测的理解，并提出了新的理论设置以反映现实世界中的挑战。


<details>
  <summary>Details</summary>
Motivation: 展示损失景观的可视化可以为理论上的研究进展提供实际的洞察。

Method: 提出了一种灵感来自于两步流程的简单解耦风险可视化方法，涉及对模型参数和数据参数的风险景观进行可视化。

Result: 提出了“扩展的可执行预测”设置，能够在分布响应与决策模型不同的情况下捕捉场景。

Conclusion: 引入了一种新的风险可视化方法，通过对风险景观的可视化理解，探索现有算法在更现实条件下的表现。

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [239] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: 研究任务向量在内嵌学习中的功能机制，提出线性组合猜想并验证，建议通过注入多个任务向量增强效果。


<details>
  <summary>Details</summary>
Motivation: 任务向量在加速内嵌学习的推理过程中展现出吸引力，尽管已经取得经验上的成功，但其原理尚不清楚。

Method: 通过理论和实证支持，为线性组合猜想提供证据，并进行损失景观分析和显著性分析与参数可视化。

Result: 通过显著性分析和参数可视化验证结果，建议通过在少样本提示中注入多个任务向量增强其效果。

Conclusion: 研究结果推进了对任务向量的理解，并揭示了基于transformer模型的内嵌学习机制。

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [240] [The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students](https://arxiv.org/abs/2506.08041)
*Siddharth Siddharth,Brainerd Prince,Amol Harsh,Shreyas Ramachandran*

Main category: cs.CY

TL;DR: 为工程本科生开设的跨学科AI课程提升了其对AI的理解及社会影响感知。


<details>
  <summary>Details</summary>
Motivation: 填补工程学生对AI及其社会影响基础知识的缺失。

Method: 将课程分三个模块，工程与人文学科的教师联合授课。

Result: 学生对AI问题的理解显著提升，特别是在AI的环境影响和公平性方面。

Conclusion: 创新的课程设计提升了学生对AI的理解和社会影响的感知。

Abstract: This work presents a novel course titled The World of AI designed for
first-year undergraduate engineering students with little to no prior exposure
to AI. The central problem addressed by this course is that engineering
students often lack foundational knowledge of AI and its broader societal
implications at the outset of their academic journeys. We believe the way to
address this gap is to design and deliver an interdisciplinary course that can
a) be accessed by first-year undergraduate engineering students across any
domain, b) enable them to understand the basic workings of AI systems sans
mathematics, and c) make them appreciate AI's far-reaching implications on our
lives. The course was divided into three modules co-delivered by faculty from
both engineering and humanities. The planetary module explored AI's dual role
as both a catalyst for sustainability and a contributor to environmental
challenges. The societal impact module focused on AI biases and concerns around
privacy and fairness. Lastly, the workplace module highlighted AI-driven job
displacement, emphasizing the importance of adaptation. The novelty of this
course lies in its interdisciplinary curriculum design and pedagogical
approach, which combines technical instruction with societal discourse. Results
revealed that students' comprehension of AI challenges improved across diverse
metrics like (a) increased awareness of AI's environmental impact, and (b)
efficient corrective solutions for AI fairness. Furthermore, it also indicated
the evolution in students' perception of AI's transformative impact on our
lives.

</details>


### [241] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Main category: cs.CY

TL;DR: 利用机器学习方法预测学生学业表现，MLPC表现最佳，通过特征选择策略显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 希望通过机器学习预测学生学业表现，从而提升教育质量和效率。

Method: 使用了多层感知器分类器（MLPC）及多种评估方法，采用特征选择策略和可解释性机器学习方法。

Result: MLPC模型在测试集上取得了最高86.46%的准确率，在10折交叉验证中测试集平均准确率为79.58%，训练集为99.65%。

Conclusion: 人工神经网络能够有效预测学生学业表现，尤其在进行特征选择后，其表现优于其他传统机器学习模型。

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


### [242] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 本文提出了一种应用于电路分析课程的AI智能导师系统，并取得了积极的学生反馈。


<details>
  <summary>Details</summary>
Motivation: 开发一个智能导师以改进电路分析课程中的作业评估和反馈能力。

Method: 设计智能导师系统，包含开放式问题回答和作业反馈生成功能，并在微软Azure平台上部署进行实际使用。

Result: 学生反馈表明，90.9%的学生对智能导师表示满意。收集的交互数据帮助识别学生在课堂上的困难，并通过改进课堂指导进行回应。

Conclusion: 这个AI智能导师有效提升了个性化指导和反馈能力，并为课程导师提供了有价值的数据洞察。将来会扩展应用范围及优化提示等技术。

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [243] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Main category: quant-ph

TL;DR: 引入NQES算法，解决多体量子系统激发态计算难题，具备高效处理大规模系统的能力。


<details>
  <summary>Details</summary>
Motivation: 计算强相互作用的量子多体系统的激发态在理论和应用领域均具有重要的意义，但由于希尔伯特空间随着系统规模呈指数增长而极具挑战性。

Method: 本文引入了一种基于神经网络的算法 -- NQES（神经量子激发态）算法，用于精确、高效地计算量子多体自旋系统的多个低激发态。

Result: NQES算法无需显式正交化过程，适用于高维度系统；通过具体实例证明其能高效计算多个激发态及其相关观测量预期。该算法成功应用于二维Wigner晶体中的长程相互作用离子系统，揭示其间隙缩放及相关特征。

Conclusion: 研究表明，NQES算法是一种可扩展且高效的计算多体量子系统激发态的方法，具有广泛的应用潜力。

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [244] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Main category: quant-ph

TL;DR: 研究提出一种结合量子退火和平方化方法的黑箱优化方案，使机器学习回归器在优化过程中处理更复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 目前用于处理复杂问题的平方化方法仍然普遍未知，此类问题中强非线性和密集的交互作用阻碍了常规方法的应用，因此需要研究新方法来扩展量子退火技术的适用范围。

Method: 使用修正线性单元基数对目标函数进行建模，实现通用逼近并获得等效的二次多项式表示。证明概念验证通过数值和分析手段进行验证。

Result: 研究验证了通过修正线性单元基数进行平方化方法的可行性，并设计出一种结合量子退火的新黑箱优化方案。

Conclusion: 通过结合量子退火技术和所提出的平方化方法，设计了一种新的黑箱优化方案，使得机器学习代理回归能够在平方化处理后输入量子退火进行优化。

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [245] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Main category: quant-ph

TL;DR: 本研究探索了量子模型中对抗性鲁棒性与泛化能力之间的关系，提出了一种新方法并在时间序列分析中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 近年来对抗性鲁棒性和泛化能力在量子机器学习中分别受到广泛关注，但两者的相互作用尚未得到充分研究。

Method: 通过Lipschitz界限来量化鲁棒性和泛化能力，并引入了一种基于正则化的训练方法，强调了可训练的数据编码策略。

Result: 理论结果在时间序列分析上的应用证明了其实用性。

Conclusion: 提出了一种新的训练方法，可以实现鲁棒且具有良好泛化能力的量子模型。

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [246] [Quantum Adiabatic Generation of Human-Like Passwords](https://arxiv.org/abs/2506.08917)
*Sascha Mücke,Raoul Heese,Thore Gerlach,David Biesner,Loong Kuan Lee,Nico Piatkowski*

Main category: quant-ph

TL;DR: 研究绝热量子计算机能否有效生成符合人类行为的密码，结果表明小样本生成的密码具有真实性和模仿能力。


<details>
  <summary>Details</summary>
Motivation: 探讨量子计算是否有潜力减少生成式人工智能模型的训练和运行所需的巨大资源。

Method: 研究不同的编码方式并提出基于QUBO和UD-MIS问题的新方法，使用绝热量子计算机生成符合人类行为模式的密码。

Result: 在QuEra Aquila 256量子位中性原子量子计算机上生成的128个密码样本中包含类似人类行为的密码，如'Tunas200992'和'teedem28iglove'。

Conclusion: 绝热量子计算机在生成真实用户行为密码方面有效，展示了其在减少资源要求方面的潜力。

Abstract: Generative Artificial Intelligence (GenAI) for Natural Language Processing
(NLP) is the predominant AI technology to date. An important perspective for
Quantum Computing (QC) is the question whether QC has the potential to reduce
the vast resource requirements for training and operating GenAI models. While
large-scale generative NLP tasks are currently out of reach for practical
quantum computers, the generation of short semantic structures such as
passwords is not. Generating passwords that mimic real user behavior has many
applications, for example to test an authentication system against realistic
threat models. Classical password generation via deep learning have recently
been investigated with significant progress in their ability to generate novel,
realistic password candidates. In the present work we investigate the utility
of adiabatic quantum computers for this task. More precisely, we study
different encodings of token strings and propose novel approaches based on the
Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum
Independent Set (UD-MIS) problems. Our approach allows us to estimate the token
distribution from data and adiabatically prepare a quantum state from which we
eventually sample the generated passwords via measurements. Our results show
that relatively small samples of 128 passwords, generated on the QuEra Aquila
256-qubit neutral atom quantum computer, contain human-like passwords such as
"Tunas200992" or "teedem28iglove".

</details>


### [247] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 研究引入了一种新的叠加参数化量子电路，解决了现有量子机器学习方法的局限性，在多种任务上显示出显著的精度与稳定性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的量子机器学习方法局限于线性幺正操作，限制了表达能力及可扩展性。为了提高量子模型的表现力及扩展能力，引入更复杂的电路架构是必要的。

Method: 结合翻转-翻转量子随机存取存储器和重复-直到成功协议，使用叠加参数化量子电路构建新模型，通过振幅变换和后选择来引入多项式激活函数。

Result: 在一维阶跃函数回归任务中，使用两量子比特的叠加参数化量子电路将均方误差减少了三个数量级；在二维星形分类任务中，引入二次激活将准确率提升至81.4%，并减少了运行间的方差。

Conclusion: 引入了叠加参数化量子电路，这种电路在硬件效率方面表现出色，可以学习复杂的决策边界。

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [248] [Confidence Boosts Trust-Based Resilience in Cooperative Multi-Robot Systems](https://arxiv.org/abs/2506.08807)
*Luca Ballotta,Áron Vékássy,Stephanie Gil,Michal Yemini*

Main category: eess.SP

TL;DR: 论文提出了一种通过分析物理通道提升多机器人系统抗网络攻击能力的协议，并进行了数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 无线通信的多机器人系统容易受到网络攻击，而物理通信通道可以用于将恶意机器人的检测与合法机器人间的任务相关数据交换分离。

Method: 提出了一个多机器人操作的稳健协议，该协议通过参数{\lambda}t来评估物理通道指示的附近机器人的合法性。并进行了数值验证实验。

Result: 通过调节{\lambda}t，设计者可以在接近最优的机器人间协调与快速任务执行之间进行权衡。数值实验验证了该方法的有效性。

Conclusion: 该协议在轻微假设下，在任意数量恶意机器人存在的情况下实现了稳健的协调。

Abstract: Wireless communication-based multi-robot systems open the door to
cyberattacks that can disrupt safety and performance of collaborative robots.
The physical channel supporting inter-robot communication offers an attractive
opportunity to decouple the detection of malicious robots from task-relevant
data exchange between legitimate robots. Yet, trustworthiness indications
coming from physical channels are uncertain and must be handled with this in
mind. In this paper, we propose a resilient protocol for multi-robot operation
wherein a parameter {\lambda}t accounts for how confident a robot is about the
legitimacy of nearby robots that the physical channel indicates. Analytical
results prove that our protocol achieves resilient coordination with
arbitrarily many malicious robots under mild assumptions. Tuning {\lambda}t
allows a designer to trade between near-optimal inter-robot coordination and
quick task execution; see Fig. 1. This is a fundamental performance tradeoff
and must be carefully evaluated based on the task at hand. The effectiveness of
our approach is numerically verified with experiments involving platoons of
autonomous cars where some vehicles are maliciously spoofed.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [249] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Main category: stat.ML

TL;DR: 该研究提出了一种名为TelePiT的新型深度学习架构，用于提高全球次季节到季节（S2S）气候预测能力，显著优于当前最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能明确建模关键的物理过程和在S2S时间尺度上的遥相关关系。

Method: 创新的深度学习架构：包括利用球谐嵌入、多尺度物理信息神经ODE、以及遥相关注意力机制。

Result: 在2米温度预测中，与之前的最佳模型比较，均方根误差减少了57.7%。

Conclusion: TelePiT显著优于最先进的数据驱动基础线和操作性数值天气预报系统。

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [250] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Main category: stat.ML

TL;DR: 提出了一种新的基于Wasserstein距离的集成方法WWAggr，提升了变点检测的效果并解决了决策阈值的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的独立深度神经网络检测器未能达到完美质量，平均等标准预测聚合技术效果不佳，需要更有效的解决方案。

Method: 引入WWAggr，一种基于Wasserstein距离的任务特定的集成聚合方法。

Result: WWAggr提高了深度变点检测模型集成的效果，并灵活应对了深度CPD模型的多样性。

Conclusion: WWAggr方法在多种深度变点检测模型的集成中表现出色，并解决了决策阈值选择的长期问题。

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [251] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Main category: stat.ML

TL;DR: 本文提出了一种新的算法用于识别符合线性约束的多目标赌博机的帕累托集，其样本复杂性接近最优，相比现有方法性能优异。


<details>
  <summary>Details</summary>
Motivation: 在多元赌博机设置下，识别符合可行性约束的帕累托集。

Method: 我们介绍了一种算法，显著优于比赛类算法和直观的两阶段方法。

Result: 证明了任意算法在约束下识别帕累托集的样本复杂性的信息论下界，并表明我们的方法的样本复杂性接近最优。

Conclusion: 我们的理论结果通过一系列基准测试的广泛实证评估得到了支持。

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [252] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Main category: stat.ML

TL;DR: 本文提出了一种基于条件深度测度的新算法，用于预测与容差区域的定义，并展示了其在模拟研究与实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然深度测度在理论上具有优势，但其与回归建模结合以提供预测区域的应用尚未深入研究。本文旨在填补这一研究空白。

Method: 该研究提出了条件深度测度算法，使用条件核均值嵌入以及集成深度测度来定义预测与容差区域。此外，还引入了保留一致性结果的保形预测变体，以提高有限样本中算法的实用性。

Result: 研究表明在同质性设置下，算法具有快速收敛的能力，并在模拟研究中表现出优异的有限样本效果。

Conclusion: 我们开发了一种基于条件深度测度的新型不确定性量化算法，为预测区域和容差区域的定义提供了新的途径，并在有限样本情况下表现出较好的实用性。

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [253] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 论文研究了在不平衡数据上训练CRF，提出去偏技术使得新估计器能有效降低方差，提升预测性能。实验支持理论分析。


<details>
  <summary>Details</summary>
Motivation: 在不平衡数据集上进行分类任务时，一种类别经常被严重低估，因此需要重新平衡数据集，以提高分类器性能。

Method: 研究使用重新平衡数据集训练Centered Random Forests (CRF) 的理论方法，并建立中心极限定理，使用重要性抽样方法进行去偏，提出IS-ICRF估计器。

Result: 提出的IS-ICRF估计器在高不平衡情况下，相比直接在原始数据上训练的ICRF，表现出方差降低的优点。

Conclusion: 理论分析表明，在重新平衡的数据集上训练随机森林并进行去偏处理可以提高预测性能，特别是减少方差。实验结果显示，这个理论结论对于Breiman随机森林而言是有效的。

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [254] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: stat.ML

TL;DR: 开发了一种在无标签情况下检测概念漂移的算法，具备高效性和灵活性，并展示了优于传统方法的统计能力。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在各领域自动化决策应用的增加，确保模型性能并及时检测概念漂移变得至关重要。

Method: 利用经典统计过程控制开发了一种无标签情况下的概念漂移检测算法，并引入新框架将检测结果有效整合。

Result: 通过数值模拟，验证了算法在计算约束下的高统计能力及其在新框架下的有效性。

Conclusion: 提出了一种在无标签环境中检测概念漂移的灵活高效算法，展示了其比现有方法在统计能力上的优越性。

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [255] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 作者提出了一种多目标内核学习方法，通过高分辨率成像数据识别并分析铁电极化开关的微观结构特征，实现高通量学习和微观机制揭示。


<details>
  <summary>Details</summary>
Motivation: 对复杂局部微结构特征的依赖使得手动或基于网格的光谱测量来系统地探索铁电极化开关变得不切实际。

Method: 引入了一种多目标内核学习工作流，该工作流直接从高分辨率成像数据中推断出决定开关行为的微观结构规则。

Result: 该框架有效地识别了畴壁配置与局部开关动力学之间的关键关系，并揭示了特定的壁几何形状和缺陷分布如何调制极化反转。

Conclusion: 该方法不仅可用于高通量主动学习，而且可以揭示微观结构对开关现象的控制机制。

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [256] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: Hackathons unite ML and microscopy experts to improve data analysis efficiency, create benchmarks, and develop future workforce, with resulting resources shared on GitHub.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in data usage and prolonged analysis time in microscopy due to the absence of standardized code ecosystems and integration strategies, and to bridge the gap between machine learning and microscopy fields.

Method: Organizing hackathons to bring together machine learning researchers and microscopy experts to collaborate and create solutions, datasets, and digital twins for standardizing workflows.

Result: The hackathon successfully produced benchmark datasets, digital twins of microscopes, and all related code is available on GitHub, promoting community growth and standardized workflows.

Conclusion: Hackathons are effective in bridging the gap between machine learning and microscopy communities by fostering collaboration, producing benchmark datasets, and encouraging the preparation of a future workforce.

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [257] [Joint Routing and Control Optimization in VANET](https://arxiv.org/abs/2506.08038)
*Chen Huang,Dingxuan Wang,Ronghui Hou*

Main category: eess.SY

TL;DR: 本文提出了DynaRoute框架，解决动态车载网络中的车队控制和数据传输问题，通过轨迹感知的路由和安全控制，实现了车队稳定、适应性路径选择及提升智能交通系统性能，仿真结果显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决动态车载网络中的车队控制和数据传输问题，通过结合轨迹感知路由和安全约束车辆协调来优化两者。

Method: DynaRoute通过实时轨迹预测和可靠的数据保证来实现车队安全控制和优化传输路径。它需要预定义的交通模型，并使用局部车辆状态信息适应动态网络条件。

Result: 综合仿真结果表明，DynaRoute在多个复杂场景中保持了控制和传输性能，并显著提高了吞吐量和可靠性。

Conclusion: DynaRoute在多个复杂场景中显著提高了吞吐量和可靠性，相较于传统方法有明显的性能提升。

Abstract: In this paper, we introduce DynaRoute, an adaptive joint optimization
framework for dynamic vehicular networks that simultaneously addresses platoon
control and data transmission through trajectory-aware routing and
safety-constrained vehicle coordination. DynaRoute guarantees continuous
vehicle movement via platoon safety control with optimizing transmission paths
through real-time trajectory prediction and ensuring reliable data. Our
solution achieves three key objectives: (1) maintaining platoon stability
through accurate data transmission, (2) enabling adaptive routing based on
vehicle movement patterns, and (3) enhancing overall intelligent transportation
system performance. DynaRoute equires predefined traffic models and adapts to
dynamic network conditions using local vehicle state information. We present
comprehensive simulation results demonstrating that DynaRoute maintains control
and transmission performance in multiple complex scenarios while significantly
improving throughput and reliability compared to traditional approaches.

</details>


### [258] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: DCIDA框架通过学习联合条件分布的设计采样策略，有效降低设计误差，尤其在复杂传递函数的设计中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 当前逆向设计探索方法在评估和拓扑方面存在差距，难以在实际设计中应用，因此有必要发展新的方法以应对不连续评估及复杂设计需求。

Method: DCIDA使用一种基于注射性的依赖映射，将原始采样设计转化为独特等价的物理表现，同时利用Transformer架构的策略网络进行设计探索。

Result: 实验表明，DCIDA的Transformer策略网络相较于当前最先进的方法在设计误差上有显著减少，特别是在处理复杂传递函数时表现更优。

Conclusion: DCIDA框架通过学习接近最优的设计采样策略显著减少了设计误差，尤其是在涉及复杂传递函数的情况下。

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [259] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: 通过使用CNN和MLP构建替代模型，显著降低计算成本，CNN在精度和稳定性上优于MLP。


<details>
  <summary>Details</summary>
Motivation: 旨在降低数值模拟的计算成本，通过适应CNN的输入来处理辐射热传递问题。

Method: 使用卷积神经网络（CNN）和多层感知机（MLP）构建替代模型，并借助ICARUS2D求解器创建的数据集进行性能比较。

Result: CNN和MLP均实现了较大的加速，并且在精度上达到工业上可接受的相对误差。CNN在精度和超参数变化的稳定性方面优于MLP。

Conclusion: CNN在处理二维箱域内具有参与性气体的辐射热传递问题中表现优于传统的MLP。

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [260] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Main category: cs.IT

TL;DR: 研究了MIMO-OFDM系统中的多用户调度问题，提出组合算法结合机器学习的方法来提高频谱效率，并在性能与复杂性之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 在混合波束成形系统中，由于多路复用增益的限制，改进调度对于提高系统的频谱效率和长期性能至关重要。

Method: 使用了基于贪心和排序算法的组合解决方案，同时还采取了机器学习的方法。

Result: 数值结果显示了所提方法在性能和复杂性之间的权衡。

Conclusion: 在多用户调度问题中，不同方法的选择取决于特定场景中的具体标准。

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [261] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*Šimon Sedláček,Bolaji Yusuf,Ján Švec,Pradyoth Hegde,Santosh Kesiraju,Oldřich Plchot,Jan Černocký*

Main category: eess.AS

TL;DR: 本文通过引入小型连接模块桥接语音编码器和大语言模型，提升了语音对话状态跟踪的性能，尤其是在SpokenWOZ数据集上取得了显著的成果。


<details>
  <summary>Details</summary>
Motivation: 探索在语音对话状态跟踪中，通过小型连接模块桥接语音编码器和大语言模型的表示空间，以实现更开放的数据和组件使用。

Method: 通过桥接语音编码器和大语言模型的表示空间，使用小型连接模块进行整合，包括WavLM-large和OLMo模块。进行全/LoRA适配器微调，以及探讨对话历史中代理回合和模糊匹配输出后处理的效果。

Result: 在SpokenWOZ数据集上，最佳的WavLM + 连接器 + OLMo-1B对齐模型在SpokenWOZ测试集上实现了34.66%的最新成绩，使用Gemma-2-9B-instruct的系统进一步提升至42.17%的JGA。

Conclusion: 通过连接语音编码器和大语言模型，改进了对话状态跟踪性能，尤其是在命名实体的对话槽值方面。

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [262] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: PARV-CE架构通过多精度支持与硬件软件协同设计，实现了在精度和能耗上的优化，适用于多种AI模型的边缘平台。


<details>
  <summary>Details</summary>
Motivation: AI模型日趋复杂，需要灵活的硬件支持多种精度格式，尤其是在能量受限的边缘平台上。

Method: 提出了一种支持多精度MAC引擎的架构PARV-CE，使用统一的数据路径进行多类精度格式的操作，并通过层级自适应精度策略优化性能和能量使用。

Result: 与现有设计相比，PDP提高了最多2倍，资源使用减少最多3倍，准确度与FP32基准仅相差1.8%。支持各种工作负载的训练和推理。

Conclusion: PARV-CE通过POLARON技术提供了一种可扩展且高效的边缘AI加速解决方案。

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [263] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: 论文提出了D-分解，一种通过交替最小化计算的非正交矩阵分解方法，并在多种数据集上表现出优于其他分解方法的重构准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提升矩阵分解在稀疏性和噪声条件下的重构性能，研究者开发了D-分解来替代传统的代数分解方法。

Method: 使用交替最小化方法计算D-分解，并通过变分定义最小化正则化的Frobenius损失。

Result: D-分解在MovieLens、MNIST、Olivetti Faces和基因表达矩阵上的重构准确性优于截断SVD、CUR和非负矩阵分解，特别是在稀疏性和噪声条件下。

Conclusion: 该论文介绍了一种新的非正交矩阵分解方法D-分解，并证明其在稀疏性和噪声条件下的重构准确性优于其他方法。

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [264] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: sparseGeoHOPCA提供一种结合几何视角的稀疏高阶主成分分析方法，提升了高维数据计算效率与可解释性，并在多项实验中展现出色表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决高维张量分解问题中的计算效率和可解释性问题，提出结合几何视角的稀疏高阶主成分分析框架。

Method: 通过将输入张量在每个模式上展开，并将所得的子问题重构为结构化的二元线性优化问题，sparseGeoHOPCA将原始的非凸稀疏目标转化为易处理的几何形式。

Result: sparseGeoHOPCA在综合实验中表现出色，能够在合成场景中准确恢复稀疏支持，在10倍压缩下保持良好的分类性能，并在ImageNet上实现高质量的图像重建。

Conclusion: sparseGeoHOPCA展示了在高维数据中提升计算效率与可解释性方面的显著优势，并证明了其几何子问题与原始SHOPCA方法之间的等价性，为数据依赖的性能提供了理论保证。

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [265] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: The paper introduces DCD, a deep learning model for precise segmentation of fetal heart structures in echocardiography, enhancing prenatal diagnosis of congenital heart diseases.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need for accurate segmentation of anatomical structures in fetal echocardiography for early diagnosis and evaluation of congenital heart disease, which remains challenging due to various factors such as ultrasound artifacts and anatomical variability.

Method: The paper proposes a model named DCD which utilizes Dense Atrous Spatial Pyramid Pooling (Dense ASPP) for multi-scale feature extraction and a Convolutional Block Attention Module (CBAM) to enhance feature representation, facilitating precise segmentation.

Result: The DCD model achieves precise and robust segmentation by effectively capturing both local and global contextual information, thus contributing to improved prenatal cardiac assessment.

Conclusion: DCD is an advanced deep learning-based model that significantly improves the accuracy and robustness of segmenting key anatomical structures in fetal echocardiography, particularly in the A4C view.

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [266] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: 提出了一种能保证单调性的新模型，增强了推广能力，尤其在数据集有限时提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的许多基于比较的偏好学习模型，包括大语言模型，未能保证单调性。以前只有广义Bradley-Terry模型被证明是单调的，但它们无法推广到未比较的数据。

Method: 提出了一种新的线性广义Bradley-Terry模型，并结合了扩散先验的使用。通过识别替代嵌入的充分条件来确保单调性。

Result: 新提出的模型在确保单调性的同时，能够推广未比较的数据，并在数据集有限时提高准确性。

Conclusion: 新的线性广义Bradley-Terry模型不仅能保证单调性，还能在数据集有限的情况下提高准确性。

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [267] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.RO

TL;DR: 代理无人机融合感知、决策、记忆与协同规划，挑战传统无人机，应用于多领域并面临技术与法规挑战，提供未来发展路线。


<details>
  <summary>Details</summary>
Motivation: 近年来Agentic AI的发展促使无人机实现自适应的复杂环境操作，突破传统无人机的限制。

Method: 通过对智能架构组件和使无人机能够执行复杂任务的技术进行详细分析，展示Agentic UAVs与传统无人机的区别。

Result: 研究了Agentic UAVs在精密农业、建筑与采矿、灾害响应、环境监测、基础设施检查、物流、安全和野生动物保护等七个领域的应用，并指出在技术、法规和数据可靠性方面的挑战，提出硬件创新、学习架构和人机交互的解决方案。

Conclusion: 本文为未来Agentic UAVs在社会和工业领域的发展、部署和治理奠定了基础框架。

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [268] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Main category: cs.RO

TL;DR: 研究了一种名为CALL的多智能体强化学习框架，通过轻量级信息共享来提高预测和规划性能，特别在自动驾驶任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 克服多智能体系统中的部分可观测性和非平稳性，同时解决信息共享的通信开销和可扩展性问题。

Method: 将生成AI与世界模型结合，通过低维潜在表示进行轻量级信息共享，促进多智能体局部学习和协调。

Result: 在CARLA平台的局部轨迹规划任务中，CALL展现出显著的性能提升。

Conclusion: CALL通过信息共享提高了预测准确性和规划性能，在CARLA平台的实验中证明了其有效性。

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [269] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: 提出Re4MPC，通过深度强化学习提高多模型运动规划的计算效率，实验验证效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多自由度机器人运动规划方法在现实世界中常常计算量过大，因此需要一种更具计算效率的解决方案。

Method: 提出了一种名为Re4MPC的新型多模型运动规划管道，使用非线性模型预测控制（NMPC）计算轨迹，并通过深度强化学习（DRL）框架来学习反应性决策的策略。

Result: 实验结果表明，Re4MPC在计算效率方面优于传统算法，并在实验中显示出更高的成功率。

Conclusion: Re4MPC比传统的NMPC基线更具计算效率，并在到达末端执行器目标方面取得了更高的成功率。

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [270] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 扩散模型生成自动驾驶潜在故障；无需外部数据集或系统先验，适于路口安全验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的安全验证面临高风险、高成本以及潜在故障的罕见性和多样性，这对实时测试带来了挑战。

Method: 训练去噪扩散模型，用于在给定任何初始交通状态的情况下生成自动驾驶汽车的潜在失败案例。

Result: 在四路交叉口问题的实验中，扩散模型能够在多种场景生成真实的失败样本，同时捕捉到大量潜在故障的多样性。

Conclusion: 应用扩散模型可以在不依赖外部数据集和对系统无先验知识的情况下，自动生成自动驾驶系统的潜在失败案例，对交通路口的安全验证具有实际应用价值。

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [271] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Main category: cs.RO

TL;DR: 本文介绍了在机器人应用中结合深度学习和结构化推理的框架，以应对未知和动态环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习架构和基础模型在处理未知和动态环境时存在效率和可靠性上的局限性。

Method: 结合数据驱动学习和结构化推理，利用可微物理进行高效的世界建模，使用贝叶斯推理进行不确定性感知决策，以及使用元学习快速适应新任务。

Result: 通过将物理符号推理嵌入到神经模型中，机器人能够超越训练数据进行泛化，对新情况进行推理，并不断扩展知识。

Conclusion: 混合神经符号架构对于下一代自主系统至关重要。我们提出了一个研究路线图以指导和加速其发展。

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [272] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.RO

TL;DR: 研发了一种通过腕部摄像机自动抓取和释放物体的假肢手系统，降低了用户的使用负担。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要用户生成肌电信号进行控制，负担较重，因此该研究旨在开发一个更轻松易用的假肢手自主控制系统。

Method: 使用遥操作系统收集人类演示数据，并通过模仿学习训练假肢手控制模型，该模型可以模拟人类的假肢手动作。

Result: 经过单一参与者的少量物体数据训练后，模仿学习算法在未见物体和不同个体上实现了高成功率，并且可以进行重量变化的泛化。

Conclusion: 该研究成功开发了一种完全自主的假肢手控制系统，仅需通过腕部摄像机即可实现抓取和释放各种形状的物体，显著降低了用户的使用负担。

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [273] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: 介绍了一种名为FreqPolicy的新方法，通过频率一致性约束和自适应一致性损失实现高效的单步动作生成，优于现有方法并实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有生成建模视动策略在推理过程中多步采样的高成本限制了其实时应用的能力，因此需要一种能够有效利用时间信息并实现高效预测的方法。

Method: 提出一种名为FreqPolicy的方法，该方法在流式视动策略中施加频率一致性约束，设计自适应一致性损失函数，并将其集成到VLA模型中实现加速。

Result: FreqPolicy优于现有方法，在真实机器人场景中达到93.5Hz的推理频率，且不损失性能。

Conclusion: FreqPolicy在多个基准测试中表现优于现有的一步动作生成器，并能在真实环境中高效应用。

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [274] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: 提出了一种多模态反馈框架MOSAIC-F，结合人工评估和数据驱动技术生成更准确的个性化反馈，并在提升口头表达能力中进行测试。


<details>
  <summary>Details</summary>
Motivation: 为了在学生学习活动中生成个性化反馈，改进学生的表现，提出了一种新颖的多模态反馈框架。

Method: MOSAIC-F框架包括四个关键步骤：首先通过标准化评分量表进行同行和教授评估；其次收集学习活动中的多模态数据；然后利用人工智能生成个性化反馈；最后通过视频记录进行自评和反馈可视化。

Result: 在提升口头表达能力的背景下测试了MOSAIC-F框架。

Conclusion: 通过结合人工评估和数据驱动的评价技术，MOSAIC-F框架能够生成更准确、个性化且可操作的反馈。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [275] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 研究探讨了t-SNE和UMAP在视觉分析中的误用现象及其原因，并提出合理使用的建议。


<details>
  <summary>Details</summary>
Motivation: t-SNE和UMAP在分析群集间关系时通常不准确，但仍然被大量使用，需要了解为何发生这种误用及如何防止。

Method: 进行文献综述并采访研究，以验证误用的普遍性并揭示使用技术的隐性动机。

Result: 研究发现，t-SNE和UMAP的误用主要源于关于其在视觉分析中适当使用的讨论的匮乏。

Conclusion: t-SNE和UMAP在视觉分析中常被误用，主要是由于缺乏关于其适当应用的讨论。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [276] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 本文提出了一种在NXP MCXN947微控制器上实现的关键词识别系统，通过结合MFCC特征提取与CNN分类器，实现了在资源受限设备上高效的实时语音交互。实验结果证明，该系统具有卓越的速度和准确率表现。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的设备上实现实时语音互动的需求推动了该研究的发展。

Method: 该系统结合了MFCC特征提取与CNN分类器，并使用量化感知训练进行优化，以在模型大小减少的情况下尽可能小地降低准确性。

Result: 实验结果表明，使用NPU可比仅使用CPU执行提高59倍的推理速度，同时实现了97.06%的准确率，模型大小为30.58 KB。

Conclusion: 该系统通过在资源受限设备上实现实时语音交互，证明了在嵌入式平台上实现高效、低功耗语音界面的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [277] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: The paper introduces an attack strategy leveraging speech elements and the Speech Large Language Model (SLLM) to enhance the effectiveness of backdoor attacks on speech classification tasks, achieving high success through the proposed Speech Prompt Backdoor Attack (SPBA).


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of speech classification systems to backdoor attacks and exploit speech elements to create more diverse and effective triggers.

Method: The authors introduce the Multiple Gradient Descent Algorithm (MGDA) as a mitigation strategy for addressing speech backdoor attacks by focusing on speech elements like timbre and emotion. They use the Speech Large Language Model (SLLM) to generate diverse triggers, increasing the number of potential backdoors.

Result: The experiments demonstrate that the proposed Speech Prompt Backdoor Attack (SPBA) is effective, achieving high success in attack metrics across tested speech classification tasks.

Conclusion: SPBA exhibits significant trigger effectiveness and achieves exceptional performance in attack metrics for speech classification tasks.

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [278] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Main category: cs.SD

TL;DR: 本文提出一种统一框架MD-ViSCo，能从单输入波形生成任何目标生命体征波形，评估结果表明该框架优于当前最优方法，解决了临床使用中的多模型问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在生成目标生命体征波形时，通常需要为每一对源与目标波形设计独特的模型。这种方法在临床环境中不够实用，因为它需要多个模型架构、优化过程和预处理流程。

Method: 提出了多方向生命体征转换器（MD-ViSCo），结合浅层1维U-Net和Swin Transformer，利用自适应实例归一化技术来捕捉不同波形样式。

Result: 在两个公开数据集上进行评估，MD-ViSCo在所有波形类型上平均超过了最先进的基线模型，包括降低了平均绝对误差（MAE）8.8%和提高了皮尔逊相关性（PC）4.9%。此外，生成的动脉血压波形满足AAMI标准，并获得BHS标准的B级，大幅超越基线模型。

Conclusion: 通过消除为每项任务开发独特模型的需求，该研究提供了一种统一框架，可以用单一模型处理任何类型的生命体征波形。

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [279] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: Step-Audio-AQAA是一个完整的端到端音频问答模型，利用音频分词器、LLM和声码器，在音频交互任务中表现卓越，尤其是在语音控制上超过了现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频-语言模型依赖文字输出，限制了其直接生成自然语音响应的能力，从而阻碍了无缝的音频交互。

Method: 该模型综合使用了双码本音频分词器来提取语言和语义特征，拥有1300亿参数的骨干LLM，以及神经声码器以进行高保真语音合成。后训练方法包括交错的文本和音频输出以增强语义连贯性，并通过直接偏好优化（DPO）与模型合并提高性能。

Result: 在StepEval-Audio-360基准测试中，Step-Audio-AQAA表现优异，特别是在语音控制方面，超越了先进的现有模型。

Conclusion: 引入的Step-Audio-AQAA模型超越了现有的LALMs，特别在语音控制中表现出色，展示了令语音问答任务更加无缝的潜力。

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


### [280] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: 本研究提出了ACORN框架，通过物理模拟和音频编码提升LLMs的物理感知能力，取得合理结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本和多模态处理方面虽表现出色，但缺乏对现实物理现象的理解，需要通过某种方式提升其物理意识。

Method: ACORN框架结合了一个基于物理的模拟器和音频编码器，与大语言模型相连，并使用AQA-PHY数据集进行训练和评估。

Result: 通过使用ACORN框架，LLMs能够在视线检测、多普勒效应估计和到达方向估计等任务中得到合理的结果。

Conclusion: ACORN展示了通过声音增强大语言模型（LLMs）对物理世界的感知能力，在仿真和现实任务中取得了合理的结果。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [281] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: 通过系统对比自回归解码和条件匹配流范式，研究揭示了它们在文本到音乐生成中的不同优劣，可为未来发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 尽管数据和架构因素重要，本研究聚焦于建模范式，试图隔离其影响并提供与之相关的权衡和新兴行为的见解，以指导未来文本到音乐生成系统。

Method: 通过在相同的数据集、训练配置和相似的基础架构上从头训练所有模型，进行系统的实证分析。

Result: 研究通过多轴评估性能，包括生成质量、推理配置的鲁棒性、可扩展性、对文本和时间对齐条件的遵循以及音频修补形式的编辑能力。

Conclusion: 本研究揭示了两种主要建模范式（自回归解码和条件匹配流）的不同优劣，这些发现可为未来的架构和训练决策提供指导。

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [282] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph通过人工智能和图神经网络模型，简化了化学与材料科学中的原子级模拟流程，不同大小的大语言模型在不同复杂任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 原子级模拟在化学和材料科学中至关重要，但由于计算方法繁多、软件生态多样以及需要专业知识的挑战，使得自动化和简化这些流程变得迫切。

Method: 提出了一种名为ChemGraph的框架，利用图神经网络基础模型和大语言模型，结合人工智能和最新的仿真工具，以直观的交互界面来优化化学和材料科学工作流程。

Result: 通过13个基准任务的评估，证明较小的大语言模型在简单工作流程中表现良好，而更复杂的任务则受益于使用更大的模型。此外，利用多代理框架将复杂任务分解为更小的子任务，小型模型可以在特定情境中与更大的模型性能相媲美或超越。

Conclusion: ChemGraph通过自动化和优化工作流程，为用户提供一种更高效和简化的进行原子模拟和计算化学的方式。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [283] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: QUITE系统使用大型语言模型（LLMs）超越规则重写SQL查询，解决了LLMs幻觉问题，并显著提升了查询执行性能和重写覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 动机来自于现有基于规则的SQL查询重写方法存在的局限性：难以发现和验证新规则，固定规则无法适应新查询模式，以及某些重写技术无法用固定规则表达。人类专家在重写能力方面表现更佳但不具备可扩展性，而大型语言模型（LLMs）则展示了近乎人类水平的语义和推理能力，因此我们提出利用LLMs超越规则进行SQL查询重写的新方法。

Method: 本文提出了一种无训练和反馈感知系统QUITE，通过使用LLM代理来重写SQL查询。设计一个通过有限状态机（FSM）控制的多代理框架使LLM能够使用外部工具，并通过实时数据库反馈增强重写过程。同时，开发了一种重写中间件来增强LLM生成优化的查询等价物的能力，并采用一种新的提示注入技术以改进重写查询的执行计划。

Result: 广泛的实验表明，QUITE系统将查询执行时间减少了35.8%，比最先进的方法多产生24.1%的重写，覆盖了以前系统未处理的查询案例。

Conclusion: QUITE系统通过无规则的LLM方法克服了传统规则匹配重写的局限，能够有效提升SQL查询的执行性能，并扩展其覆盖范围在更大程度上满足复杂查询模式。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


### [284] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: RADAR是一种基准测试框架，用于评估语言模型在表格数据上的数据意识推理能力，通过模拟数据伪影揭露其在处理数据异常时的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在数据分析领域的自动化应用越来越多，其数据意识方面的能力仍然是一个亟待探索的领域。这些能力包括识别、推理和适当处理数据伪影，如缺失值、异常值和逻辑性不一致问题，尤其是在现实世界的表格数据中更为普遍，如果处理不当，将显著影响分析结论的有效性。

Method: 本研究提出了一种用于系统评估表格数据上的数据意识推理的基准测试——RADAR。我们开发了一个框架，通过程序性扰动来模拟数据伪影，以便对模型行为进行有针对性的评估。RADAR包括2980个表查询对，涵盖了来自9个领域和5种数据伪影类型的真实世界数据。

Result: 评估结果表明，尽管在没有数据伪影的表格上表现不错，但在引入数据伪影后，前沿模型显著退化，暴露了其在稳健的数据意识分析能力方面的关键差距。

Conclusion: RADAR设计灵活且可扩展，支持多样的扰动类型和可控的表格大小，为推进表格推理提供了有价值的资源。

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [285] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: LEANN提供了一种高效的存储解决方案，显著降低嵌入式搜索索引的存储需求，同时保持良好的搜索性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 因为嵌入式搜索数据结构的高存储开销，使得在个人设备上本地存储和维护变得不可行。

Method: 提出了一种名为LEANN的存储高效的近似最近邻搜索索引，结合紧凑的图结构和高效的动态重计算策略。

Result: LEANN将索引大小减少至原始数据的5%以下，存储效率达到了标准索引的50倍，同时在真实世界的问答基准上，在不到2秒内保持90%的top-3召回率。

Conclusion: LEANN有效解决了在资源受限的个人设备上进行嵌入式搜索的存储挑战，提供了较小的存储需求并保持较高的搜索准确性。

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [286] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Main category: q-bio.QM

TL;DR: 结合多种分子特征和自动化机器学习方法来预测Caco-2透过性，CaliciBoost模型结合3D描述符显著提升了预测效果。


<details>
  <summary>Details</summary>
Motivation: 提高计算预测的精确性和效率以在早期药物发现阶段准确预测药物候选物的口服吸收特性。

Method: 本文利用自动化机器学习方法，结合不同的分子特征表示方法(包括2D/3D描述符、结构指纹和基于深度学习的嵌入)来预测Caco-2透过性。

Result: 在评估中，PaDEL、Mordred和RDKit描述符表现出良好的预测效果，CaliciBoost模型表现最佳，通过结合3D描述符，MAE减少了15.73%。

Conclusion: 研究表明，使用自动化机器学习方法提升Caco-2透过性的预测精度，特别是CaliciBoost模型结合PaDEL和Mordred表示的3D描述符显著提高了预测效果。

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [287] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Main category: q-bio.QM

TL;DR: Protriever is a fast, adaptable framework for retrieving homologous protein sequences, enhancing protein modeling tasks without relying on MSA.


<details>
  <summary>Details</summary>
Motivation: Current methods for retrieving homologous protein sequences are computationally expensive and struggle with divergent sequences, necessitating a new approach for efficient retrieval to improve protein modeling tasks.

Method: Protriever employs an end-to-end differentiable framework for retrieving homologous protein sequences, focused on efficient vector search.

Result: Protriever outperforms sequence-based models reliant on MSA in terms of speed and accuracy for protein fitness prediction.

Conclusion: Protriever is a scalable, efficient alternative to MSA-based retrieval, achieving state-of-the-art performance in protein fitness prediction.

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [288] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: 引入EDINET-Bench以评估LLM在日本金融任务中的表现，揭示了其在实务中的应用挑战，并提供公开数据和代码支持进一步研究。


<details>
  <summary>Details</summary>
Motivation: 解决日本金融数据集不足的问题，为LLM在财务分析中的发展和评估提供资源。

Method: 通过从EDINET下载过去十年的年度报告，为每个评估任务自动分配标签，构建了EDINET-Bench用于评估LLM在金融任务中的性能。

Result: 实验表明，即使是最先进的LLM在财务欺诈检测和收益预测的二元分类中也仅比逻辑回归略好，突显了LLM在实际金融应用中的应用困难。

Conclusion: EDINET-Bench为日本金融数据提供了一个独特的新基准，揭示了LLM在实际金融应用中遇到的显著挑战，并强调了技术在金融领域专门适应的必要性。

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [289] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Main category: physics.geo-ph

TL;DR: 研究提出了一种新的物理信息机器学习方法TS-PIELM，提高了土壤固结分析的计算效率和精度，超过传统PINN方法的1000倍效率和100倍精度。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络(PINN)在进行土壤固结分析时的准确性和效率有限，需要改进以成为一个具有竞争力的替代方案。

Method: 提出了一种称为时间步长物理信息极限学习机（TS-PIELM）的新型物理信息机器学习（PIML）方法。该方法通过将固结过程分为多个时间间隔，并利用单层前馈极限学习机（ELM）来加速网络训练。而输入层权重在训练过程中保持随机生成并固定，输出层权重则通过求解线性方程组直接计算获得。

Result: 新方法TS-PIELM在解决三个典型的Terzaghi固结问题中表现优越。

Conclusion: 该研究证明了TS-PIELM在计算地质技术中潜力巨大，相较于传统的PINN方法，该框架在一维情况下的计算效率提高了1000倍，精度提高了100倍。

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [290] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 论文探索机器学习系统的复杂性，提出了一种基于指标的架构模型用于支持系统的架构决策。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效管理机器学习系统的复杂性，并为架构决策提供支持。

Method: 通过扩展一个参考架构来收集和评估机器学习系统的复杂性指标。

Result: 提出了一个基于指标的架构模型的初步概念，能够用于描述机器学习系统并收集其复杂性指标。

Conclusion: 论文提出了一个基于指标的架构模型，用于评估和管理机器学习系统的复杂性。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [291] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: 该论文探讨大型语言模型（LLMs）在符号约束分析中评估程序最坏情况执行能力，提出并解决了这一任务，并通过符号推理引导的微调改进模型性能，实验表明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 探索LLM解决更加复杂的符号推理任务的能力，特别是分析程序最坏情况执行的符号约束，以连接LLM和符号推理方法。

Method: 通过SMT（Satisfiability Modulo Theories）约束解决进行符号推理引导的微调，支持使用专门设计的符号约束数据集进行评估以及改进现有LLM的能力。

Result: 实验结果表明，经过微调的模型WARP-1.0-3B在性能上持续超越了尺寸匹配甚至更大的基线模型，证明了能够恢复算法最坏情况行为约束的能力。

Conclusion: 实验结果表明，经过符号推理引导的微调，3B规模的LLM能够通过强化学习方法恢复算法的最坏情况行为的约束。这表明LLMs能够进行更深入的符号推理，并支持神经网络学习与形式化方法之间的紧密结合。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [292] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: Repeton is an open-source framework using LLMs for automated code manipulation, offering a patch-and-test pipeline that enhances code debugging scalability and transparency.


<details>
  <summary>Details</summary>
Motivation: To address low precision and limited interpretability of LLMs in complex software engineering tasks related to code generation and comprehension.

Method: Repeton uses a structured patch-and-test pipeline that iteratively diagnoses issues, proposes code changes, and validates each patch through automated testing. This process is guided by lightweight heuristics and development tools, without relying on embedding-based retrieval systems.

Result: Repeton shows good performance compared to RAG-based methods in both patch validity and interpretability when evaluated on the SWE-bench Lite benchmark.

Conclusion: Repeton provides a practical path for scalable and transparent autonomous debugging by decomposing software engineering tasks into modular and verifiable stages.

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [293] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: 本文通过执行轨迹分析提出了软件工程代理的决策路径分类，研究了成功成分及策略，揭示代理补丁与开发者补丁在结构和风格上的不同。


<details>
  <summary>Details</summary>
Motivation: 虽然多个代理架构表现出强大的经验性能，但支持其行为的内部决策工作流程仍不被理解。深入了解这些工作流程有望提高代理的可靠性和效率。因此，本文首次对SWE代理行为进行系统研究，旨在提供新的见解以改善代理设计。

Method: 本文采用执行轨迹分析的方法，对五个代表性代理的决策路径进行了分类，并深入研究了错误定位、补丁生成和再现测试生成的三个核心组件。此外，进行了大规模代码克隆分析，对比代理生成的补丁与开发者编写的补丁，同时提供了补丁内容的定性研究。

Result: 通过研究，本文提出了决策路径分类法，识别了代理成功的三个核心组成部分，并分析测试生成对补丁生产的影响。还进行了代码克隆分析，比较了代理生成和开发者编写的补丁，提炼了结构和风格上的差异，这些发现为代理设计提供了新见解。

Conclusion: 本文对软件工程代理（SWE agents）行为进行了系统研究，提出了决策路径分类法，并识别了代理成功的三个核心组成部分：错误定位、补丁生成和再现测试生成。研究显示测试生成对成功的补丁生成有重要影响，分析了成功生成测试的策略。通过对代理生成和开发者编写的补丁进行大规模代码克隆分析，揭示了补丁内容在结构和风格上的差异。

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [294] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文探讨了生成型AI生成代码的环保性，结果显示这些工具在代码生成过程中未采用环保视角，需要进一步研究和应对策略。


<details>
  <summary>Details</summary>
Motivation: 探讨生成型AI是否能生成具有可持续性的代码以及它们的环保友好程度。

Method: 对ChatGPT、BARD和Copilot三个流行的生成型AI工具进行早期调查，评估其生成代码的可持续性。

Result: 结果显示这些工具在生成代码时存在默认的非环保行为，并强调需要进一步深入研究和实施有效补救策略。

Conclusion: 生成型AI工具在生成代码的过程中未遵循可持续编程实践，存在较大的改进空间，研究呼吁更深入的调查和改进措施。

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [295] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper highlights deviations in code review processes, introduces a method to detect these with 91% accuracy, and demonstrates that excluding them improves machine learning model performance by 53.33% in predicting review times.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bias and inefficiencies in MR process analytics caused by non-standardized purposes such as drafts and dependency updates. Removing these deviations is hypothesized to enhance the accuracy of ML models in review analysis.

Method: The paper proposes a few-shot learning detection method with 91% accuracy to identify and exclude deviation categories from MR workflows, improving ML model performance for predicting review completion time.

Result: The study identifies seven categories of deviations, found in 37.02% of MRs. By excluding these, the performance of ML models predicting review completion time improves in 53.33% of cases, with performance increases up to 2.25 times and a significant change in feature importance.

Conclusion: Ignoring deviations in Merge Request (MR) workflows can lead to biased analytics and ineffective machine learning (ML) models for review analysis. Proper identification and exclusion of these deviations improve model performance and reliability.

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [296] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: 研究提出了一种高效准确的AR手术导航变形建模算法，通过人机交互提升术中解剖对齐。实验显示误差减少至2.78毫米，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前有限元方法的计算成本过高限制了其在术中应用，并且现有算法常常无法处理术中发生的大规模解剖变化，导致解剖对应关系不准确和增强现实导航受影响。该研究旨在解决这些挑战。

Method: 该研究提出了一种融合有限元方法（水准）精确度的数据驱动生物力学算法，并引入了一种新的人机交互机制，使外科医生能够提供提示以纠正解剖错位。

Result: 实验表明，该算法在公开数据集上实现了平均目标注册误差为3.42毫米，引入外科医生的交互后误差进一步降低到2.78毫米，超越了现有方法的体积精度。

Conclusion: 该研究提出了一种结合数据驱动生物力学算法和人机交互机制的变形建模框架，可提高手术导航中的计算效率和注册准确性，有望提升计算机辅助手术的安全性和可靠性。

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [297] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS框架通过结合多模态数据和图理论精化基础设施网络的真相数据，提升了语义分割模型的性能，实现了更准确的渠系网络映射。


<details>
  <summary>Details</summary>
Motivation: 为了改善水管理中的灌溉规划和基础设施维护，需要准确的渠系网络映射。然而，现有的学习方法受限于不完整或不充分的真相数据集。

Method: 提出了一种名为IGraSS的新型迭代框架，结合了语义分割模块和基于图的真相精化模块，使用多模态数据（包括RGB、NDWI和DEM）进行处理。语义分割模块处理卫星影像数据块，而精化模块则从整体上将基础设施网络视为一个图进行操作。

Result: 实验表明，IGraSS框架能够显著减少无法访问的渠系段，从约18%减少到3%。使用精化后的真相进行训练明显提高了渠系识别的准确性。

Conclusion: IGraSS展示了在基础设施网络，包括道路和渠系网络的准确映射中提高地图精度的能力。通过融合多种数据源和利用图理论约束条件，IGraSS能够有效地改进语义分割模型的训练数据，从而提高模型性能。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [298] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 通过整合离散扩散框架和VLA管道，实现个性化的外科手术风格指纹建模，尽管提高了准确度，但增加了身份泄露风险。


<details>
  <summary>Details</summary>
Motivation: 目前的AI系统往往忽略了个性化信号，而外科医生由于训练、经验和运动行为的差异，展现出不同的操作风格。

Method: 使用离散扩散框架，并与视觉-语言-动作管道集成，通过自然语言提示编码个性化外科医生指纹。

Result: 在JIGSAWS数据集上评估方法，成功地重建手势序列，并学习到每位外科医生独特的运动指纹。身份泄露风险是通过会员推理攻击量化的。

Conclusion: 个性化嵌入可以提高任务性能，但同时也增加了身份泄露的风险，因此在外科建模中需要平衡个性化与隐私风险。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [299] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 本文探讨了使用现代解码器型LLMs作为文本编码器在文本到图像生成中的有效性，发现调整嵌入方式能够显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 许多文本到图像模型仍然使用较为过时的T5和CLIP作为文本编码器，本文旨在探索现代解码器型大语言模型作为文本编码器的有效性。

Method: 构建标准化的训练和评估流程，训练和评估了27个文本到图像模型，涉及12种文本编码器，通过不同的文本嵌入方式进行实验对比。

Result: 通过实验发现，使用经过层归一化的全层平均嵌入可以显著改善与复杂提示的对齐，大多数使用这种条件的LLMs在高级视觉语言推理中性能优于基线T5模型。

Conclusion: 使用现代解码器型大语言模型（LLM）作为文本编码器可以显著提高文本到图像生成模型的性能，尤其是在处理复杂提示时。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [300] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 研究利用1D图像标记器进行图像编辑和生成，通过标记操作和优化，实现高效图像编辑和生成，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 受1D标记器潜在空间的表现力启发，探索其在图像编辑和生成中的应用能力。

Method: 构建了一个图像生成管道，通过梯度优化和即插即用损失函数执行标记优化，并在图像编辑和生成任务中应用这些标记。

Result: 在图像修复和文本指导的图像编辑应用中，展示了1D标记器的优越性，能生成多样化和逼真的图像，无需训练任何生成模型。

Conclusion: 使用1D图像标记器进行图像编辑和生成具有很大潜力，可以在不训练生成模型的情况下实现多样化和逼真的样本生成。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [301] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 Mirage 的音频到视频生成模型，利用自注意力机制实现了在主观质量上优于现有方法的逼真视频输出。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法要么忽略声音，只关注无声图像序列的生成，要么关注有限的应用领域如重新配音。引入一种能够从音频输入生成逼真、生动视频的模型，以实现音视频的和谐结合。

Method: 采用自注意力机制进行音频到视频生成模型的统一训练方法，无论是从头开始训练还是基于现有权重训练。

Result: Mirage 能够生成逼真的视频输出，特别是当与文本到语音（TTS）方法结合时，实现令人信服的多模态视频。

Conclusion: Mirage 生成的视频比现有结合音频特定架构或损失组件的方法在主观质量上更出色。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [302] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: SEMA offers a scalable, efficient alternative to traditional and linear attention, enhancing focus and performance on large-scale image tasks in computer vision.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational complexity of traditional attention mechanisms and the challenges faced by linear attention in maintaining focus, particularly in computer vision applications.

Method: The paper introduces Scalable and Efficient Mamba-like Attention (SEMA), which integrates token localization and arithmetic averaging to enhance the ability to focus compared to traditional attention methods.

Result: The proposed SEMA model performs well on the Imagenet-1k dataset, outperforming recent vision Mamba models in classification tasks, even with larger images and similar model parameter sizes.

Conclusion: SEMA is a scalable and efficient alternative to traditional linear attention, offering better performance in computer vision tasks.

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [303] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 该论文提出了一个用于文本到图像生成的自适应引导策略Step AG，有效提升了图像质量，并在推理速度上提高了20%到30%。


<details>
  <summary>Details</summary>
Motivation: 当前的无分类器引导方法在条件生成中需要的计算步骤是无条件生成的两倍，导致了显著的高成本，因此需要一种更高效的方法。

Method: 提出了一种简单且普遍适用的自适应引导策略Step AG，通过限制无分类器引导在前几个去噪步骤中的适用性，提高了图像质量和图像-文本对齐，同时加速了生成过程。

Result: 在图像质量和图像-文本对齐方面的评估表明，无分类器引导仅需在最初几个去噪步骤中使用即可生成高质量、良好条件的图像，且能够平均加速20%到30%。

Conclusion: 我们的方法Step AG在不同的设置下以及多种模型（包括视频生成模型）中都表现出了卓越的性能。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [304] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 研究发现文本生成图像模型在文化上下文的准确性上存在显著问题，并提供了一种新的评价基准来量化这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成图像模型在视觉内容生成中的普及，关于其准确代表多元文化背景的能力引发了担忧。

Method: 提出了CulturalFrames，一个用于严格评估视觉生成中的文化表现的人类评价基准，包含从10个国家和5个社会文化领域中收集的983个提示和3637张由4个先进T2I模型生成的对应图像，并辅以超过10k的详细人类标注。

Result: 平均而言，文化预期被错过44%的时间，其中明示性预期错失率达到68%，而隐性预期失败率为49%。现有的T2I评估度量与人类对文化一致性的判断之间的相关性较差。

Conclusion: 当前的文本生成图像模型在文化上下文的准确呈现上存在显著差距，这些模型在文化预期的明示性和隐性方面均未能很好地对齐。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [305] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 论文提出用类似MCTS的算法改善已有非推理VLMs的推理能力，无需再训练，效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索一种无需额外训练或监督即可在非推理视觉-语言模型中引发隐藏知识和推理轨迹的方法，给予已部署模型新的应用潜力。

Method: 使用一种受蒙特卡洛树搜索（MCTS）启发的算法，将子问题-子答案对注入模型的输出流，从而实现推理能力的增强。

Result: 在三个基准上的评估表明，该方法可以带来一致的改进，尤其是在MMMU-PRO基准上实现了总体2%的提升，其中在文科方向取得了显著的9%的提升。

Conclusion: 通过将推理过程视为搜索过程，即将子问题作为更广泛推理轨迹内的潜在决策，可以帮助非推理模型在碎片化的知识之间建立联系，从而产生更长的推理轨迹。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [306] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出ASVR机制，通过对图像语义表示的自回归重建来改善多模态理解，结果表明在多个基准测试上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 典型的大型视觉-语言模型在训练中仅对文本序列进行自回归监督，未充分利用视觉模态，导致模型无法有效利用无字幕的图像、字幕可能遗漏重要的视觉细节、以及某些视觉内容难以通过文字传达等问题。

Method: 本文提出了自回归语义视觉重建（ASVR）方法，该方法在统一的自回归框架内实现了视觉和文本模态的联合学习。

Result: ASVR方法即使在提供连续的图像特征作为输入时，也可以有效地重建离散的语义标记，在多模态理解基准上实现稳定和一致的改进，特别是在LLaVA-1.5上将平均分数提高了5%。

Conclusion: ASVR显著提高了多模态理解的性能，无论在数据规模还是在LLM骨架类型上都表现出一致的改进。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [307] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: 提出MLVTG框架，通过增强多模态对齐与时序建模，提高视频定位任务的精度，获得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer方法在处理视频时间定位任务时，存在注意力冗余和多模态对齐效果不佳的问题。

Method: 提出了MLVTG框架，其中包括MambaAligner和LLMRefiner两个关键模块。MambaAligner使用Vision Mamba模块代替Transformers，以更好地处理时间依赖性并提取视频表征用于多模态对齐；LLMRefiner利用预训练大语言模型的固定层，增强多模态对齐。

Result: MLVTG在QVHighlights, Charades-STA, 和TVSum数据集上取得了最新的性能表现，大幅超越现有基线方法。

Conclusion: MLVTG通过时序建模和语义纯化策略提高了多模态视频定位的精度，显示了其方法的有效性。

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [308] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow is a novel framework for motion prediction in autonomous driving, reducing computation and improving performance.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient and accurate motion prediction in autonomous driving to ensure safety and informed decision-making under dynamic conditions.

Method: TrajFlow uses a flow matching-based framework for motion prediction, employs a ranking loss based on Plackett-Luce distribution, and self-conditioning training technique.

Result: TrajFlow significantly reduces computational overhead while maintaining coherence across predictions and improves uncertainty estimation. It achieves state-of-the-art performance on the Waymo Open Motion Dataset.

Conclusion: TrajFlow achieves state-of-the-art performance on motion prediction for autonomous vehicles and is effective for safety-critical applications.

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [309] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: 首次全面综述了基于Transformer的HSI分类，提供领域进展映射和未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer在高光谱成像中的应用现状，分析其在长距离依赖学习中的潜力和面临的挑战，为研究者提供选择和改进模型的指导。

Method: 对300多篇相关文献进行了综述和分类分析，详细审视从数据预处理到损失设计的各个阶段，结合高光谱成像的独特性评估各种设计选择。

Result: 划分了Transformer在高光谱成像应用中的各个阶段，评估了领域进展并提出了研究计划以应对现有挑战。

Conclusion: 本文首次对基于Transformer的高光谱成像（HSI）分类进行了全面综述，总结并对比了不同的设计选择，并为未来的研究方向提出建议。

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [310] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: 本文提出ECMNet，结合CNN与Mamba构建轻量化语义分割网络，通过多模块增强精准性和效率，在标准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决CNN与Transformer不足之处，尤其是全球上下文建模不足的问题，并利用Mamba在视觉任务中的优势。

Method: 结合CNN与Mamba的轻量化Efficient CNN-Mamba网络，通过胶囊框架融入，设计增强的双注意力模块和多尺度注意力单元，以及Mamba增强的特征融合模块。

Result: 在Cityscapes和CamVid测试数据集上分别实现了70.6%和73.6%的mIoU，仅需0.87M参数和8.27G FLOPs。

Conclusion: 提出的ECMNet在语义分割任务中表现出高效的精度和平衡性。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [311] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 提出了使用SE(3)对称变压器模型预测AAA生长的方法，能够高精度预测AAA直径变化，并识别患者是否在两年内需要手术，可用于个性化监测策略。


<details>
  <summary>Details</summary>
Motivation: 目前的AAA监测标准基于最大直径，但未考虑AAA三维形状与其生长的复杂关系，导致标准化间隔可能不合适。个性化的AAA生长预测能够改进监控策略。

Method: 采用SE(3)对称变压器模型，使用24名AAA患者的纵向数据集进行训练，包含113次CTA扫描，以不规则间隔采样进行预测，验证集来自不同医院的7名AAA患者的25次CTA扫描。

Result: 模型能够在下次扫描时预测AAA生长，直径误差中值为1.18毫米，并能预测患者在两年内是否会符合选择性修复的资格(准确率为0.93)。模型在不同医院的外部验证集中表现良好。

Conclusion: 通过使用SE(3)对称变压器模型，能够直接在富含局部多物理特征的血管模型表面上预测AAA的生长。这比传统参数化的AAA形状预测方法更能保留血管表面的解剖结构和几何保真度。

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [312] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 该研究开发了一种基于对比学习的深度学习方法，用于从全切片图像预测空间分辨的基因表达，相较现有方法在多个基因预测准确性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决高成本的数据采集问题，改进空间转录组学数据在临床诊断癌症中的应用。

Method: 对比学习深度学习方法

Result: 在六个不同疾病数据集中的评估表明，与现有研究相比，我们的方法在高表达、高变异和标记基因预测的Pearson相关系数上分别提高了6.27%、6.11%、11.26%。

Conclusion: 我们的方法改善了现有研究中高表达基因、高变异基因和标记基因的预测准确性，并能够应用于样本数量有限的数据集。

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [313] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出一种专家乘积框架（PoE），不需训练，通过采样实现专家知识组合，改善图像和视频生成任务的控制性。


<details>
  <summary>Details</summary>
Motivation: 整合视觉生成、视觉语言模型以及人造知识来源，如图形引擎和物理模拟器，研究较少，因此需要探索如何从多源头中集成不同的知识

Method: 采用退火重要性采样（AIS）从专家的乘积分布中进行采样，进行推理时的知识组合。

Result: 在图像和视频生成任务中，比单一模型方法具有更好的可控性，并且提供灵活的用户界面。

Conclusion: 提出了一种专家乘积框架（PoE），用于实现异构模型的知识组合，在图像和视频生成任务中表现出优越的控性和灵活的用户界面。

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [314] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: 提出了一种基于注意力的二阶段方法，提高对虚假相关性和分布外背景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 上下文可强烈影响对象感知，特别是在对象出现在分布外背景时会导致偏差表征；而许多图像级以对象为中心的任务需要识别相关区域，因此需解决如何利用上下文信息来减少偏差表征的问题。

Method: 提出了一种基于注意力的二阶段框架：第一阶段处理整个图像以发现对象部分并识别任务相关区域；第二阶段利用输入注意力屏蔽限制接收域，以进行集中分析，同时过滤掉可能的虚假信息；同时对两个阶段进行联合训练。

Result: 广泛的实验表明，该方法在多样化基准上显著提高了模型应对虚假相关性和分布外背景的鲁棒性。

Conclusion: 本文提出的注意力机制方法极大地提高了模型对虚假相关性和分布外背景的鲁棒性。

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [315] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 研究提出SEE框架，利用视觉模型SAM生成伪标签并通过特征聚类提升隐匿物体分割，取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 不完全监督隐匿物体分割（ISCOS）任务因不完全标注数据的有限监督和隐藏对象与背景之间的相似性导致的识别难度而面临挑战。为解决这些问题，本文提出一种方法。

Method: 该方法提出了一种统一的平均教师框架SEE，利用视觉基础模型“分割任何模型（SAM）”生成伪标签，通过教师模型产生的粗略掩码作为提示。同时，设计了一系列伪标签生成、存储和监督的策略以提升伪标签质量，并通过混合粒度特征分组模块促进特征聚类与分割一致性。

Result: 实验证明该方法在多个ISCOS任务中实现了最先进的性能，并可作为即插即用的解决方案来提升现有模型的性能。

Conclusion: 该研究提出了一种统一的方法，名为SEE，解决不完全监督隐匿物体分割问题，并在多个任务中验证了其有效性，达到了最先进的性能。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [316] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: DGMR方法通过多样性指导的MLP剪枝，显著减少了大规模视觉Transformer的参数和FLOPs，性能几乎无损。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优秀，但大规模的模型参数导致计算和内存成本过高。因此需要有效的压缩方法来降低成本。

Method: 提出了一种多样性指导的MLP剪枝方法，即Diversity-Guided MLP Reduction (DGMR)，并通过Gram-Schmidt权重剪枝策略来实现。

Result: 实验结果显示，DGMR方法在多个先进的大规模视觉Transformer模型上实现了超过57.0%的参数和FLOPs减少，且性能几乎无损。特别是在EVA-CLIP-E (4.4B)模型上，实现了71.5%的参数和FLOPs减少，并且没有性能下降。

Conclusion: DGMR方法有效地减少了大规模视觉Transformer模型的参数和计算量，同时保持了接近无损的性能。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [317] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA模型通过减少训练参数和计算成本，实现了对预训练视觉模型的适应，在医疗图像-文本匹配任务中表现优异，提升了文本到图像以及图像到文本的检索准确度。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉-语言对齐方法（基于CLIP）在视觉表示能力方面表现不佳，限制了其在视觉-语言对齐中的效果。尽管多模态屏蔽建模预训练的模型在直接跨模态匹配时表现不佳，但在视觉表示中非常出色。为了解决这一矛盾，该论文提出了一种高效的医疗视觉-语言对齐方法ALTA。

Method: 使用ALTA模型，只需要约8%的可训练参数和少于五分之一的计算耗费，通过适应来自屏蔽记录建模的预训练视觉模型实现视觉-语言匹配任务中的卓越性能。它通过整合时间-多视角X光片输入提高信息一致性。

Result: ALTA在文本到图像准确度和图像到文本检索准确度方面分别提高了超过4%绝对点和约6%绝对点。ALTA的适应性调整促进了更好的视觉和语言理解。

Conclusion: ALTA模型通过自适应预训练视觉模型实现了医疗视觉-语言的高效对齐，在检索和零样本分类任务中表现出色。通过整合时间-多视角X光片输入，进一步提高了X光片与其报告说明之间的信息一致性。实验评估显示，ALTA在文本到图像准确度和图像到文本检索准确度方面超过最佳表现模型4%到6%。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [318] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出了一种新的正则化损失函数，进一步提升了扩散生成模型的性能，无需预训练和额外数据。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常依赖于回归目标，缺乏显式的正则化。我们希望通过新的正则化方法来改善这一点。

Method: 我们提出了一种新的损失函数——Dispersive Loss，它能促进隐藏空间内部表示的分散。

Result: 在ImageNet数据集上的多种模型进行评估，Dispersive Loss在强大的基准上取得了一致的性能提升。

Conclusion: 我们的工作展示了Dispersive Loss如何在不需要额外数据和参数的情况下提高扩散生成模型的性能。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [319] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: Landsat-Bench是一个新的基准测试工具，证明了SSL4EO-L预训练模型比ImageNet提供了更好的特征表示。


<details>
  <summary>Details</summary>
Motivation: 尽管Landsat计划提供了50多年全球一致的地球影像，但缺乏基准测试限制了基于Landsat的地理空间基础模型的发展。为此，提出Landsat-Bench以推动这一方向上的进步。

Method: 引入了一套新的基准测试Landsat-Bench，适应于现有的遥感数据集，并使用SSL4EO-L数据集预训练的Landsat基础模型进行基线和标准化评估。

Result: 在EuroSAT-L和BigEarthNet-L数据集上，使用SSL4EO-L预训练的模型相较于ImageNet分别提高了+4%总体准确率和+5.1%的平均精度。

Conclusion: Landsat-Bench提供了一套针对Landsat影像的基准测试，证明了使用SSL4EO-L预训练的模型在下游任务中比ImageNet具有更好的表现。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [320] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一种新方法，可从未校准视频流实时重建动态3D场景，具备高重建质量和有效的动态场景建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时应对实时处理非校准输入、准确建模动态场景演变和保持长期稳定性及计算效率这三大挑战。

Method: StreamSplat是一个完全前馈的框架，利用静态编码器中的概率采样机制用于3DGS位置预测，并使用动态解码器中的双向变形场进行动态建模。

Result: StreamSplat在静态和动态基准上的实验中持续优于以往工作，并独特地支持任意长度视频流的在线重建。

Conclusion: StreamSplat实现了在线重建任意长度视频流的动态3D场景，且在重建质量和动态场景建模上优于现有方法。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [321] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 研究提出了一种基于Fast AutoAugment的最佳数据增强方法，有效提升小物体检测性能，在DOTA数据集上取得20%性能提升。


<details>
  <summary>Details</summary>
Motivation: 小物体的检测性能远逊于大物体，对于小物体检测效果的提升，是计算机视觉领域最具挑战且重要的问题之一。

Method: 提出了一种使用Fast AutoAugment的最佳数据增强方法，快速找到最优的增强策略以改善小物体检测性能。

Result: 在DOTA数据集上，小物体检测性能提高了20%。

Conclusion: 通过使用基于Fast AutoAugment的最佳数据增强方法，可以显著提高小物体的检测性能，测试表明在DOTA数据集上获得了20%的性能提升。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [322] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 提出了DIsoN，解决了OOD检测中数据共享问题，在多个医学影像数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 在医学影像等安全关键领域，ML模型的安全部署需要检测出在训练中未见的输入特性，以防止不可靠的预测。然而，现有OOD检测方法无法在不共享实际训练数据的情况下有效进行检测。

Method: 提出了一种名为Isolation Network的OOD检测框架，通过解决二分类任务量化将测试样本与训练数据分离的难度，并在数据共享不可能的情况下，通过交换模型参数，实现训练和测试数据的比较。

Result: 引入了DIsoN，能够在数据共享不可能的情况下比较训练和测试数据，且该方法在12个OOD检测任务中表现良好。

Conclusion: DIsoN在四个医学影像数据集上的12个OOD检测任务中表现出色，同时尊重数据隐私。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [323] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: We developed a physics-informed neural simulator using Kelvinlet-based priors and FEM simulations to enhance the real-time accuracy and consistency of soft tissue deformations in surgical applications.


<details>
  <summary>Details</summary>
Motivation: Fast and accurate simulation of soft tissue deformation is crucial for surgical robotics and medical training.

Method: Our framework integrates Kelvinlet-based priors into neural simulators and incorporates large-scale Finite Element Method (FEM) simulations of both linear and nonlinear soft tissue responses.

Result: Our method improves neural network predictions across diverse architectures by enhancing accuracy and physical consistency while maintaining low latency for real-time performance.

Conclusion: Kelvinlet-augmented learning is a powerful and efficient strategy for real-time, physics-aware soft tissue simulation in surgical applications.

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [324] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Main category: stat.ME

TL;DR: 引入了R包midr，可以通过最大解释分解法解释黑箱模型，构建全球替代模型。


<details>
  <summary>Details</summary>
Motivation: 在需要模型和预测可解释性的领域，采用黑箱预测模型需要合适的可解释机器学习（IML）和可解释人工智能（XAI）方法。

Method: 使用最大解释分解法（MID），通过最小化模型预测函数与加性表示之间的平方误差，来得到黑箱模型的低阶加性表示。

Result: midr可用来从黑箱模型中学习，通过构建具有高级分析能力的全球替代模型实现。

Conclusion: 研究引入了R包midr，该工具有效实现了最大解释分解法（MID），可以用于解释黑箱模型。

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [325] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Main category: math.OC

TL;DR: MOSS是一个多目标优化框架，结合了稀疏性、准确性和稳定性，开发的算法可快速计算帕累托前沿，并在预测性能和稳定性上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: MOSS将稀疏性、准确性和稳定性三个重要的可解释性标准纳入到一个多目标优化框架中，帮助实践者快速评估准确性和稳定性之间的权衡。

Method: 我们开发了一种专门的割平面算法来快速计算帕累托前沿，这一算法可以处理超出商业优化求解器能力范围的问题实例。

Result: 实验表明，MOSS在预测性能和稳定性上优于现有的最先进规则集。

Conclusion: MOSS在预测性能和稳定性方面优于现有的规则集方法。

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [326] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Main category: math.OC

TL;DR: 提出一种连续策略-价值迭代算法，利用Langevin型动力学同时更新价值函数和最优控制，证明了收敛性。


<details>
  <summary>Details</summary>
Motivation: 提出一个新的连续策略-价值迭代算法，以同时更新随机控制问题的价值函数和最优控制。

Method: 通过Langevin型动力学进行更新，并通过连续的策略迭代方向实现分布采样和非凸学习技术。

Result: 在哈密顿函数的单调性条件下，实现策略改进并收敛到最优控制。

Conclusion: 连续更新策略迭代可以有效地优化价值函数和识别最优控制。

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [327] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Main category: math.OC

TL;DR: 论文通过改进二阶方法，实现了处理凸-凹极小极大化问题的上界改进，达到更好的复杂度。


<details>
  <summary>Details</summary>
Motivation: 之前的自然推广方法已达到上界，但是作者认为可以进一步优化，所以尝试推广更优的方法。

Method: 推广凸优化的二阶方法，并将其应用于拖延Hessian算法，提出了一个可加速全局收敛算法的二阶Catalyst框架。

Result: 提出的算法将复杂度的上界从O(ε^-2/3)改进至O(ε^-4/7)，实现了加速效果。

Conclusion: 通过推广优化的二阶方法，该论文在解决凸-凹极小极大化问题上实现了改进的上界。

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [328] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Main category: math.OC

TL;DR: 本文提出了一种框架，通过减缩映射改善优化问题的曲率性质，从而加速梯度方法的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 许多高维优化问题在其最小化集合中表现出丰富的几何结构，通常由于过参数化或对称性而形成光滑的流形。本文的动机是利用这些结构信息至少在局部优化问题上，以提高优化算法的效率。

Method: 本文采用对优化问题进行减缩映射的方法，通过内优化问题自然产生的减缩映射来重新参数化参数空间的一部分，使其位于解决方案流形上。

Result: 设计良好的减缩映射能够改善目标的曲率特性，导致问题条件更好，理论上基于梯度的方法收敛速度更快。

Conclusion: 本论文提出了一种通用框架，利用在优点处的结构信息，通过优化问题的减缩映射加速收敛，并解释了在经验中观察到的优化算法增益。

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [329] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Main category: math.OC

TL;DR: 研究稀疏优化中的额外约束问题，提出迭代硬阈值算法变体，通过二步连续投影算子提供全球性收敛保证，改进现有技术。


<details>
  <summary>Details</summary>
Motivation: 在稀疏优化中，虽然使用凸松弛能控制稀疏性，但是许多实际应用不仅需要稀疏性约束，还需要额外的约束。现有算法处理具有混合组合及凸性约束的复杂场景时，通常需要封闭形式的投影，其未必存在，且通常仅提供局部收敛保证。因此，本研究旨在填补这一空缺。

Method: 提出了一种新的迭代硬阈值算法变体，并配备了用于这些混合约束的二步连续投影算子，作为对欧式投影的简单替代。

Result: 在确定性、随机性和零阶设置下，算法输出的目标值提供全球性保证，并且在无附加约束的情况下，通过新技术移除了不消失的系统误差。

Conclusion: 本文提出的方法能够在稀疏性松弛和次优性之间引入新型折中，为算法输出提供全球性收敛保证，改进了现有技术。

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [330] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: 本研究使用符号回归构建了重子物理对物质幂谱影响的解析近似，提供了辨别模型差异的可能。


<details>
  <summary>Details</summary>
Motivation: 重子物理在当前和未来宇宙学调查中对物质分布影响显著，是分析中的关键系统问题。研究旨在寻找简单的符号参数化方法来解析重子物理对物质幂谱的影响。

Method: 使用符号回归构建了重子物理对物质幂谱影响的解析近似。通过CAMELS水力动力学模拟和重子化算法，用以生成在有和无重子效应下物质幂谱的比值函数。

Result: 成功获得了对四个不同水力学模拟以及重子化算法的独立函数，预测误差与之前数值模拟器相类似。提供了对预测不确定性的解析函数，其误差在大尺度和高红移条件下表现物理正确。

Conclusion: 本研究通过符号回归建立了可简化分析重子物理对物质幂谱影响的解析近似模型。这些模型允许直接解释宇宙学和反馈参数的变化影响，并识别影响较小的参数。结果与之前数值模拟器的误差相似，并为实际数据模型辨别提供了可能。

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [331] [Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata](https://arxiv.org/abs/2506.08569)
*Erwan Plantec,Gautier Hamon,Mayalen Etcheverry,Bert Wang-Chak Chan,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Main category: nlin.CG

TL;DR: 论文提出Flow-Lenia，通过优化参数生成复杂生物，实现多物种仿真，并揭示其进化动态。


<details>
  <summary>Details</summary>
Motivation: 创造能够自发产生如自我生成、自我复制、进化和开放性等生命世界中的性质的人工系统。

Method: 提出一种称为Flow-Lenia的质量守恒扩展，并进行了实验以展示其产生空间局部化模式（SLPs）的效果及复杂行为。通过优化更新规则参数，实现OptFlow-Lenia中复杂生物表现的重要行为。

Result: 实验表明，Flow-Lenia能够有效生成具有复杂行为的空间局部化模式，并可优化规则参数以产生显示重要行为的复杂生物。嵌入模型参数允许进行多物种模拟，揭示了系统中的进化动态。

Conclusion: 我们提出了Flow-Lenia，一种Lenia的质量守恒扩展，能够生成具有复杂行为的空间局部化模式。在Flow-Lenia中，模型参数嵌入于其自身的动态中，从而实现多物种仿真。通过使用进化活动框架及其他指标，揭示了系统中出现的进化动态。

Abstract: Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [332] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Main category: physics.med-ph

TL;DR: 本文提出了一种新的联合学习方法用于CBCT到sCT合成，通过在多个中心协作训练，解决了数据隐私和异质性问题，并展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于机构异质性、扫描仪依赖的变化和数据隐私法规，这些因素阻碍了多中心数据共享。提出的方法旨在克服这些挑战。

Method: 提出了一种跨筒体水平的联合学习（FL）方法，用于头颈部区域的CBCT到sCT合成。采用条件生成对抗网络，在三个欧洲医学中心的公开SynthRAD2025挑战数据集上进行协作训练。

Result: 联合模型展示了跨中心的有效泛化能力，平均绝对误差（MAE）在64.38±13.63到85.90±7.10 HU之间，结构相似性指数（SSIM）在0.882±0.022到0.922±0.039之间，峰值信噪比（PSNR）在32.86±0.94到34.91±1.04 dB之间。在外部验证数据集上获得了可比的性能（MAE：75.22±11.81 HU，SSIM：0.904±0.034，PSNR：33.52±2.06 dB），无须额外训练，证实了尽管存在协议、扫描仪差异和配准误差，依然具有稳健的泛化能力。

Conclusion: 该研究展示了利用联合学习（FL）进行CBCT到sCT合成的技术可行性，同时保留了数据隐私。这为跨机构开发通用模型提供了一种协作解决方案，无需集中数据共享或站点特定的微调。

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [333] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 我们开发了一种CLIP风格的框架，利用对比学习将3D蛋白结构与功能注释对齐,在大规模蛋白数据集上实现了高效的零样本检索。


<details>
  <summary>Details</summary>
Motivation: 受到视觉语言模型(VLMs)的最新进展的启发，我们希望通过检索具有相似结构和语义的蛋白质，来促进蛋白质功能的解释。

Method: 我们提出了一种类似CLIP的框架，通过对比学习将三维蛋白质结构与功能注释对齐。

Result: 我们的方法在蛋白质数据库的零样本检索任务中表现出了有前途的性能，尤其是在更具挑战性的跨数据库检索中。

Conclusion: 我们的研究表明，利用多模态基础模型可以在蛋白生物学中实现结构与功能的理解。我们的框架在蛋白质数据的零样本检索任务中表现出了有前途的性能。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [334] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Main category: astro-ph.IM

TL;DR: Astro-DSB模型改进了星形成过程中观测逆预测任务的性能，通过生成式建模提高了解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决星际分子云中的观测逆预测任务，并提高在天体动力学中的模型的解释性和效率。通过扩展扩散模型的应用，考察其在天体物理系统中的生成能力。

Method: 我们采用了Astro-DSB模型，结合配对领域假设，使用生成概率建模进行物理模拟数据和真实观测数据的学习和预测。在生成建模的基础上，进行Out-Of-Distribution测试以评估其在未见初始条件下的表现。

Result: Astro-DSB模型在解释性、学习效率和预测性能方面优于传统天体统计和其他机器学习方法。生成模型在未见条件下的物理仿真中展现出比像素对像素建模更好的性能。

Conclusion: 我们提出的Astro-DSB模型通过配对领域假设在天体动力学中提高了解释性、学习效率和预测性能。基于生成式模型的方法在未见初始条件和不同主导物理过程的物理仿真中优于传统的像素对像素建模。此外，此研究展示了生成模型在物理系统中的学习能力，推动了物理意识生成模型的发展。

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [335] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Main category: cs.CE

TL;DR: 引入KP-PINNs框架，利用再生核Hilbert空间（RKHS）范数和Kernel Packet方法提升PINNs的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络（PINNs）在求解复杂微分方程时，由于通常采用L2损失函数，其数值解在某些情况下可能不准确且不稳定。因此，需要寻找新的方法提升PINNs在科学计算中的稳定性和准确性。

Method: 本研究提出了一个名为Kernel Packet加速的PINNs（KP-PINNs）的框架，利用再生核Hilbert空间（RKHS）范数表达损失函数，并使用Kernel Packet（KP）方法进行计算加速。

Result: 理论结果表明KP-PINNs可以在多种微分方程上保持稳定，数值实验表明它能够有效且高效地求解微分方程。

Conclusion: 本文提出了一种新的物理信息神经网络框架（KP-PINNs），通过使用再生核Hilbert空间（RKHS）范数来改进损失函数表达，并采用Kernel Packet（KP）方法加速计算，能够在多种微分方程上保持稳定。

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [336] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: 引入GradEscape，一个用于攻击AI生成文本检测器的基于梯度的规避器，解决了文本离散性和标记化器不匹配问题，性能优于其他方法，并提出防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成文本(AIGT)检测器面临通过文本离散性造成的不可导计算问题，以及句法需求和不同标记化器间的不匹配问题，因此需要一种新方法来解决这些挑战。

Method: 论文介绍了GradEscape，这是一种基于梯度的AIGT检测器攻击者。它采用新的加权嵌入构建方法以应对文本的离散特性，并通过引入暖启动方法解决了与检测器的标记化器不匹配问题。此外，还使用新式标记化器推断和模型提取技术，即便是仅查询访问的情况下，也能有效逃避检测。

Result: 在四个数据集和三个流行语言模型上的实验结果表明，GradEscape在不同场景下优于现有的AIGT规避器，能在仅使用139M参数的情况下击败拥有11B参数的释义模型。还成功应用于两个真实商业AIGT检测器。分析还揭示了训练数据中文本表达风格差异带来的主要漏洞。

Conclusion: GradEscape是第一个专为攻击AIGT检测器而设计的基于梯度的规避器，通过新方法解决计算和标记化不匹配的问题。此外还提出了一个潜在的防御策略，并开源以改进检测器的鲁棒性。

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


### [337] [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)
*Vivek Vaidya,Aditya Patwardhan,Ashish Kundu*

Main category: cs.CR

TL;DR: 研究了LLMs在生成密码政策中的一致性和准确性问题，结果显示显著挑战。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域, 如访问控制中应用LLMs时，一致性和可预测性问题变得突出，亟需研究其对安全操作的影响。

Method: 本文采用了两种实验方法来研究LLMs在网络安全访问控制系统中的应用。首先，利用预训练LLMs从自然语言提示生成配置文件；其次，为这些模型提供官方的配置文档作为信息基准。

Result: 研究结果显示，目前的LLMs在生成密码策略时存在明显挑战，这为更好地部署LLMs在访问控制系统提供了有价值的见解。

Conclusion: 本研究强调了生成型AI，特别是大型语言模型(LLMs)，在当前的密码政策生成中的显著不一致和准确性问题。这些问题对安全访问控制系统的部署构成重大挑战。

Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are
rapidly being adopted across industry, academia, and government sectors, owing
to their remarkable capabilities in natural language processing. However,
despite their strengths, the inconsistency and unpredictability of LLM outputs
present substantial challenges, especially in security-critical domains such as
access control. One critical issue that emerges prominently is the consistency
of LLM-generated responses, which is paramount for ensuring secure and reliable
operations.
  In this paper, we study the application of LLMs within the context of
Cybersecurity Access Control Systems. Specifically, we investigate the
consistency and accuracy of LLM-generated password policies, translating
natural language prompts into executable pwquality.conf configuration files.
Our experimental methodology adopts two distinct approaches: firstly, we
utilize pre-trained LLMs to generate configuration files purely from natural
language prompts without additional guidance. Secondly, we provide these models
with official pwquality.conf documentation to serve as an informative baseline.
We systematically assess the soundness, accuracy, and consistency of these
AI-generated configurations. Our findings underscore significant challenges in
the current generation of LLMs and contribute valuable insights into refining
the deployment of LLMs in Access Control Systems.

</details>


### [338] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Main category: cs.CR

TL;DR: ReAgent是一种新型防御机制，通过检测代理的一致性，有效应对大型语言模型代理的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型代理在培训和微调过程中面临严重的安全风险，特别是来自后门攻击的风险。需要一种新的方法来解决这些安全风险。

Method: 采用双层检测机制，在执行层面验证代理的思维与行动的一致性；在计划层面通过代理推断指令的一致性检查来检测潜在后门。

Result: ReAgent能够大幅减少后门攻击的成功率，在各种任务中表现出色，尤其在数据库操作任务中可以将攻击成功率降低到90%。

Conclusion: ReAgent显著降低了后门攻击的成功率，特别是在数据库操作任务中表现出色。

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


### [339] [WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks](https://arxiv.org/abs/2506.08602)
*Tingzhi Li,Xuefeng Liu*

Main category: cs.CR

TL;DR: 提出了名为WGLE的新型GNN黑箱水印方法，实现了高效的多比特信息嵌入与所有权验证，且对模型性能影响小。


<details>
  <summary>Details</summary>
Motivation: 现有的指纹识别和黑箱水印方法在计算成本、健壮性以及信息传达方面存在不足，因此需要一种新的水印范式。

Method: WGLE建立在称为Layer-wise Distance Difference on an Edge (LDDE)的关键思想基础上，通过为选定的边预定义正负LDDE值来嵌入水印。

Result: 在六个公共数据集和六个主流GNN架构上，WGLE展示了100%的所有权验证准确率，平均保真度下降仅为0.85%，并具有较强的抗攻击能力与较低的嵌入开销。

Conclusion: WGLE在所有测试的数据集和GNN架构上有效地实现了100%的所有权验证准确率，同时保持了较低的原模型性能损失。

Abstract: Graph Neural Networks (GNNs) are increasingly deployed in graph-related
applications, making ownership verification critical to protect their
intellectual property against model theft. Fingerprinting and black-box
watermarking are two main methods. However, the former relies on determining
model similarity, which is computationally expensive and prone to ownership
collisions after model post-processing such as model pruning or fine-tuning.
The latter embeds backdoors, exposing watermarked models to the risk of
backdoor attacks. Moreover, both methods enable ownership verification but do
not convey additional information. As a result, each distributed model requires
a unique trigger graph, and all trigger graphs must be used to query the
suspect model during verification. Multiple queries increase the financial cost
and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box
watermarking paradigm for GNNs that enables embedding the multi-bit string as
the ownership information without using backdoors. WGLE builds on a key insight
we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the
difference between the feature distance and the prediction distance of two
connected nodes. By predefining positive or negative LDDE values for multiple
selected edges, WGLE embeds the watermark encoding the intended information
without introducing incorrect mappings that compromise the primary task. WGLE
is evaluated on six public datasets and six mainstream GNN architectures along
with state-of-the-art methods. The results show that WGLE achieves 100%
ownership verification accuracy, an average fidelity degradation of 0.85%,
comparable robustness against potential attacks, and low embedding overhead.
The code is available in the repository.

</details>


### [340] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Main category: cs.CR

TL;DR: 通过分析网络攻防代理，揭示其行为模式及某些动作和诱饵的有效性，提出提高可解释性的方法并评估挑战的现实性。


<details>
  <summary>Details</summary>
Motivation: 参加CAGE Challenge 2网络防御挑战，通过分析网络防御和攻击代理的表现来了解它们的行为模式及有效性。

Method: 分析评估集中的重要事件，识别出入侵和清除事件的模式，以评估攻击者和防守者的表现。通过分析环境状态的转换来确定有效和无效的动作，以及诱饵服务对攻击成功率的影响。

Result: 确定了某些动作的无效性介于40%到99%之间；诱饵服务可以阻止高达94%的利用行为；防御者通常可在主机被利用后的一到两个时间步内清除入侵。

Conclusion: 通过简化其复杂状态和动作空间，并跟踪重要事件，可以提高代理成功和失败的可解释性，并揭示赛中防御和攻击代理的细粒度行为。

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [341] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 指令微调的多模态大语言模型能显著提升与大脑活动的对齐度，尤其在处理视频和音频任务上。


<details>
  <summary>Details</summary>
Motivation: 过去的研究主要关注非指令微调的多模态模型或单模态环境的对齐情况，本文旨在通过指令微调的模型来填补这一空白并提升对齐度。

Method: 利用六种视频和两种音频指令微调的多模态大语言模型，通过13个视频任务特定指令评估其与大脑活动的对齐度。

Result: 指令调谐的视频多模态大语言模型在视频任务中特定指令对齐上明显优于非指令调谐的多模态模型（提高15%）和单模态模型（提高20%）。

Conclusion: 该研究表明指令微调的多模态大语言模型能够显著提高与大脑活动的对齐度，尤其是在处理视频和音频任务时。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [342] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Main category: physics.soc-ph

TL;DR: 该研究通过使用强化学习解决马尔科夫决策过程模型，以缓解电力系统中的级联故障风险。结果表明，主动断开某些关键线路能有效减少故障传播。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的渗透率增加，现代电力系统面临着级联故障的风险，因此需要快速、复杂的操作决策来实时缓解这些故障。

Method: 将影响图扩展到马尔科夫决策过程模型，并使用强化学习求解，包括不采取行动的选项以实现保守决策。

Result: 在IEEE 14-bus和IEEE 118-bus系统中验证了模型，结果表明主动断开线路可以有效降低级联风险，某些线路在缓解级联传播中始终表现得尤为重要。

Conclusion: 使用强化学习的策略梯度方法能够有效减小电力系统中的级联故障风险，并且在不恶化系统状态的情况下采取保守行动。

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [343] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 引入了Exp4Fuse框架，通过零样本LLM扩展与融合排名方法提升稀疏检索，实验表明其效果优于现有方法，且在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM-based查询扩展方法在提高信息检索性能时依赖于生成文档的质量，通常需要复杂的提示策略和高级密集检索技术，因此代价高昂且计算密集。为了解决这些限制，作者探索使用零样本LLM-based查询扩展来改善稀疏检索，特别是学习型稀疏检索器。

Method: 引入了一种名为Exp4Fuse的新型融合排名框架，通过间接应用零样本LLM-based查询扩展来增强稀疏检索器的性能。Exp4Fuse通过同时考虑基于原始查询和LLM增强查询的两种检索方法来生成两个排名列表，并使用修改后的倒数排名融合方法将其融合。

Result: Exp4Fuse不仅在提高稀疏检索的性能方面超越了现有的LLM-based查询扩展方法，而且当与高级稀疏检索器结合使用时，在多个基准测试中实现了SOTA结果。

Conclusion: Exp4Fuse在提高稀疏检索性能方面表现出色，不仅超越了现有的LLM-based查询扩展方法，还在与先进稀疏检索技术结合时在多个基准上实现了SOTA结果。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [344] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: 提出了Hierarchical Lexical Graph (HLG)和两种补充检索器以改进RAG在多文档检索中的表现，显著提高了检索准确性，代码已开源。


<details>
  <summary>Details</summary>
Motivation: RAG在处理需要跨语义文档整合答案时表现不佳，需要改进以提高复杂多文档系统的检索性能。

Method: 引入三层级的Hierarchical Lexical Graph (HLG)索引系统和两个补充检索器（StatementGraphRAG和TopicGraphRAG）来进行语义文档细粒度和话题相关性检索。

Result: 在五个数据集上的广泛实验表明，与简单的基于片段的RAG相比，该方法在检索召回率和正确性上平均有23.1%的相对改善。

Conclusion: 通过Hierarchical Lexical Graph (HLG)及其相应的补充检索器（StatementGraphRAG和TopicGraphRAG）的引入，解决了RAG在处理跨语义文档时的不足，显著提高了检索召回率和正确性。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


### [345] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: 本文提出结合 RDF 知识图谱和 GNN 的方法，显著提高了推荐系统的性能，有效利用了 RDF 图的语义信息。


<details>
  <summary>Details</summary>
Motivation: 尽管已创建了逾千个 W3C 标准的 RDF 知识图谱，但其丰富的语义信息尚未在基于 GNN 的推荐系统中得到充分利用。因此提出整合 RDF 知识图谱与图神经网络的方法。

Method: 本文提出将 RDF 知识图谱与图神经网络全面结合的方法，利用 RDF 对象属性的拓扑信息和 RDF 数据类型属性的内容信息，并对多种图神经网络进行了深入评估，分析不同的语义特征初始化和图结构异质性对推荐任务表现的影响。

Result: 通过在多种含有数百万节点的 RDF 图中的推荐场景进行实验，证明利用 RDF 知识图谱的语义丰富性显著改善了推荐系统。

Conclusion: 研究表明利用 RDF 知识图谱的语义丰富性可以显著提高推荐系统的性能，并为基于 GNN 的推荐系统在链接开放数据云上的应用奠定了基础。

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


### [346] [Multimodal Representation Alignment for Cross-modal Information Retrieval](https://arxiv.org/abs/2506.08774)
*Fan Xu,Luis A. Leiva*

Main category: cs.IR

TL;DR: 研究了跨模态检索任务中不同模型的特征对齐问题，提出余弦相似性在对齐任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究不同机器学习模型在多模态检索任务中对于相同概念的不同表示，目标是跨模态的特征对齐。

Method: 通过四种标准相似性度量以及两种通过神经网络实现的学习度量来对视觉和文本嵌入进行对齐。

Result: 研究发现Wasserstein距离是衡量模态间差距的有效方法，而余弦相似性在特征对齐任务中的表现超越了其他几种度量方法。

Conclusion: Wasserstein距离可以作为衡量模态间差距的信息指标，而余弦相似性在特征对齐任务中表现优于其他度量。

Abstract: Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [347] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: PerfTracker在线故障诊断系统可诊断大规模模型训练中的性能问题，成功部署在现代GPU集群中并解决多个复杂问题。


<details>
  <summary>Details</summary>
Motivation: 传统分布式系统或数据中心网络设计的故障诊断方法无法有效应用于实际训练系统中，因此需要能够处理现代GPU集群的复杂性和规模的数据密集训练过程的故障诊断系统。

Method: PerfTracker通过在线的细粒度分析有效总结大规模模型训练(LMT)的函数运行时行为模式，并利用差异化可观察性，以最小的生产影响定位问题根源。

Result: PerfTracker成功应用于具有大量GPU的大规模集群，能诊断出各种复杂性能问题，已在生产环境中作为服务进行部署。

Conclusion: PerfTracker是一款首次运用细粒度分析的在线故障诊断系统，能够在生产环境中诊断大规模模型训练时的性能问题。

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>
