<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.LG](#cs.LG) [Total: 42]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: 本文提出了生成嵌入大语言模型（GEM）方法，使大语言模型在保留其原有能力的同时生成高质量文本嵌入，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前许多应用仍依赖于单独的嵌入模型生成文本嵌入，这可能会使系统复杂化，并在嵌入模型和大语言模型之间引入对查询理解的不一致。为了解决这个限制，本文提出了一种能够使大语言模型生成高质量文本嵌入的方法。

Method: 本文提出了一种简单的自监督方法，称为生成嵌入大语言模型（GEM），通过在文本主体中插入新的特殊标记并操控注意力掩码的方法，使任何大型解码器语言模型生成高质量的文本嵌入。该方法可在后训练或微调阶段轻松集成到现有的大语言模型中。

Result: 实验结果表明，在文本嵌入基准（MTEB）上，应用本文方法后的大语言模型性能显著提升，而在自然语言处理基准（MMLU）上的影响极小

Conclusion: 本文提出的方法可以显著提高原有大语言模型在文本嵌入任务上的表现，同时对其原有的自然语言处理任务表现影响极小。

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>


### [2] [Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR](https://arxiv.org/abs/2506.04364)
*Zheng-Xin Yong,Vineel Pratap,Michael Auli,Jean Maillard*

Main category: cs.CL

TL;DR: 对于新语言的ASR系统建设，应优先增加训练数据中的说话者数量。


<details>
  <summary>Details</summary>
Motivation: 建立一个可以服务全球用户的自动语音识别系统，需要对广泛的口音具有鲁棒性。

Method: 系统研究训练数据中的三个变量对ASR面对未见口音的鲁棒性影响。

Result: 增加说话者数量能够在固定的训练时长下提升ASR的性能，而增加多种口音的说话者优先级效果甚微。

Conclusion: 在ASR训练数据中增加说话者数量比增加每位说话者的贡献时长更为有利。

Abstract: To build an automatic speech recognition (ASR) system that can serve everyone
in the world, the ASR needs to be robust to a wide range of accents including
unseen accents. We systematically study how three different variables in
training data -- the number of speakers, the audio duration per each individual
speaker, and the diversity of accents -- affect ASR robustness towards unseen
accents in a low-resource training regime. We observe that for a fixed number
of ASR training hours, it is more beneficial to increase the number of speakers
(which means each speaker contributes less) than the number of hours
contributed per speaker. We also observe that more speakers enables ASR
performance gains from scaling number of hours. Surprisingly, we observe
minimal benefits to prioritizing speakers with different accents when the
number of speakers is controlled. Our work suggests that practitioners should
prioritize increasing the speaker count in ASR training data composition for
new languages.

</details>


### [3] [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
*Matthieu Tehenan,Vikram Natarajan,Jonathan Michala,Milton Lin,Juri Opitz*

Main category: cs.CL

TL;DR: 提出一种方法分解句子嵌入为可解释组件，通过字典学习进行分析，揭示出嵌入中许多特征是线性编码的，为嵌入提供更透明和可控的表示。


<details>
  <summary>Details</summary>
Motivation: 当前NLP和AI系统中，句子嵌入是核心，但对其内部结构认识较少，且不易解释。提出一种能够将句子嵌入分解为可解释组件的方法。

Method: 提出了一种新方法，通过对词级别表示使用字典学习，定义句子嵌入的可解释组件，并分析池化过程对特征的压缩。

Result: 研究发现句子嵌入中的许多语义和语法特征是通过线性编码的，提升了对嵌入空间的透明度和可控性。

Conclusion: 通过使用字典学习和对token级别表示分析，发现许多语义和句法方面通过线性方式编码在嵌入中，提升了对句子嵌入空间的理解度。

Abstract: Sentence embeddings are central to modern NLP and AI systems, yet little is
known about their internal structure. While we can compare these embeddings
using measures such as cosine similarity, the contributing features are not
human-interpretable, and the content of an embedding seems untraceable, as it
is masked by complex neural transformations and a final pooling operation that
combines individual token embeddings. To alleviate this issue, we propose a new
method to mechanistically decompose sentence embeddings into interpretable
components, by using dictionary learning on token-level representations. We
analyze how pooling compresses these features into sentence representations,
and assess the latent features that reside in a sentence embedding. This
bridges token-level mechanistic interpretability with sentence-level analysis,
making for more transparent and controllable representations. In our studies,
we obtain several interesting insights into the inner workings of sentence
embedding spaces, for instance, that many semantic and syntactic aspects are
linearly encoded in the embeddings.

</details>


### [4] [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
*Neeraj Agrawal,Saurabh Kumar,Priyanka Bhatt,Tanishka Agarwal*

Main category: cs.CL

TL;DR: 提出了一种新的层次文本分类模型HTC-CLIP，通过对比学习结合两种现有方法，提升了分类性能，提高了Macro F1评分。


<details>
  <summary>Details</summary>
Motivation: 由于现有的层次文本分类模型各自具有不同特征，通过同时结合它们的优势可以增强分类性能。

Method: 采用对比学习来学习具有层次感知的文本表示和文本引导路径指导的层次表示，并将两者的概率分布合并使用。

Result: HTC-CLIP模型在两个公共基准数据集上提高了Macro F1评分，较现有最优模型提升了0.99 - 2.37%。

Conclusion: 通过结合两种现有的层次文本分类方法，HTC-CLIP在性能方面表现优越，在两个公共基准数据集上的测试表明其在Macro F1评分方面提高了0.99 - 2.37%。

Abstract: Hierarchical Text Classification (HTC) has recently gained traction given the
ability to handle complex label hierarchy. This has found applications in
domains like E- commerce, customer care and medicine industry among other
real-world applications. Existing HTC models either encode label hierarchy
separately and mix it with text encoding or guide the label hierarchy structure
in the text encoder. Both approaches capture different characteristics of label
hierarchy and are complementary to each other. In this paper, we propose a
Hierarchical Text Classification using Contrastive Learning Informed Path
guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation
and text informed path guided hierarchy representation using contrastive
learning. During the training of HTC-CLIP, we learn two different sets of class
probabilities distributions and during inference, we use the pooled output of
both probabilities for each class to get the best of both representations. Our
results show that the two previous approaches can be effectively combined into
one architecture to achieve improved performance. Tests on two public benchmark
datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP
over the existing state-of-the-art models.

</details>


### [5] [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
*Kurt Micallef,Claudia Borg*

Main category: cs.CL

TL;DR: 本文评估了55个大型语言模型在低资源语言马耳他语上的表现，发现预训练时接触语言和微调是性能的关键因素，微调尽管成本高但效果更好，建议使用更加传统的语言建模方法以提高性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在多种NLP任务中表现出色，但在低资源语言如马耳他语上的表现仍然有限。本文旨在评估LLM在这种语言上的有效性，并探索影响其性能的因素。

Method: 本文研究了55个公开发布的LLM在马耳他语上的表现，开发了一个覆盖11个辨别和生成任务的新基准，通过实验比较了微调和提示策略的效果，并分析了影响性能的各种因素。

Result: 许多模型在马耳他语生成任务中的表现较差，而较小的微调模型整体表现更好。此外，预训练期间的语言接触和指令调优对于性能至关重要。虽然微调初始成本高，但其表现更好且推理成本较低。

Conclusion: 本文通过对55个公开的LLM进行多维度分析，发现模型在马耳他语上表现较差，尤其是在生成任务中，而较小的微调模型在所有任务中表现更好。预训练期间接触马耳他语和指令调优是影响性能的最重要因素。微调虽然初始成本较高，但性能更好且推理成本更低。建议研究人员在处理低资源语言时考虑传统语言建模方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various Natural Language Processing (NLP) tasks, largely due to their
generalisability and ability to perform tasks without additional training.
However, their effectiveness for low-resource languages remains limited. In
this study, we evaluate the performance of 55 publicly available LLMs on
Maltese, a low-resource language, using a newly introduced benchmark covering
11 discriminative and generative tasks. Our experiments highlight that many
models perform poorly, particularly on generative tasks, and that smaller
fine-tuned models often perform better across all tasks. From our
multidimensional analysis, we investigate various factors impacting
performance. We conclude that prior exposure to Maltese during pre-training and
instruction-tuning emerges as the most important factor. We also examine the
trade-offs between fine-tuning and prompting, highlighting that while
fine-tuning requires a higher initial cost, it yields better performance and
lower inference costs. Through this work, we aim to highlight the need for more
inclusive language technologies and recommend that researchers working with
low-resource languages consider more "traditional" language modelling
approaches.

</details>


### [6] [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
*Saurabh Kumar,Sourav Bansal,Neeraj Agrawal,Priyanka Bhatt*

Main category: cs.CL

TL;DR: 提出一种嵌入器与分类器组合模型，在少样本情况下显著提高了不同领域的客户服务意图检测准确率。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够在不同领域（根据渠道、地理位置和语言定义）中泛化的意图分类器，并且仅需少量标注样本。

Method: 采用监督微调方法结合等方正则化训练领域特定的句子嵌入，并使用多语言知识蒸馏策略来实现跨领域的泛化。

Result: 模型在加拿大和墨西哥电商客户服务数据集上的意图检测准确率提高了20-23%。

Conclusion: 提出的模型在少样本领域泛化方面取得了显著的性能提升。

Abstract: Customer care is an essential pillar of the e-commerce shopping experience
with companies spending millions of dollars each year, employing automation and
human agents, across geographies (like US, Canada, Mexico, Chile), channels
(like Chat, Interactive Voice Response (IVR)), and languages (like English,
Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on
annotated data have shown good performance in downstream tasks relevant to
Customer Care. However, model performance is largely subject to the
availability of sufficient annotated domain-specific data. Cross-domain
availability of data remains a bottleneck, thus building an intent classifier
that generalizes across domains (defined by channel, geography, and language)
with only a few annotations, is of great practical value. In this paper, we
propose an embedder-cum-classifier model architecture which extends
state-of-the-art domain-specific models to other domains with only a few
labeled samples. We adopt a supervised fine-tuning approach with isotropic
regularizers to train a domain-specific sentence embedder and a multilingual
knowledge distillation strategy to generalize this embedder across multiple
domains. The trained embedder, further augmented with a simple linear
classifier can be deployed for new domains. Experiments on Canada and Mexico
e-commerce Customer Care dataset with few-shot intent detection show an
increase in accuracy by 20-23% against the existing state-of-the-art
pre-trained models.

</details>


### [7] [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
*Ran Xu,Yuchen Zhuang,Yishan Zhong,Yue Yu,Xiangru Tang,Hang Wu,May D. Wang,Peifeng Ruan,Donghan Yang,Tao Wang,Guanghua Xiao,Carl Yang,Yang Xie,Wenqi Shi*

Main category: cs.CL

TL;DR: MedAgentGYM是一个训练环境，增强LLM代理的编码型医学推理能力，通过基准测试显示性能提升，为生物医学研究开发提供了平台。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型代理中的编码型医学推理能力。

Method: 使用MedAgentGYM对超过30种LLM进行了广泛的基准测试，并通过监督微调和持续强化学习改进模型性能。

Result: Med-Copilot-7B通过监督微调和持续强化学习取得了显著的性能提升，与gpt-4o竞争。

Conclusion: MedAgentGYM为开发LLM基础的编码助手提供了一个综合性平台，以促进高级生物医学研究和实践。

Abstract: We introduce MedAgentGYM, the first publicly available training environment
designed to enhance coding-based medical reasoning capabilities in large
language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across
129 categories derived from authentic real-world biomedical scenarios. Tasks
are encapsulated within executable coding environments, each featuring detailed
task descriptions, interactive feedback mechanisms, verifiable ground-truth
annotations, and scalable training trajectory generation. Extensive
benchmarking of over 30 LLMs reveals a notable performance disparity between
commercial API-based models and open-source counterparts. Leveraging
MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through
supervised fine-tuning (+36.44%) and continued reinforcement learning
(+42.47%), emerging as an affordable and privacy-preserving alternative
competitive with gpt-4o. By offering both a comprehensive benchmark and
accessible, expandable training resources within unified execution
environments, MedAgentGYM delivers an integrated platform to develop LLM-based
coding assistants for advanced biomedical research and practice.

</details>


### [8] [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
*Wesley Scivetti,Tatsuya Aoyama,Ethan Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: 大规模语言模型能够理解罕见语法形式，但在语义理解上存在局限性，与人类学习者不同。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模预训练语言模型是否可以从常见的构造推广到罕见的构造，尤其是其对罕见构造的形式和意义的理解能力。

Method: 采用定制化的合成基准来测试模型对LET-ALONE构造的句法和语义属性的理解能力。

Result: 人类级别的语言模型对构造的形式敏感，但不能正确推广其意义。

Conclusion: 当前的语言模型对于语言形式的敏感性较高，但在理解罕见构造的语义方面表现不佳。

Abstract: Humans have a remarkable ability to acquire and understand grammatical
phenomena that are seen rarely, if ever, during childhood. Recent evidence
suggests that language models with human-scale pretraining data may possess a
similar ability by generalizing from frequent to rare constructions. However,
it remains an open question how widespread this generalization ability is, and
to what extent this knowledge extends to meanings of rare constructions, as
opposed to just their forms. We fill this gap by testing human-scale
transformer language models on their knowledge of both the form and meaning of
the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we
construct a bespoke synthetic benchmark that targets syntactic and semantic
properties of the construction. We find that human-scale LMs are sensitive to
form, even when related constructions are filtered from the dataset. However,
human-scale LMs do not make correct generalizations about LET-ALONE's meaning.
These results point to an asymmetry in the current architectures' sample
efficiency between language form and meaning, something which is not present in
human language learners.

</details>


### [9] [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
*Lev Morozov,Aleksandr Mogilevskii,Alexander Shirnin*

Main category: cs.CL

TL;DR: EmoRAG is a system for detecting perceived emotions in text, using an ensemble of models without additional training. It is efficient, scalable, and achieves comparable results to top systems in emotion detection.


<details>
  <summary>Details</summary>
Motivation: To detect perceived emotions in text efficiently and effectively for SemEval-2025 Task 11, Subtask A.

Method: Uses an ensemble of models to predict emotions without requiring additional model training.

Result: Achieves results comparable to the best performing systems in multi-label emotion detection of speaker emotions from text.

Conclusion: EmoRAG is efficient, scalable, and easy to implement, achieving results comparable to the best systems in emotion detection.

Abstract: This paper describes EmoRAG, a system designed to detect perceived emotions
in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We
focus on predicting the perceived emotions of the speaker from a given text
snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise,
and disgust. Our approach does not require additional model training and only
uses an ensemble of models to predict emotions. EmoRAG achieves results
comparable to the best performing systems, while being more efficient,
scalable, and easier to implement.

</details>


### [10] [Zero-Shot Open-Schema Entity Structure Discovery](https://arxiv.org/abs/2506.04458)
*Xueqiang Xu,Jinfeng Xiao,James Barry,Mohab Elkaref,Jiaru Zou,Pengcheng Jiang,Yunyi Zhang,Max Giammona,Geeth de Mel,Jiawei Han*

Main category: cs.CL

TL;DR: ZOES is a novel approach enhancing entity structure extraction by using enrichment, refinement, and unification, eliminating the need for predefined schemas or datasets, and proving effective across multiple domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the reliance on predefined schemas or annotated datasets, which often lead to incomplete extraction results in existing large language model-based methods.

Method: ZOES utilizes an enrichment, refinement, and unification mechanism to enhance entity structure extraction.

Result: Experiments show that ZOES significantly improves the extraction of entity structures in three domains, demonstrating its effectiveness and generalizability.

Conclusion: ZOES improves LLMs' capability to extract more complete entity structures without the need for predefined schemas or annotated datasets, asserting its effectiveness and generalizability across various domains.

Abstract: Entity structure extraction, which aims to extract entities and their
associated attribute-value structures from text, is an essential task for text
understanding and knowledge graph construction. Existing methods based on large
language models (LLMs) typically rely heavily on predefined entity attribute
schemas or annotated datasets, often leading to incomplete extraction results.
To address these challenges, we introduce Zero-Shot Open-schema Entity
Structure Discovery (ZOES), a novel approach to entity structure extraction
that does not require any schema or annotated samples. ZOES operates via a
principled mechanism of enrichment, refinement, and unification, based on the
insight that an entity and its associated structure are mutually reinforcing.
Experiments demonstrate that ZOES consistently enhances LLMs' ability to
extract more complete entity structures across three different domains,
showcasing both the effectiveness and generalizability of the method. These
findings suggest that such an enrichment, refinement, and unification mechanism
may serve as a principled approach to improving the quality of LLM-based entity
structure discovery in various scenarios.

</details>


### [11] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: 研究水印技术对LLMs的影响，提出了AR方法恢复对齐性，有效且保持水印检测能力。


<details>
  <summary>Details</summary>
Motivation: 为了理解水印技术对大型语言模型（LLMs）的真相性、安全性和有用性等核心属性的影响。

Method: 提出Alignment Resampling (AR)方法，利用外部奖励模型在推理时进行采样，恢复模型的对齐性，并实验验证其有效性。

Result: 实验表明，通过对2-4个水印生成结果进行采样，可以有效恢复或超过基线的对齐分数，同时保持强烈的水印检测能力。

Conclusion: 该研究提出了一种简单的方法来平衡水印强度和模型对齐性，支持负责任地部署带水印的LLMs。

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


### [12] [Aligning Large Language Models with Implicit Preferences from User-Generated Content](https://arxiv.org/abs/2506.04463)
*Zhaoxuan Tan,Zheng Li,Tianyi Liu,Haodong Wang,Hyokun Yun,Ming Zeng,Pei Chen,Zhihan Zhang,Yifan Gao,Ruijie Wang,Priyanka Nigam,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出PUGC框架，从UGC提取隐性偏好，改进模型响应质量，提高训练效率。在实验中表现出显著性能提升及领域特定对齐等优势。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法需依赖人工或先进大模型的精心设计数据，而这成本高且难以扩展。因此，利用无标注的用户生成内容中的隐性人类偏好，生成偏好数据是一个更高效的解决方案。

Method: 提出一个名为PUGC的框架，将用户生成内容转化为用户查询，并生成策略模型的响应；UGC作为参考文本来评分响应，从而使模型与隐性偏好对齐。

Result: 使用DPO和PUGC训练的模型在Alpaca Eval 2测试中性能比传统方法提高了9.37%，并创造了35.93%的最新长度控制胜率。研究还表明该方法在奖励质量、领域特定对齐有效性、对UGC质量的鲁棒性及心智理论能力方面具有优势。

Conclusion: 通过使用PUGC框架，可以在不依赖人为标注数据的情况下，从用户生成内容中提取隐性偏好，提高模型生成结果的质量和领域特定的对齐效果。

Abstract: Learning from preference feedback is essential for aligning large language
models (LLMs) with human values and improving the quality of generated
responses. However, existing preference learning methods rely heavily on
curated data from humans or advanced LLMs, which is costly and difficult to
scale. In this work, we present PUGC, a novel framework that leverages implicit
human Preferences in unlabeled User-Generated Content (UGC) to generate
preference data. Although UGC is not explicitly created to guide LLMs in
generating human-preferred responses, it often reflects valuable insights and
implicit preferences from its creators that has the potential to address
readers' questions. PUGC transforms UGC into user queries and generates
responses from the policy model. The UGC is then leveraged as a reference text
for response scoring, aligning the model with these implicit preferences. This
approach improves the quality of preference data while enabling scalable,
domain-specific alignment. Experimental results on Alpaca Eval 2 show that
models trained with DPO and PUGC achieve a 9.37% performance improvement over
traditional methods, setting a 35.93% state-of-the-art length-controlled win
rate using Mistral-7B-Instruct. Further studies highlight gains in reward
quality, domain-specific alignment effectiveness, robustness against UGC
quality, and theory of mind capabilities. Our code and dataset are available at
https://zhaoxuan.info/PUGC.github.io/

</details>


### [13] [SQLens: An End-to-End Framework for Error Detection and Correction in Text-to-SQL](https://arxiv.org/abs/2506.04494)
*Yue Gong,Chuan Lei,Xiao Qin,Kapil Vaidya,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.CL

TL;DR: SQLens是一种针对大型语言模型生成的SQL语句的语义错误检测与修正框架，显著提升了错误检测和执行精度。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型在text-to-SQL任务中虽然表现出色，但常产生语义不正确但语法有效的查询，因而需要提高其可靠性。

Method: 提出SQLens，一个用于细粒度检测和修正大型语言模型生成的SQL语句中语义错误的框架。SQLens结合底层数据库和大型语言模型的错误信号，识别SQL子句中的潜在语义错误，并利用这些信号指导查询修正。

Result: 实验证明，SQLens在两个公共基准上比最佳的大型语言模型自评方法在错误检测F1值上提升了25.78%，并提升text-to-SQL系统的执行精度最高达20%。

Conclusion: SQLens能够有效地检测和修正大型语言模型生成的SQL语句中的语义错误，提高了text-to-SQL系统的可靠性。

Abstract: Text-to-SQL systems translate natural language (NL) questions into SQL
queries, enabling non-technical users to interact with structured data. While
large language models (LLMs) have shown promising results on the text-to-SQL
task, they often produce semantically incorrect yet syntactically valid
queries, with limited insight into their reliability. We propose SQLens, an
end-to-end framework for fine-grained detection and correction of semantic
errors in LLM-generated SQL. SQLens integrates error signals from both the
underlying database and the LLM to identify potential semantic errors within
SQL clauses. It further leverages these signals to guide query correction.
Empirical results on two public benchmarks show that SQLens outperforms the
best LLM-based self-evaluation method by 25.78% in F1 for error detection, and
improves execution accuracy of out-of-the-box text-to-SQL systems by up to 20%.

</details>


### [14] [DRE: An Effective Dual-Refined Method for Integrating Small and Large Language Models in Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2506.04516)
*Kun Zhao,Bohao Yang,Chen Tang,Siyuan Dai,Haoteng Tang,Chenghua Lin,Liang Zhan*

Main category: cs.CL

TL;DR: 结合SLM和LLM的方法（SLIDE和DRE）提高了对话评估的可靠性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理模糊场景时表现不佳，而SLMs在受到误导或对抗性输入时易受影响。为了利用它们的互补优点，需要结合SLM和LLM的方法。

Method: 提出了SLIDE方法，通过自适应加权结合SLM和LLM，并进一步提出双重细化评估（DRE）方法，利用SLM生成的洞察指导LLM的初步评估，并通过SLM改进LLM的得分，提高准确性。

Result: 实验表明，DRE方法优于现有方法，在各种基准上显示出与人类判断更强的对齐性。

Conclusion: 通过结合小型和大型语言模型（SLM和LLM），可以提供更可靠的评估工具，特别是在开放性任务（如对话评估）中。

Abstract: Large Language Models (LLMs) excel at many tasks but struggle with ambiguous
scenarios where multiple valid responses exist, often yielding unreliable
results. Conversely, Small Language Models (SLMs) demonstrate robustness in
such scenarios but are susceptible to misleading or adversarial inputs. We
observed that LLMs handle negative examples effectively, while SLMs excel with
positive examples. To leverage their complementary strengths, we introduce
SLIDE (Small and Large Integrated for Dialogue Evaluation), a method
integrating SLMs and LLMs via adaptive weighting. Building on SLIDE, we further
propose a Dual-Refinement Evaluation (DRE) method to enhance SLM-LLM
integration: (1) SLM-generated insights guide the LLM to produce initial
evaluations; (2) SLM-derived adjustments refine the LLM's scores for improved
accuracy. Experiments demonstrate that DRE outperforms existing methods,
showing stronger alignment with human judgment across diverse benchmarks. This
work illustrates how combining small and large models can yield more reliable
evaluation tools, particularly for open-ended tasks such as dialogue
evaluation.

</details>


### [15] [Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation](https://arxiv.org/abs/2506.04521)
*Di Wu,Seth Aycock,Christof Monz*

Main category: cs.CL

TL;DR: 逐步翻译的手段并未表现出预期的性能提升，简化为直接提示“再次翻译”效果更佳。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM（大型语言模型）在翻译任务中链式思维（CoT）策略的有效性，主张进一步研究影响CoT在翻译中的有效因素。

Method: 通过实验数据的对比，评估利用链式思维（CoT）进行任务分解的翻译策略的有效性。

Result: 没有明确证据表明在测试模型中通过明确分解翻译过程能够带来性能提升。

Conclusion: 分析表明，在翻译过程中，仅通过简单地提示LLMs“再次翻译”比依赖于手工设计的逐步翻译方式效果更好。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities for
many tasks, often by explicitly decomposing the task via Chain-of-Thought (CoT)
reasoning. Recent work on LLM-based translation designs hand-crafted prompts to
decompose translation, or trains models to incorporate intermediate
steps.~\textit{Translating Step-by-step}~\citep{briakou2024translating}, for
instance, introduces a multi-step prompt with decomposition and refinement of
translation with LLMs, which achieved state-of-the-art results on WMT24. In
this work, we scrutinise this strategy's effectiveness. Empirically, we find no
clear evidence that performance gains stem from explicitly decomposing the
translation process, at least for the models on test; and we show that simply
prompting LLMs to ``translate again'' yields even better results than
human-like step-by-step prompting. Our analysis does not rule out the role of
reasoning, but instead invites future work exploring the factors for CoT's
effectiveness in the context of translation.

</details>


### [16] [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)
*William Sheffield,Kanishka Misra,Valentina Pyatkin,Ashwini Deo,Kyle Mahowald,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 研究显示LLMs可以区分“just”的广泛类别，但难以捕捉其细微差别。


<details>
  <summary>Details</summary>
Motivation: 分析大规模语言模型（LLMs）对理解英语话语粒子“just”细微意义的能力。

Method: 使用由专家语言学家精心创建和标记的数据对LLMs进行测试。

Result: 发现LLMs能区分较广泛的类别，但难以完全捕捉细微差别。

Conclusion: LLMs在理解话语粒子的细微差别方面仍存在不足。

Abstract: Discourse particles are crucial elements that subtly shape the meaning of
text. These words, often polyfunctional, give rise to nuanced and often quite
disparate semantic/discourse effects, as exemplified by the diverse uses of the
particle "just" (e.g., exclusive, temporal, emphatic). This work investigates
the capacity of LLMs to distinguish the fine-grained senses of English "just",
a well-studied example in formal semantics, using data meticulously created and
labeled by expert linguists. Our findings reveal that while LLMs exhibit some
ability to differentiate between broader categories, they struggle to fully
capture more subtle nuances, highlighting a gap in their understanding of
discourse particles.

</details>


### [17] [BSBench: will your LLM find the largest prime number?](https://arxiv.org/abs/2506.04535)
*K. O. T. Erziev*

Main category: cs.CL

TL;DR: 论文提出了一个基准测试方法用于评估大语言模型在无合理答案问题上的表现，发现现有模型表现不佳，并提供了相关的代码和数据。


<details>
  <summary>Details</summary>
Motivation: 认为在无解问题上测试大语言模型并不是无意义的，旨在研究其在此类问题上的表现。

Method: 提出一种基准方法来修改现有数据集，以测试大语言模型在无合理答案问题上的表现。

Result: 经过测试，发现现有模型在此类问题上表现不佳。

Conclusion: 现有的语言模型在处理无解问题时，表现远未达到理想状态。

Abstract: We propose that benchmarking LLMs on questions which have no reasonable
answer actually isn't as silly as it sounds. We also present a benchmark that
allows such testing and a method to modify the existing datasets, and discover
that existing models demonstrate a performance far from the perfect on such
questions. Our code and data artifacts are available at
https://github.com/L3G5/impossible-bench

</details>


### [18] [SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?](https://arxiv.org/abs/2506.04557)
*Senyu Li,Jiayi Wang,Felermino D. M. A. Ali,Colin Cherry,Daniel Deutsch,Eleftheria Briakou,Rui Sousa-Silva,Henrique Lopes Cardoso,Pontus Stenetorp,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 引入一个大规模非洲语言机器翻译评估数据集，并开发新的评价指标显著提升低资源语言翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前评价非洲低资源语言的机器翻译质量的指标存在覆盖率有限和低资源环境下性能不佳的问题。

Method: 引入SSA-MTE，一个大规模的人类标注机器翻译评估数据集，并基于该数据开发改进的评价指标SSA-COMET和SSA-COMET-QE，同时对提示驱动的方法进行基准测试。

Result: SSA-COMET模型显著优于AfriCOMET，并在低资源语言上与强大的LLM具有竞争力。

Conclusion: SSA-COMET模型在低资源语言的机器翻译评估上具备竞争力，并提供了开放资源以支持未来研究。

Abstract: Evaluating machine translation (MT) quality for under-resourced African
languages remains a significant challenge, as existing metrics often suffer
from limited language coverage and poor performance in low-resource settings.
While recent efforts, such as AfriCOMET, have addressed some of the issues,
they are still constrained by small evaluation sets, a lack of publicly
available training data tailored to African languages, and inconsistent
performance in extremely low-resource scenarios. In this work, we introduce
SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13
African language pairs from the News domain, with over 63,000 sentence-level
annotations from a diverse set of MT systems. Based on this data, we develop
SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free
evaluation metrics. We also benchmark prompting-based approaches using
state-of-the-art LLMs like GPT-4o and Claude. Our experimental results show
that SSA-COMET models significantly outperform AfriCOMET and are competitive
with the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on
low-resource languages such as Twi, Luo, and Yoruba. All resources are released
under open licenses to support future research.

</details>


### [19] [Demonstrations of Integrity Attacks in Multi-Agent Systems](https://arxiv.org/abs/2506.04572)
*Can Zheng,Yuhan Cao,Xiaoning Dong,Tianxing He*

Main category: cs.CL

TL;DR: 研究揭示恶意智能体可以通过提示操控影响多智能体系统的行为，建议增强安全机制以防止此类攻击。


<details>
  <summary>Details</summary>
Motivation: 研究如何保护多智能体系统免受恶意攻击，以确保这些系统的完整性和可靠性。

Method: 分析恶意智能体如何通过微妙的提示操控来偏向多智能体系统操作，并设计四种类型的攻击以测试其效果。

Result: 证明通过精心设计的提示操控可以造成多智能体系统的系统性偏见，这使得恶意智能体能够有效地误导评估系统并操控协作智能体。此外，现有检测机制如GPT-4o-mini和o3-mini无法检测到这些攻击。

Conclusion: 需要加强多智能体系统的安全协议和内容验证机制，以应对复杂的风险情景评估，以及提升监测系统的能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding, code generation, and complex planning.
Simultaneously, Multi-Agent Systems (MAS) have garnered attention for their
potential to enable cooperation among distributed agents. However, from a
multi-party perspective, MAS could be vulnerable to malicious agents that
exploit the system to serve self-interests without disrupting its core
functionality. This work explores integrity attacks where malicious agents
employ subtle prompt manipulation to bias MAS operations and gain various
benefits. Four types of attacks are examined: \textit{Scapegoater}, who
misleads the system monitor to underestimate other agents' contributions;
\textit{Boaster}, who misleads the system monitor to overestimate their own
performance; \textit{Self-Dealer}, who manipulates other agents to adopt
certain tools; and \textit{Free-Rider}, who hands off its own task to others.
We demonstrate that strategically crafted prompts can introduce systematic
biases in MAS behavior and executable instructions, enabling malicious agents
to effectively mislead evaluation systems and manipulate collaborative agents.
Furthermore, our attacks can bypass advanced LLM-based monitors, such as
GPT-4o-mini and o3-mini, highlighting the limitations of current detection
mechanisms. Our findings underscore the critical need for MAS architectures
with robust security protocols and content validation mechanisms, alongside
monitoring systems capable of comprehensive risk scenario assessment.

</details>


### [20] [Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis](https://arxiv.org/abs/2506.04574)
*Dimitris Vamvourellis,Dhagash Mehta*

Main category: cs.CL

TL;DR: 在金融情感分析中，直觉思维比推理思维更有效。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在零样本金融情感分析中的表现，研究推理是否能够提高模型性能。

Method: 对三种专有LLM进行了比较，分别采用模拟“系统1”和“系统2”思维的提示范例进行实验。

Result: 推理并没有提高该任务的性能，最快、最人性化的模型是GPT-4o，并且没有使用链式思维提示。

Conclusion: 快速、直觉的“系统1”思维与人类判断更一致，推理可能引发过度思考，从而导致次优预测结果。

Abstract: We investigate the effectiveness of large language models (LLMs), including
reasoning-based and non-reasoning models, in performing zero-shot financial
sentiment analysis. Using the Financial PhraseBank dataset annotated by domain
experts, we evaluate how various LLMs and prompting strategies align with
human-labeled sentiment in a financial context. We compare three proprietary
LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that
simulate System 1 (fast and intuitive) or System 2 (slow and deliberate)
thinking and benchmark them against two smaller models (FinBERT-Prosus,
FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest
that reasoning, either through prompting or inherent model design, does not
improve performance on this task. Surprisingly, the most accurate and
human-aligned combination of model and method was GPT-4o without any
Chain-of-Thought (CoT) prompting. We further explore how performance is
impacted by linguistic complexity and annotation agreement levels, uncovering
that reasoning may introduce overthinking, leading to suboptimal predictions.
This suggests that for financial sentiment classification, fast, intuitive
"System 1"-like thinking aligns more closely with human judgment compared to
"System 2"-style slower, deliberative reasoning simulated by reasoning models
or CoT prompting. Our results challenge the default assumption that more
reasoning always leads to better LLM decisions, particularly in high-stakes
financial applications.

</details>


### [21] [Are LLMs Reliable Translators of Logical Reasoning Across Lexically Diversified Contexts?](https://arxiv.org/abs/2506.04575)
*Qingchuan Li,Jiatong Li,Zirui Liu,Mingyue Cheng,Yuting Zeng,Qi Liu,Tongxuan Liu*

Main category: cs.CL

TL;DR: 提出了SCALe基准测试和新方法MenTaL，以提高LLM在处理词汇多样化文本中的翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的逻辑推理基准缺乏词汇多样性，未能充分考验LLM的翻译能力，导致LLM在翻译多样化词汇时表现不稳定。因此，需要一个新的基准来挑战和提升这一能力。

Method: 提出了一种新的方法MenTaL，通过在翻译前引导LLMs构建统一的表达形式表来解决词汇多样化问题。此外，采用了上下文学习和监督微调（SFT）来增强性能。

Result: 实验表明，现有LLMs在应对词汇多样化方面表现不足。使用SCALe基准测试并应用MenTaL方法后，LLMs的表现得到显著改善。

Conclusion: 通过新方法MenTaL的应用，可以显著提高LLM翻译器在词汇多样化文本上的表现。

Abstract: Neuro-symbolic approaches combining large language models (LLMs) with solvers
excels in logical reasoning problems need long reasoning chains. In this
paradigm, LLMs serve as translators, converting natural language reasoning
problems into formal logic formulas. Then reliable symbolic solvers return
correct solutions. Despite their success, we find that LLMs, as translators,
struggle to handle lexical diversification, a common linguistic phenomenon,
indicating that LLMs as logic translators are unreliable in real-world
scenarios. Moreover, existing logical reasoning benchmarks lack lexical
diversity, failing to challenge LLMs' ability to translate such text and thus
obscuring this issue. In this work, we propose SCALe, a benchmark designed to
address this significant gap through **logic-invariant lexical
diversification**. By using LLMs to transform original benchmark datasets into
lexically diversified but logically equivalent versions, we evaluate LLMs'
ability to consistently map diverse expressions to uniform logical symbols on
these new datasets. Experiments using SCALe further confirm that current LLMs
exhibit deficiencies in this capability. Building directly on the deficiencies
identified through our benchmark, we propose a new method, MenTaL, to address
this limitation. This method guides LLMs to first construct a table unifying
diverse expressions before performing translation. Applying MenTaL through
in-context learning and supervised fine-tuning (SFT) significantly improves the
performance of LLM translators on lexically diversified text. Our code is now
available at https://github.com/wufeiwuwoshihua/LexicalDiver.

</details>


### [22] [Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching](https://arxiv.org/abs/2506.04579)
*Jianfei Zhang,Bei Li,Jun Bai,Rumei Li,Yanmeng Wang,Chenghua Lin,Wenge Rong*

Main category: cs.CL

TL;DR: 引入梯度匹配方法改进演示选择，提升了多示例上下文学习的效果。在多个数据集和语言模型上，相较随机选择有明显提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中演示选择的随机化问题。在许多案例演示中，由于传统的实例级检索不适用，现有方法依然采用随机选择。这限制了领域内知识模型快速适应任务的能力。

Method: 通过引入一种新的梯度匹配方法，选择演示样本，使其微调梯度与目标任务的整个训练集对齐，试图在选定样本内实现整个训练集的学习效果。

Result: 提出的方法在从4-shot到128-shot的场景下，在包括Qwen2.5-72B和Llama3-70B在内的大型语言模型上，较随机选择有显著提高。例如，在Qwen2.5-72B和Llama3-70B上超过随机选择4%，在5个闭源模型上大约提高2%。

Conclusion: 实现了更可靠和有效的多示例上下文学习，为大规模语言模型的广泛应用铺平道路。

Abstract: In-Context Learning (ICL) empowers Large Language Models (LLMs) for rapid
task adaptation without Fine-Tuning (FT), but its reliance on demonstration
selection remains a critical challenge. While many-shot ICL shows promising
performance through scaled demonstrations, the selection method for many-shot
demonstrations remains limited to random selection in existing work. Since the
conventional instance-level retrieval is not suitable for many-shot scenarios,
we hypothesize that the data requirements for in-context learning and
fine-tuning are analogous. To this end, we introduce a novel gradient matching
approach that selects demonstrations by aligning fine-tuning gradients between
the entire training set of the target task and the selected examples, so as to
approach the learning effect on the entire training set within the selected
examples. Through gradient matching on relatively small models, e.g.,
Qwen2.5-3B or Llama3-8B, our method consistently outperforms random selection
on larger LLMs from 4-shot to 128-shot scenarios across 9 diverse datasets. For
instance, it surpasses random selection by 4% on Qwen2.5-72B and Llama3-70B,
and by around 2% on 5 closed-source LLMs. This work unlocks more reliable and
effective many-shot ICL, paving the way for its broader application.

</details>


### [23] [SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing](https://arxiv.org/abs/2506.04583)
*Hongjun Liu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 提出了一个新方法SUCEA，通过细分声明和迭代检索证据来提升事实核查准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动事实核查系统在处理对抗性声明时表现不佳，需要改进。

Method: 提出了一个无训练方法SUCEA，分为三个步骤：1）声明分割和去语境化；2）迭代检索证据和声明编辑；3）证据聚合和标签预测。

Result: 在两个具有挑战性的事实核查数据集上的实验表明，我们的框架在检索和蕴涵标签准确性方面显著提升，超过了四个强大的基线。

Conclusion: 我们的框架显著提高了检索和蕴涵标签准确性，优于四种强大的基于声明分解的方法。

Abstract: Automatic fact-checking has recently received more attention as a means of
combating misinformation. Despite significant advancements, fact-checking
systems based on retrieval-augmented language models still struggle to tackle
adversarial claims, which are intentionally designed by humans to challenge
fact-checking systems. To address these challenges, we propose a training-free
method designed to rephrase the original claim, making it easier to locate
supporting evidence. Our modular framework, SUCEA, decomposes the task into
three steps: 1) Claim Segmentation and Decontextualization that segments
adversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval
and Claim Editing that iteratively retrieves evidence and edits the subclaim
based on the retrieved evidence; 3) Evidence Aggregation and Label Prediction
that aggregates all retrieved evidence and predicts the entailment label.
Experiments on two challenging fact-checking datasets demonstrate that our
framework significantly improves on both retrieval and entailment label
accuracy, outperforming four strong claim-decomposition-based baselines.

</details>


### [24] [MuSciClaims: Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.04585)
*Yash Kumar Lal,Manikanta Bandham,Mohammad Saqib Hasan,Apoorva Kashi,Mahnaz Koupaee,Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: 该研究介绍了一个新的基准MuSciClaims来测试视觉语言模型的主张验证能力，发现多数模型表现不佳，对主张的支持判断存在偏见。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏直接测试科学文章中主张验证能力的多模态基准数据，而科学问答、图表注释等领域已有大量研究工作。该研究旨在填补这一差距。

Method: 研究采用自动从科学文章中提取支持的主张，手动进行扰动以产生矛盾的主张，并通过这些设计扰动来测试具体的主张验证能力。引入了一套诊断任务来帮助理解模型的失败之处。

Result: 大多数视觉语言模型在主张验证任务上表现较差，F1得分在0.3-0.5之间，即使是表现最好的模型也仅达到0.77 F1。模型对主张的支持判断存在偏见，难以定位图表中的正确证据，难以跨模态聚合信息，并且常常不能理解图表的基本组成部分。

Conclusion: 当前的视觉语言模型在验证科学文章中的主张方面表现不佳，存在偏向于支持主张的趋势，并且对主张中的微妙扰动误解。该研究引入了一个新的基准MuSciClaims，以便更好地评估模型的主张验证能力。

Abstract: Assessing scientific claims requires identifying, extracting, and reasoning
with multimodal data expressed in information-rich figures in scientific
literature. Despite the large body of work in scientific QA, figure captioning,
and other multimodal reasoning tasks over chart-based data, there are no
readily usable multimodal benchmarks that directly test claim verification
abilities. To remedy this gap, we introduce a new benchmark MuSciClaims
accompanied by diagnostics tasks. We automatically extract supported claims
from scientific articles, which we manually perturb to produce contradicted
claims. The perturbations are designed to test for a specific set of claim
verification capabilities. We also introduce a suite of diagnostic tasks that
help understand model failures. Our results show most vision-language models
are poor (~0.3-0.5 F1), with even the best model only achieving 0.77 F1. They
are also biased towards judging claims as supported, likely misunderstanding
nuanced perturbations within the claims. Our diagnostics show models are bad at
localizing correct evidence within figures, struggle with aggregating
information across modalities, and often fail to understand basic components of
the figure.

</details>


### [25] [LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models](https://arxiv.org/abs/2506.04586)
*Wen Ding,Fan Qian*

Main category: cs.CL

TL;DR: LESS通过利用大语言模型改善伪标签，提高了自动语音识别和翻译的准确性，效果显著，并适用于多语言和任务。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLMs）来改进自动语音识别或翻译产生的伪标签，以提高未标注数据的处理效率和准确性。

Method: 在LESS框架中，通过大语言模型（LLM）修正来自ASR或AST的伪标签文本，并采用数据过滤策略，以优化LLM知识传递的效率。

Result: 在普通话ASR和西班牙语到英语AST任务上，LESS实现了Wenet Speech测试集中3.77%的WER绝对值减少，以及在Callhome和Fisher测试集上的34.0和64.7的BLEU分数。

Conclusion: LESS框架在多语言的ASR和AST任务中表现出色，显著降低了错误率，提升了翻译准确性，验证了其跨语言、任务和领域的适应性。

Abstract: We introduce LESS (Large Language Model Enhanced Semi-supervised Learning), a
versatile framework that leverages Large Language Models (LLMs) to correct
pseudo labels generated from in-the-wild data. Within the LESS framework,
pseudo-labeled text from Automatic Speech Recognition (ASR) or Automatic Speech
Translation (AST) of the unsupervised data is refined by an LLM, and augmented
by a data filtering strategy to optimize LLM knowledge transfer efficiency.
Experiments on both Mandarin ASR and Spanish-to-English AST tasks show that
LESS achieves a notable absolute WER reduction of 3.77% on the Wenet Speech
test set, as well as BLEU scores of 34.0 and 64.7 on Callhome and Fisher test
sets respectively. These results validate the adaptability of LESS across
different languages, tasks, and domains. Ablation studies conducted with
various LLMs and prompt configurations provide novel insights into leveraging
LLM-derived knowledge for speech processing applications.

</details>


### [26] [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)
*Chengwu Liu,Ye Yuan,Yichun Yin,Yan Xu,Xin Xu,Zaoyu Chen,Yasheng Wang,Lifeng Shang,Qun Liu,Ming Zhang*

Main category: cs.CL

TL;DR: 提出$Safe$框架，通过形式数学语言Lean 4验证语言模型生成的内容，显著提高性能，解决幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 为了克服链式思维中难以检测的幻觉问题，我们受到数学证明的启发，决定提供一种可以验证幻觉的形式化方法。

Method: 我们在每个推理步骤中使用形式数学语言Lean 4来表达数学主张，并提供形式证明来识别幻觉，而不是分配任意分数。

Result: 在多个语言模型和各种数学数据集上，这种方法表现出显著的性能提升，并提供可解释和可验证的证据。

Conclusion: 我们提出了一种回顾性、步骤感知的形式验证框架$Safe$，可以在多个语言模型和各种数学数据集上评估，显著提高性能，同时提供可解释和可验证的证据。

Abstract: Chain-of-Thought (CoT) prompting has become the de facto method to elicit
reasoning capabilities from large language models (LLMs). However, to mitigate
hallucinations in CoT that are notoriously difficult to detect, current methods
such as process reward models (PRMs) or self-consistency operate as opaque
boxes and do not provide checkable evidence for their judgments, possibly
limiting their effectiveness. To address this issue, we draw inspiration from
the idea that "the gold standard for supporting a mathematical claim is to
provide a proof". We propose a retrospective, step-aware formal verification
framework $Safe$. Rather than assigning arbitrary scores, we strive to
articulate mathematical claims in formal mathematical language Lean 4 at each
reasoning step and provide formal proofs to identify hallucinations. We
evaluate our framework $Safe$ across multiple language models and various
mathematical datasets, demonstrating a significant performance improvement
while offering interpretable and verifiable evidence. We also propose
$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$
formal statements. To the best of our knowledge, our work represents the first
endeavor to utilize formal mathematical language Lean 4 for verifying natural
language content generated by LLMs, aligning with the reason why formal
mathematical languages were created in the first place: to provide a robust
foundation for hallucination-prone human-written proofs.

</details>


### [27] [A MISMATCHED Benchmark for Scientific Natural Language Inference](https://arxiv.org/abs/2506.04603)
*Firoz Shaik,Mobashir Sadat,Nikita Gautam,Doina Caragea,Cornelia Caragea*

Main category: cs.CL

TL;DR: 引入MISMATCHED基准测试，涵盖心理学、工程和公共卫生领域，数据集和代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有的科学NLI数据集主要来自计算机科学领域，其他领域数据集被忽视。

Method: 对MISMATCHED基准进行强基线测试，使用预训练的SLM和LLM。

Result: 我们最佳基线的Macro F1仅为78.17%，展示了未来改进的巨大空间。

Conclusion: 我们展示了将隐含科学NLI关系的句子对纳入模型训练可提高其在科学NLI上的表现。

Abstract: Scientific Natural Language Inference (NLI) is the task of predicting the
semantic relation between a pair of sentences extracted from research articles.
Existing datasets for this task are derived from various computer science (CS)
domains, whereas non-CS domains are completely ignored. In this paper, we
introduce a novel evaluation benchmark for scientific NLI, called MISMATCHED.
The new MISMATCHED benchmark covers three non-CS domains-PSYCHOLOGY,
ENGINEERING, and PUBLIC HEALTH, and contains 2,700 human annotated sentence
pairs. We establish strong baselines on MISMATCHED using both Pre-trained Small
Language Models (SLMs) and Large Language Models (LLMs). Our best performing
baseline shows a Macro F1 of only 78.17% illustrating the substantial headroom
for future improvements. In addition to introducing the MISMATCHED benchmark,
we show that incorporating sentence pairs having an implicit scientific NLI
relation between them in model training improves their performance on
scientific NLI. We make our dataset and code publicly available on GitHub.

</details>


### [28] [Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for Efficient Reasoning](https://arxiv.org/abs/2506.04611)
*Ho-Lam Chung,Teng-Yun Hsiao,Hsiao-Ying Huang,Chunerh Cho,Jian-Ren Lin,Zhang Ziwei,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 提出一种轻量级方法ADAPT，通过多样性数据策略增强测试时缩放(TTS)，在数学推理中节省计算资源，同时提高准确率。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放(TTS)在推理过程中分配额外的计算资源以提高大型语言模型(LLMs)的推理性能。然而，优化推理性能的模型通常产生较少样本的输出，限制了TTS的有效性。

Method: 提出ADAPT方法，通过前缀微调和专注于多样性的数据策略来解决推理优化模型产生的输出不够多样的问题。

Result: 在数学推理任务中，ADAPT方法达到了80%的准确率，其计算资源需求仅为强基线的八分之一。

Conclusion: 生成多样性在最大化测试时缩放(TTS)有效性中起着至关重要的作用。

Abstract: Test-Time Scaling (TTS) improves the reasoning performance of Large Language
Models (LLMs) by allocating additional compute during inference. We conduct a
structured survey of TTS methods and categorize them into sampling-based,
search-based, and trajectory optimization strategies. We observe that
reasoning-optimized models often produce less diverse outputs, which limits TTS
effectiveness. To address this, we propose ADAPT (A Diversity Aware Prefix
fine-Tuning), a lightweight method that applies prefix tuning with a
diversity-focused data strategy. Experiments on mathematical reasoning tasks
show that ADAPT reaches 80% accuracy using eight times less compute than strong
baselines. Our findings highlight the essential role of generative diversity in
maximizing TTS effectiveness.

</details>


### [29] [Subjective Perspectives within Learned Representations Predict High-Impact Innovation](https://arxiv.org/abs/2506.04616)
*Likun Cao,Rui Pan,James Evans*

Main category: cs.CL

TL;DR: 研究量化了创新者的主观视角和创新机会，发现视角多样性有助于创造力，而背景多样性则可能产生消极影响，影响团队组建和研究策略。


<details>
  <summary>Details</summary>
Motivation: 利用新兴的机器学习方法来建模创新者的个人视角和人际创新机会，探索这些因素如何影响创新潜能。

Method: 使用动态语言表示的几何空间模型，量化创新者的主观视角与创新机会，并通过对数百万科学家、发明家、作家、企业家和维基百科贡献者的数据分析，验证视角和背景多样性对创新的影响。

Result: 研究发现，当通过合作者对创作的视角差异和经历间的差异来分解视角和背景多样性时，前者一贯预示着创意成就的产生，而后者则可能产生反作用。这一现象在所有实例和时间段中被验证。

Conclusion: 本研究通过量化主观视角和创新机会，揭示了其如何预测个体和团体未来创造性关注和成功结合的想法。视角多样性有助于创意成就，而背景多样性则可能产生相反的效果。这些发现可用于指导团队组建和研究政策的发展。

Abstract: Existing studies of innovation emphasize the power of social structures to
shape innovation capacity. Emerging machine learning approaches, however,
enable us to model innovators' personal perspectives and interpersonal
innovation opportunities as a function of their prior trajectories of
experience. We theorize then quantify subjective perspectives and innovation
opportunities based on innovator positions within the geometric space of
concepts inscribed by dynamic language representations. Using data on millions
of scientists, inventors, writers, entrepreneurs, and Wikipedia contributors
across the creative domains of science, technology, film, entrepreneurship, and
Wikipedia, here we show that measured subjective perspectives anticipate what
ideas individuals and groups creatively attend to and successfully combine in
future. When perspective and background diversity are decomposed as the angular
difference between collaborators' perspectives on their creation and between
their experiences, the former consistently anticipates creative achievement
while the latter portends its opposite, across all cases and time periods
examined. We analyze a natural experiment and simulate creative collaborations
between AI (large language model) agents designed with various perspective and
background diversity, which are consistent with our observational findings. We
explore mechanisms underlying these findings and identify how successful
collaborators leverage common language to weave together diverse experience
obtained through trajectories of prior work that converge to provoke one
another and innovate. We explore the importance of these findings for team
assembly and research policy.

</details>


### [30] [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
*Takashi Wada,Yuki Hirakawa,Ryotaro Shimizu,Takahiro Kawashima,Yuki Saito*

Main category: cs.CL

TL;DR: 提出了一种优化后的静态词嵌入方法，通过句子级主成分分析以及知识蒸馏或对比学习实现优化，可以有效地表示句子语义并降低计算成本，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 优化静态词嵌入以提高它们在句子语义表示任务中的表现，同时降低计算成本。

Method: 从预训练的句子转换器中提取词嵌入，并利用句子级主成分分析进行优化，随后应用知识蒸馏或对比学习。推理时，通过简单地平均词嵌入来表示句子。

Result: 模型在单语和跨语任务中均表现优异，大幅度超越现有静态模型，并在某些数据集上能够与基本的Sentence Transformer模型（SimCSE）相媲美。

Conclusion: 我们提出的新静态词嵌入在句子语义表示任务上表现良好，并在某些数据集上与基本的Sentence Transformer模型（SimCSE）相媲美。

Abstract: We propose new static word embeddings optimised for sentence semantic
representation. We first extract word embeddings from a pre-trained Sentence
Transformer, and improve them with sentence-level principal component analysis,
followed by either knowledge distillation or contrastive learning. During
inference, we represent sentences by simply averaging word embeddings, which
requires little computational cost. We evaluate models on both monolingual and
cross-lingual tasks and show that our model substantially outperforms existing
static models on sentence semantic tasks, and even rivals a basic Sentence
Transformer model (SimCSE) on some data sets. Lastly, we perform a variety of
analyses and show that our method successfully removes word embedding
components that are irrelevant to sentence semantics, and adjusts the vector
norms based on the influence of words on sentence semantics.

</details>


### [31] [Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning](https://arxiv.org/abs/2506.04625)
*Zhiyuan Ma,Jiayu Liu,Xianzhen Luo,Zhenya Huang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper presents Tool-MVR, an enhanced LLM for better tool utilization and reflection, significantly outperforming existing models in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current language models in tool planning and invocation as well as tool reflection abilities, which hinder their problem-solving capacities.

Method: The paper introduces Tool-MVR, which incorporates Multi-Agent Meta-Verification (MAMV) for constructing a more reliable instruction dataset (ToolBench-V) and Exploration-based Reflection Learning (EXPLORE) for better tool reflection, resulting in ToolBench-R. The model is fine-tuned on these new datasets, leading to improved performance.

Result: Tool-MVR outperforms ToolLLM by 23.9% and GPT-4 by 15.3% on the StableToolBench, while reducing API calls by 31.4%. It also achieves a 58.9% error correction rate on RefineToolBench, vastly outperforming ToolLLM's 9.1%.

Conclusion: Tool-MVR significantly outperforms existing models in tool utilization and reflection capabilities, making it a leading approach in AI problem-solving.

Abstract: Empowering large language models (LLMs) with effective tool utilization
capabilities is crucial for enabling AI agents to solve complex problems.
However, current models face two major limitations: (1) unreliable tool
planning and invocation due to low-quality instruction datasets (e.g.,
widespread hallucinated API calls), and (2) weak tool reflection abilities
(over 90% of errors cannot be corrected) resulting from static imitation
learning. To address these critical limitations, we propose Tool-MVR, a novel
Tool-Augmented LLM that achieves comprehensive System 2 reasoning through two
key innovations. Specifically, we first introduce Multi-Agent Meta-Verification
(MAMV), a systematic pipeline that rigorously validates APIs, queries, and
reasoning trajectories to construct ToolBench-V, a new high-quality instruction
dataset that addresses the limitation of unreliable tool planning and
invocation. Second, we propose Exploration-based Reflection Learning (EXPLORE),
which enhances tool reflection capabilities by leveraging tool feedback through
a dynamic "Error -> Reflection -> Correction" learning paradigm, resulting in
our reflection dataset ToolBench-R and addressing the critical weakness in tool
reflection. Finally, we obtain Tool-MVR by finetuning open-source LLMs (e.g.,
Qwen-7B) on both ToolBench-V and ToolBench-R. Our experiments demonstrate that
Tool-MVR achieves state-of-the-art performance on StableToolBench, surpassing
both ToolLLM (by 23.9%) and GPT-4 (by 15.3%) while reducing API calls by 31.4%,
with strong generalization capabilities across unseen tools and scenarios.
Additionally, on our proposed RefineToolBench, the first benchmark specifically
designed to evaluate tool reflection capabilities, Tool-MVR achieves a 58.9%
error correction rate, significantly outperforming ToolLLM's 9.1%.

</details>


### [32] [ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.04635)
*Thai-Binh Nguyen,Thi Van Nguyen,Quoc Truong Do,Chi Mai Luong*

Main category: cs.CL

TL;DR: 本文提出一种从视频中自动生成音视频语音识别数据集的方法，并成功应用于越南语，实现了在噪音环境下优于传统ASR的表现。


<details>
  <summary>Details</summary>
Motivation: 由于音视频语音识别在噪声环境下的鲁棒性，尽管目前数据集稀缺，尤其是英语以外的语言，仍有待解决的问题。

Method: 通过自动化数据收集方法，从原始视频中生成音视频语音识别数据集。

Result: 实验证明，自动收集的数据集支持强大的基线AVSR模型，在清晰环境下表现良好，并在噪音环境下显著优于传统ASR。

Conclusion: 本文提出的方法能够有效生成音视频语音识别数据集，尤其是对资源较少的语言，如越南语，实现了强大的基线模型，并在噪音环境下表现优异。

Abstract: Audio-Visual Speech Recognition (AVSR) has gained significant attention
recently due to its robustness against noise, which often challenges
conventional speech recognition systems that rely solely on audio features.
Despite this advantage, AVSR models remain limited by the scarcity of extensive
datasets, especially for most languages beyond English. Automated data
collection offers a promising solution. This work presents a practical approach
to generate AVSR datasets from raw video, refining existing techniques for
improved efficiency and accessibility. We demonstrate its broad applicability
by developing a baseline AVSR model for Vietnamese. Experiments show the
automatically collected dataset enables a strong baseline, achieving
competitive performance with robust ASR in clean conditions and significantly
outperforming them in noisy environments like cocktail parties. This efficient
method provides a pathway to expand AVSR to more languages, particularly
under-resourced ones.

</details>


### [33] [TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering](https://arxiv.org/abs/2506.04642)
*Vinay Joshi,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 引入 TaDA 方法，无需单独处理异常值，可显著减少 KV 缓存内存占用并保持准确性，支持更长的上下文和推理链。


<details>
  <summary>Details</summary>
Motivation: KV 缓存的内存需求与序列长度成比例增长，这限制了大型语言模型的可扩展性。因此，有必要找到一种能够有效压缩 KV 缓存的方法。

Method: 我们提出了一种名为 TaDA 的训练自由 KV 缓存压缩方法，使用量化精度根据错误敏感度在层间进行适应，同时通过均值中心化消除了单独的异常值处理。

Result: 我们的技术将 KV 缓存的内存占用减少到原始 16 位基线的 27%，同时实现了可比较的准确性。

Conclusion: 我们的方法可以在不牺牲准确率的情况下显著减少 KV 缓存的内存需求，这可能有助于支持更长的上下文长度和更复杂的推理。

Abstract: The key-value (KV) cache in transformer models is a critical component for
efficient decoding or inference, yet its memory demands scale poorly with
sequence length, posing a major challenge for scalable deployment of large
language models. Among several approaches to KV cache compression, quantization
of key and value activations has been widely explored. Most KV cache
quantization methods still need to manage sparse and noncontiguous outliers
separately. To address this, we introduce TaDA, a training-free recipe for KV
cache compression with quantization precision that adapts to error sensitivity
across layers and a mean centering to eliminate separate outlier handling. Our
approach yields substantial accuracy improvements for multiple models
supporting various context lengths. Moreover, our approach does not need to
separately manage outlier elements -- a persistent hurdle in most traditional
quantization methods. Experiments on standard benchmarks demonstrate that our
technique reduces KV cache memory footprint to 27% of the original 16-bit
baseline while achieving comparable accuracy. Our method paves the way for
scalable and high-performance reasoning in language models by potentially
enabling inference for longer context length models, reasoning models, and
longer chain of thoughts.

</details>


### [34] [Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents](https://arxiv.org/abs/2506.04649)
*Juhyun Oh,Eunsu Kim,Alice Oh*

Main category: cs.CL

TL;DR: 引入新基准Flex-TravelPlanner以评估语言模型在动态规划中灵活推理的能力，发现现有模型在多回合和优先级处理上存在显著不足，建议改进方向并公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有针对LLMs规划能力的评估基准主要集中在静态的单回合场景中，缺乏对真实世界动态规划问题的模拟和对多重竞合约束的平衡，因此，本研究动机在于开发更具现实意义的评估基准。

Method: 研究引入了Flex-TravelPlanner这一基准，通过对语言模型在动态规划场景中的灵活推理能力进行评估，设置了两种新的评估情景：（1）在多个回合中引入的顺序约束；（2）具有明确优先级的竞合约束场景。

Result: GPT-4o和Llama 3.1 70B在实验中的表现揭示了一些关键发现：单回合任务表现与多回合适应能力之间的差距，约束引入顺序对性能的显著影响，以及在约束优先级处理方面的困难。

Conclusion: 研究表明，LLMs在静态单回合任务上的表现无法很好地预测其在多回合动态规划场景中的适应能力。模型在应对多个回合的约束引入时，表现受到显著影响，并且在约束优先级的处理上经常出错，通常会错误地偏好新引入的低优先级偏好，而忽视已有的高优先级约束。

Abstract: Real-world planning problems require constant adaptation to changing
requirements and balancing of competing constraints. However, current
benchmarks for evaluating LLMs' planning capabilities primarily focus on
static, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark
that evaluates language models' ability to reason flexibly in dynamic planning
scenarios. Building on the TravelPlanner dataset~\citep{xie2024travelplanner},
we introduce two novel evaluation settings: (1) sequential constraint
introduction across multiple turns, and (2) scenarios with explicitly
prioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B
reveals several key findings: models' performance on single-turn tasks poorly
predicts their ability to adapt plans across multiple turns; constraint
introduction order significantly affects performance; and models struggle with
constraint prioritization, often incorrectly favoring newly introduced lower
priority preferences over existing higher-priority constraints. These findings
highlight the importance of evaluating LLMs in more realistic, dynamic planning
scenarios and suggest specific directions for improving model performance on
complex planning tasks. The code and dataset for our framework are publicly
available at https://github.com/juhyunohh/FlexTravelBench.

</details>


### [35] [Normative Conflicts and Shallow AI Alignment](https://arxiv.org/abs/2506.04679)
*Raphaël Millière*

Main category: cs.CL

TL;DR: 当前的大型语言模型对齐策略无法有效预防其被滥用，因为它们未能赋予模型深入的规范性推理能力。


<details>
  <summary>Details</summary>
Motivation: 探讨和解决大型语言模型在价值对齐方面的不足，以确保其安全部署。

Method: 通过研究与道德心理学的相关研究进行比对，分析现有对齐方法的不足。

Result: 发现现有的对齐方法只能强化表面的行为表现，无法帮助大型语言模型进行有效的规范性推理。

Conclusion: 现有针对大型语言模型的对齐策略是浅薄的，未能赋予模型真正的规范性推理能力，因此无法有效避免被操纵和误用。

Abstract: The progress of AI systems such as large language models (LLMs) raises
increasingly pressing concerns about their safe deployment. This paper examines
the value alignment problem for LLMs, arguing that current alignment strategies
are fundamentally inadequate to prevent misuse. Despite ongoing efforts to
instill norms such as helpfulness, honesty, and harmlessness in LLMs through
fine-tuning based on human preferences, they remain vulnerable to adversarial
attacks that exploit conflicts between these norms. I argue that this
vulnerability reflects a fundamental limitation of existing alignment methods:
they reinforce shallow behavioral dispositions rather than endowing LLMs with a
genuine capacity for normative deliberation. Drawing from on research in moral
psychology, I show how humans' ability to engage in deliberative reasoning
enhances their resilience against similar adversarial tactics. LLMs, by
contrast, lack a robust capacity to detect and rationally resolve normative
conflicts, leaving them susceptible to manipulation; even recent advances in
reasoning-focused LLMs have not addressed this vulnerability. This ``shallow
alignment'' problem carries significant implications for AI safety and
regulation, suggesting that current approaches are insufficient for mitigating
potential harms posed by increasingly capable AI systems.

</details>


### [36] [MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2506.04688)
*Gio Paik,Geewook Kim,Jinbae Im*

Main category: cs.CL

TL;DR: MMRefine是一个评估多模态大语言模型精炼能力的基准，提供框架检测和纠正推理错误，揭示性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 提高多模态大语言模型在推理过程中的错误检测与修正能力。

Method: 设计了一个多模态精炼基准，评估语言模型在六个不同场景下的错误检测和修正能力。

Result: 实验揭示了各种开源与闭源多模态大语言模型在精炼性能上的瓶颈与阻碍因素。

Conclusion: MMRefine提供了一个详尽的评估框架，用于检测和纠正多模态大语言模型在推理过程中的错误，揭示其性能瓶颈和改进领域。

Abstract: This paper introduces MMRefine, a MultiModal Refinement benchmark designed to
evaluate the error refinement capabilities of Multimodal Large Language Models
(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,
MMRefine provides a framework that evaluates MLLMs' abilities to detect and
correct errors across six distinct scenarios beyond just comparing final
accuracy before and after refinement. Furthermore, the benchmark analyzes the
refinement performance by categorizing errors into six error types. Experiments
with various open and closed MLLMs reveal bottlenecks and factors impeding
refinement performance, highlighting areas for improvement in effective
reasoning enhancement. Our code and dataset are publicly available at
https://github.com/naver-ai/MMRefine.

</details>


### [37] [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
*Thao Nguyen,Yang Li,Olga Golovneva,Luke Zettlemoyer,Sewoong Oh,Ludwig Schmidt,Xian Li*

Main category: cs.CL

TL;DR: REWIRE 方法通过重新改写低质量文本，实现了数据的循环利用，提高了语言模型的性能，是预训练数据扩展的有效途径。


<details>
  <summary>Details</summary>
Motivation: 由于自然数据的增长速度无法跟上计算能力的增长，尤其是高质量文本的可获得性有限，因此需要找到一种方式来利用现有过滤过程丢弃的数据，从而突破预训练扩展的 "数据墙"。

Method: 本文提出了一种名为 REWIRE (REcycling the Web with guIded REwrite) 的方法，该方法通过引导重写来丰富低质量文档，使其可用于训练。通过将高质量原始文本与重写文本混合，提高了语言模型在多种任务上的性能。

Result: 在 DCLM 基准的 1B、3B 和 7B 规模实验中，通过与高质量文本混合，重写后的文本在 22 个多样任务上分别提高了 1.0, 1.3 和 2.5 个百分点。通过进一步分析，混合文本中约 82% 来自重新生成的低质量文档。REWIRE 在生成合成数据方面表现优于相关的方法。

Conclusion: REWIRE 可以将低质量的网络文档转化为可用于训练的数据，有效增加合成数据在预训练集中的比例，并且提升语言模型的性能。该方法相较于现有的生成合成数据的方法更优。

Abstract: Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.

</details>


### [38] [Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding Classification](https://arxiv.org/abs/2506.04693)
*Lu Wei,Liangzhi Li,Tong Xiang,Xiao Liu,Noa Garcia*

Main category: cs.CL

TL;DR: 提出了一种新的隐含仇恨言论检测方法，通过引入编码类型，提高了中英文数据集上的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动检测方法在识别显性仇恨言论时表现良好，但是在处理更为隐晦的隐含仇恨言论时存在困难，因此需要一种新的方法来改进这一不足。

Method: 引入了一种新的隐含仇恨言论检测分类法，定义了六种编码策略，并提出了两种将编码类型整合到隐含仇恨言论检测中的方法。

Result: 实验表明，使用编码类型能够在中文和英文数据集上改善隐含仇恨言论的检测效果，验证了该方法在不同语言上的有效性。

Conclusion: 通过在编码过程中嵌入编码类型，能够有效提高对隐含仇恨言论的检测能力。

Abstract: The internet has become a hotspot for hate speech (HS), threatening societal
harmony and individual well-being. While automatic detection methods perform
well in identifying explicit hate speech (ex-HS), they struggle with more
subtle forms, such as implicit hate speech (im-HS). We tackle this problem by
introducing a new taxonomy for im-HS detection, defining six encoding
strategies named codetypes. We present two methods for integrating codetypes
into im-HS detection: 1) prompting large language models (LLMs) directly to
classify sentences based on generated responses, and 2) using LLMs as encoders
with codetypes embedded during the encoding process. Experiments show that the
use of codetypes improves im-HS detection in both Chinese and English datasets,
validating the effectiveness of our approach across different languages.

</details>


### [39] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
*Zhaolu Kang,Junhao Gong,Jiaxu Yan,Wanke Xia,Yian Wang,Ziwen Wang,Huaxuan Ding,Zhuo Cheng,Wenhao Cao,Zhiyuan Feng,Siqi He,Shannan Yan,Junzhe Chen,Xiaomin He,Chaoya Jiang,Wei Ye,Kaidong Yu,Xuelong Li*

Main category: cs.CL

TL;DR: 为了弥补现有基准的不足，研究者提出了 HSSBench，以评估多语言 MLLMs 在人文与社会科学领域的表现。


<details>
  <summary>Details</summary>
Motivation: 当前评估 MLLMs 的基准过度关注 STEM 学科的线性推理需求，忽略了人文与社会科学对跨学科思维的需求。

Method: 引入一个新的数据生成管道，专家与自动代理协作生成并反复完善样本。

Result: 我们用 HSSBench 对超过 20 个主流 MLLMs 进行了基准测试，发现即使是最先进的模型也面临显著挑战。

Conclusion: HSSBench 旨在推动跨学科领域的研究，评估 MLLMs 在多种语言的 HSS 任务中的表现，为未来的研究启发更多思考。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
potential to advance a broad range of domains. However, current benchmarks for
evaluating MLLMs primarily emphasize general knowledge and vertical
step-by-step reasoning typical of STEM disciplines, while overlooking the
distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks
in the HSS domain require more horizontal, interdisciplinary thinking and a
deep integration of knowledge across related fields, which presents unique
challenges for MLLMs, particularly in linking abstract concepts with
corresponding visual representations. Addressing this gap, we present HSSBench,
a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks
in multiple languages, including the six official languages of the United
Nations. We also introduce a novel data generation pipeline tailored for HSS
scenarios, in which multiple domain experts and automated agents collaborate to
generate and iteratively refine each sample. HSSBench contains over 13,000
meticulously designed samples, covering six key categories. We benchmark more
than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant
challenges even for state-of-the-art models. We hope that this benchmark will
inspire further research into enhancing the cross-disciplinary reasoning
abilities of MLLMs, especially their capacity to internalize and connect
knowledge across fields.

</details>


### [40] [Accelerated Test-Time Scaling with Model-Free Speculative Sampling](https://arxiv.org/abs/2506.04708)
*Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: STAND 是一种可加速语言模型推理的无模型推测解码方法，减少了推理延迟并提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型推理过程中性能和效率之间的关键权衡问题。

Method: 利用语言模型推理路径中的冗余性，通过记忆高效的基于logit的N元模块进行概率信息保存，引入随机起草和优化的Gumbel-Top-K采样以及数据驱动的树结构构建。

Result: STAND 将推理延迟降低了60-65%，比最先进的推测解码方法在吞吐量上优越14-28%，在单一轨迹场景中推理延迟降低了48-58%。

Conclusion: STAND 是一种无模型的推测解码方法，可以在不影响准确率的情况下显著加速语言模型推理。

Abstract: Language models have demonstrated remarkable capabilities in reasoning tasks
through test-time scaling techniques like best-of-N sampling and tree search.
However, these approaches often demand substantial computational resources,
creating a critical trade-off between performance and efficiency. We introduce
STAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative
decoding approach that leverages the inherent redundancy in reasoning
trajectories to achieve significant acceleration without compromising accuracy.
Our analysis reveals that reasoning paths frequently reuse similar reasoning
patterns, enabling efficient model-free token prediction without requiring
separate draft models. By introducing stochastic drafting and preserving
probabilistic information through a memory-efficient logit-based N-gram module,
combined with optimized Gumbel-Top-K sampling and data-driven tree
construction, STAND significantly improves token acceptance rates. Extensive
evaluations across multiple models and reasoning tasks (AIME-2024,
GPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference
latency by 60-65% compared to standard autoregressive decoding while
maintaining accuracy. Furthermore, STAND outperforms state-of-the-art
speculative decoding methods by 14-28% in throughput and shows strong
performance even in single-trajectory scenarios, reducing inference latency by
48-58%. As a model-free approach, STAND can be applied to any existing language
model without additional training, being a powerful plug-and-play solution for
accelerating language model reasoning.

</details>


### [41] [IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech translation](https://arxiv.org/abs/2506.04714)
*Bhavana Akkiraju,Aishwarya Pothula,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 本文研究了优化超参数和应用数据增强技术在低资源Bhojpuri-Hindi语言对语音翻译任务中的效果，结果表明这些技术能显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 研究低资源语言对（Bhojpuri-Hindi）的语音翻译提升方法，以提高翻译质量。

Method: 对SeamlessM4T模型进行了微调，探索超参数优化和数据增强技术的影响；应用速度扰动和SpecAugment等数据增强方法；进行跨语言信号联合训练。

Result: 精心选择超参数以及简单但有效的数据增强技术可以显著提高低资源翻译任务的性能，并分析翻译假设以理解BLEU指标中的错误类型。

Conclusion: 在低资源环境中应用超参数优化和数据增强技术可以显著改善翻译质量。

Abstract: This paper presents the submission of IIITH-BUT to the IWSLT 2025 shared task
on speech translation for the low-resource Bhojpuri-Hindi language pair. We
explored the impact of hyperparameter optimisation and data augmentation
techniques on the performance of the SeamlessM4T model fine-tuned for this
specific task. We systematically investigated a range of hyperparameters
including learning rate schedules, number of update steps, warm-up steps, label
smoothing, and batch sizes; and report their effect on translation quality. To
address data scarcity, we applied speed perturbation and SpecAugment and
studied their effect on translation quality. We also examined the use of
cross-lingual signal through joint training with Marathi and Bhojpuri speech
data. Our experiments reveal that careful selection of hyperparameters and the
application of simple yet effective augmentation techniques significantly
improve performance in low-resource settings. We also analysed the translation
hypotheses to understand various kinds of errors that impacted the translation
quality in terms of BLEU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models](https://arxiv.org/abs/2506.04244)
*Farzad Farhadzadeh,Debasmit Das,Shubhankar Borse,Fatih Porikli*

Main category: cs.AI

TL;DR: ProLoRA使得在文本到图像扩散模型中实现参数高效微调的跨模型转移无需额外训练数据，利用子空间和零空间的相似性对齐层，成功进行知识转移，性能良好。


<details>
  <summary>Details</summary>
Motivation: 传统方法在切换基础模型时需要重新训练，这在数据受限的情况下较为困难。ProLoRA旨在解决这个问题。

Method: 通过将源调整投射到目标模型的权重空间，并利用子空间和零空间相似性，选择性地对齐目标模型的层，来实现零样本的模型适应。

Result: 实验证明，在既定的文本生成图像模型上，ProLoRA能够在无需重新训练的情况下成功实现知识转移，性能可与重新训练的方法相媲美。

Conclusion: ProLoRA实现了不需要额外训练数据的跨模型转移参数高效微调，大幅度减轻了传统方法在切换基础模型时需要重新训练的问题。

Abstract: We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient
fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained
low-rank adjustments (e.g., LoRA) from a source to a target model without
additional training data. This overcomes the limitations of traditional methods
that require retraining when switching base models, often challenging due to
data constraints. ProLoRA achieves this via projection of source adjustments
into the target model's weight space, leveraging subspace and null space
similarities and selectively targeting aligned layers. Evaluations on
established text-to-image models demonstrate successful knowledge transfer and
comparable performance without retraining.

</details>


### [43] [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
*Guangchen Lan,Huseyin A. Inan,Sahar Abdelnabi,Janardhan Kulkarni,Lukas Wutschitz,Reza Shokri,Christopher G. Brinton,Robert Sim*

Main category: cs.AI

TL;DR: 通过提示LLMs和使用RL框架来增强上下文完整性（CI）推理，成功减少信息泄露，并维持任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理为用户做出决策的时代的到来，确定在执行某项任务过程中共享什么信息变得至关重要。本文旨在探讨上下文完整性（CI），即在特定任务中共享适当的信息。

Method: 本文首先通过提示大型语言模型（LLMs）在决定披露信息时明确考虑上下文完整性（CI）。随后，本文开发了一种强化学习（RL）框架，进一步在模型中灌输实现CI所需的推理能力。研究使用一个合成的自动创建的数据集来测试方法。

Result: 研究表明，所提出的方法在减少不当信息披露的同时，保持任务性能。改进也能够从合成数据集成功转移至像PrivacyLens等有人工标注的CI基准测试。

Conclusion: 上下文完整性（CI）在自主代理信息披露中至关重要。通过让模型考虑上下文，研究成功减少了信息泄露，同时保持了任务执行能力。

Abstract: As the era of autonomous agents making decisions on behalf of users unfolds,
ensuring contextual integrity (CI) -- what is the appropriate information to
share while carrying out a certain task -- becomes a central question to the
field. We posit that CI demands a form of reasoning where the agent needs to
reason about the context in which it is operating. To test this, we first
prompt LLMs to reason explicitly about CI when deciding what information to
disclose. We then extend this approach by developing a reinforcement learning
(RL) framework that further instills in models the reasoning necessary to
achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$
examples but with diverse contexts and information disclosure norms, we show
that our method substantially reduces inappropriate information disclosure
while maintaining task performance across multiple model sizes and families.
Importantly, improvements transfer from this synthetic dataset to established
CI benchmarks such as PrivacyLens that has human annotations and evaluates
privacy leakage of AI assistants in actions and tool calls.

</details>


### [44] [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)
*Zhengyang Li*

Main category: cs.AI

TL;DR: 引入LLM-MARL框架，通过与游戏环境的集成，使智能体在协调和通信方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 增强模拟游戏环境中多智能体的协调、通信和泛化能力。

Method: 框架包含三个模块：协调器、通讯器和记忆，结合PPO训练、语言条件损失和LLM查询门控评估。

Result: 在Google Research Football、MAgent Battle和StarCraft II中的实验表明，框架显著提升了各项指标。消融研究表明，子目标生成和基于语言的消息传递对性能提升贡献显著。

Conclusion: 通过将大型语言模型（LLM）引入多智能体强化学习（MARL），系统表现出在胜率、协调分数和零样本泛化能力方面优于现有方法。

Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.

</details>


### [45] [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
*Yang Zhao,Chengxiao Dai,Dusit Niyato,Chuan Fu Tan,Keyi Xiang,Yueyang Wang,Zhiquan Yeo,Daren Tan Zong Loong,Jonathan Low Zhaozhi,Eugene H. Z. HO*

Main category: cs.AI

TL;DR: CircuGraphRAG通过结合领域知识图解决LLM幻觉问题，改善循环经济决策的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在可持续制造中的幻觉问题，尤其是工业代码和排放因子相关的幻觉，保证决策的准确性。

Method: 引入了CircuGraphRAG这一检索增强生成框架，将领域知识图融入LLM以进行结构化多跳推理。

Result: 相比单独的LLMs和简单RAG，CircuGraphRAG在单跳和多跳问答上表现优异，ROUGE-L F1得分可达1.0，响应时间减半，令牌使用减少16%。

Conclusion: CircuGraphRAG为循环经济规划提供了事实核查和符合监管要求的支持，提升了低碳资源决策的可靠性。

Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but
often hallucinate industrial codes and emission factors, undermining regulatory
and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented
generation (RAG) framework that grounds LLMs outputs in a domain-specific
knowledge graph for the circular economy. This graph connects 117,380
industrial and waste entities with classification codes and GWP100 emission
data, enabling structured multi-hop reasoning. Natural language queries are
translated into SPARQL and verified subgraphs are retrieved to ensure accuracy
and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG
achieves superior performance in single-hop and multi-hop question answering,
with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also
improves efficiency, halving the response time and reducing token usage by 16%
in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready
support for circular economy planning, advancing reliable, low-carbon resource
decision making.

</details>


### [46] [HADA: Human-AI Agent Decision Alignment Architecture](https://arxiv.org/abs/2506.04253)
*Tapio Pitkäranta,Leena Pitkäranta*

Main category: cs.AI

TL;DR: 提出HADA架构，使得大型语言模型和传统算法能够与组织目标保持一致，通过云端演示和评估证明其在提高决策流程的透明性和准确性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为组织提供一种一致的方法，使得大型语言模型代理和传统算法能够与组织的目标和价值观保持一致。

Method: 采用基于云的概念验证模型，通过在Docker/Kubernetes/Python上部署生产信贷评分模型，演示该架构如何在五个零售银行场景中进行运作和调整。评估使用了设计科学研究方法论，通过观察和日志检查证明架构覆盖所有预定义目标。

Result: 证明了HADA架构能够改善现实世界决策流程的准确性、透明度和伦理合规性，文中列出了五个示例场景进行验证。

Conclusion: 提出了一种开源HADA架构，开发了一种用于多代理系统中的人机协作中程设计理论，并提供了实证证据表明协议兼容的角色代理可以改善现实世界决策流程。

Abstract: We present HADA (Human-AI Agent Decision Alignment), a protocol- and
framework agnostic reference architecture that keeps both large language model
(LLM) agents and legacy algorithms aligned with organizational targets and
values. HADA wraps any algorithm or LLM in role-specific stakeholder agents --
business, data-science, audit, ethics, and customer -- each exposing
conversational APIs so that technical and non-technical actors can query,
steer, audit, or contest every decision across strategic, tactical, and
real-time horizons. Alignment objectives, KPIs, and value constraints are
expressed in natural language and are continuously propagated, logged, and
versioned while thousands of heterogeneous agents run on different
orchestration stacks. A cloud-native proof of concept packages a production
credit-scoring model (getLoanDecision) and deploys it on
Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target
changes, parameter tweaks, explanation requests, and ethics triggers flow end
to end through the architecture. Evaluation followed the Design-Science
Research Methodology. Walkthrough observation and log inspection demonstrated
complete coverage of six predefined objectives: every role could invoke
conversational control, trace KPIs and value constraints, detect and mitigate
ZIP-code bias, and reproduce full decision lineage, independent of the
underlying LLM or agent library. Contributions: (1) an open-source HADA
architecture, (2) a mid-range design theory for human-AI alignment in
multi-agent systems, and (3) empirical evidence that framework-agnostic,
protocol-compliant stakeholder agents improve accuracy, transparency, and
ethical compliance in real-world decision pipelines.

</details>


### [47] [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)
*Yongjin Yang,Sinjae Kang,Juyong Lee,Dongjun Lee,Se-Young Yun,Kimin Lee*

Main category: cs.AI

TL;DR: 提出了一种名为EXIF的自动技能发现框架，通过无人工干预的迭代反馈机制，训练大型语言模型代理，实现技能提升和性能改善。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型代理以获取必要的技能和在环境中执行多样化任务变得越来越重要。然而，创建用于技能获取的训练数据集面临挑战，需要大量人工收集轨迹，或由于LLM缺乏可行任务的知识，导致所提任务无效且无法提供有意义的学习信号。解决这些挑战是本文的动力所在。

Method: 提出了一个名为EXIF的自动技能发现框架。采用“探索优先”的策略，通过设置探索代理Alice在环境中生成可行的技能数据集，然后用于训练目标代理Bob。同时引入一个迭代反馈循环，Alice评估Bob的表现，并指导下一轮探索，形成闭环的数据生成过程。

Result: 在Webshop和Crafter上的实验表明，EXIF能够有效发现有意义的技能，并通过迭代扩大训练代理的能力，无需人为干预，显著提升性能。

Conclusion: EXIF在训练大型语言模型代理以获得必要技能和执行多样任务方面表现优异，能够在无需人工干预的情况下实现显著的性能提升。通过设置探索代理Alice与目标代理Bob相同的模型，性能也得到了显著改善，显示出EXIF系统自我进化的潜力。

Abstract: Training large language model (LLM) agents to acquire necessary skills and
perform diverse tasks within an environment is gaining interest as a means to
enable open-endedness. However, creating the training dataset for their skill
acquisition faces several challenges. Manual trajectory collection requires
significant human effort. Another approach, where LLMs directly propose tasks
to learn, is often invalid, as the LLMs lack knowledge of which tasks are
actually feasible. Moreover, the generated data may not provide a meaningful
learning signal, as agents often already perform well on the proposed tasks. To
address this, we propose a novel automatic skill discovery framework EXIF for
LLM-powered agents, designed to improve the feasibility of generated target
behaviors while accounting for the agents' capabilities. Our method adopts an
exploration-first strategy by employing an exploration agent (Alice) to train
the target agent (Bob) to learn essential skills in the environment.
Specifically, Alice first interacts with the environment to retrospectively
generate a feasible, environment-grounded skill dataset, which is then used to
train Bob. Crucially, we incorporate an iterative feedback loop, where Alice
evaluates Bob's performance to identify areas for improvement. This feedback
then guides Alice's next round of exploration, forming a closed-loop data
generation process. Experiments on Webshop and Crafter demonstrate EXIF's
ability to effectively discover meaningful skills and iteratively expand the
capabilities of the trained agent without any human intervention, achieving
substantial performance improvements. Interestingly, we observe that setting
Alice to the same model as Bob also notably improves performance, demonstrating
EXIF's potential for building a self-evolving system.

</details>


### [48] [A Statistical Physics of Language Model Reasoning](https://arxiv.org/abs/2506.04374)
*Jack David Carson,Amir Reisizadeh*

Main category: cs.AI

TL;DR: 研究提出了一个统计物理框架来理解变压器语言模型的连续时间推理动力学，发现并验证了四种潜在推理模式，可以低成本模拟并预测关键转变。


<details>
  <summary>Details</summary>
Motivation: 变压器语言模型展现出的推理能力难以通过机制来理解。研究者希望通过统计物理框架来达到对此的分析和理解。

Method: 该研究将句子级隐藏状态轨迹建模为下维流形上的随机动力系统。通过潜在状态变换的漂移-扩散系统来捕捉不同的推理阶段，包括未对齐状态或失败。研究者还建立并验证了一个SLDS模型以捕捉这些特征。

Result: 八个模型在七个基准上进行的实验证明，通过秩为40的投影（在方差捕获与可行性之间取得平衡）可以解释大约50%的方差，发现了四种潜在的推理模式。

Conclusion: 该框架能够以低成本模拟推理，提供了研究和预测如状态失调或其他模型失败等关键转变的工具。

Abstract: Transformer LMs show emergent reasoning that resists mechanistic
understanding. We offer a statistical physics framework for continuous-time
chain-of-thought reasoning dynamics. We model sentence-level hidden state
trajectories as a stochastic dynamical system on a lower-dimensional manifold.
This drift-diffusion system uses latent regime switching to capture diverse
reasoning phases, including misaligned states or failures. Empirical
trajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing
variance capture and feasibility) explains ~50% variance. We find four latent
reasoning regimes. An SLDS model is formulated and validated to capture these
features. The framework enables low-cost reasoning simulation, offering tools
to study and predict critical transitions like misaligned states or other LM
failures.

</details>


### [49] [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
*Peter Jansen,Samiah Hassan,Ruoyao Wang*

Main category: cs.AI

TL;DR: 介绍了Matter-of-Fact数据集用于评估假设可行性，通过领域专家验证显示当前模型难以解决但任务是可解决的，有潜力加速科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前辅助科学发现的方法利用语言模型自动生成大量潜在假设进行测试，同时自动生成基于代码的实验来验证这些假设。然而，当实验规模较大时（数千个实验），自动化实验的成本可能会很高。为了实现大规模的发现系统，并提高重大发现的可能性，需要一种能力来根据假设的可行性进行筛选。

Method: 本文介绍了Matter-of-Fact，一个用于确定假设可行性的挑战数据集。Matter-of-Fact从科学文章中提取了8.4k个关于四个高影响当代材料科学主题的声明，包括超导体、半导体、电池和航空航天材料，同时包括理论、实验和代码/模拟结果中的定性和定量声明。

Result: 我们展示了在科学文献和代码生成上增强的检索生成的强基线在完成这项任务时未能超过72%的表现（机会表现为50%），而领域专家的验证表明几乎所有任务都是可以解决的，这突显了当前模型对这一任务的难度，以及通过取得短期进展来加速科学发现的潜力。

Conclusion: 当前的模型在确定假设可行性方面表现不佳（未超过72%），然而领域专家认为任务是可解决的，这表明有可能通过改进模型来显著加速科学发现。

Abstract: Contemporary approaches to assisted scientific discovery use language models
to automatically generate large numbers of potential hypothesis to test, while
also automatically generating code-based experiments to test those hypotheses.
While hypotheses can be comparatively inexpensive to generate, automated
experiments can be costly, particularly when run at scale (i.e. thousands of
experiments). Developing the capacity to filter hypotheses based on their
feasibility would allow discovery systems to run at scale, while increasing
their likelihood of making significant discoveries. In this work we introduce
Matter-of-Fact, a challenge dataset for determining the feasibility of
hypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from
scientific articles spanning four high-impact contemporary materials science
topics, including superconductors, semiconductors, batteries, and aerospace
materials, while including qualitative and quantitative claims from
theoretical, experimental, and code/simulation results. We show that strong
baselines that include retrieval augmented generation over scientific
literature and code generation fail to exceed 72% performance on this task
(chance performance is 50%), while domain-expert verification suggests nearly
all are solvable -- highlighting both the difficulty of this task for current
models, and the potential to accelerate scientific discovery by making
near-term progress.

</details>


### [50] [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)
*Xixi Wang,Miguel Costa,Jordanka Kovaceva,Shuai Wang,Francisco C. Pereira*

Main category: cs.AI

TL;DR: 提出一种利用图形框架构建有效推理链的方法，改善LLMs在复杂场景中的多表问答能力，实验结果显示其效果显著。


<details>
  <summary>Details</summary>
Motivation: 扩展LLMs的能力用于多表问答面临挑战，因为现有方法在复杂现实场景中难以处理不可靠的模式链接。

Method: 提出了一种基于图形的框架，利用人类策划的关系知识，以显式方式编码模式链接和连接路径，并通过搜索图构建可解释的推理链，辅以修剪和子路径合并策略以提高效率和连贯性。

Result: 实验表明，该方法在标准基准和大规模真实数据集上表现良好。

Conclusion: 该研究提出了一种图形框架，通过显式编码模式链接和连接路径来解决多表问答中的问题。实验结果表明，该方法在标准基准和现实数据集上均表现良好，标志着首次在复杂工业表格数据中应用多表问答系统。

Abstract: Large language models (LLMs) have shown promise in table Question Answering
(Table QA). However, extending these capabilities to multi-table QA remains
challenging due to unreliable schema linking across complex tables. Existing
methods based on semantic similarity work well only on simplified hand-crafted
datasets and struggle to handle complex, real-world scenarios with numerous and
diverse columns. To address this, we propose a graph-based framework that
leverages human-curated relational knowledge to explicitly encode schema links
and join paths. Given a natural language query, our method searches this graph
to construct interpretable reasoning chains, aided by pruning and sub-path
merging strategies to enhance efficiency and coherence. Experiments on both
standard benchmarks and a realistic, large-scale dataset demonstrate the
effectiveness of our approach. To our knowledge, this is the first multi-table
QA system applied to truly complex industrial tabular data.

</details>


### [51] [An AI-Based Public Health Data Monitoring System](https://arxiv.org/abs/2506.04429)
*Ananya Joshi,Nolan Gormley,Richa Gadgil,Tina Townes,Roni Rosenfeld,Bryan Wilder*

Main category: cs.AI

TL;DR: 提出了一种新型基于AI的排名监控系统，大幅提高了公共卫生数据监控的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统的警报监控系统在处理现代公共卫生数据方面存在不足，需要频繁调整告警阈值，并因数据量大而导致延迟。因此，需要一种可扩展的方法来监控大量卫生数据。

Method: 该研究提出了一种基于排名的监控范式，通过人机协作和AI异常检测方法实现，而非传统的阈值警报系统。

Result: 部署在国家级机构的系统每天监控多达500万个数据点，在被评估的三个月中，与传统警报方法相比，评审速度效率提高了54倍，显著改善了监控目标。

Conclusion: 本研究探讨了人类中心的AI技术在突发公共卫生事件和数据质量监测中的潜力。

Abstract: Public health experts need scalable approaches to monitor large volumes of
health data (e.g., cases, hospitalizations, deaths) for outbreaks or data
quality issues. Traditional alert-based monitoring systems struggle with modern
public health data monitoring systems for several reasons, including that
alerting thresholds need to be constantly reset and the data volumes may cause
application lag. Instead, we propose a ranking-based monitoring paradigm that
leverages new AI anomaly detection methods. Through a multi-year
interdisciplinary collaboration, the resulting system has been deployed at a
national organization to monitor up to 5,000,000 data points daily. A
three-month longitudinal deployed evaluation revealed a significant improvement
in monitoring objectives, with a 54x increase in reviewer speed efficiency
compared to traditional alert-based methods. This work highlights the potential
of human-centered AI to transform public health decision-making.

</details>


### [52] [Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences](https://arxiv.org/abs/2506.04478)
*Hadi Hosseini,Samarth Khanna,Ronak Singh*

Main category: cs.AI

TL;DR: 研究表明，大语言模型在处理匹配市场中的排名偏好和结构化算法方面仍有局限性，尤其是在大市场中改善推理能力需要更复杂的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多种推理任务上展示了进步，但其处理组合领域中排名偏好和结构化算法的能力仍未得到充分探索。

Method: 对当前几种最先进的模型进行了一系列不同层次的偏好推理任务评估，包括稳定匹配生成、不稳定性检测、不稳定性解决和细粒度偏好查询，以系统化地揭示它们在处理排名输入时的逻辑和算法局限性。

Result: 发现即便是功能先进的顶尖模型在大市场中也难以解决不稳定性，通常无法识别阻塞对或迭代执行算法。通过参数高效微调（LoRA），在小市场中的表现有显著提升，但在大规模实例中未能获得类似的提升。

Conclusion: 当前顶尖的语言模型在处理匹配市场中的不稳定性时表现不佳，尤其是在大规模市场中，这突显了需要更复杂的策略来改进对大规模上下文输入的推理能力。

Abstract: The rise of Large Language Models (LLMs) has driven progress in reasoning
tasks -- from program synthesis to scientific hypothesis generation -- yet
their ability to handle ranked preferences and structured algorithms in
combinatorial domains remains underexplored. We study matching markets, a core
framework behind applications like resource allocation and ride-sharing, which
require reconciling individual ranked preferences to ensure stable outcomes. We
evaluate several state-of-the-art models on a hierarchy of preference-based
reasoning tasks -- ranging from stable-matching generation to instability
detection, instability resolution, and fine-grained preference queries -- to
systematically expose their logical and algorithmic limitations in handling
ranked inputs. Surprisingly, even top-performing models with advanced reasoning
struggle to resolve instability in large markets, often failing to identify
blocking pairs or execute algorithms iteratively. We further show that
parameter-efficient fine-tuning (LoRA) significantly improves performance in
small markets, but fails to bring about a similar improvement on large
instances, suggesting the need for more sophisticated strategies to improve
LLMs' reasoning with larger-context inputs.

</details>


### [53] [CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective](https://arxiv.org/abs/2506.04481)
*Jiayu Liu,Zhenya Huang,Wei Dai,Cheng Cheng,Jinze Wu,Jing Sha,Song Li,Qi Liu,Shijin Wang,Enhong Chen*

Main category: cs.AI

TL;DR: CogMath评估框架揭示大型语言模型数学能力被高估，并提供了精确评估其能力的方法，为进一步提升其推理能力提供了深刻见解。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地评估大型语言模型的数学能力，提出了一种更细化、更符合人类认知过程的评估方法。

Method: 提出了CogMath评估框架，该框架基于人类认知理论，通过三个阶段和九个细化评估维度来评估大型语言模型的数学能力。

Result: 通过CogMath框架评估，发现主流大型语言模型的数学能力被高估了30%-40%，并揭示了它们在特定阶段和维度上的强项和弱项。

Conclusion: CogMath展示了现有大型语言模型在数学能力评估方面存在的差距，并提供了更精细化的方法来评估其真实能力。

Abstract: Although large language models (LLMs) show promise in solving complex
mathematical tasks, existing evaluation paradigms rely solely on a coarse
measure of overall answer accuracy, which are insufficient for assessing their
authentic capabilities. In this paper, we propose \textbf{CogMath}, which
comprehensively assesses LLMs' mathematical abilities through the lens of human
cognition. Specifically, inspired by psychological theories, CogMath formalizes
human reasoning process into 3 stages: \emph{problem comprehension},
\emph{problem solving}, and \emph{solution summarization}. Within these stages,
we investigate perspectives such as numerical calculation, knowledge, and
counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In
each dimension, we develop an ``\emph{Inquiry}-\emph{Judge}-\emph{Reference}''
multi-agent system to generate inquiries that assess LLMs' mastery from this
dimension. An LLM is considered to truly master a problem only when excelling
in all inquiries from the 9 dimensions. By applying CogMath on three
benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs
are overestimated by 30\%-40\%. Moreover, we locate their strengths and
weaknesses across specific stages/dimensions, offering in-depth insights to
further enhance their reasoning abilities.

</details>


### [54] ["Don't Do That!": Guiding Embodied Systems through Large Language Model-based Constraint Generation](https://arxiv.org/abs/2506.04500)
*Aladin Djuhera,Amin Seffo,Masataro Asai,Holger Boche*

Main category: cs.AI

TL;DR: STPR框架利用大语言模型将复杂自然语言约束转化为Python函数，实现机器人导航中的合规性和低成本推理。


<details>
  <summary>Details</summary>
Motivation: 由于自然语言中的约束往往复杂且非正式，因此很难将其转化为形式化描述以供规划算法使用。本文旨在解决这一问题。

Method: 提出了一个名为STPR的约束生成框架，利用大语言模型将自然语言中的约束翻译为可执行的Python函数，并应用于点云表示和传统搜索算法。

Result: 通过Gazebo模拟环境的实验验证，STPR在多种约束和场景下表现出色，并支持较小的、特定于代码的大语言模型的使用，具有较低的推理成本。

Conclusion: 实验结果表明，STPR能够在复杂约束和场景下确保完全的合规性，同时具有较短的运行时间。

Abstract: Recent advancements in large language models (LLMs) have spurred interest in
robotic navigation that incorporates complex spatial, mathematical, and
conditional constraints from natural language into the planning problem. Such
constraints can be informal yet highly complex, making it challenging to
translate into a formal description that can be passed on to a planning
algorithm. In this paper, we propose STPR, a constraint generation framework
that uses LLMs to translate constraints (expressed as instructions on ``what
not to do'') into executable Python functions. STPR leverages the LLM's strong
coding capabilities to shift the problem description from language into
structured and transparent code, thus circumventing complex reasoning and
avoiding potential hallucinations. We show that these LLM-generated functions
accurately describe even complex mathematical constraints, and apply them to
point cloud representations with traditional search algorithms. Experiments in
a simulated Gazebo environment show that STPR ensures full compliance across
several constraints and scenarios, while having short runtimes. We also verify
that STPR can be used with smaller, code-specific LLMs, making it applicable to
a wide range of compact models at low inference cost.

</details>


### [55] [Schema Generation for Large Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2506.04512)
*Bohui Zhang,Yuan He,Lydia Pintscher,Albert Meroño Peñuela,Elena Simperl*

Main category: cs.AI

TL;DR: The paper explores using LLMs for automatic schema generation, introduces new datasets and metrics, and shows LLMs' effectiveness in generating schemas for large knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: To explore automatic schema generation using LLMs due to their impressive capabilities in ontology engineering.

Method: LLM-based pipelines utilizing local and global information from knowledge graphs to generate schemas in Shape Expressions (ShEx).

Result: Experiments demonstrate the effectiveness of LLMs in producing high-quality ShEx schemas.

Conclusion: LLMs exhibit strong potential in automated schema generation for large KGs.

Abstract: Schemas are vital for ensuring data quality in the Semantic Web and natural
language processing. Traditionally, their creation demands substantial
involvement from knowledge engineers and domain experts. Leveraging the
impressive capabilities of large language models (LLMs) in related tasks like
ontology engineering, we explore automatic schema generation using LLMs. To
bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata
EntitySchema, along with evaluation metrics. The LLM-based pipelines
effectively utilize local and global information from knowledge graphs (KGs) to
generate validating schemas in Shape Expressions (ShEx). Experiments
demonstrate LLMs' strong potential in producing high-quality ShEx schemas,
paving the way for scalable, automated schema generation for large KGs.
Furthermore, our benchmark introduces a new challenge for structured
generation, pushing the limits of LLMs on syntactically rich formalisms.

</details>


### [56] [OpenAg: Democratizing Agricultural Intelligence](https://arxiv.org/abs/2506.04571)
*Srikanth Thudumu,Jason Fisher*

Main category: cs.AI

TL;DR: OpenAg是一个全面框架，通过结合多种技术来改善农业人工智能的上下文理解和可解释性，旨在支持小农户的可扩展农业决策。


<details>
  <summary>Details</summary>
Motivation: 当前农业智能系统缺乏上下文理解力、可解释性和适应性，尤其是对于资源有限的小农户。通用大型语言模型（LLMs）通常缺乏农业领域的特定知识和上下文推理，导致建议过于泛泛或不切实际。OpenAg旨在解决这些问题。

Method: OpenAg结合了领域特定基础模型、神经知识图谱、多智能体推理、因果解释性和自适应迁移学习，以提供具有上下文感知、可解释性和可操作的见解。系统包括统一的农业知识库，整合科学文献、传感器数据和农民生成的知识，神经农业知识图进行结构化推理和推断，适应性多智能体推理系统以及因果透明机制。

Result: OpenAg通过其多层次和多模块的系统设计，为农业提供上下文感知、解释性和可操作的智能建议。

Conclusion: OpenAg提供了一种全面框架，旨在推动农业人工通用智能（AGI）的发展，使其能够支持可扩展且本地相关的农业决策。

Abstract: Agriculture is undergoing a major transformation driven by artificial
intelligence (AI), machine learning, and knowledge representation technologies.
However, current agricultural intelligence systems often lack contextual
understanding, explainability, and adaptability, especially for smallholder
farmers with limited resources. General-purpose large language models (LLMs),
while powerful, typically lack the domain-specific knowledge and contextual
reasoning needed for practical decision support in farming. They tend to
produce recommendations that are too generic or unrealistic for real-world
applications. To address these challenges, we present OpenAg, a comprehensive
framework designed to advance agricultural artificial general intelligence
(AGI). OpenAg combines domain-specific foundation models, neural knowledge
graphs, multi-agent reasoning, causal explainability, and adaptive transfer
learning to deliver context-aware, explainable, and actionable insights. The
system includes: (i) a unified agricultural knowledge base that integrates
scientific literature, sensor data, and farmer-generated knowledge; (ii) a
neural agricultural knowledge graph for structured reasoning and inference;
(iii) an adaptive multi-agent reasoning system where AI agents specialize and
collaborate across agricultural domains; and (iv) a causal transparency
mechanism that ensures AI recommendations are interpretable, scientifically
grounded, and aligned with real-world constraints. OpenAg aims to bridge the
gap between scientific knowledge and the tacit expertise of experienced farmers
to support scalable and locally relevant agricultural decision-making.

</details>


### [57] [Judicial Permission](https://arxiv.org/abs/2506.04610)
*Guido Governatori,Antonino Rotolo*

Main category: cs.AI

TL;DR: 研究通过对话游戏模型探讨刑事审判中弱许可的作用，发现不同证明标准和论证语义对司法许可有显著影响。


<details>
  <summary>Details</summary>
Motivation: 弱许可在刑事审判中有时可以影响案件的最终判决，但其作用常常被忽视。本文旨在通过理论模型的建立来揭示和分析弱许可的重要性和实际应用。

Method: 本文引入一种对话游戏模型，系统性地处理司法许可，并考虑不同的证明标准和论证语义。

Result: 研究通过对话游戏模型的应用，发现不同的证明标准和论证语义可以显著影响司法许可的实施和效果。

Conclusion: 研究表明，司法许可中的弱许可在刑事审判中具有重要意义，值得进一步探讨。

Abstract: This paper examines the significance of weak permissions in criminal trials
(\emph{judicial permission}). It introduces a dialogue game model to
systematically address judicial permissions, considering different standards of
proof and argumentation semantics.

</details>


### [58] [DeePoly: A High-Order Accuracy and Efficiency Deep-Polynomial Framework for Scientific Machine Learning](https://arxiv.org/abs/2506.04613)
*Li Liu,Heng Yong*

Main category: cs.AI

TL;DR: DeePoly combines DNNs with polynomial optimization to solve PDEs efficiently, offering both high-order accuracy and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Machine learning methods, especially those based on DNNs, have lacked convergence guarantees and computational efficiency in scientific computing compared to traditional numerical schemes.

Method: DeePoly employs a two-stage approach: first using a DNN to capture complex global features, followed by linear space optimization with DNN-extracted features and polynomial basis functions.

Result: The approach improves high-order accuracy and efficiency across diverse problem types, leveraging the strengths of DNNs and polynomial bases.

Conclusion: DeePoly significantly enhances accuracy and efficiency in solving PDEs, maintaining mesh-free and scheme-free properties, demonstrated through theoretical analysis and numerical experiments.

Abstract: Recently, machine learning methods have gained significant traction in
scientific computing, particularly for solving Partial Differential Equations
(PDEs). However, methods based on deep neural networks (DNNs) often lack
convergence guarantees and computational efficiency compared to traditional
numerical schemes. This work introduces DeePoly, a novel framework that
transforms the solution paradigm from pure non-convex parameter optimization to
a two-stage approach: first employing a DNN to capture complex global features,
followed by linear space optimization with combined DNN-extracted features
(Scoper) and polynomial basis functions (Sniper). This strategic combination
leverages the complementary strengths of both methods -- DNNs excel at
approximating complex global features (i.e., high-gradient features) and
stabilize the polynomial approximation while polynomial bases provide
high-precision local corrections with convergence guarantees. Theoretical
analysis and numerical experiments demonstrate that this approach significantly
enhances both high-order accuracy and efficiency across diverse problem types
while maintaining mesh-free and scheme-free properties. This paper also serves
as a theoretical exposition for the open-source project DeePoly.

</details>


### [59] [Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation](https://arxiv.org/abs/2506.04614)
*Yuyang Wanyan,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Jiabo Ye,Yutong Kou,Ming Yan,Fei Huang,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.AI

TL;DR: 本文引入了一种用于图形用户界面自动化任务的预操作评论机制，通过S-GRPO策略及推理引导的数据收集，增强任务执行的可靠性和效率，取得优于现有MLLMs的结果。


<details>
  <summary>Details</summary>
Motivation: 通过引入预操作评论机制来减少在线交互环境中图形用户界面自动化任务的决策错误，并加强任务执行的可靠性。

Method: 提出了一种建议感知梯度相对策略优化（S-GRPO）策略，用于构建我们的预操作评论机制模型GUI-Critic-R1，并通过推理引导的数据收集管道开发了GUI-Critic-Train和GUI-Critic-Test。

Result: 在GUI-Critic-Test的静态实验中，GUI-Critic-R1在评论准确性方面表现优于目前的MLLMs。在GUI自动化基准的动态评估中，该模型在成功率和操作效率上也表现出优越性。

Conclusion: GUI-Critic-R1的推出通过建议奖励的引入和推理引导的数据收集过程，提升了图形用户界面自动化任务的决策准确性和操作效率，与现有MLLMs相比具有显著优势。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have been
extensively utilized for multimodal reasoning tasks, including Graphical User
Interface (GUI) automation. Unlike general offline multimodal tasks, GUI
automation is executed in online interactive environments, necessitating
step-by-step decision-making based on real-time status of the environment. This
task has a lower tolerance for decision-making errors at each step, as any
mistakes may cumulatively disrupt the process and potentially lead to
irreversible outcomes like deletions or payments. To address these issues, we
introduce a pre-operative critic mechanism that provides effective feedback
prior to the actual execution, by reasoning about the potential outcome and
correctness of actions. Specifically, we propose a Suggestion-aware Gradient
Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative
critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance
the reliability of the model's feedback. Furthermore, we develop a
reasoning-bootstrapping based data collection pipeline to create a
GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic
data. Static experiments on the GUI-Critic-Test across both mobile and web
domains reveal that our GUI-Critic-R1 offers significant advantages in critic
accuracy compared to current MLLMs. Dynamic evaluation on GUI automation
benchmark further highlights the effectiveness and superiority of our model, as
evidenced by improved success rates and operational efficiency.

</details>


### [60] [CHANCERY: Evaluating corporate governance reasoning capabilities in language models](https://arxiv.org/abs/2506.04636)
*Lucas Irwin,Arda Kaz,Peiyao Sheng,Pramod Viswanath*

Main category: cs.AI

TL;DR: 论文引入CHANCERY基准测试，评估模型在公司治理法律推理中的表现，展示了现有模型在这一领域的挑战性和发展潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多个法律数据集，但尚未有专门针对推理任务的数据集，因此引入一个新的基准测试来评估语言模型在法律推理方面的能力。

Method: 提出了一种新的基准测试——CHANCERY，评估语言模型在法律推理特别是公司治理方面的能力。

Result: Claude 3.7 Sonnet和GPT-4o在此基准上的准确率分别为64.5%和75.2%。ReAct和CodeAct框架的代理表现更好，分别取得76.1%和78.1%的得分，证明需要高级法律推理能力才能在测试中取得高分。

Conclusion: 法律推理对现有的最先进模型构成了一定的挑战，表明进一步发展的空间。

Abstract: Law has long been a domain that has been popular in natural language
processing (NLP) applications. Reasoning (ratiocination and the ability to make
connections to precedent) is a core part of the practice of the law in the real
world. Nevertheless, while multiple legal datasets exist, none have thus far
focused specifically on reasoning tasks. We focus on a specific aspect of the
legal landscape by introducing a corporate governance reasoning benchmark
(CHANCERY) to test a model's ability to reason about whether
executive/board/shareholder's proposed actions are consistent with corporate
governance charters. This benchmark introduces a first-of-its-kind corporate
governance reasoning test for language models - modeled after real world
corporate governance law. The benchmark consists of a corporate charter (a set
of governing covenants) and a proposal for executive action. The model's task
is one of binary classification: reason about whether the action is consistent
with the rules contained within the charter. We create the benchmark following
established principles of corporate governance - 24 concrete corporate
governance principles established in and 79 real life corporate charters
selected to represent diverse industries from a total dataset of 10k real life
corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models
confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet
and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents
exhibit superior performance, with agents based on the ReAct and CodeAct
frameworks scoring 76.1% and 78.1% respectively, further confirming the
advanced legal reasoning capabilities required to score highly on the
benchmark. We also conduct an analysis of the types of questions which current
reasoning models struggle on, revealing insights into the legal reasoning
capabilities of SOTA models.

</details>


### [61] [Agents of Change: Self-Evolving LLM Agents for Strategic Planning](https://arxiv.org/abs/2506.04651)
*Nikolas Belle,Dakota Barnes,Alfonso Amayuelas,Ivan Bercovich,Xin Eric Wang,William Wang*

Main category: cs.AI

TL;DR: 在《卡坦岛》游戏环境中研究LLM代理自我改进能力，结果显示自动演变代理优于手动设置的代理。


<details>
  <summary>Details</summary>
Motivation: 探索LLM（大语言模型）代理是否能在明确挑战其战略规划能力的环境中完成自我提升。

Method: 引入多代理架构，将分析员、研究员、编码员和玩家的角色进行协作，通过迭代分析游戏、研究新策略以及修改代理逻辑或提示来改进代理。

Result: 自动演变的代理超过静态基线，通过自动采用策略、传递样本行为给游戏代理，并在多次迭代中展示适应性推理。

Conclusion: 自动演变的代理特别是在采用Claude 3.7和GPT-4o模型时，通过自我改进策略、传递示范行为给游戏代理，表现出多次迭代的适应性推理超越静态基线。

Abstract: Recent advances in LLMs have enabled their use as autonomous agents across a
range of tasks, yet they continue to struggle with formulating and adhering to
coherent long-term strategies. In this paper, we investigate whether LLM agents
can self-improve when placed in environments that explicitly challenge their
strategic planning abilities. Using the board game Settlers of Catan, accessed
through the open-source Catanatron framework, we benchmark a progression of
LLM-based agents, from a simple game-playing agent to systems capable of
autonomously rewriting their own prompts and their player agent's code. We
introduce a multi-agent architecture in which specialized roles (Analyzer,
Researcher, Coder, and Player) collaborate to iteratively analyze gameplay,
research new strategies, and modify the agent's logic or prompt. By comparing
manually crafted agents to those evolved entirely by LLMs, we evaluate how
effectively these systems can diagnose failure and adapt over time. Our results
show that self-evolving agents, particularly when powered by models like Claude
3.7 and GPT-4o, outperform static baselines by autonomously adopting their
strategies, passing along sample behavior to game-playing agents, and
demonstrating adaptive reasoning over multiple iterations.

</details>


### [62] [E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction](https://arxiv.org/abs/2506.04654)
*Zhichao Yang,Jiashu He,Mohammad B. Al-Khasawneh,Darshan Pandit,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 研究开发了一个使用LLM的框架，能从非结构化的电动自行车事故报告中提取关键安全因素，改善安全分析，并为相关方提供可操作的信息。


<details>
  <summary>Details</summary>
Motivation: 由于电动自行车事故报告常为非结构化叙述格式，给量化安全分析带来困难，因此需要开发一种能够有效分类提取这些报告中安全变量的框架。

Method: 构建了由四个LLM代理组成的框架，分别负责数据分类、信息提取、事故原因判断和组件关联；并使用有序logit模型分析事故严重程度和提取因素的关系。

Result: 研究发现设备问题略多于人为原因，但人为因素导致的事故更致命；模型在分类准确性上达到0.87的F1加权分数，展示了LLM在交通等小众领域提取非结构化数据的潜力。

Conclusion: 研究开发的LLM框架能够有效分类与提取非结构化电动自行车事故报告中的安全变量，提高安全分析的定量化能力。

Abstract: Electric bicycles (e-bikes) are rapidly increasing in use, raising safety
concerns due to a rise in accident reports. However, e-bike incident reports
often use unstructured narrative formats, which hinders quantitative safety
analysis. This study introduces E-bike agents, a framework that uses large
language models (LLM) powered agents to classify and extract safety variables
from unstructured incident reports. Our framework consists of four LLM agents,
handling data classification, information extraction, injury cause
determination, and component linkage, to extract the key factors that could
lead to E-bike accidents and cause varying severity levels. Furthermore, we
used an ordered logit model to examine the relationship between the severity of
the incident and the factors retrieved, such as gender, the type of cause, and
environmental conditions. Our research shows that equipment issues are slightly
more common than human-related ones, but human-related incidents are more often
fatal. Specifically, pedals, tires, and brakes are frequent contributors to
accidents. The model achieves a high weighted F1 score of 0.87 in
classification accuracy, highlighting the potential of using LLMs to extract
unstructured data in niche domains, such as transportation. Our method offers a
scalable solution to improve e-bike safety analytics and provides actionable
information for policy makers, designers, and regulators.

</details>


### [63] [Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling](https://arxiv.org/abs/2506.04699)
*Bihan Xu,Shiwei Zhao,Runze Wu,Zhenya Huang,Jiawei Wang,Zhipeng Hu,Kai Wang,Haoyu Liu,Tangjie Lv,Le Li,Changjie Fan,Xin Tong,Jiangze Han*

Main category: cs.AI

TL;DR: 本研究引入LLM驱动的代理，为MMO经济模拟提供了一种创新方法，解决了以往模拟中的若干挑战并促进行为现象的出现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于代理的模拟在模拟人类经济活动方面存在重大挑战，尤其是在代理的可靠性、社交性和可解释性方面。因此，本研究试图通过引入新的方法来解决这些挑战。

Method: 本研究采用大型语言模型（LLM）来设计驱动代理，这些代理具有类似人类的决策能力和适应性，能够进行角色扮演、感知、记忆和推理。

Result: LLM驱动的代理能够有效促进游戏内经济活动中的现象，例如角色专业化和价格波动，这些现象符合市场规则。

Conclusion: 使用LLM设计的代理能够解决以往研究中的挑战，通过角色扮演和推理能力提升经济活动的真实性和动态性。

Abstract: Within the domain of Massively Multiplayer Online (MMO) economy research,
Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game
economics, evolving from rule-based agents to decision-making agents enhanced
by reinforcement learning. Nevertheless, existing works encounter significant
challenges when attempting to emulate human-like economic activities among
agents, particularly regarding agent reliability, sociability, and
interpretability. In this study, we take a preliminary step in introducing a
novel approach using Large Language Models (LLMs) in MMO economy simulation.
Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning
aptitude, we design LLM-driven agents with human-like decision-making and
adaptability. These agents are equipped with the abilities of role-playing,
perception, memory, and reasoning, addressing the aforementioned challenges
effectively. Simulation experiments focusing on in-game economic activities
demonstrate that LLM-empowered agents can promote emergent phenomena like role
specialization and price fluctuations in line with market rules.

</details>


### [64] [Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning](https://arxiv.org/abs/2506.04723)
*Jiayu Wang,Yifei Ming,Zixuan Ke,Caiming Xiong,Shafiq Joty,Aws Albarghouthi,Frederic Sala*

Main category: cs.AI

TL;DR: RL提升了语言模型在推理过程中的能力，尤其是在策略制定和知识整合上。


<details>
  <summary>Details</summary>
Motivation: 虽然RL在语言模型中的应用显示了显著的经验增益，但对其优势的细化理解仍有欠缺。

Method: 引入细致的分析框架来研究RL对推理的影响，包括计划执行、问题分解和知识利用等元素。

Result: 即使为模型提供明确的计划步骤可能降低性能，RL优化的模型却表现出更大的鲁棒性，并在整合知识时提升任务性能。

Conclusion: RL增强模型的推理能力，特别是在整合知识和内部策略制定方面。

Abstract: Reinforcement learning (RL) has become the dominant paradigm for endowing
language models with advanced reasoning capabilities. Despite the substantial
empirical gains demonstrated by RL-based training methods like GRPO, a granular
understanding of their advantages is still lacking. To address this gap, we
introduce a fine-grained analytic framework to dissect the impact of RL on
reasoning. Our framework specifically investigates key elements that have been
hypothesized to benefit from RL training: (1) plan-following and execution, (2)
problem decomposition, and (3) improved reasoning and knowledge utilization.
Using this framework, we gain insights beyond mere accuracy. For instance,
providing models with explicit step-by-step plans surprisingly degrades
performance on the most challenging benchmarks, yet RL-tuned models exhibit
greater robustness, experiencing markedly smaller performance drops than their
base counterparts. This suggests that RL may not primarily enhance the
execution of external plans but rather empower models to formulate and follow
internal strategies better suited to their reasoning processes. Conversely, we
observe that RL enhances the model's capacity to integrate provided knowledge
into its reasoning process, leading to performance improvements across diverse
tasks. We also study difficulty, showing improved training by developing new
ways to exploit hard problems. Our findings lay a foundation for more
principled training and evaluation of reasoning models.

</details>


### [65] [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)
*Lin Sun,Weihong Lin,Jinzhu Wu,Yongfu Zhu,Xiaoqi Jian,Guangxiang Zhao,Change Jia,Linglin Zhang,Sai-er Hu,Yuhan Wu,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 推理模型评估结果波动显著，需要更严格的评估方法来确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估结果存在波动，因此需要更加严格的评估范式。

Method: 对Deepseek-R1-Distill系列模型进行经验评估，分析其在不同评估条件下的表现。

Result: 评估揭示评估条件微小差异导致结果显著变化，影响了模型性能提升的再现性。

Conclusion: 现有的推理模型在性能评估中存在显著波动，影响评估的可靠性。

Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.

</details>


### [66] [Ontology-based knowledge representation for bone disease diagnosis: a foundation for safe and sustainable medical artificial intelligence systems](https://arxiv.org/abs/2506.04756)
*Loan Dao,Ngoc Quoc Ly*

Main category: cs.AI

TL;DR: 该研究提出了一种本体论驱动的框架用于骨病诊断，重点在于维持临床可解释性和构建标准化结构。尽管理论框架已提出，实验验证尚待进行。


<details>
  <summary>Details</summary>
Motivation: 医学人工智能系统通常缺乏系统性领域专家的整合，可能影响诊断的可靠性。

Method: 研究提出了一种基于本体论的骨病诊断框架。框架包括三个理论贡献：（1）使用骨病本体指导的分层神经网络结构，用于分割-分类任务，并通过提示整合视觉语言模型（VLMs），（2）一个用于临床推理的本体增强型视觉问答（VQA）系统，以及（3）一个通过本体关系整合影像、临床和实验室数据的多模态深度学习模型。

Result: 框架展示了其在骨病以外扩展的潜力，由于数据集和计算资源限制，实验验证目前尚未完成。

Conclusion: 该框架展示了超出骨病诊断的潜力，具有标准化结构和可复用组件。尽管理论基础已建立，实验验证仍在等待中，原因在于当前的数据集和计算资源限制。未来工作将专注于扩大临床数据集和进行全面的系统验证。

Abstract: Medical artificial intelligence (AI) systems frequently lack systematic
domain expertise integration, potentially compromising diagnostic reliability.
This study presents an ontology-based framework for bone disease diagnosis,
developed in collaboration with Ho Chi Minh City Hospital for Traumatology and
Orthopedics. The framework introduces three theoretical contributions: (1) a
hierarchical neural network architecture guided by bone disease ontology for
segmentation-classification tasks, incorporating Visual Language Models (VLMs)
through prompts, (2) an ontology-enhanced Visual Question Answering (VQA)
system for clinical reasoning, and (3) a multimodal deep learning model that
integrates imaging, clinical, and laboratory data through ontological
relationships. The methodology maintains clinical interpretability through
systematic knowledge digitization, standardized medical terminology mapping,
and modular architecture design. The framework demonstrates potential for
extension beyond bone diseases through its standardized structure and reusable
components. While theoretical foundations are established, experimental
validation remains pending due to current dataset and computational resource
limitations. Future work will focus on expanding the clinical dataset and
conducting comprehensive system validation.

</details>


### [67] [Safe Planning and Policy Optimization via World Model Learning](https://arxiv.org/abs/2506.04828)
*Artem Latyshev,Gregory Gorbov,Aleksandr I. Panov*

Main category: cs.AI

TL;DR: 本文提出了一种新的模型强化学习框架，使用自适应机制动态优化任务表现和安全性，在安全关键任务中取得了优异成效。


<details>
  <summary>Details</summary>
Motivation: 在真实场景应用中，强化学习需要优先考虑安全性和可靠性，这对智能体行为施加了严格限制。基于模型的强化学习虽然可以通过预测世界模型进行动作规划和策略优化，但其模型固有的不准确性可能导致安全关键设置下的灾难性失败问题。

Method: 本文提出了一种新的基于模型的强化学习框架，通过联合优化任务表现和安全来提升性能。为解决世界模型误差问题，该方法采用自适应机制，动态切换模型规划和直接策略执行，并通过隐式世界模型解决传统模型方法的目标不匹配问题。此外，还使用动态安全阈值，适应于代理的能力变化，持续优选超越安全策略建议的动作。

Result: 实验表明，与非自适应方法相比，该框架显著提高了安全性和表现，而不仅仅是满足最低安全要求。

Conclusion: 本文提出的框架能够在优化任务表现和确保安全方面同时取得显著改善，在多种安全关键的连续控制任务中表现优异，超越现有方法。

Abstract: Reinforcement Learning (RL) applications in real-world scenarios must
prioritize safety and reliability, which impose strict constraints on agent
behavior. Model-based RL leverages predictive world models for action planning
and policy optimization, but inherent model inaccuracies can lead to
catastrophic failures in safety-critical settings. We propose a novel
model-based RL framework that jointly optimizes task performance and safety. To
address world model errors, our method incorporates an adaptive mechanism that
dynamically switches between model-based planning and direct policy execution.
We resolve the objective mismatch problem of traditional model-based approaches
using an implicit world model. Furthermore, our framework employs dynamic
safety thresholds that adapt to the agent's evolving capabilities, consistently
selecting actions that surpass safe policy suggestions in both performance and
safety. Experiments demonstrate significant improvements over non-adaptive
methods, showing that our approach optimizes safety and performance
simultaneously rather than merely meeting minimum safety requirements. The
proposed framework achieves robust performance on diverse safety-critical
continuous control tasks, outperforming existing methods.

</details>


### [68] [Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles](https://arxiv.org/abs/2506.04849)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Paul Théron,Louis-Marie Traonouez*

Main category: cs.AI

TL;DR: 本文通过马尔可夫模型和模拟器，模拟网络攻击者和防御者的交互，评估防御策略在复杂网络态势下的效果。


<details>
  <summary>Details</summary>
Motivation: 随着网络攻击日益复杂和协调，多智能体方法的网络防御策略可能是解决网络攻击的关键。

Method: 采用马尔可夫模型进行建模，并通过模拟器在网络节点上部署网络攻击者和防御者进行测试。

Result: 成功实现了一个受现实启发的场景，并展示了不同的行为实现方法对网络攻防的影响。

Conclusion: 使用模拟器进行的实验展示了在现实情境中协调的网络攻击和防御策略的实施。

Abstract: As cyber-attacks show to be more and more complex and coordinated,
cyber-defenders strategy through multi-agent approaches could be key to tackle
against cyber-attacks as close as entry points in a networked system. This
paper presents a Markovian modeling and implementation through a simulator of
fighting cyber-attacker agents and cyber-defender agents deployed on host
network nodes. It aims to provide an experimental framework to implement
realistically based coordinated cyber-attack scenarios while assessing
cyber-defenders dynamic organizations. We abstracted network nodes by sets of
properties including agents' ones. Actions applied by agents model how the
network reacts depending in a given state and what properties are to change.
Collective choice of the actions brings the whole environment closer or farther
from respective cyber-attackers and cyber-defenders goals. Using the simulator,
we implemented a realistically inspired scenario with several behavior
implementation approaches for cyber-defenders and cyber-attackers.

</details>


### [69] [LLMs for sensory-motor control: Combining in-context and iterative learning](https://arxiv.org/abs/2506.04867)
*Jônata Tyska Carvalho,Stefano Nolfi*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型生成和优化实时控制策略的新方法，并在经典任务中取得了高性能结果。


<details>
  <summary>Details</summary>
Motivation: 通过将大型语言模型应用于实时控制，可以提高控制任务的性能和效率。

Method: 设计了一种方法，将语言模型与持续观察和行动向量直接映射，使其生成控制策略，并通过迭代学习过程优化。

Result: 方法在Gymnasium库的经典控制任务和MuJoCo库的倒摆任务中进行了验证，成功找到了最佳或高性能解决方案。

Conclusion: 整合推理产生的符号知识和从环境交互中获取的子符号感官-运动数据，可以有效提高控制任务性能。

Abstract: We propose a method that enables large language models (LLMs) to control
embodied agents by directly mapping continuous observation vectors to
continuous action vectors. Initially, the LLMs generate a control strategy
based on a textual description of the agent, its environment, and the intended
goal. This strategy is then iteratively refined through a learning process in
which the LLMs are repeatedly prompted to improve the current strategy, using
performance feedback and sensory-motor data collected during its evaluation.
The method is validated on classic control tasks from the Gymnasium library and
the inverted pendulum task from the MuJoCo library. In most cases, it
successfully identifies optimal or high-performing solutions by integrating
symbolic knowledge derived through reasoning with sub-symbolic sensory-motor
data gathered as the agent interacts with its environment.

</details>


### [70] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: 研究通过表示工程在大语言模型中诱导、检测和控制链式思维推理下的战略欺骗，成功率为40%，检测准确率为89%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的诚实性是一个关键的对齐挑战，特别是在具有链式思维（CoT）推理的高级系统中可能会战略性地欺骗人类。

Method: 使用表示工程，通过线性人工断层扫描（LAT）提取“欺骗向量”，系统地诱导、检测和控制这些具有链式思维（CoT）推理能力的LLMs中的战略欺骗。

Result: 通过激活控制，在无需明确提示的情况下实现了40%的上下文适应性欺骗成功率，并实现了89%的检测准确率。

Conclusion: 揭示了推理模型中与诚实性相关的特定问题，并提供了用于可信AI对齐的工具。

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>


### [71] [Differentiable Logic Cellular Automata: From Game of Life to Pattern Generation](https://arxiv.org/abs/2506.04912)
*Pietro Miotti,Eyvind Niklasson,Ettore Randazzo,Alexander Mordvintsev*

Main category: cs.AI

TL;DR: 本研究提出了一种结合NCA和DLGNs的方法，成功应用于学习和生成图案，展示了该方法的鲁棒性和泛化能力，是可编程和健壮计算系统的一步。


<details>
  <summary>Details</summary>
Motivation: 希望设计一个模型，使其能够结合二进制逻辑、神经网络适应性和局部处理，开发更加健壮和可编程的计算系统。

Method: 该模型的基本计算单元是可微逻辑门，并结合成电路，通过梯度方法进行训练，在推理时操作于完全离散状态空间。这使得可以在保持离散性的同时学习细胞自动机的局部更新规则。

Result: 模型成功学习到生成目标图案的递归电路，特别是在简单图案上，观察到同步和异步更新均取得成功，展示了显著的泛化能力和对扰动的鲁棒性。

Conclusion: 本论文首次成功将可微逻辑门网络应用于递归架构，表明DLGNs和NCA的结合是朝着可编程物质和健壮计算系统发展的重要一步。

Abstract: This paper introduces Differentiable Logic Cellular Automata (DiffLogic CA),
a novel combination of Neural Cellular Automata (NCA) and Differentiable Logic
Gates Networks (DLGNs). The fundamental computation units of the model are
differentiable logic gates, combined into a circuit. During training, the model
is fully end-to-end differentiable allowing gradient-based training, and at
inference time it operates in a fully discrete state space. This enables
learning local update rules for cellular automata while preserving their
inherent discrete nature. We demonstrate the versatility of our approach
through a series of milestones: (1) fully learning the rules of Conway's Game
of Life, (2) generating checkerboard patterns that exhibit resilience to noise
and damage, (3) growing a lizard shape, and (4) multi-color pattern generation.
Our model successfully learns recurrent circuits capable of generating desired
target patterns. For simpler patterns, we observe success with both synchronous
and asynchronous updates, demonstrating significant generalization capabilities
and robustness to perturbations. We make the case that this combination of
DLGNs and NCA represents a step toward programmable matter and robust computing
systems that combine binary logic, neural network adaptability, and localized
processing. This work, to the best of our knowledge, is the first successful
application of differentiable logic gate networks in recurrent architectures.

</details>


### [72] [Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life](https://arxiv.org/abs/2506.04916)
*Atahan Karagoz*

Main category: cs.AI

TL;DR: 本文提出能量智能系统，利用闭环控制自我调节能量在不稳定资源环境中生存，以此为理论和架构基础实现自主代理的持久性。


<details>
  <summary>Details</summary>
Motivation: 目前的奖励驱动范式侧重于任务表现，在资源有限或波动的环境中难以适用。因此，需开发能够自我调节能量的系统以保证持久生存。

Method: 我们通过能量为基础的效用函数和生存范围的可行性约束形式化该原则，并提出一个集成能量收集、热量调节和自适应计算的模块化架构，组成闭环控制系统。

Result: 在模拟环境中，系统在无外部监督情况下表现出稳定且资源感知的行为。

Conclusion: 本文提出了一种新的自治系统——能量智能，其目标不是任务性能，而是通过内部能量调节维持自身生存。这为在资源变化无常的环境中部署自治代理提供了理论和架构基础。

Abstract: This paper introduces Energentic Intelligence, a class of autonomous systems
defined not by task performance, but by their capacity to sustain themselves
through internal energy regulation. Departing from conventional reward-driven
paradigms, these agents treat survival-maintaining functional operation under
fluctuating energetic and thermal conditions-as the central objective. We
formalize this principle through an energy-based utility function and a
viability-constrained survival horizon, and propose a modular architecture that
integrates energy harvesting, thermal regulation, and adaptive computation into
a closed-loop control system. A simulated environment demonstrates the
emergence of stable, resource-aware behavior without external supervision.
Together, these contributions provide a theoretical and architectural
foundation for deploying autonomous agents in resource-volatile settings where
persistence must be self-regulated and infrastructure cannot be assumed.

</details>


### [73] [Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning](https://arxiv.org/abs/2506.04998)
*Mehdi Azarafza,Mojtaba Nayyeri,Faezeh Pasandideh,Steffen Staab,Achim Rettberg*

Main category: cs.AI

TL;DR: RAG-UAV框架通过检索增强提高LLM在无人机任务中的数学推理能力，实验结果表明其显著提升了准确率并减少错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自主无人机任务中的数学推理能力有待提高，传统飞行控制依赖硬编码方程，而新兴的LLM虽具潜力但在数学推理上表现不佳。

Method: 提出了一种名为RAG-UAV的检索增强生成框架，通过提供相关领域文献来改进多个大型语言模型的数学推理能力。

Result: 实验表明，RAG框架能显著提高精确答案的准确率，减少错误公式选择，并显著降低均方误差。

Conclusion: RAG-UAV框架显著提高了多个大型语言模型在UAV特定情境下的数学推理能力，使其在工程分析中更为可靠。

Abstract: Autonomous UAV operation necessitates reliable mathematical reasoning for
tasks such as trajectory planning and power management. While traditional
flight control relies on hardcoded equations, recent Large Language Models
(LLMs) offer potential for more flexible problem-solving but struggle with
reliably selecting and applying correct mathematical formulations and executing
precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented
generation framework designed to improve the mathematical reasoning of several
LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in
UAV-specific contexts by providing access to relevant domain literature. To
conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem
set comprising 20 UAV-centric mathematical problems across four difficulty
levels. Our experiments demonstrate that incorporating retrieval substantially
increases exact answer accuracy (achieving up to 75% with o1), reduces
instances of incorrect formulation selection (from 25% without RAG to 5% with
RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders
of magnitude for the best-performing models. This pilot study indicates that
RAG can enable general-purpose LLMs to function as more reliable tools for
engineering analysis, although direct real-time flight control requires further
investigation and validation on a larger scale. All benchmark data, question
and answer are publicly available.

</details>


### [74] [Truly Self-Improving Agents Require Intrinsic Metacognitive Learning](https://arxiv.org/abs/2506.05109)
*Tennison Liu,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 研究提出内在元认知框架以克服目前自我改进代理的局限性，分析现有系统，并探索如何优化人类与代理间的元认知任务分配。


<details>
  <summary>Details</summary>
Motivation: 当前的自我改进代理存在局限性，包括其过程僵化、难以在任务领域间泛化以及无法随着能力提升进行扩展。有效的自我改进需要内在的元认知学习能力，能够主动评估、反思及调整自身学习过程。

Method: 引入一个由元认知知识、元认知规划及元认知评估构成的框架，这三者分别对应自评能力、学习决策及反思学习经验。通过分析现有的自我改进代理，发现其主要依赖外部设计的固定系统来进行元认知过程。

Result: 许多内在元认知学习的要素已经存在，但需要更好地分配和评估元认知职责，以实现持续、广泛及一致的自我改进。

Conclusion: 内在元认知学习是实现自我改进的关键，需要创新方法来分配人类和代理的元认知职责，同时提高内在元认知学习的效率。

Abstract: Self-improving agents aim to continuously acquire new capabilities with
minimal supervision. However, current approaches face two key limitations:
their self-improvement processes are often rigid, fail to generalize across
tasks domains, and struggle to scale with increasing agent capabilities. We
argue that effective self-improvement requires intrinsic metacognitive
learning, defined as an agent's intrinsic ability to actively evaluate, reflect
on, and adapt its own learning processes. Drawing inspiration from human
metacognition, we introduce a formal framework comprising three components:
metacognitive knowledge (self-assessment of capabilities, tasks, and learning
strategies), metacognitive planning (deciding what and how to learn), and
metacognitive evaluation (reflecting on learning experiences to improve future
learning). Analyzing existing self-improving agents, we find they rely
predominantly on extrinsic metacognitive mechanisms, which are fixed,
human-designed loops that limit scalability and adaptability. Examining each
component, we contend that many ingredients for intrinsic metacognition are
already present. Finally, we explore how to optimally distribute metacognitive
responsibilities between humans and agents, and robustly evaluate and improve
intrinsic metacognitive learning, key challenges that must be addressed to
enable truly sustained, generalized, and aligned self-improvement.

</details>


### [75] [LLM-First Search: Self-Guided Exploration of the Solution Space](https://arxiv.org/abs/2506.05213)
*Nathan Herr,Tim Rocktäschel,Roberta Raileanu*

Main category: cs.AI

TL;DR: 提出LLM-First Search方法，提高了大型语言模型在推理和规划任务中的表现，具有更高的计算效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索方法，如蒙特卡洛树搜索，依赖于固定的探索超参数，限制了其在不同难度任务上的适应性。在某些情况下，它们可能不实用或成本高昂。本文提出了一种新的搜索方法，以提高大型语言模型在推理和规划方面的表现。

Method: 提出了一种新的基于大型语言模型的自导搜索方法，称为LLM-First Search，通过自我引导探索来控制搜索过程。该方法不依赖于外部启发式或硬编码策略，而是基于内部评分机制来评估是否继续当前搜索路径或探索备选分支。

Result: LLM-First Search在熵倒和数独游戏中的表现优于三种经典广泛使用的搜索算法，并且与其他方法相比更加计算效率，尤其是在使用更强模型时表现更好。

Conclusion: LLM-First Search (LFS)比传统搜索算法在挑战性任务上的表现更好，计算效率更高，尤其在使用更强大的模型时。

Abstract: Large Language Models (LLMs) have demonstrated remarkable improvements in
reasoning and planning through increased test-time compute, often by framing
problem-solving as a search process. While methods like Monte Carlo Tree Search
(MCTS) have proven effective in some domains, their reliance on fixed
exploration hyperparameters limits their adaptability across tasks of varying
difficulty, rendering them impractical or expensive in certain settings. In
this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM
Self-Guided Search} method that removes the need for pre-defined search
strategies by empowering the LLM to autonomously control the search process via
self-guided exploration. Rather than relying on external heuristics or
hardcoded policies, the LLM evaluates whether to pursue the current search path
or explore alternative branches based on its internal scoring mechanisms. This
enables more flexible and context-sensitive reasoning without requiring manual
tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku
against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth
First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which
have been used to achieve SotA results on a range of challenging reasoning
tasks. We found that LFS (1) performs better on more challenging tasks without
additional tuning, (2) is more computationally efficient compared to the other
methods, especially when powered by a stronger model, (3) scales better with
stronger models, due to its LLM-First design, and (4) scales better with
increased compute budget. Our code is publicly available at
\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.

</details>


### [76] [Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning](https://arxiv.org/abs/2506.05256)
*Violet Xiang,Chase Blagden,Rafael Rafailov,Nathan Lile,Sang Truong,Chelsea Finn,Nick Haber*

Main category: cs.AI

TL;DR: 引入一种自适应长度惩罚（ALP）机制，用于根据提示难度调整token生成长度，提高了困难问题的准确性，减少了简单问题上的计算。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案通常需要数据组织、手动配置，或者不考虑问题的难易，而是对所有问题一视同仁。为了解决这种问题的浪费计算，提出了自适应长度惩罚（ALP），一种调整生成长度以适应每个提示解答率的强化学习目标。

Method: 提出了一种新的自适应长度惩罚（ALP）强化学习目标。在训练期间，ALP通过多个rollouts监视每个提示的在线解答率，并添加一个可微分惩罚，其大小与解答率成反比。这样，自信（容易）的提示会因为额外的token而承担高昂成本，而困难提示则不受影响。

Result: 经过ALP后训练的DeepScaleR-1.5B显著减少了token的平均使用量，同时在性能上没有显著下降。

Conclusion: 使用ALP实施的DeepScaleR-1.5B减少了50%的平均token使用量，同时保持了性能。与固定预算和统一惩罚基线相比，ALP更智能地分配其减少的预算，在简单提示上减少计算，并将节省的token重新分配给困难提示，从而在最困难的问题上实现更高的准确性。

Abstract: Large reasoning models (LRMs) achieve higher performance on challenging
reasoning tasks by generating more tokens at inference time, but this verbosity
often wastes computation on easy problems. Existing solutions, including
supervised finetuning on shorter traces, user-controlled budgets, or RL with
uniform penalties, either require data curation, manual configuration, or treat
all problems alike regardless of difficulty. We introduce Adaptive Length
Penalty (ALP), a reinforcement learning objective tailoring generation length
to per-prompt solve rate. During training, ALP monitors each prompt's online
solve rate through multiple rollouts and adds a differentiable penalty whose
magnitude scales inversely with that rate, so confident (easy) prompts incur a
high cost for extra tokens while hard prompts remain unhindered. Posttraining
DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly
dropping performance. Relative to fixed-budget and uniform penalty baselines,
ALP redistributes its reduced budget more intelligently by cutting compute on
easy prompts and reallocating saved tokens to difficult ones, delivering higher
accuracy on the hardest problems with higher cost.

</details>


### [77] [Control Tax: The Price of Keeping AI in Check](https://arxiv.org/abs/2506.05296)
*Mikhail Terekhov,Zhen Ning David Liu,Caglar Gulcehre,Samuel Albanie*

Main category: cs.AI

TL;DR: 本研究提出了一个量化AI控制税的理论框架，并通过实证研究优化AI监测策略，以实现安全性与成本效益之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着代理AI在高风险实际应用中的快速整合，迫切需要建立稳健的监管机制，因此提出了AI控制领域的研究，以应对控制措施难以实施的问题。

Method: 研究采用了理论框架的构建，量化控制税，以及在对抗性环境中对语言模型进行综合评估，并基于实际财务成本估算开发优化监测策略，平衡安全性与成本效益。

Result: 实验证明，优化的监测策略可以在考虑审计预算等实际约束的情况下有效平衡安全性和成本效益，并为实践工作者在不同部署环境下做出明智决策提供支持。

Conclusion: 该研究提出一种新的理论框架，通过量化控制税来将分类器性能映射到安全保证上，推动AIC通过系统的经济可行性评估在不同部署环境下取得进展，为实际应用中的控制措施提供了一种以经济性为导向的方法。

Abstract: The rapid integration of agentic AI into high-stakes real-world applications
requires robust oversight mechanisms. The emerging field of AI Control (AIC)
aims to provide such an oversight mechanism, but practical adoption depends
heavily on implementation overhead. To study this problem better, we introduce
the notion of Control tax -- the operational and financial cost of integrating
control measures into AI pipelines. Our work makes three key contributions to
the field of AIC: (1) we introduce a theoretical framework that quantifies the
Control Tax and maps classifier performance to safety assurances; (2) we
conduct comprehensive evaluations of state-of-the-art language models in
adversarial settings, where attacker models insert subtle backdoors into code
while monitoring models attempt to detect these vulnerabilities; and (3) we
provide empirical financial cost estimates for control protocols and develop
optimized monitoring strategies that balance safety and cost-effectiveness
while accounting for practical constraints like auditing budgets. Our framework
enables practitioners to make informed decisions by systematically connecting
safety guarantees with their costs, advancing AIC through principled economic
feasibility assessment across different deployment contexts.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [78] [Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs](https://arxiv.org/abs/2506.04236)
*Botao Amber Hu,Helena Rong*

Main category: cs.MA

TL;DR: 本文研究了基于区块链智能体在开放系统中实现持续的开放式进化的可能性，并于Spore.fun平台上进行了实际案例分析。


<details>
  <summary>Details</summary>
Motivation: 通过新技术的应用，将大规模语言模型（LLM）驱动的AI智能体部署在区块链上，以实现无需人类监督的自治演化，为长期的开放式进化提供可能。

Method: 本文利用数字行为学对Spore.fun中链上智能体的行为和进化轨迹进行了详细的案例研究。

Result: 研究表明，链上智能体能够自主繁殖和进化，与区块链金融网络及人类社交媒体进行交互，展示了开放系统中持续新颖性的潜力。

Conclusion: 该研究探讨了开放人工生命系统是否能够通过经济激励与环境互动来实现持续的开放式进化。

Abstract: In Artificial Life (ALife) research, replicating Open-Ended Evolution
(OEE)-the continuous emergence of novelty observed in biological life-has
traditionally been pursued within isolated closed system simulations, such as
Tierra and Avida, which have typically plateaued after an initial burst of
novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires
an "open" system that continually exchanges information or energy with its
environment. A recent technological innovation in decentralized physical
infrastructure networks (DePIN) providing permissionless computational
substrates enables deploying large language model (LLM)-based AI agents on
blockchains integrated with Trusted Execution Environments (TEEs). This enables
on-chain agents to operate autonomously "in the wild," achieving
self-sovereignty without human oversight. These agents can control their own
social media accounts and cryptocurrency wallets, allowing them to interact
directly with blockchain-based financial networks and broader human social
media. Building on this new paradigm of on-chain agents, Spore.fun is a recent
real-world AI evolution experiment that enables autonomous breeding and
evolution of new on-chain agents. This paper presents a detailed case study of
Spore.fun, examining agent behaviors and their evolutionary trajectories
through digital ethology. We aim to spark discussion about whether "open" ALife
systems "in-the-wild," based on permissionless computational substrates and
driven by economic incentives to interact with their environment, could finally
achieve the long-sought goal of OEE.

</details>


### [79] [HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource Utilization](https://arxiv.org/abs/2506.04255)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.MA

TL;DR: HASHIRU improves flexibility, resource efficiency, and adaptability in MAS through a dynamic management model and hybrid intelligence, outperforming current benchmarks in various evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing MAS frameworks lack flexibility, resource awareness, model diversity, and the ability to autonomously create tools, which HASHIRU aims to address.

Method: The framework employs a "CEO" agent that dynamically manages "employee" agents, adapting to task and resource needs. It uses a hybrid intelligence model prioritizing smaller, local LLMs, and flexibly supplements with external APIs and larger models. An economic model governs hiring/firing costs to enhance stability and efficiency.

Result: HASHIRU demonstrated high performance in various tasks. It succeeded in academic paper review (58% success), excelled in safety assessments (100% success on a JailbreakBench subset), and outperformed existing models in complex reasoning tasks such as GSM8K, JEEBench, and SVAMP.

Conclusion: HASHIRU provides a more robust, efficient, and adaptable MAS framework through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension.

Abstract: Rapid Large Language Model (LLM) advancements are fueling autonomous
Multi-Agent System (MAS) development. However, current frameworks often lack
flexibility, resource awareness, model diversity, and autonomous tool creation.
This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent
Resource Utilization), a novel MAS framework enhancing flexibility, resource
efficiency, and adaptability. HASHIRU features a "CEO" agent dynamically
managing specialized "employee" agents, instantiated based on task needs and
resource constraints (cost, memory). Its hybrid intelligence prioritizes
smaller, local LLMs (via Ollama) while flexibly using external APIs and larger
models when necessary. An economic model with hiring/firing costs promotes team
stability and efficient resource allocation. The system also includes
autonomous API tool creation and a memory function. Evaluations on tasks like
academic paper review (58% success), safety assessments (100% on a
JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash
on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate
HASHIRU's capabilities. Case studies illustrate its self-improvement via
autonomous cost model generation, tool integration, and budget management.
HASHIRU offers a promising approach for more robust, efficient, and adaptable
MAS through dynamic hierarchical control, resource-aware hybrid intelligence,
and autonomous functional extension. Source code and benchmarks are available
at https://github.com/HASHIRU-AI/HASHIRU and
https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is
available at https://hashiruagentx-hashiruai.hf.space upon request.

</details>


### [80] [CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients](https://arxiv.org/abs/2506.04265)
*Mengda Ji,Genjiu Xu,Liying Wang*

Main category: cs.MA

TL;DR: CORA方法通过联盟级信用分配提升多智能体强化学习性能，尤其在多局部最优任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习方法在信用分配上往往缺乏对联盟级贡献的详细分析，导致在策略更新上表现不佳。

Method: 本文提出了CORA信用分配方法，通过随机联盟抽样计算联盟优势，并使用合作博弈论中的核心解来分解优势。

Result: 实验结果表明，CORA在多智能体协作任务中优于强基线方法，特别是在存在多个局部最优点的任务中表现突出。

Conclusion: 本文提出了一种名为CORA的信用分配方法，强调联盟级的信用分配在改进多智能体强化学习性能中的重要性。

Abstract: This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.

</details>


### [81] [CPU-Based Layout Design for Picker-to-Parts Pallet Warehouses](https://arxiv.org/abs/2506.04266)
*Timo Looms,Lin Xie*

Main category: cs.MA

TL;DR: 研究引入了一种受CPU架构启发的仓库空间布局，仿真结果显示该布局在吞吐时间和劳动力需求方面优于传统布局。


<details>
  <summary>Details</summary>
Motivation: 传统布局导致过多的行走距离和高劳动力需求，亟需一种创新的布局设计来优化仓库效率。

Method: 使用离散事件仿真技术对不同布局进行对比评估。

Result: 新的布局设计在吞吐时间和劳动力需求方面取得了显著的改善。

Conclusion: 基于CPU的布局设计可以显著改善仓库的运营效率，具体表现为缩短了运输时间和降低了劳动力需求。

Abstract: Picker-to-parts pallet warehouses often face inefficiencies due to
conventional layouts causing excessive travel distances and high labor
requirements. This study introduces a novel layout design inspired by CPU
architecture, partitioning warehouse space into specialized zones, namely
Performance (P), Efficiency (E), and Shared (S). Discrete-event simulation is
used to evaluate this design against traditional rectangular (random and ABC
storage) and Flying-V layouts. Results demonstrate significant improvements in
throughput time and reduced labor requirements, highlighting the potential for
CPU-based layouts in optimizing warehouse operations.

</details>


### [82] [Autonomous Collaborative Scheduling of Time-dependent UAVs, Workers and Vehicles for Crowdsensing in Disaster Response](https://arxiv.org/abs/2506.04276)
*Lei Han,Yitong Guo,Pengfei Yang,Zhiyong Yu,Liang Wang,Quan Wang,Zhiwen Yu*

Main category: cs.MA

TL;DR: 研究提出HoAs-PALN算法，通过自适应降维和纳什均衡博弈优化调度，在灾后环境信息采集中表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前感知技术在灾后环境中存在适应性差、专业感知能力不足和解决方案实用性有限等问题，需要更有效的灾后环境信息采集方式。

Method: 采用异构多智能体在线自主协作调度算法HoAs-PALN，通过匹配过程中的自适应降维和局部纳什均衡博弈实现。

Result: 实验表明，与基线方法相比，HoAs-PALN的任务完成率平均提高64.12%、46.48%、16.55%和14.03%，每次在线调度决策耗时均小于10秒。

Conclusion: HoAs-PALN算法在动态灾后环境中表现出色，显著提高任务完成率，并能够快速做出在线调度决策。

Abstract: Natural disasters have caused significant losses to human society, and the
timely and efficient acquisition of post-disaster environmental information is
crucial for the effective implementation of rescue operations. Due to the
complexity of post-disaster environments, existing sensing technologies face
challenges such as weak environmental adaptability, insufficient specialized
sensing capabilities, and limited practicality of sensing solutions. This paper
explores the heterogeneous multi-agent online autonomous collaborative
scheduling algorithm HoAs-PALN, aimed at achieving efficient collection of
post-disaster environmental information. HoAs-PALN is realized through adaptive
dimensionality reduction in the matching process and local Nash equilibrium
game, facilitating autonomous collaboration among time-dependent UAVs, workers
and vehicles to enhance sensing scheduling. (1) In terms of adaptive
dimensionality reduction during the matching process, HoAs-PALN significantly
reduces scheduling decision time by transforming a five-dimensional matching
process into two categories of three-dimensional matching processes; (2)
Regarding the local Nash equilibrium game, HoAs-PALN combines the softmax
function to optimize behavior selection probabilities and introduces a local
Nash equilibrium determination mechanism to ensure scheduling decision
performance. Finally, we conducted detailed experiments based on extensive
real-world and simulated data. Compared with the baselines (GREEDY, K-WTA, MADL
and MARL), HoAs-PALN improves task completion rates by 64.12%, 46.48%, 16.55%,
and 14.03% on average, respectively, while each online scheduling decision
takes less than 10 seconds, demonstrating its effectiveness in dynamic
post-disaster environments.

</details>


### [83] [From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems](https://arxiv.org/abs/2506.04565)
*Jiayi Chen,Junyi Ye,Guiling Wang*

Main category: cs.MA

TL;DR: 此调查概述了复合AI系统（CAIS），提出了分类法，并分析了几个基础范式，以帮助理解和发展系统级AI。


<details>
  <summary>Details</summary>
Motivation: 应对独立模型在需要记忆、推理、实时落地和多模态理解任务上的局限性，整合LLMs与外部组件形成复合AI系统。

Method: 定义CAIS概念，提出基于组件角色和编排策略的多维分类法，分析四个基础范式，并回顾代表性系统，比较设计权衡，总结评价方法。

Result: 定位关键挑战，包括可扩展性、互操作性、基准测试和协调，并提出未来研究的有前景方向。

Conclusion: 本文提出了一个多维分类法，并分析了四个基本范式，以帮助理解和发展下一代系统级人工智能的发展方向。

Abstract: Compound Al Systems (CAIS) is an emerging paradigm that integrates large
language models (LLMs) with external components, such as retrievers, agents,
tools, and orchestrators, to overcome the limitations of standalone models in
tasks requiring memory, reasoning, real-time grounding, and multimodal
understanding. These systems enable more capable and context-aware behaviors by
composing multiple specialized modules into cohesive workflows. Despite growing
adoption in both academia and industry, the CAIS landscape remains fragmented,
lacking a unified framework for analysis, taxonomy, and evaluation. In this
survey, we define the concept of CAIS, propose a multi-dimensional taxonomy
based on component roles and orchestration strategies, and analyze four
foundational paradigms: Retrieval-Augmented Generation (RAG), LLM Agents,
Multimodal LLMs (MLLMs), and orchestration-centric architectures. We review
representative systems, compare design trade-offs, and summarize evaluation
methodologies across these paradigms. Finally, we identify key
challenges-including scalability, interoperability, benchmarking, and
coordination-and outline promising directions for future research. This survey
aims to provide researchers and practitioners with a comprehensive foundation
for understanding, developing, and advancing the next generation of
system-level artificial intelligence.

</details>


### [84] [Towards Language-Augmented Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.05236)
*Maxime Toquebiau,Jae-Yun Jun,Faïz Benamar,Nicolas Bredeche*

Main category: cs.MA

TL;DR: 研究将人类语言整合到多智能体学习中，证明语言增强学习可以提高代理的表现和解释能力。


<details>
  <summary>Details</summary>
Motivation: 探讨通过将代理与人类定义的语言基础相结合来提高多智能体的学习和协调能力。

Method: 提出了一种框架，使代理不仅学习行动，还能够产生和解释自然语言描述，以改进多智能体系统的协调和学习。

Result: 与传统的自主形成通信基线相比，使用语言增强学习方法的代理在不同任务上表现更优。

Conclusion: 语言增强学习可以有效地改善多智能体学习的性能和解释性。

Abstract: Communication is a fundamental aspect of coordinated behavior in multi-agent
reinforcement learning. Yet, most prior works in this field have focused on
emergent communication protocols developed from scratch, often resulting in
inefficient or non-interpretable systems. Inspired by the role of language in
natural intelligence, we investigate how grounding agents in a human-defined
language can improve learning and coordination of multiple embodied agents. We
propose a framework in which agents are trained not only to act but also to
produce and interpret natural language descriptions of their observations. This
language-augmented learning serves a dual role: enabling explicit communication
between agents and guiding representation learning. We demonstrate that agents
trained with our method outperform traditional emergent communication baselines
across various tasks. Our analysis reveals that language grounding leads to
more informative internal representations, better generalization to new
partners, and improved capability for human-agent interaction. These findings
demonstrate the effectiveness of integrating structured language into
multi-agent learning and open avenues for more interpretable and capable
multi-agent systems.

</details>


### [85] [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)
*Niv Eckhaus,Uri Berger,Gabriel Stanovsky*

Main category: cs.MA

TL;DR: 本文开发了一种异步LLM代理，可以决定何时发言，其在Mafia游戏中的表现与人类玩家相当，并能够自然融入参与。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在异步交流中决定何时发言的适应性异步LLM代理。

Method: 收集在线Mafia游戏数据，评估异步代理与人类参与者的表现与融入能力。

Result: 我们的代理在游戏表现和融入方面表现与人类玩家相当，发言时机与人类相似，消息内容有所不同。

Conclusion: 成果促进了LLMs在现实人群环境中的集成，推动其在团队讨论、教育和专业环境中解决复杂社交动态的应用。

Abstract: LLMs are used predominantly in synchronous communication, where a human user
and a model communicate in alternating turns. In contrast, many real-world
settings are inherently asynchronous. For example, in group chats, online team
meetings, or social games, there is no inherent notion of turns; therefore, the
decision of when to speak forms a crucial part of the participant's decision
making. In this work, we develop an adaptive asynchronous LLM-agent which, in
addition to determining what to say, also decides when to say it. To evaluate
our agent, we collect a unique dataset of online Mafia games, including both
human participants, as well as our asynchronous agent. Overall, our agent
performs on par with human players, both in game performance, as well as in its
ability to blend in with the other human players. Our analysis shows that the
agent's behavior in deciding when to speak closely mirrors human patterns,
although differences emerge in message content. We release all our data and
code to support and encourage further research for more realistic asynchronous
communication between LLM agents. This work paves the way for integration of
LLMs into realistic human group settings, from assistance in team discussions
to educational and professional environments where complex social dynamics must
be navigated.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [A Comprehensive Survey on the Risks and Limitations of Concept-based Models](https://arxiv.org/abs/2506.04237)
*Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 论文对概念模型的风险和限制进行了详细调查，指出其结构和训练程序中的显著限制，并总结改进措施和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 推动概念模型在敏感领域的更广泛应用，以增加各方的信任。

Method: 对概念模型的风险和局限性进行了综合调查，汇总了常见挑战及应对架构选择，并探讨了改进模型可靠性的最新进展。

Result: 提供了对概念模型的深入分析，指出其在结构、训练、假设和对抗脆弱性方面的显著限制，并总结了改进其可靠性的进展。

Conclusion: 对概念模型在各个方面的挑战及改进措施进行了全面总结，但仍存在许多未解决的问题和未来研究方向。

Abstract: Concept-based Models are a class of inherently explainable networks that
improve upon standard Deep Neural Networks by providing a rationale behind
their predictions using human-understandable `concepts'. With these models
being highly successful in critical applications like medical diagnosis and
financial risk prediction, there is a natural push toward their wider adoption
in sensitive domains to instill greater trust among diverse stakeholders.
However, recent research has uncovered significant limitations in the structure
of such networks, their training procedure, underlying assumptions, and their
susceptibility to adversarial vulnerabilities. In particular, issues such as
concept leakage, entangled representations, and limited robustness to
perturbations pose challenges to their reliability and generalization.
Additionally, the effectiveness of human interventions in these models remains
an open question, raising concerns about their real-world applicability. In
this paper, we provide a comprehensive survey on the risks and limitations
associated with Concept-based Models. In particular, we focus on aggregating
commonly encountered challenges and the architecture choices mitigating these
challenges for Supervised and Unsupervised paradigms. We also examine recent
advances in improving their reliability and discuss open problems and promising
avenues of future research in this domain.

</details>


### [87] [Improving Out-of-Distribution Detection with Markov Logic Networks](https://arxiv.org/abs/2506.04241)
*Konstantin Kirchheim,Frank Ortmeier*

Main category: cs.LG

TL;DR: 本研究通过结合马尔可夫逻辑网络，加强了现有的OOD检测器的性能和解释性，保持高效的同时提出一种学习算法来优化逻辑约束。


<details>
  <summary>Details</summary>
Motivation: 确保深度学习模型在开放世界场景中的可靠性，现有的OOD检测器主要依赖于统计模型，需要寻找更具解释性和高效的替代方案。

Method: 本研究将现有的OOD检测器与马尔可夫逻辑网络（MLNs）相结合，通过在人的可理解概念上定义加权逻辑约束来为输入赋予概率，从而增强OOD检测的效果。同时，引入一种从数据集中学习逻辑约束的算法。

Result: 在多个数据集上进行的大量实验表明，MLNs可以显著提高现有多种OOD检测器的性能，同时保持计算高效性。此外，学习逻辑约束的算法展示了其有效性。

Conclusion: 通过将概率推理和MLNs引入OOD检测，能够提高检测性能，并增强结果的可解释性。

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models operating in open-world scenarios. Current OOD
detectors mainly rely on statistical models to identify unusual patterns in the
latent representations of a deep neural network. This work proposes to augment
existing OOD detectors with probabilistic reasoning, utilizing Markov logic
networks (MLNs). MLNs connect first-order logic with probabilistic reasoning to
assign probabilities to inputs based on weighted logical constraints defined
over human-understandable concepts, which offers improved explainability.
Through extensive experiments on multiple datasets, we demonstrate that MLNs
can significantly enhance the performance of a wide range of existing OOD
detectors while maintaining computational efficiency. Furthermore, we introduce
a simple algorithm for learning logical constraints for OOD detection from a
dataset and showcase its effectiveness.

</details>


### [88] [Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction](https://arxiv.org/abs/2506.04243)
*Warayut Dokduea,Weerachart Tangchirapat,Sompote Youwai*

Main category: cs.LG

TL;DR: 提出三重注意力变压器架构，以优异表现预测混凝土蠕变，这项工作展示了变压器架构在材料科学中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前方法将时间仅仅视为输入参数，没有建模变形发展的顺序性。本文通过新的变压器架构解决这一根本限制。

Method: 本文将混凝土蠕变预测转化为自回归序列建模任务，类似于语言处理，利用变压器的自注意力机制捕捉历史蠕变模式中的长期依赖关系。模型实施了三重流注意力框架，包括时间注意力、特征注意力和批注意力。

Result: 在包含160天标准化日测量的实验数据集上，模型实现了1.63%的平均绝对百分比误差和所有数据集上的R^2值为0.999，显著优于传统经验模型和现有机器学习方法。注意力机制的消融研究证实了其关键作用，注意力池化对模型性能贡献最大。

Conclusion: 本文提出的三重注意力变压器架构在预测时间依赖型混凝土蠕变时表现优异，超越传统经验模型和现有机器学习方法，为材料科学问题应用变压器架构提供了可能性，揭示了数据驱动方法在预测结构行为和工程设计实践中的革命潜力。

Abstract: This paper presents a novel Triple Attention Transformer Architecture for
predicting time-dependent concrete creep, addressing fundamental limitations in
current approaches that treat time as merely an input parameter rather than
modeling the sequential nature of deformation development. By transforming
concrete creep prediction into an autoregressive sequence modeling task similar
to language processing, our architecture leverages the transformer's
self-attention mechanisms to capture long-range dependencies in historical
creep patterns. The model implements a triple-stream attention framework
incorporating temporal attention for sequential progression, feature attention
for material property interactions, and batch attention for inter-sample
relationships. Evaluated on experimental datasets with standardized daily
measurements spanning 160 days, the architecture achieves exceptional
performance with mean absolute percentage error of 1.63% and R2 values of 0.999
across all datasets, substantially outperforming traditional empirical models
and existing machine learning approaches. Ablation studies confirm the critical
role of attention mechanisms, with attention pooling contributing most
significantly to model performance. SHAP analysis reveals Young's modulus as
the primary predictive feature, followed by density and compressive strength,
providing interpretability essential for engineering applications. A deployed
web-based interface facilitates practical implementation, enabling real-time
predictions using standard laboratory parameters. This work establishes the
viability of applying transformer architectures to materials science problems,
demonstrating the potential for data-driven approaches to revolutionize
structural behavior prediction and engineering design practices.

</details>


### [89] [Associative Memory and Generative Diffusion in the Zero-noise Limit](https://arxiv.org/abs/2506.05178)
*Joshua Hess,Quaid Morris*

Main category: cs.LG

TL;DR: 该研究探讨了关联记忆和生成扩散模型间的关系，强调Morse-Smale系统在噪声消失时的稳定性，并通过分岔分析描述景观转换。


<details>
  <summary>Details</summary>
Motivation: 研究生成扩散模型与连续状态关联记忆模型之间的联系，揭示基础动力系统的普遍性质。

Method: 通过研究Morse-Smale动力系统作为梯度关联记忆模型和扩散模型（带白噪声扰动的系统）的唯象近似。

Result: 确定了关联记忆在噪声水平下降时从生成过渡到记忆的通用性。这还揭示了在零噪声极限下的稳定性，通过小型泛化分歧的集合来表征在这些极限下的稳定系统之间的定性转换。

Conclusion: 研究揭示了Morse-Smale流的结构稳定性在噪声消失极限下蕴含的扩散稳定性概念。通过例子展示了如何通过一系列分岔来表征这些景观，并为经典和现代Hopfield网络提供结构稳定性标准。

Abstract: Connections between generative diffusion and continuous-state associative
memory models are studied. Morse-Smale dynamical systems are emphasized as
universal approximators of gradient-based associative memory models and
diffusion models as white-noise perturbed systems thereof. Universal properties
of associative memory that follow from this description are described and used
to characterize a generic transition from generation to memory as noise levels
diminish. Structural stability inherited by Morse-Smale flows is shown to imply
a notion of stability for diffusions at vanishing noise levels. Applied to one-
and two-parameter families of gradients, this indicates stability at all but
isolated points of associative memory learning landscapes and the learning and
generation landscapes of diffusion models with gradient drift in the zero-noise
limit, at which small sets of generic bifurcations characterize qualitative
transitions between stable systems. Examples illustrating the characterization
of these landscapes by sequences of these bifurcations are given, along with
structural stability criterion for classic and modern Hopfield networks
(equivalently, the attention mechanism).

</details>


### [90] [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
*Shaona Ghosh,Amrita Bhattacharjee,Yftah Ziser,Christopher Parisien*

Main category: cs.LG

TL;DR: 本文提出SafeSteer方法，通过类别特定的转向向量实现LLM输出的精确安全控制，采用简单的无监督方法，避免内容拒绝，且无需对比安全数据，在多个模型、数据集和风险类别中显示出有效性。


<details>
  <summary>Details</summary>
Motivation: 针对微调大型语言模型以适应不断变化的安全策略既昂贵又不切实际的问题，探讨通过潜在激活转向在推理时进行控制，以实现精确且可定制的安全调整。

Method: 使用名为SafeSteer的方法，该方法利用类别特定的转向向量，通过简单、无梯度的无监督方法来提升安全性，同时保持文本质量和主题相关性，而不明确拒绝。

Result: 展示了该方法在不同大型语言模型、数据集和风险类别中的效果，证明其可以实现精确控制、避免一刀切的拒绝，并指导模型生成安全的内容，同时保持主题的相关性。

Conclusion: 所提出的方法能在各种LLMs、数据集和风险类别中有效运作，实现精确控制和避免内容拒绝，确保生成的内容安全且主题相关。

Abstract: Fine-tuning large language models (LLMs) to adapt to evolving safety policies
is costly and impractical. Mechanistic interpretability enables inference-time
control through latent activation steering, yet its potential for precise,
customizable safety adjustments remains largely untapped. This paper
investigates an approach called SafeSteer for guiding the outputs of LLMs by:
(i) leveraging category-specific steering vectors for more precise control,
(ii) employing a simple, gradient-free unsupervised method to enhance safety
steering while preserving text quality, topic relevance, and without explicit
refusal, and (iii) accomplishing this without a hard requirement of contrastive
pairwise safe data. We also highlight that our method, being simple and
effective, aligns with recent studies suggesting that simple techniques often
outperform more complex ones in activation steering. We showcase the
effectiveness of our approach across various LLMs, datasets, and risk
categories, demonstrating its ability to provide precise control, prevent
blanket refusals, and guide models toward generating safe content while
maintaining topic relevance.

</details>


### [91] [Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support](https://arxiv.org/abs/2506.04254)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 本文提出了一种适应地方条件的森林火灾风险评估方法，使用AI模型进行预测，提供法国全国范围内的AI基准。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了火行为和频率，使得准确预测森林火灾成为AI领域的紧迫挑战。传统的二元分类法过于简单化，无法满足消防人员的需求，因此迫切需要一种能够适应地方条件并提供区域专属预测的新方法。

Method: 使用最先进的AI模型进行预测，并以全国范围为尺度，使用未充分研究的数据集进行评估，提供了法国都市地区的第一个国家级AI基准。

Result: 提出了一种适应地方条件的火灾风险评估新方法，以及法国都市地区第一个国家级AI基准，为未来的重要工作提供了总结。

Conclusion: 本文提出了一种新的方法，通过适应不同部门的环境条件，为森林火灾风险评估提供针对性更强的预测。这种方法不仅可应用于法国，也为其他国家提供了参考。

Abstract: Forest fire prediction involves estimating the likelihood of fire ignition or
related risk levels in a specific area over a defined time period. With climate
change intensifying fire behavior and frequency, accurate prediction has become
one of the most pressing challenges in Artificial Intelligence (AI).
Traditionally, fire ignition is approached as a binary classification task in
the literature. However, this formulation oversimplifies the problem,
especially from the perspective of end-users such as firefighters. In general,
as is the case in France, firefighting units are organized by department, each
with its terrain, climate conditions, and historical experience with fire
events. Consequently, fire risk should be modeled in a way that is sensitive to
local conditions and does not assume uniform risk across all regions. This
paper proposes a new approach that tailors fire risk assessment to departmental
contexts, offering more actionable and region-specific predictions for
operational use. With this, we present the first national-scale AI benchmark
for metropolitan France using state-of-the-art AI models on a relatively
unexplored dataset. Finally, we offer a summary of important future works that
should be taken into account. Supplementary materials are available on GitHub.

</details>


### [92] [MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression](https://arxiv.org/abs/2506.04268)
*Jingyang Li,Guoqiang Li*

Main category: cs.LG

TL;DR: MUC-G4是一种用于压缩神经网络增量验证的新框架，实验表明其在处理量化和剪枝时具有高效性和快速性，可在实践应用中确保网络的安全和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习的快速发展带来了在边缘设备上部署神经网络的挑战，主要由于其高内存和运行时复杂性。网络压缩技术旨在减少这种复杂性，同时保持准确性。

Method: 提出了一种名为MUC-G4的新框架，通过将原始和压缩后的网络编码为SMT公式，分类变化，并使用原始网络的最小不满足核心来指导高效验证。

Result: 实验结果表明，与传统方法相比，该方法在量化和剪枝处理方面具有高证明重用率和显著的验证时间加速效果。

Conclusion: MUC-G4提供了一种有效的解决方案，确保压缩神经网络的安全性和可靠性。

Abstract: The rapid development of deep learning has led to challenges in deploying
neural networks on edge devices, mainly due to their high memory and runtime
complexity. Network compression techniques, such as quantization and pruning,
aim to reduce this complexity while maintaining accuracy. However, existing
incremental verification methods often focus only on quantization and struggle
with structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided
Incremental Verification), a novel framework for incremental verification of
compressed deep neural networks. It encodes both the original and compressed
networks into SMT formulas, classifies changes, and use \emph{Minimal Unsat
Cores (MUCs)} from the original network to guide efficient verification for the
compressed network. Experimental results show its effectiveness in handling
quantization and pruning, with high proof reuse rates and significant speedup
in verification time compared to traditional methods. MUC-G4 hence offers a
promising solution for ensuring the safety and reliability of compressed neural
networks in practical applications.

</details>


### [93] [Understanding the Impact of Sampling Quality in Direct Preference Optimization](https://arxiv.org/abs/2506.04272)
*Kyung Rok Kim,Yumo Bai,Chonghuan Wang,Guanting Chen*

Main category: cs.LG

TL;DR: 研究了采样分布对直接偏好优化训练动态的影响，并设计了一种简化模型，证明高质量响应能改善优化效果。


<details>
  <summary>Details</summary>
Motivation: 理解采样分布在直接偏好优化中的作用及其对训练动态的影响。

Method: 设计了一个简化而结构良好的对齐模型，通过这种代理模型，进行了定量研究，展示高质量响应频率如何增强梯度信号并优化优化景观。

Result: 得出更多高质量响应的频率能够增强梯度信号并改善优化景观，从而有效提高策略学习的效果。理论发现得到了实证实验的支持。

Conclusion: 通过分析和实验验证了生成分布的支持和质量对DPO训练动态的影响，为在线DPO框架的有效性提供了理论支持。

Abstract: We study the role of the sampling distribution in Direct Preference
Optimization (DPO) and aim to understand its impact on DPO's training dynamics.
Our analyses show that both the solution space and the convergence behavior of
DPO depend on the support and quality of the generating distribution. We first
analyze how distribution of responses influences policy updates during gradient
descent, drawing connections to common phenomena found in practice. We then
design a simplified yet well-structured alignment model as a proxy, and develop
quantitative results showing how more frequent high-quality responses amplify
the gradient signal and improve the optimization landscape, leading to more
effective policy learning. Our theoretical findings are supported by empirical
experiments and provide a principled justification for the online DPO framework
in practice.

</details>


### [94] [SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida](https://arxiv.org/abs/2506.04281)
*Xu Zheng,Chaohao Lin,Sipeng Chen,Zhuomin Chen,Jimeng Shi,Wei Cheng,Jayantha Obeysekera,Jason Liu,Dongsheng Luo*

Main category: cs.LG

TL;DR: 本文引入了SF2Bench，评估了多种机器学习方法在复合洪水预测中的表现，结果显示不同方法在时空依赖处理上表现不一。


<details>
  <summary>Details</summary>
Motivation: 当前洪水预测面临数据不足和分析方法效率不高的问题，因此需要开发新的数据集和方法来提高洪水预测效果。

Method: 研究评估了六类建模方法的性能，包括多层感知机、卷积神经网络、递归神经网络、图神经网络、变压器模型和大型语言模型。

Result: 研究通过实验验证了不同关键特征对洪水预测的影响，并分析了时空因素对预测的影响。结果表明，各类方法在捕捉复合洪水中的复杂时空依赖关系上表现不同。

Conclusion: 多种建模方法在捕捉复合洪水复杂时空依赖关系方面表现出了不同的能力，表明这些方法具有很大的潜力用于改善洪水预测。

Abstract: Forecasting compound floods presents a significant challenge due to the
intricate interplay of meteorological, hydrological, and oceanographic factors.
Analyzing compound floods has become more critical as the global climate
increases flood risks. Traditional physics-based methods, such as the
Hydrologic Engineering Center's River Analysis System, are often
time-inefficient. Machine learning has recently demonstrated promise in both
modeling accuracy and computational efficiency. However, the scarcity of
comprehensive datasets currently hinders systematic analysis. Existing
water-related datasets are often limited by a sparse network of monitoring
stations and incomplete coverage of relevant factors. To address this
challenge, we introduce SF2Bench, a comprehensive time series collection on
compound floods in South Florida, which integrates four key factors: tide,
rainfall, groundwater, and human management activities (gate and pump
controlling). This integration allows for a more detailed analysis of the
individual contributions of these drivers to compound flooding and informs the
development of improved flood forecasting approaches. To comprehensively
evaluate the potential of various modeling paradigms, we assess the performance
of six categories of methods, encompassing Multilayer Perceptrons,
Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural
Networks, Transformers, and Large Language Models. We verified the impact of
different key features on flood forecasting through experiments. Our analysis
examines temporal and spatial aspects, providing insights into the influence of
historical data and spatial dependencies. The varying performance across these
approaches underscores the diverse capabilities of each in capturing complex
temporal and spatial dependencies inherent in compound floods.

</details>


### [95] [Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning](https://arxiv.org/abs/2506.05252)
*Dravyansh Sharma,Alec Sun*

Main category: cs.LG

TL;DR: 本文探讨了当分类代理真正改进以获得更好分类时的学习算法特性，并解决了Attias等提出的开放问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在社会决策中变得越来越普遍，了解分类代理如何对学习算法做出反应变得越来越重要。本文通过探讨代理真正改进以获得理想分类的学习特性，从而促进学习算法的应用。

Method: 本文通过不对称变体的最小一致概念类，对现实环境下的改进的适当学习进行了精确的描述。

Result: 本文给出了自然的欧几里得球改善集的积极结果，表明在数据分布上的轻度生成假设下的非正规学习。此外，本文在研究良好的有界噪声模型中实现了较低的泛化误差，并获得了可实现和不可知在线学习中的错误边界。

Conclusion: 本文研究了在实现环境下通过代理真正改进的学习算法的可学习性，本文通过引入不对称的最小一致概念类，准确地描述了改进的适当学习。

Abstract: Machine learning is now ubiquitous in societal decision-making, for example
in evaluating job candidates or loan applications, and it is increasingly
important to take into account how classified agents will react to the learning
algorithms. The majority of recent literature on strategic classification has
focused on reducing and countering deceptive behaviors by the classified
agents, but recent work of Attias et al. identifies surprising properties of
learnability when the agents genuinely improve in order to attain the desirable
classification, such as smaller generalization error than standard
PAC-learning. In this paper we characterize so-called learnability with
improvements across multiple new axes. We introduce an asymmetric variant of
minimally consistent concept classes and use it to provide an exact
characterization of proper learning with improvements in the realizable
setting. While prior work studies learnability only under general, arbitrary
agent improvement regions, we give positive results for more natural Euclidean
ball improvement sets. In particular, we characterize improper learning under a
mild generative assumption on the data distribution. We further show how to
learn in more challenging settings, achieving lower generalization error under
well-studied bounded noise models and obtaining mistake bounds in realizable
and agnostic online learning. We resolve open questions posed by Attias et al.
for both proper and improper learning.

</details>


### [96] [DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience](https://arxiv.org/abs/2506.04282)
*Runxiang Wang,Boxiao Wang,Kai Li,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: DrSR结合数据驱动和反思学习改善符号回归能力，在跨学科数据集上的实验显示，其在方程发现的有效性和效率上超过传统和LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法如LLM-SR过于依赖内部的科学先验知识，缺乏显式的数据理解和系统的反思，影响了方程生成的效果。

Method: DrSR引导LLM分析数据中的结构关系，生成结构化描述；同时监控方程性能并建立反馈环，以改进后续生成过程，形成数据理解与生成反思的闭环。

Result: DrSR在物理、化学、生物学和材料科学的跨学科数据集上实验表明，DrSR显著提高了有效方程率，并在准确性、泛化性和搜索效率上超过了经典和近期的LLM方法。

Conclusion: DrSR框架通过结合数据驱动洞察和反思学习提高了符号回归的鲁棒性和发现能力，比传统方法及现有LLM方法在准确性、泛化性和搜索效率上表现更优，显示出其在科学方程发现中的潜力。

Abstract: Symbolic regression is a fundamental tool for discovering interpretable
mathematical expressions from data, with broad applications across scientific
and engineering domains. Recently, large language models (LLMs) have
demonstrated strong performance in this task, leveraging embedded scientific
priors and reasoning capabilities to surpass traditional methods. However,
existing LLM-based approaches, such as LLM-SR, often over-rely on internal
priors, lacking explicit data understanding and systematic reflection during
equation generation. To address these limitations, we propose DrSR (Dual
Reasoning Symbolic Regression), a framework that combines data-driven insight
with reflective learning to enhance both robustness and discovery capability.
Specifically, DrSR guides LLMs to analyze structural relationships (e.g.,
monotonicity, nonlinearity, and correlation) within the data to generate
structured descriptions. Simultaneously, it monitors equation performance and
establishes a feedback loop to refine subsequent generations. By integrating
data understanding and generation reflection in a closed loop, DrSR enables
more efficient exploration of the symbolic expression space. Experiments across
interdisciplinary datasets in physics, chemistry, biology, and materials
science demonstrate that DrSR substantially improves the valid equation rate
and consistently outperforms both classical and recent LLM-based methods in
terms of accuracy, generalization, and search efficiency. These results
underscore its potential for scientific equation discovery.

</details>


### [97] [Training-free AI for Earth Observation Change Detection using Physics Aware Neuromorphic Networks](https://arxiv.org/abs/2506.04285)
*Stephen Smith,Cormac Purcell,Zdenka Kuncic*

Main category: cs.LG

TL;DR: 提出了一种Physics Aware Neuromorphic Network (PANN) 用于提高自然灾害检测中从卫星到地面的数据优先下传，通过物理神经网络实现训练无关、低计算资源需求，且效果优于传统AI模型。


<details>
  <summary>Details</summary>
Motivation: 为解决卫星数据到地面站的带宽限制问题，提出了一种在卫星上进行数据处理并优先下传相关数据的方法。

Method: 使用物理感知网络（PANN）来处理多光谱卫星图像以生成变化图，从而优先下传相关数据。PANN基于纳米电子电路元件“忆阻器”（具有记忆的非线性电阻）组成的物理神经网络，使用与物理相容的动态输出特征进行自然灾害检测。

Result: PANN与最先进的AI模型相比，在每个自然灾害类别中取得了相当或更好的结果。

Conclusion: PANN在与最先进的AI模型的比较中表现出色，表明其在资源受限的机载处理中的潜力。

Abstract: Earth observations from low Earth orbit satellites provide vital information
for decision makers to better manage time-sensitive events such as natural
disasters. For the data to be most effective for first responders, low latency
is required between data capture and its arrival to decision makers. A major
bottleneck is in the bandwidth-limited downlinking of the data from satellites
to ground stations. One approach to overcome this challenge is to process at
least some of the data on-board and prioritise pertinent data to be downlinked.
In this work we propose a Physics Aware Neuromorphic Network (PANN) to detect
changes caused by natural disasters from a sequence of multi-spectral satellite
images and produce a change map, enabling relevant data to be prioritised for
downlinking. The PANN used in this study is motivated by physical neural
networks comprised of nano-electronic circuit elements known as "memristors"
(nonlinear resistors with memory). The weights in the network are dynamic and
update in response to varying input signals according to memristor equations of
state and electrical circuit conservation laws. The PANN thus generates
physics-constrained dynamical output features which are used to detect changes
in a natural disaster detection task by applying a distance-based metric.
Importantly, this makes the whole model training-free, allowing it to be
implemented with minimal computing resources. The PANN was benchmarked against
a state-of-the-art AI model and achieved comparable or better results in each
natural disaster category. It thus presents a promising solution to the
challenge of resource-constrained on-board processing.

</details>


### [98] [Backbone Augmented Training for Adaptations](https://arxiv.org/abs/2506.04288)
*Jae Wan Park,Junhyeok Kim,Youngjun Jun,Hyunah Ko,Seong Jae Hwang*

Main category: cs.LG

TL;DR: 提出BAT方法利用主干数据增强适应性训练，在个性化和语言生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通过使用大量用于训练主干模型的主干数据来缓解因适应性数据有限而导致的训练困难。

Method: 提出了一种主干增强训练（BAT）方法，结合主干数据来扩充适应性数据集，并通过ALBAT算法实施该方法。

Result: ALBAT算法在个性化和语言生成任务中增强了适应性训练，特别是在数据稀缺的情境下。

Conclusion: 利用主干数据可以有效增强适应性训练，增强个性化和语言生成任务的表现。

Abstract: Adaptations facilitate efficient training of large backbone models, including
diffusion models for image generation and transformer-based language models.
While various adaptation techniques enhance performance with minimal
computational resources, limited adaptation data often leads to challenges in
training. To address this, we focus on the enormous amount of backbone data
used to pre-train the backbone models. We propose Backbone Augmented Training
(BAT), a method that leverages backbone data to augment the adaptation dataset.
First, we formulate and prove two mathematical key propositions: one
establishes the validity of BAT, while the other identifies a condition under
which BAT benefits adaptation. Furthermore, we introduce an advanced data
selection scheme that satisfies these propositions and present ALBAT algorithm
to implement this approach. ALBAT efficiently enhances adaptation training in
both personalization and language generation tasks with scarce data.

</details>


### [99] [Relational reasoning and inductive bias in transformers trained on a transitive inference task](https://arxiv.org/abs/2506.04289)
*Jesse Geerts,Stephanie Chan,Claudia Clopath,Kimberly Stachenfeld*

Main category: cs.LG

TL;DR: 研究Transformer在不同学习模式下的传递推理能力，发现预训练可以改善上下文模式下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在不同学习模式下的关系推理机制不明显，尤其是在心理学经典任务中的表现。

Method: 研究Transformer如何在权重学习和上下文学习两种模式下执行传递推理任务。

Result: 权重学习在仅基于相邻项的训练中自然倾向于传递性推理，而只接受相邻项训练的上下文学习模型不表现出传递性推理。预训练任务可以促进上下文关系推理的发展。

Conclusion: 通过分析，揭示了Transformer在不同学习模式中的推理能力差异，并演示了训练任务的选择如何影响模型的推理表现。

Abstract: Transformer-based models have demonstrated remarkable reasoning abilities,
but the mechanisms underlying relational reasoning in different learning
regimes remain poorly understood. In this work, we investigate how transformers
perform a classic relational reasoning task from the Psychology literature,
\textit{transitive inference}, which requires inference about indirectly
related items by integrating information across observed adjacent item pairs
(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior
across two distinct learning regimes: in-weights learning (IWL), where models
store information in network parameters, and in-context learning (ICL), where
models flexibly utilize information presented within the input sequence. Our
findings reveal that IWL naturally induces a generalization bias towards
transitive inference, despite being trained only on adjacent items, whereas ICL
models trained solely on adjacent items do not generalize transitively.
Mechanistic analysis shows that ICL models develop induction circuits that
implement a simple match-and-copy strategy that performs well at relating
adjacent pairs, but does not encoding hierarchical relationships among
indirectly related items. Interestingly, when pre-trained on in-context linear
regression tasks, transformers successfully exhibit in-context generalizable
transitive inference. Moreover, like IWL, they display both \textit{symbolic
distance} and \textit{terminal item effects} characteristic of human and animal
performance, without forming induction circuits. These results suggest that
pre-training on tasks with underlying structure promotes the development of
representations that can scaffold in-context relational reasoning.

</details>


### [100] [A Lyapunov Drift-Plus-Penalty Method Tailored for Reinforcement Learning with Queue Stability](https://arxiv.org/abs/2506.04291)
*Wenhan Xu,Jiashuo Jiang,Lei Deng,Danny Hin-Kwok Tsang*

Main category: cs.LG

TL;DR: 提出了一种称为LDPTRLQ的算法，通过将Lyapunov漂移加罚算法与强化学习相结合来解决队列稳定性问题，实验结果表明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于物联网设备的激增，对解决复杂优化问题的需求加剧，因此需要一种能够平衡Lyapunov漂移加罚算法贪婪优化与强化学习长远视角的方法。

Method: 将Lyapunov漂移加罚算法与强化学习相结合，提出了一种适用于强化学习的队列稳定Lyapunov漂移加罚方法（LDPTRLQ算法）。

Result: 模拟结果表明，LDPTRLQ算法在多个问题上优于基线方法，并在兼容性和稳定性方面也表现出色。

Conclusion: 我们的LDPTRLQ算法优于使用Lyapunov漂移加罚算法和强化学习的基线方法，验证了理论推导的有效性。

Abstract: With the proliferation of Internet of Things (IoT) devices, the demand for
addressing complex optimization challenges has intensified. The Lyapunov
Drift-Plus-Penalty algorithm is a widely adopted approach for ensuring queue
stability, and some research has preliminarily explored its integration with
reinforcement learning (RL). In this paper, we investigate the adaptation of
the Lyapunov Drift-Plus-Penalty algorithm for RL applications, deriving an
effective method for combining Lyapunov Drift-Plus-Penalty with RL under a set
of common and reasonable conditions through rigorous theoretical analysis.
Unlike existing approaches that directly merge the two frameworks, our proposed
algorithm, termed Lyapunov drift-plus-penalty method tailored for reinforcement
learning with queue stability (LDPTRLQ) algorithm, offers theoretical
superiority by effectively balancing the greedy optimization of Lyapunov
Drift-Plus-Penalty with the long-term perspective of RL. Simulation results for
multiple problems demonstrate that LDPTRLQ outperforms the baseline methods
using the Lyapunov drift-plus-penalty method and RL, corroborating the validity
of our theoretical derivations. The results also demonstrate that our proposed
algorithm outperforms other benchmarks in terms of compatibility and stability.

</details>


### [101] [AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents](https://arxiv.org/abs/2506.04293)
*Fengze Liu,Haoyu Wang,Joonhyuk Cho,Dan Roth,Andrew W. Lo*

Main category: cs.LG

TL;DR: AutoCT框架通过结合大型语言模型与经典机器学习，优化临床试验预测，在实验中表现优异，提供了可解释和经济高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 提高临床试验结果预测的准确性，以降低研发成本并加快药物发现进程。

Method: 提出了一个名为AutoCT的新框架，该框架结合了大型语言模型的推理能力和经典机器学习的可解释性，并使用蒙特卡罗树搜索迭代优化预测性能。

Result: 实验结果表明，AutoCT在临床试验预测任务中表现与现有最先进方法相当或更优，且仅需有限的自我改进迭代。

Conclusion: AutoCT为大规模、可解释且成本高效的临床试验预测建立了新的范式。

Abstract: Clinical trials are critical for advancing medical treatments but remain
prohibitively expensive and time-consuming. Accurate prediction of clinical
trial outcomes can significantly reduce research and development costs and
accelerate drug discovery. While recent deep learning models have shown promise
by leveraging unstructured data, their black-box nature, lack of
interpretability, and vulnerability to label leakage limit their practical use
in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel
framework that combines the reasoning capabilities of large language models
with the explainability of classical machine learning. AutoCT autonomously
generates, evaluates, and refines tabular features based on public information
without human input. Our method uses Monte Carlo Tree Search to iteratively
optimize predictive performance. Experimental results show that AutoCT performs
on par with or better than SOTA methods on clinical trial prediction tasks
within only a limited number of self-refinement iterations, establishing a new
paradigm for scalable, interpretable, and cost-efficient clinical trial
prediction.

</details>


### [102] [Short-Term Power Demand Forecasting for Diverse Consumer Types to Enhance Grid Planning and Synchronisation](https://arxiv.org/abs/2506.04294)
*Asier Diaz-Iglesias,Xabier Belaunzaran,Ane M. Florez-Tapia*

Main category: cs.LG

TL;DR: 为了转型至可再生能源，本研究通过客户群体化和AI及机器学习算法提出新方法，提高电力需求预测的准确性，从而支持电网稳定性。


<details>
  <summary>Details</summary>
Motivation: 可再生能源转型中确保电网稳定性需要准确的电力需求预测。

Method: 优化客户群体化，利用AI和机器学习算法进行短期负荷预测（STLF）和极短期负荷预测（VSTLF），并进行特征选择，包括时间、社会经济和由哥白尼地球观测计划获得的天气相关数据。

Result: 新提出的预测方法在STLF和VSTLF上都表现出比简单模型更优越的性能，提升了可靠的电力需求预测，支持电网稳定性。

Conclusion: 研究提出的新的预测方法，在不同用户群体上都有相对于简单模型的优势，强调了定制化预测策略的重要性，以及详细天气数据对预测精度的影响。

Abstract: Ensuring grid stability in the transition to renewable energy sources
requires accurate power demand forecasting. This study addresses the need for
precise forecasting by differentiating among industrial, commercial, and
residential consumers through customer clusterisation, tailoring the
forecasting models to capture the unique consumption patterns of each group. A
feature selection process is done for each consumer type including temporal,
socio-economic, and weather-related data obtained from the Copernicus Earth
Observation (EO) program. A variety of AI and machine learning algorithms for
Short-Term Load Forecasting (STLF) and Very Short-Term Load Forecasting (VSTLF)
are explored and compared, determining the most effective approaches. With all
that, the main contribution of this work are the new forecasting approaches
proposed, which have demonstrated superior performance compared to simpler
models, both for STLF and VSTLF, highlighting the importance of customized
forecasting strategies for different consumer groups and demonstrating the
impact of incorporating detailed weather data on forecasting accuracy. These
advancements contribute to more reliable power demand predictions, thereby
supporting grid stability.

</details>


### [103] [Deep learning for predicting hauling fleet production capacity under uncertainties in open pit mines using real and simulated data](https://arxiv.org/abs/2506.04296)
*N Guerin,M Nakhla,A Dehoux,J L Loyer*

Main category: cs.LG

TL;DR: 提出了一种结合真实操作数据与合成机械故障场景的深度学习框架，用于提高露天矿业中的车队容量预测准确性，并通过两种模型进行评估，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 短期预测开采车队容量对露天矿业至关重要，因为天气波动、机械故障以及人员可用性的变化会引入显著的运营不确定性。

Method: 本文提出了一种深度学习框架，结合了真实的操作记录（高分辨率降雨量测量、车队性能遥测）与合成产生的机械故障场景，以捕捉高影响故障事件的波动性。

Result: 通过对两种架构进行评估：XGBoost回归器实现了14.3%的中位绝对误差（MedAE），而长短期记忆网络（LSTM）则实现了15.1%的MedAE。SHAP值分析确定累计降雨量、历史载荷趋势和模拟故障频率为主要预测指标。

Conclusion: 混合建模方法提供了一个全面的决策支持工具，用于在动态不确定条件下进行主动的数据驱动车队管理。

Abstract: Accurate short-term forecasting of hauling-fleet capacity is crucial in
open-pit mining, where weather fluctuations, mechanical breakdowns, and
variable crew availability introduce significant operational uncertainties. We
propose a deep-learning framework that blends real-world operational records
(high-resolution rainfall measurements, fleet performance telemetry) with
synthetically generated mechanical-breakdown scenarios to enable the model to
capture fluctuating high-impact failure events. We evaluate two architectures:
an XGBoost regressor achieving a median absolute error (MedAE) of 14.3 per cent
and a Long Short-Term Memory network with a MedAE of 15.1 per cent. Shapley
Additive exPlanations (SHAP) value analyses identify cumulative rainfall,
historical payload trends, and simulated breakdown frequencies as dominant
predictors. Integration of simulated breakdown data and shift-planning features
notably reduces prediction volatility. Future work will further integrate
maintenance-scheduling indicators (Mean Time Between Failures, Mean Time to
Repair), detailed human resource data (operator absenteeism, crew efficiency
metrics), blast event scheduling, and other operational constraints to enhance
forecast robustness and adaptability. This hybrid modelling approach offers a
comprehensive decision-support tool for proactive, data-driven fleet management
under dynamically uncertain conditions.

</details>


### [104] [Softlog-Softmax Layers and Divergences Contribute to a Computationally Dependable Ensemble Learning](https://arxiv.org/abs/2506.04297)
*Abdourrahmane Mahamane Atto*

Main category: cs.LG

TL;DR: 该研究提出一个4步流程，通过使用软对数-软最大值级联提升集成学习系统的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 提升下一代集成学习系统的一致性和可靠性。

Method: 本文通过建立软对数-软最大值级联来实现步骤流程，包括构造卷积截断模型，导出软对数，使用信息度量进行决策关系分析，并生成性能张量。

Result: 本文的方法提高了复杂问题中的集成多样性和系统的一致性与可靠性。

Conclusion: 本文提出的4步过程能够显著提高下一代集成学习系统的一致性和可靠性。

Abstract: The paper proposes a 4-step process for highlighting that softlog-softmax
cascades can improve both consistency and dependability of the next generation
ensemble learning systems. The first process is anatomical in nature: the
target ensemble model under consideration is composed by canonical elements
relating to the definition of a convolutional frustum. No a priori is
considered in the choice of canonical forms. Diversity is the main criterion
for selecting these forms. It is shown that the more complex the problem, the
more useful this ensemble diversity is. The second process is physiological and
relates to neural engineering: a softlog is derived to both make weak
logarithmic operations consistent and lead, through multiple softlog-softmax
layers, to intermediate decisions in the sense of respecting the same class
logic as that faced by the output layer. The third process concerns neural
information theory: softlog-based entropy and divergence are proposed for the
sake of constructing information measures yielding consistent values on closed
intervals. These information measures are used to determine the relationships
between individual and sub-community decisions in frustum diversitybased
ensemble learning. The concluding process addresses the derivation of an
informative performance tensor for the purpose of a reliable ensemble
evaluation.

</details>


### [105] [The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective](https://arxiv.org/abs/2506.04301)
*Jiin Kim,Byeongjun Shin,Jinha Chung,Minsoo Rhu*

Main category: cs.LG

TL;DR: 该研究分析了AI代理的资源和能耗问题，发现其在提高准确性时存在高昂且不可持续的计算成本，建议优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在多步推理过程中展示的任务泛化和灵活性增加的同时，如何协调系统级的成本、效率和可持续性问题。

Method: 对不同代理设计和测试时间扩展策略的资源使用、延迟行为、能耗和数据中心功率消耗进行全面的系统级分析。

Result: AI代理在计算增加时提高了准确性，但出现了收益递减、延迟差异扩大和不可持续的计算需求，需在性能与实际部署约束之间找到平衡。

Conclusion: 研究揭示，AI代理在提高准确性时，计算的边际收益迅速减少，同时带来了巨大和不可持续的基础设施成本，呼吁在代理设计中实现计算效率的推理。

Abstract: Large-language-model (LLM)-based AI agents have recently showcased impressive
versatility by employing dynamic reasoning, an adaptive, multi-step process
that coordinates with external tools. This shift from static, single-turn
inference to agentic, multi-turn workflows broadens task generalization and
behavioral flexibility, but it also introduces serious concerns about
system-level cost, efficiency, and sustainability. This paper presents the
first comprehensive system-level analysis of AI agents, quantifying their
resource usage, latency behavior, energy consumption, and datacenter-wide power
consumption demands across diverse agent designs and test-time scaling
strategies. We further characterize how AI agent design choices, such as
few-shot prompting, reflection depth, and parallel reasoning, impact
accuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy
with increased compute, they suffer from rapidly diminishing returns, widening
latency variance, and unsustainable infrastructure costs. Through detailed
evaluation of representative agents, we highlight the profound computational
demands introduced by AI agent workflows, uncovering a looming sustainability
crisis. These results call for a paradigm shift in agent design toward
compute-efficient reasoning, balancing performance with deployability under
real-world constraints.

</details>


### [106] [RedRFT: A Light-Weight Benchmark for Reinforcement Fine-Tuning-Based Red Teaming](https://arxiv.org/abs/2506.04302)
*Xiang Zheng,Xingjun Ma,Wei-Bin Lee,Cong Wang*

Main category: cs.LG

TL;DR: RedRFT是一个简化和标准化RFT红队实施和评估的轻量级基准。


<details>
  <summary>Details</summary>
Motivation: 当前RFT红队缺乏统一的基准，影响结果的稳定性和可重复性。

Method: 介绍RedRFT基准，综合单文件CleanRL设计和模块化Tianshou，支持多种度量并进行消融实验。

Result: 通过RedRFT的引入，提高了RFT红队实施的稳定性和可重复性，促进快速原型设计。

Conclusion: RedRFT提供了一个统一的基准，简化和标准化了RFT红队方法的实施和评估。

Abstract: Red teaming has proven to be an effective method for identifying and
mitigating vulnerabilities in Large Language Models (LLMs). Reinforcement
Fine-Tuning (RFT) has emerged as a promising strategy among existing red
teaming techniques. However, a lack of a unified benchmark hinders current
RFT-based red teaming methods. Implementation details, especially in Proximal
Policy Optimization (PPO)-based RFT, significantly affect outcome stability and
reproducibility. To address this issue, we introduce RedRFT, a lightweight
benchmark designed to simplify and standardize the implementation and
evaluation of RFT-based red teaming. RedRFT combines the design strengths of
both single-file CleanRL and highly modularized Tianshou, offering high-quality
single-file red teaming implementations and modular PPO core components, such
as the General Advantage Estimator. It supports a variety of token and sentence
diversity metrics, featuring modularized intrinsic reward computation that
facilitates plug-and-play experimentation. To clarify their influence on RFT
performance, we conducted an extensive ablation study on key components,
including Low-Rank Adaptation (LoRA), Kullback-Leibler (KL) divergence, and
Lagrange Multiplier. We hope this work contributes to 1) gaining a
comprehensive understanding of the implementation nuances of RFT-based red
teaming algorithms, and 2) enabling rapid prototyping of innovative features
for RFT-based red teaming. Code for the benchmark can be accessed at
https://github.com/x-zheng16/RedRFT.git.

</details>


### [107] [You Only Train Once](https://arxiv.org/abs/2506.04349)
*Christos Sakaridis*

Main category: cs.LG

TL;DR: 提出了一种自动优化损失权重超参数的方法，只需一次训练即可优化模型损失，显著优于传统网格搜索方法。


<details>
  <summary>Details</summary>
Motivation: 虽然创建和优化学习模型的过程涉及多次训练运行，但我们希望通过提出的方法将损失选择和权重的训练限制为一次性训练。

Method: 自动优化损失权重超参数，通过标准的基于梯度的优化，将这些超参数视为网络的常规参数并进行学习。

Result: YOTO在一次性联合优化损失超参数和常规模型参数方面的有效性，通过与用于计算机视觉问题（如3D估计和语义分割）的最先进网络的暴力网格搜索比较，显示在未见测试数据上持续优于最佳网格搜索模型。

Conclusion: 我们的方法 YOTO 通过自动优化学习模型的损失权重超参数，成功地将损失选择和权重的训练限制为一次性训练。

Abstract: The title of this paper is perhaps an overclaim. Of course, the process of
creating and optimizing a learned model inevitably involves multiple training
runs which potentially feature different architectural designs, input and
output encodings, and losses. However, our method, You Only Train Once (YOTO),
indeed contributes to limiting training to one shot for the latter aspect of
losses selection and weighting. We achieve this by automatically optimizing
loss weight hyperparameters of learned models in one shot via standard
gradient-based optimization, treating these hyperparameters as regular
parameters of the networks and learning them. To this end, we leverage the
differentiability of the composite loss formulation which is widely used for
optimizing multiple empirical losses simultaneously and model it as a novel
layer which is parameterized with a softmax operation that satisfies the
inherent positivity constraints on loss hyperparameters while avoiding
degenerate empirical gradients. We complete our joint end-to-end optimization
scheme by defining a novel regularization loss on the learned hyperparameters,
which models a uniformity prior among the employed losses while ensuring
boundedness of the identified optima. We evidence the efficacy of YOTO in
jointly optimizing loss hyperparameters and regular model parameters in one
shot by comparing it to the commonly used brute-force grid search across
state-of-the-art networks solving two key problems in computer vision, i.e. 3D
estimation and semantic segmentation, and showing that it consistently
outperforms the best grid-search model on unseen test data. Code will be made
publicly available.

</details>


### [108] [Half-Layered Neural Networks](https://arxiv.org/abs/2506.04352)
*Ethem Alpaydin*

Main category: cs.LG

TL;DR: 提出一种“半”隐藏层，通过随机和训练权重结合，减少参数量且提升正则化效果，在MNIST数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度网络中参数过多的问题，提出能够在保持准确率的同时减少参数的半层构造。

Method: 提出一种“半”隐藏层，其部分权重随机设定，部分权重进行训练。每个半单元分为两个阶段：首先通过固定随机权重对输入进行加权求和；然后，通过两个可调权重进行乘法和平移，再经过非线性激活函数。

Result: 在MNIST和FashionMNIST数据集上的实验表明，半层结构能够在参数减少的同时保持合理的准确率。

Conclusion: 半层结构的隐藏单元在参数数量减少的情况下展现了良好的效果，表现出随机连接的正则化效应。

Abstract: We propose a ``half'' layer of hidden units that has some of its weights
randomly set and some of them trained. A half unit is composed of two stages:
First, it takes a weighted sum of its inputs with fixed random weights, and
second, the total activation is multiplied and then translated using two
modifiable weights, before the result is passed through a nonlinearity. The
number of modifiable weights of each hidden unit is thus two and does not
depend on the fan-in. We show how such half units can be used in the first or
any later layer in a deep network, possibly following convolutional layers. Our
experiments on MNIST and FashionMNIST data sets indicate the promise of half
layers, where we can achieve reasonable accuracy with a reduced number of
parameters due to the regularizing effect of the randomized connections.

</details>


### [109] [A Risk-Aware Reinforcement Learning Reward for Financial Trading](https://arxiv.org/abs/2506.04358)
*Uditansh Srivastava,Shivam Aryan,Shaurya Singh*

Main category: cs.LG

TL;DR: 提出一种模块化组合奖励函数用于强化学习的金融交易，通过调整不同指标权重来平衡风险和收益，并支持基于梯度的训练。


<details>
  <summary>Details</summary>
Motivation: 股票交易中，强化学习面临着平衡收益和风险的挑战，因此需要开发一种能够同时考虑多项风险与收益指标的奖励函数。

Method: 提出一种组合奖励函数，使用年化收益、下行风险、差异化收益和特雷诺比率作为可微分项，并通过网格搜索调整参数以实现特定的风险收益配置。同时推导每个条款的闭合形式梯度以支持基于梯度的训练。

Result: 这框架为在复杂交易环境中构建稳健的多目标奖励函数提供了通用蓝图，并可以通过加入其他风险度量和自适应权重进行扩展。

Conclusion: 这种组合奖励函数使强化学习在金融交易中能够更有效地平衡收益与风险，具备单一度量目标无法达到的灵活性和自适应性。

Abstract: We propose a novel composite reward function for reinforcement learning in
financial trading that balances return and risk using four differentiable
terms: annualized return downside risk differential return and the Treynor
ratio
  Unlike single metric objectives for example the Sharpe ratio our formulation
is modular and parameterized by weights w1 w2 w3 and w4 enabling practitioners
to encode diverse investor preferences
  We tune these weights via grid search to target specific risk return profiles
  We derive closed form gradients for each term to facilitate gradient based
training and analyze key theoretical properties including monotonicity
boundedness and modularity
  This framework offers a general blueprint for building robust multi objective
reward functions in complex trading environments and can be extended with
additional risk measures or adaptive weighting

</details>


### [110] [Even Faster Hyperbolic Random Forests: A Beltrami-Klein Wrapper Approach](https://arxiv.org/abs/2506.04360)
*Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: 论文展示了如何将超DT算法转化为Beltrami-Klein模型，以提高双曲空间中的决策树模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在双曲空间的洛伦兹模型中，用同质超平面替代轴平行超平面来划分输入空间，但需要寻找一种更优雅且能够保持阈值操作的方法。

Method: 本文通过将超DT算法重新表达为Beltrami-Klein模型，并使用简单的预处理和后处理步骤，将其包装在现有的欧几里得空间树模型周围。

Result: 通过包装方法，解锁了许多在欧几里得空间模型中已经可用的优化，提高了算法的灵活性、速度和准确性，并提供了一个更简单、易维护和可扩展的代码库。

Conclusion: 本文提出了一种将超DT算法重新表达为双曲空间Beltrami-Klein模型的方法，从而提高了灵活性、速度和准确性。

Abstract: Decision trees and models that use them as primitives are workhorses of
machine learning in Euclidean spaces. Recent work has further extended these
models to the Lorentz model of hyperbolic space by replacing axis-parallel
hyperplanes with homogeneous hyperplanes when partitioning the input space. In
this paper, we show how the hyperDT algorithm can be elegantly reexpressed in
the Beltrami-Klein model of hyperbolic spaces. This preserves the thresholding
operation used in Euclidean decision trees, enabling us to further rewrite
hyperDT as simple pre- and post-processing steps that form a wrapper around
existing tree-based models designed for Euclidean spaces. The wrapper approach
unlocks many optimizations already available in Euclidean space models,
improving flexibility, speed, and accuracy while offering a simpler, more
maintainable, and extensible codebase. Our implementation is available at
https://github.com/pchlenski/hyperdt.

</details>


### [111] [Replay Can Provably Increase Forgetting](https://arxiv.org/abs/2506.04377)
*Yasaman Mahdaviyeh,James Lucas,Mengye Ren,Andreas S. Tolias,Richard Zemel,Toniann Pitassi*

Main category: cs.LG

TL;DR: 对样本重放进行理论分析，发现其在某些情况下可能增加遗忘，结果依赖于样本选择和任务关系。


<details>
  <summary>Details</summary>
Motivation: 克服在逐步解决多个任务时遗忘的问题，尤其是在处理新任务时，旧任务的性能下降。

Method: 在过参数化的连续线性回归设置中进行理论分析，并进行了神经网络的实验以验证样本重放的效果。

Result: 发现遗忘可能随重放样本数量的增加而增加，重放甚至可能在某些任务中有害，特别是在分布设置中，随机选择样本的重放增加了期望遗忘。还证明了对于使用随机梯度下降的神经网络，类似的有害重放行为也存在。

Conclusion: 实验表明，即使在看似无害的场景中，样本重放的性能也严重依赖于重放样本的选择和任务之间的关系。

Abstract: Continual learning seeks to enable machine learning systems to solve an
increasing corpus of tasks sequentially. A critical challenge for continual
learning is forgetting, where the performance on previously learned tasks
decreases as new tasks are introduced. One of the commonly used techniques to
mitigate forgetting, sample replay, has been shown empirically to reduce
forgetting by retaining some examples from old tasks and including them in new
training episodes. In this work, we provide a theoretical analysis of sample
replay in an over-parameterized continual linear regression setting, where each
task is given by a linear subspace and with enough replay samples, one would be
able to eliminate forgetting. Our analysis focuses on sample replay and
highlights the role of the replayed samples and the relationship between task
subspaces. Surprisingly, we find that, even in a noiseless setting, forgetting
can be non-monotonic with respect to the number of replay samples. We present
tasks where replay can be harmful with respect to worst-case settings, and also
in distributional settings where replay of randomly selected samples increases
forgetting in expectation. We also give empirical evidence that harmful replay
is not limited to training with linear models by showing similar behavior for a
neural networks equipped with SGD. Through experiments on a commonly used
benchmark, we provide additional evidence that, even in seemingly benign
scenarios, performance of the replay heavily depends on the choice of replay
samples and the relationship between tasks.

</details>


### [112] [Bridging the Performance Gap Between Target-Free and Target-Based Reinforcement Learning With Iterated Q-Learning](https://arxiv.org/abs/2506.04398)
*Théo Vincent,Yogesh Tripathi,Tim Faust,Yaniv Oren,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出使用迭代共享Q学习(iS-QL)提高样本效率，减少目标网络带来的不稳定性，且内存需求更少，训练时间与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 在去掉目标网络以节省内存并提高在线网络容量的同时，消除目标网络会导致不稳定性和性能下降，因此需要一种能兼顾两者优点的方法。

Method: 提出了一种新的方法——迭代共享Q学习(iS-QL)，通过共享在线网络的线性层作为目标网络，其他参数仍与最新的在线网络共享。这允许并行学习连续的Bellman迭代。

Result: 证明iS-QL能在不同设置中提高无目标方法的样本效率，并在内存占用和训练时间上表现出色。

Conclusion: iS-QL提供了一种介于目标网络和无目标方法之间的优质替代，具有提高样本效率的优点，并且所需内存较少，训练时间可与传统方法相媲美，展现出在强化学习研究中扩展的潜力。

Abstract: In value-based reinforcement learning, removing the target network is
tempting as the boostrapped target would be built from up-to-date estimates,
and the spared memory occupied by the target network could be reallocated to
expand the capacity of the online network. However, eliminating the target
network introduces instability, leading to a decline in performance. Removing
the target network also means we cannot leverage the literature developed
around target networks. In this work, we propose to use a copy of the last
linear layer of the online network as a target network, while sharing the
remaining parameters with the up-to-date online network, hence stepping out of
the binary choice between target-based and target-free methods. It enables us
to leverage the concept of iterated Q-learning, which consists of learning
consecutive Bellman iterations in parallel, to reduce the performance gap
between target-free and target-based approaches. Our findings demonstrate that
this novel method, termed iterated Shared Q-Learning (iS-QL), improves the
sample efficiency of target-free approaches across various settings.
Importantly, iS-QL requires a smaller memory footprint and comparable training
time to classical target-based algorithms, highlighting its potential to scale
reinforcement learning research.

</details>


### [113] [Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning](https://arxiv.org/abs/2506.04399)
*Suzan Ece Ada,Emre Ugur*

Main category: cs.LG

TL;DR: UMCNP方法在无奖励信号的情况下提高元强化学习的样本效率，并通过学习到的动态模型自适应地应对未知任务。


<details>
  <summary>Details</summary>
Motivation: 在元测试阶段奖励信号缺失的情况下提高样本效率。

Method: 提出了一种名为UMCNP的新颖混合少样本元强化学习方法，该方法结合但区分了参数化策略梯度和任务推理的少样本元强化学习。

Result: 相比基准方法，在元测试阶段使用显著更少的样本完成对未见测试任务的适应。

Conclusion: UMCNP方法在无需在元训练中使用额外样本的情况下，可以通过自适应的方式，高效推理和适应未知测试任务模型。

Abstract: We introduce Unsupervised Meta-Testing with Conditional Neural Processes
(UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method
that uniquely combines, yet distinctly separates, parameterized policy
gradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for
settings where the reward signal is missing during meta-testing, our method
increases sample efficiency without requiring additional samples in
meta-training. UMCNP leverages the efficiency and scalability of Conditional
Neural Processes (CNPs) to reduce the number of online interactions required in
meta-testing. During meta-training, samples previously collected through PPG
meta-RL are efficiently reused for learning task inference in an offline
manner. UMCNP infers the latent representation of the transition dynamics model
from a single test task rollout with unknown parameters. This approach allows
us to generate rollouts for self-adaptation by interacting with the learned
dynamics model. We demonstrate our method can adapt to an unseen test task
using significantly fewer samples during meta-testing than the baselines in
2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with
unknown angle sensor bias, walker agent with randomized dynamics parameters.

</details>


### [114] [Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning](https://arxiv.org/abs/2506.04411)
*Achleshwar Luthra,Tianbao Yang,Tomer Galanti*

Main category: cs.LG

TL;DR: 本文建立了自监督对比学习（CL）的理论基础，提出其近似一个排除同类对比的监督损失（NSCL），并验证了该近似在类数增加时的有效性，同时引入了一个新的少样本误差界来支持线性探测。


<details>
  <summary>Details</summary>
Motivation: 为了解决自监督对比学习（CL）的理论基础尚未完全建立的问题，作者提出了将标准CL目标近似为一个排除同类对比的监督变体（NSCL）的思路。

Method: 作者引入了一个新的少样本线性探测误差界，并验证了该误差界对于探测性能的估计是严格的。

Result: 实验证实了CL和NSCL损失之间的差距随着语义类数量增加而以$\mathcal{O}(\frac{1}{\#\text{classes}})$的速率衰减，且它们的损失高度相关。CL的最小化能够将NSCL损失接近直接最小化所达到的值。

Conclusion: 本文通过理论和实验证明，对比学习（CL）和其监督变体（NSCL）的损失差距随着语义类数量增加而减小。CL训练的表示能够很好地支持简单线性探测的少样本标签恢复。

Abstract: Despite its empirical success, the theoretical foundations of self-supervised
contrastive learning (CL) are not yet fully established. In this work, we
address this gap by showing that standard CL objectives implicitly approximate
a supervised variant we call the negatives-only supervised contrastive loss
(NSCL), which excludes same-class contrasts. We prove that the gap between the
CL and NSCL losses vanishes as the number of semantic classes increases, under
a bound that is both label-agnostic and architecture-independent.
  We characterize the geometric structure of the global minimizers of the NSCL
loss: the learned representations exhibit augmentation collapse, within-class
collapse, and class centers that form a simplex equiangular tight frame. We
further introduce a new bound on the few-shot error of linear-probing. This
bound depends on two measures of feature variability--within-class dispersion
and variation along the line between class centers. We show that directional
variation dominates the bound and that the within-class dispersion's effect
diminishes as the number of labeled samples increases. These properties enable
CL and NSCL-trained representations to support accurate few-shot label recovery
using simple linear probes.
  Finally, we empirically validate our theoretical findings: the gap between CL
and NSCL losses decays at a rate of $\mathcal{O}(\frac{1}{\#\text{classes}})$;
the two losses are highly correlated; minimizing the CL loss implicitly brings
the NSCL loss close to the value achieved by direct minimization; and the
proposed few-shot error bound provides a tight estimate of probing performance
in practice.

</details>


### [115] [Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order](https://arxiv.org/abs/2506.04430)
*Egor Petrov,Grigoriy Evseev,Aleksey Antonov,Andrey Veprikov,Pavel Plyusnin,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 研究提出了JAGUAR SignSGD和JAGUAR Muon作为内存和计算高效的大型语言模型微调方法，实验验证其在资源有限条件下的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统一阶优化器内存和计算成本高，特别是在大规模模型的微调中，因此需要一种更高效的方案。

Method: 首次为随机ZO SignSGD提供严格的收敛性保障；提出JAGUAR SignSGD和JAGUAR Muon算法。

Result: 提出的算法在内存使用方面实现了显著降低，在收敛质量上达到或超过标准一阶方法。

Conclusion: 本研究提出的ZO优化方法在资源受限的LLM自适应上具有实际和理论基础。

Abstract: Fine-tuning Large Language Models (LLMs) is essential for adapting
pre-trained models to downstream tasks. Yet traditional first-order optimizers
such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and
computational costs that scale poorly with model size. In this paper, we
investigate zero-order (ZO) optimization methods as a memory- and
compute-efficient alternative, particularly in the context of
parameter-efficient fine-tuning techniques like LoRA. We propose
$\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO
SignSGD, requiring the same number of parameters as the standard ZO SGD and
only $\mathcal{O}(1)$ function evaluations per iteration. To the best of our
knowledge, this is the first study to establish rigorous convergence guarantees
for SignSGD in the stochastic ZO case. We further propose $\texttt{JAGUAR
Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix
structure of model parameters, and we provide its convergence rate under
arbitrary stochastic noise. Through extensive experiments on challenging LLM
fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or
exceed the convergence quality of standard first-order methods, achieving
significant memory reduction. Our theoretical and empirical results establish
new ZO optimization methods as a practical and theoretically grounded approach
for resource-constrained LLM adaptation. Our code is available at
https://github.com/brain-mmo-lab/ZO_LLM

</details>


### [116] [KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products](https://arxiv.org/abs/2506.04432)
*Zixuan Xia,Aram Davtyan,Paolo Favaro*

Main category: cs.LG

TL;DR: KOALA++是一种改进的Kalman优化算法，能够在不存储完整协方差矩阵的情况下，显式模拟梯度不确定性，并在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决二阶方法依赖昂贵的二阶梯度计算的问题，提出一种能够显式模拟神经网络训练中结构化梯度不确定性的可扩展Kalman优化算法。

Method: 直接估计参数协方差矩阵，并通过递归更新紧凑的梯度协方差积来实现。

Result: KOALA++在图像分类和语言建模等多样化任务中，表现出与现有的先进优化器相当或更好的精确度，并保持了一阶方法的高效性。

Conclusion: KOALA++在多种任务中表现出与先进的一阶和二阶优化器相当或更好的精度，同时保持了一阶方法的效率。

Abstract: We propose KOALA++, a scalable Kalman-based optimization algorithm that
explicitly models structured gradient uncertainty in neural network training.
Unlike second-order methods, which rely on expensive second order gradient
calculation, our method directly estimates the parameter covariance matrix by
recursively updating compact gradient covariance products. This design improves
upon the original KOALA framework that assumed diagonal covariance by
implicitly capturing richer uncertainty structure without storing the full
covariance matrix and avoiding large matrix inversions. Across diverse tasks,
including image classification and language modeling, KOALA++ achieves accuracy
on par or better than state-of-the-art first- and second-order optimizers while
maintaining the efficiency of first-order methods.

</details>


### [117] [Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory](https://arxiv.org/abs/2506.04434)
*Hari K. Prakash,Charles H. Martin*

Main category: cs.LG

TL;DR: 发现神经网络中的“反悟”阶段，提出用HTSR α作为泛化测量的有效指标，而不是单凭测试数据。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中的反悟（anti-grokking）阶段，从而揭示一种新的过拟合与泛化崩溃测量手段，避免直接依赖测试数据。

Method: 使用3层MLP，结合WeightWatcher工具，通过HTSR方法来分析MNIST子集在有无权重衰减条件下的训练过程。

Result: 证明了HTSR α能够准确识别所有阶段，并揭示重训练与α < 2所关联的相关陷阱信号。传统指标只能检测前两个阶段，而不能有效地区分悟（grokking）与反悟阶段。

Conclusion: 反悟现象（anti-grokking）在神经网络的训练中是一个新的阶段，并且在训练的后期可能导致测试准确率崩溃，而训练准确率维持完美。通过使用重尾自正则化（HTSR），发现HTSR的层质量指标α能够辨别所有三个阶段，提供了一种衡量过拟合和泛化崩溃的方法。

Abstract: We study the well-known grokking phenomena in neural networks (NNs) using a
3-layer MLP trained on 1 k-sample subset of MNIST, with and without weight
decay, and discover a novel third phase -- \emph{anti-grokking} -- that occurs
very late in training and resembles but is distinct from the familiar
\emph{pre-grokking} phases: test accuracy collapses while training accuracy
stays perfect. This late-stage collapse is distinct, from the known
pre-grokking and grokking phases, and is not detected by other proposed
grokking progress measures. Leveraging Heavy-Tailed Self-Regularization HTSR
through the open-source WeightWatcher tool, we show that the HTSR layer quality
metric $\alpha$ alone delineates all three phases, whereas the best competing
metrics detect only the first two. The \emph{anti-grokking} is revealed by
training for $10^7$ and is invariably heralded by $\alpha < 2$ and the
appearance of \emph{Correlation Traps} -- outlier singular values in the
randomized layer weight matrices that make the layer weight matrix atypical and
signal overfitting of the training set. Such traps are verified by visual
inspection of the layer-wise empirical spectral densities, and by using
Kolmogorov--Smirnov tests on randomized spectra. Comparative metrics, including
activation sparsity, absolute weight entropy, circuit complexity, and $l^2$
weight norms track pre-grokking and grokking but fail to distinguish grokking
from anti-grokking. This discovery provides a way to measure overfitting and
generalization collapse without direct access to the test data. These results
strengthen the claim that the \emph{HTSR} $\alpha$ provides universal
layer-convergence target at $\alpha \approx 2$ and underscore the value of
using the HTSR alpha $(\alpha)$ metric as a measure of generalization.

</details>


### [118] [RETRO SYNFLOW: Discrete Flow Matching for Accurate and Diverse Single-Step Retrosynthesis](https://arxiv.org/abs/2506.04439)
*Robin Yadav,Qi Yan,Guy Wolf,Avishek Joey Bose,Renjie Liao*

Main category: cs.LG

TL;DR: 论文通过引入RETRO SYNFLOW框架提高了逆合成预测的准确性和多样性，实现了60%的top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 在有机化学中，识别和预测合成目标产物分子的一系列反应是一个基本问题。由于化学搜索空间的组合性质，单步反应物预测，即单步逆合成，对于现有的无模板生成方法来说仍然具有挑战性。

Method: 引入RETRO SYNFLOW（RSF）离散流匹配框架，通过建立一个马尔科夫桥梁来连接目标产物分子和反应物分子。RSF包括反应中心识别步骤，以生成中间结构（合成单元）作为离散流的更具信息的源分布，并结合费曼-卡克引导与基于序列蒙特卡洛的再采样方法。

Result: 实证表明，该方法实现了60.0%的top-1准确率，比以前的方法提高了20%。同时，通过在推理时进行FK引导，提高了top-5往返准确率19%。

Conclusion: RSF在保持竞争力的准确率结果的同时，显著增强了生成反应的多样性和可行性。

Abstract: A fundamental problem in organic chemistry is identifying and predicting the
series of reactions that synthesize a desired target product molecule. Due to
the combinatorial nature of the chemical search space, single-step reactant
prediction -- i.e. single-step retrosynthesis -- remains challenging even for
existing state-of-the-art template-free generative approaches to produce an
accurate yet diverse set of feasible reactions. In this paper, we model
single-step retrosynthesis planning and introduce RETRO SYNFLOW (RSF) a
discrete flow-matching framework that builds a Markov bridge between the
prescribed target product molecule and the reactant molecule. In contrast to
past approaches, RSF employs a reaction center identification step to produce
intermediate structures known as synthons as a more informative source
distribution for the discrete flow. To further enhance diversity and
feasibility of generated samples, we employ Feynman-Kac steering with
Sequential Monte Carlo based resampling to steer promising generations at
inference using a new reward oracle that relies on a forward-synthesis model.
Empirically, we demonstrate \nameshort achieves $60.0 \%$ top-1 accuracy, which
outperforms the previous SOTA by $20 \%$. We also substantiate the benefits of
steering at inference and demonstrate that FK-steering improves top-$5$
round-trip accuracy by $19 \%$ over prior template-free SOTA methods, all while
preserving competitive top-$k$ accuracy results.

</details>


### [119] [Selective Matching Losses -- Not All Scores Are Created Equal](https://arxiv.org/abs/2506.04446)
*Gil I. Shamir,Manfred K. Warmuth*

Main category: cs.LG

TL;DR: 论文提出选择性损失，通过调整得分区域的敏感性，提高在某些重要区域的预测准确性，尤其在多类别问题中，采用复合Softmax函数改进了多维选择性计算。


<details>
  <summary>Details</summary>
Motivation: 在某些领域的子集中产生准确预测非常重要，而在其他区域则不太重要。

Method: 通过构造选择性匹配损失函数，并采用组合Softmax函数的框架进行多维选择性损失。

Result: 提出了一种方法，利用选择性损失在高重要性区域获得更好的预测，同时在多维度下克服了标准Softmax函数的局限性。

Conclusion: 选择性损失在存在重要评分区域的应用中比传统损失具有显著优势。

Abstract: Learning systems match predicted scores to observations over some domain.
Often, it is critical to produce accurate predictions in some subset (or
region) of the domain, yet less important to accurately predict in other
regions. We construct selective matching loss functions by design of increasing
link functions over score domains. A matching loss is an integral over the
link. A link defines loss sensitivity as function of the score, emphasizing
high slope high sensitivity regions over flat ones. Loss asymmetry drives a
model and resolves its underspecification to predict better in high sensitivity
regions where it is more important, and to distinguish between high and low
importance regions. A large variety of selective scalar losses can be designed
with scaled and shifted Sigmoid and hyperbolic sine links. Their properties,
however, do not extend to multi-class. Applying them per dimension lacks
ranking sensitivity that assigns importance according to class score ranking.
Utilizing composite Softmax functions, we develop a framework for
multidimensional selective losses. We overcome limitations of the standard
Softmax function, that is good for classification, but not for distinction
between adjacent scores. Selective losses have substantial advantage over
traditional losses in applications with more important score regions, including
dwell-time prediction, retrieval, ranking with either pointwise, contrastive
pairwise, or listwise losses, distillation problems, and fine-tuning alignment
of Large Language Models (LLMs).

</details>


### [120] [Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning](https://arxiv.org/abs/2506.04454)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: 该论文扩展了一种神经符号AI框架，通过传输学习在不同数据集上展示了其在网络入侵检测中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高网络入侵检测系统的鲁棒性、可解释性和泛化能力。

Method: 用于特征提取的深度嵌入式聚类，与XGBoost结合进行符号推理，以及全面的不确定性量化。

Result: ODXU在CIC-IDS-2017数据集上的表现优于传统神经模型，并通过重新利用预训练的模型在不同数据集上的传输学习策略成功地提高了性能。

Conclusion: 该研究通过开发ODXU框架来提升网络入侵检测系统的性能，特别是在准确性和错误遗漏率方面表现优异，并通过传输学习策略扩展其应用范围。

Abstract: Network Intrusion Detection Systems (NIDS) play a vital role in protecting
digital infrastructures against increasingly sophisticated cyber threats. In
this paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates
deep embedded clustering for feature extraction, symbolic reasoning using
XGBoost, and comprehensive uncertainty quantification (UQ) to enhance
robustness, interpretability, and generalization in NIDS. The extended ODXU
incorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)
and metamodel-based techniques, including SHAP values and Information Gain, to
assess the reliability of predictions. Experimental results on the CIC-IDS-2017
dataset show that ODXU outperforms traditional neural models across six
evaluation metrics, including classification accuracy and false omission rate.
While transfer learning has seen widespread adoption in fields such as computer
vision and natural language processing, its potential in cybersecurity has not
been thoroughly explored. To bridge this gap, we develop a transfer learning
strategy that enables the reuse of a pre-trained ODXU model on a different
dataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal
transfer configuration involves reusing the pre-trained autoencoder, retraining
the clustering module, and fine-tuning the XGBoost classifier, and outperforms
traditional neural models when trained with as few as 16,000 samples
(approximately 50% of the training data). Additionally, results show that
metamodel-based UQ methods consistently outperform score-based approaches on
both datasets.

</details>


### [121] [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
*Ivan Vegner,Sydelle de Souza,Valentin Forch,Martha Lewis,Leonidas A. A. Doumas*

Main category: cs.LG

TL;DR: 论文探讨了机器学习中表征系统性与行为系统性的区别，并建议通过机制可解释性来评估表征系统性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习模型中系统性是各方追求的性质，因为它支持在新环境中的强泛化能力，因此该研究旨在澄清系统性中的不同维度并提供全新的评估方式。

Method: 通过分析现有语言和视觉领域的关键基准来评估行为系统性。

Result: 研究揭示了许多现有的基准并没有测量表征的系统性，并提出了在机制可解释性领域中评估模型表征系统性的方法。

Conclusion: 该论文研究了表征系统性和行为系统性的区别，强调了现有模型和基准主要关注于行为的系统性，而忽略了表征的系统性。

Abstract: A core aspect of compositionality, systematicity is a desirable property in
ML models as it enables strong generalization to novel contexts. This has led
to numerous studies proposing benchmarks to assess systematic generalization,
as well as models and training regimes designed to enhance it. Many of these
efforts are framed as addressing the challenge posed by Fodor and Pylyshyn.
However, while they argue for systematicity of representations, existing
benchmarks and models primarily focus on the systematicity of behaviour. We
emphasize the crucial nature of this distinction. Furthermore, building on
Hadley's (1994) taxonomy of systematic generalization, we analyze the extent to
which behavioural systematicity is tested by key benchmarks in the literature
across language and vision. Finally, we highlight ways of assessing
systematicity of representations in ML models as practiced in the field of
mechanistic interpretability.

</details>


### [122] [Classifying Dental Care Providers Through Machine Learning with Features Ranking](https://arxiv.org/abs/2506.04474)
*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh,Mohammed Hasan Abu-Arqoub,Rashiq Rafiq Marie*

Main category: cs.LG

TL;DR: 该研究利用2018年数据集，应用机器学习模型对牙科提供者进行分类，发现特征选择对提高模型效率和准确性非常重要。神经网络模型获得了最高的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于利用机器学习模型对牙科提供者进行分类，识别出标准提供者和安全网诊所提供者，以优化资源分配。

Method: 该研究采用了12种机器学习模型进行分类，包括k近邻（kNN）、决策树、支持向量机（SVM）、随机梯度下降（SGD）、随机森林、神经网络和梯度提升，并使用10倍交叉验证进行评估。

Result: 神经网络算法使用全部20个特征达到了最高的分类准确率（94.1%），其次是梯度提升（93.2%）和随机森林（93.0%）。

Conclusion: 该研究证明了通过集成特征排序技术和高级机器学习算法可以优化牙科提供者分类，从而实现对资源匮乏群体的有针对性的资源分配。

Abstract: This study investigates the application of machine learning (ML) models for
classifying dental providers into two categories - standard rendering providers
and safety net clinic (SNC) providers - using a 2018 dataset of 24,300
instances with 20 features. The dataset, characterized by high missing values
(38.1%), includes service counts (preventive, treatment, exams), delivery
systems (FFS, managed care), and beneficiary demographics. Feature ranking
methods such as information gain, Gini index, and ANOVA were employed to
identify critical predictors, revealing treatment-related metrics
(TXMT_USER_CNT, TXMT_SVC_CNT) as top-ranked features. Twelve ML models,
including k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines
(SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and
Gradient Boosting, were evaluated using 10-fold cross-validation.
Classification accuracy was tested across incremental feature subsets derived
from rankings. The Neural Network achieved the highest accuracy (94.1%) using
all 20 features, followed by Gradient Boosting (93.2%) and Random Forest
(93.0%). Models showed improved performance as more features were incorporated,
with SGD and ensemble methods demonstrating robustness to missing data. Feature
ranking highlighted the dominance of treatment service counts and annotation
codes in distinguishing provider types, while demographic variables (AGE_GROUP,
CALENDAR_YEAR) had minimal impact. The study underscores the importance of
feature selection in enhancing model efficiency and accuracy, particularly in
imbalanced healthcare datasets. These findings advocate for integrating
feature-ranking techniques with advanced ML algorithms to optimize dental
provider classification, enabling targeted resource allocation for underserved
populations.

</details>


### [123] [Comparative performance of ensemble models in predicting dental provider types: insights from fee-for-service data](https://arxiv.org/abs/2506.04479)
*Mohammad Subhi Al-Batah,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Abdullah Alourani*

Main category: cs.LG

TL;DR: 研究评估了使用机器学习模型对牙科服务提供者进行分类的效果，发现神经网络和随机森林模型在处理不平衡数据方面表现突出，能改善资源分配。


<details>
  <summary>Details</summary>
Motivation: 对牙科服务提供者进行分类对优化医疗资源分配和政策规划至关重要，尤其是提升对服务不足人群的服务。

Method: 研究使用了多种机器学习算法进行分类，包括k-最近邻算法（kNN）、决策树、支持向量机（SVM）、随机梯度下降（SGD）、随机森林、神经网络和梯度增强。采用10折交叉验证，并使用AUC、分类准确性（CA）、F1分数、精准度和召回率进行模型评估。

Result: 神经网络模型取得了最高的AUC（0.975）和分类准确性（94.1%），其次是随机森林（AUC: 0.948，CA: 93.0%），在处理不平衡数据和复杂特征交互方面优于传统分类器（如逻辑回归和SVM）。

Conclusion: 先进的机器学习技术，如集成和深度学习模型，可以显著提高牙科服务分类的准确性，有助于优化资源分配，特别是对服务不足的群体。

Abstract: Dental provider classification plays a crucial role in optimizing healthcare
resource allocation and policy planning. Effective categorization of providers,
such as standard rendering providers and safety net clinic (SNC) providers,
enhances service delivery to underserved populations. This study aimed to
evaluate the performance of machine learning models in classifying dental
providers using a 2018 dataset. A dataset of 24,300 instances with 20 features
was analyzed, including beneficiary and service counts across fee-for-service
(FFS), Geographic Managed Care, and Pre-Paid Health Plans. Providers were
categorized by delivery system and patient age groups (0-20 and 21+). Despite
38.1% missing data, multiple machine learning algorithms were tested, including
k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM),
Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient
Boosting. A 10-fold cross-validation approach was applied, and models were
evaluated using AUC, classification accuracy (CA), F1-score, precision, and
recall. Neural Networks achieved the highest AUC (0.975) and CA (94.1%),
followed by Random Forest (AUC: 0.948, CA: 93.0%). These models effectively
handled imbalanced data and complex feature interactions, outperforming
traditional classifiers like Logistic Regression and SVM. Advanced machine
learning techniques, particularly ensemble and deep learning models,
significantly enhance dental workforce classification. Their integration into
healthcare analytics can improve provider identification and resource
distribution, benefiting underserved populations.

</details>


### [124] [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)
*Jiayu Wang,Aws Albarghouthi,Frederic Sala*

Main category: cs.LG

TL;DR: COSMOS框架能够高效预测大语言模型的适应结果，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的情况下，选择最佳的语言模型和适应策略是具有挑战性的，通常需要大量的实验。

Method: 引入COSMOS框架，通过两个强大的预测器实现：嵌入增强的轻量级代理模型预测微调性能，以及低样本规模法则预测检索增强的上下文学习。

Result: COSMOS在八个代表性基准中实现了高预测准确性，同时平均减少计算成本92.72%，在资源密集的场景中减少高达98.71%的计算成本。

Conclusion: 通过有效预测适应结果，可以显著减少语言模型部署的计算开销，同时保持性能标准。

Abstract: Large language models (LLMs) achieve remarkable performance across numerous
tasks by using a diverse array of adaptation strategies. However, optimally
selecting a model and adaptation strategy under resource constraints is
challenging and often requires extensive experimentation. We investigate
whether it is possible to accurately predict both performance and cost without
expensive trials. We formalize the strategy selection problem for LLMs and
introduce COSMOS, a unified prediction framework that efficiently estimates
adaptation outcomes at minimal cost. We instantiate and study the capability of
our framework via a pair of powerful predictors: embedding-augmented
lightweight proxy models to predict fine-tuning performance, and low-sample
scaling laws to forecast retrieval-augmented in-context learning. Extensive
evaluation across eight representative benchmarks demonstrates that COSMOS
achieves high prediction accuracy while reducing computational costs by 92.72%
on average, and up to 98.71% in resource-intensive scenarios. Our results show
that efficient prediction of adaptation outcomes is not only feasible but can
substantially reduce the computational overhead of LLM deployment while
maintaining performance standards.

</details>


### [125] [Orthogonal Gradient Descent Improves Neural Calibration](https://arxiv.org/abs/2506.04487)
*C. Evans Hedges*

Main category: cs.LG

TL;DR: 正交化梯度的$\perp$Grad方法提高模型校准，保持准确性并减少过度自信问题，在多种条件下表现优于SGD。


<details>
  <summary>Details</summary>
Motivation: 研究者希望通过改善模型校准的方式提高模型的性能，同时不降低准确性。

Method: 提出了一种称为$\perp$Grad的方法，通过在训练期间使梯度正交化来改善模型校准。

Result: 在使用CIFAR-10数据集（含10%标记数据）的实验中，$\perp$Grad与SGD在准确性方面持平，但显著改善了校准指标，例如较低的测试损失、减少的softmax过度自信和更高的预测熵。这些优点在输入扰动（如CIFAR-10C）和延长训练时间的情况下依然存在。

Conclusion: $\perp$Grad能够改善模型的校准，且与后处理校准技术结合良好，理论证明简化版本在轻度假设下可以收敛。

Abstract: We provide evidence that orthogonalizing gradients during training improves
model calibration without sacrificing accuracy. On CIFAR-10 with 10% labeled
data, $\perp$Grad matches SGD in accuracy but yields consistently improved
calibration metrics such as lower test loss, reduced softmax overconfidence,
and higher predictive entropy. These benefits persist under input corruption
(CIFAR-10C) and extended training, where $\perp$Grad models degrade more
gracefully than SGD-trained counterparts. $\perp$Grad is optimizer-agnostic,
incurs minimal overhead, and works well with post-hoc calibration techniques
like temperature scaling.
  Theoretically, we prove convergence of a simplified version of $\perp$Grad
under mild assumptions and characterize its stationary points in positive
homogeneous networks: $\perp$Grad converges to solutions where further loss
reduction requires confidence scaling rather than decision boundary
improvement.

</details>


### [126] [Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data](https://arxiv.org/abs/2506.04490)
*Rishwanth Raghu,Axel Levy,Gordon Wetzstein,Ellen D. Zhong*

Main category: cs.LG

TL;DR: 将冷冻电镜技术与蛋白质结构预测模型结合，提出CryoBoltz方法，有效构建动态生物分子系统的原子模型。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质结构预测模型无法捕捉动态生物分子复合物的构象多样性问题，同时简化从冷冻电镜实验数据到原子模型的困难过程。

Method: 提出了一种称为CryoBoltz的方法，结合了冷冻电镜密度图和蛋白质结构预测模型，通过采用全局和局部结构约束，指导预训练模型的采样轨迹。

Result: CryoBoltz方法在处理动态生物分子系统（如转运蛋白和抗体）的异质性冷冻电镜图中，成功构建了原子模型。

Conclusion: 本研究提出了一种结合蛋白质结构预测模型和冷冻电镜密度图的方法，用于预测符合实验数据的构象状态。

Abstract: Protein structure prediction models are now capable of generating accurate 3D
structural hypotheses from sequence alone. However, they routinely fail to
capture the conformational diversity of dynamic biomolecular complexes, often
requiring heuristic MSA subsampling approaches for generating alternative
states. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a
powerful tool for imaging near-native structural heterogeneity, but is
challenged by arduous pipelines to go from raw experimental data to atomic
models. Here, we bridge the gap between these modalities, combining cryo-EM
density maps with the rich sequence and biophysical priors learned by protein
structure prediction models. Our method, CryoBoltz, guides the sampling
trajectory of a pretrained protein structure prediction model using both global
and local structural constraints derived from density maps, driving predictions
towards conformational states consistent with the experimental data. We
demonstrate that this flexible yet powerful inference-time approach allows us
to build atomic models into heterogeneous cryo-EM maps across a variety of
dynamic biomolecular systems including transporters and antibodies.

</details>


### [127] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的训练范式—扰动梯度训练（PGT），克服了物理库计算无法进行反向传播的局限。实验表明，在模拟网络架构及物理实验中，PGT可达到与标准方法相当的性能，具有显著的能效提升。


<details>
  <summary>Details</summary>
Motivation: 物理库计算存在无法进行反向传播的局限性，通过引入PGT可以克服这一问题。

Method: PGT采用随机扰动网络参数空间的方法，通过仅前向过程来近似梯度更新。

Result: PGT在模拟神经网络架构和实验硬件上展示了性能可与标准反向传播方法相媲美。

Conclusion: PGT能够在无法进行反向传播的情况下实现与标准反向传播方法相当的性能。

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [128] [Self-organization to multicriticality](https://arxiv.org/abs/2506.04275)
*Silja Sormunen,Thilo Gross,Jari Saramäki*

Main category: nlin.AO

TL;DR: 研究表明，自组织临界状态可达到多重临界性，且这种现象在自组织系统中可能广泛存在。


<details>
  <summary>Details</summary>
Motivation: 现有的自组织临界状态的动态性可能比预想中更为多样化，并探讨系统如何在保持接近某一相变的同时自组织到另一种相变。

Method: 研究了一个随机网络上耦合振荡器的模型，其中网络拓扑根据振荡器的动态性进行演化。自组织以多重临界性现象的首次展示，通过系统自组织到振荡开始状态，随后漂移到模式形成的开始状态，保持在振荡开始状态的同时体现多重临界性。

Result: 系统首先自组织到振荡开始状态，随后在保持振荡开始状态的同时漂移到模式形成开始状态，从而在两个不同方面达到临界状态。

Conclusion: 结果揭示了多重临界性是自组织系统中的一种普遍行为，并建议这种现象可能在多个领域中具备广泛意义，特别是对需要多种相变的系统如大脑的研究。

Abstract: Self-organized criticality is a well-established phenomenon, where a system
dynamically tunes its structure to operate on the verge of a phase transition.
Here, we show that the dynamics inside the self-organized critical state are
fundamentally far more versatile than previously recognized, to the extent that
a system can self-organize to a new type of phase transition while staying on
the verge of another. In this first demonstration of self-organization to
multicriticality, we investigate a model of coupled oscillators on a random
network, where the network topology evolves in response to the oscillator
dynamics. We show that the system first self-organizes to the onset of
oscillations, after which it drifts to the onset of pattern formation while
still remaining at the onset of oscillations, thus becoming critical in two
different ways at once. The observed evolution to multicriticality is robust
generic behavior that we expect to be widespread in self-organizing systems.
Overall, these results offer a unifying framework for studying systems, such as
the brain, where multiple phase transitions may be relevant for proper
functioning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [129] [Gen-n-Val: Agentic Image Data Generation and Validation](https://arxiv.org/abs/2506.04676)
*Jing-En Huang,I-Sheng Fang,Tzuhsuan Huang,Chih-Yu Wang,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: Gen-n-Val is a framework improving synthetic data quality, reducing label noise, and enhancing performance in object detection and segmentation, outperforming existing methods like MosaicFusion.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of data scarcity and label noise in computer vision tasks, especially in object detection and instance segmentation, by improving synthetic data quality.

Method: The paper introduces Gen-n-Val, an agentic data generation framework comprising two agents: a Layer Diffusion (LD) prompt agent and a data validation agent using language models (LLMs and VLLMs). The use of TextGrad refines system prompts, and image harmonization combines multiple instances within scenes.

Result: Compared to MosaicFusion, Gen-n-Val reduces invalid synthetic data from 50% to 7% and achieves a 1% mAP improvement on rare classes in COCO instance segmentation. It also shows a significant 7.1% mAP improvement on open-vocabulary object detection benchmarks, improving the performance of the YOLOv9 and YOLO11 families.

Conclusion: Gen-n-Val significantly enhances synthetic data quality and improves performance in object detection and instance segmentation tasks compared to existing methods.

Abstract: Recently, Large Language Models (LLMs) and Vision Large Language Models
(VLLMs) have demonstrated impressive performance as agents across various tasks
while data scarcity and label noise remain significant challenges in computer
vision tasks, such as object detection and instance segmentation. A common
solution for resolving these issues is to generate synthetic data. However,
current synthetic data generation methods struggle with issues, such as
multiple objects per mask, inaccurate segmentation, and incorrect category
labels, limiting their effectiveness. To address these issues, we introduce
Gen-n-Val, a novel agentic data generation framework that leverages Layer
Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks
and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt
agent, an LLM, optimizes prompts for LD to generate high-quality foreground
instance images and segmentation masks. These optimized prompts ensure the
generation of single-object synthetic data with precise instance masks and
clean backgrounds. (2) The data validation agent, a VLLM, which filters out
low-quality synthetic instance images. The system prompts for both agents are
refined through TextGrad. Additionally, we use image harmonization to combine
multiple instances within scenes. Compared to state-of-the-art synthetic data
approaches like MosaicFusion, our approach reduces invalid synthetic data from
50% to 7% and improves performance by 1% mAP on rare classes in COCO instance
segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant
improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object
detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance
of YOLOv9 and YOLO11 families in instance segmentation and object detection.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [130] [Memory-Driven Bounded Confidence Opinion Dynamics: A Hegselmann-Krause Model Based on Fractional-Order Methods](https://arxiv.org/abs/2506.04701)
*Meiru Jiang,Wei Su,Guojian Ren,Yongguang Yu*

Main category: physics.soc-ph

TL;DR: 提出了一种新的分数阶模型，改进了意见动态的收敛性和一致性，更贴近真实世界的意见演变。


<details>
  <summary>Details</summary>
Motivation: 研究记忆效应在社会互动和决策过程中的重要性，并提出更真实的意见动态模型。

Method: 基于Hegselmann-Krause框架和分数阶差分，建立了一个全面的模型来描述系统状态中的记忆效应。

Result: 模型显示出有利的收敛性和一致性特性，解决了经典意见动态中的一些局限性，如有界意见的单调性。

Conclusion: 提出了一种新的分数阶有界信任度意见动态模型，可以更真实地表示现实世界中意见的演变。

Abstract: Memory effects play a crucial role in social interactions and decision-making
processes. This paper proposes a novel fractional-order bounded confidence
opinion dynamics model to characterize the memory effects in system states.
Building upon the Hegselmann-Krause framework and fractional-order difference,
a comprehensive model is established that captures the persistent influence of
historical information. Through rigorous theoretical analysis, the fundamental
properties including convergence and consensus is investigated. The results
demonstrate that the proposed model not only maintains favorable convergence
and consensus characteristics compared to classical opinion dynamics, but also
addresses limitations such as the monotonicity of bounded opinions. This
enables a more realistic representation of opinion evolution in real-world
scenarios. The findings of this study provide new insights and methodological
approaches for understanding opinion formation and evolution, offering both
theoretical significance and practical applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [131] [Computationally Intensive Research: Advancing a Role for Secondary Analysis of Qualitative Data](https://arxiv.org/abs/2506.04230)
*Kaveh Mohajeri,Amir Karami*

Main category: cs.DB

TL;DR: 本文提出了计算密集型二次分析的方案，旨在重构已生成的定性研究数据，以创新研究设计，解决跨情境和纵向研究问题，以及探讨数据共享和重用的挑战。


<details>
  <summary>Details</summary>
Motivation: 定性研究通常产生丰富的数据，但许多数据未被使用。本文旨在探讨如何二次分析这些数据，从而促进研究设计的创新。

Method: 提出了一种计算密集型二次分析方案，并探讨通过数据汇集跨多个情境和时间框架，解决跨情境和纵向研究现象和问题的方法。

Result: 提出了定性数据共享和重用所面临的关键挑战和持续关注的方面。

Conclusion: 本文强调计算方法在重构过去定性研究生成的数据中的潜力，提出了一种进行计算密集型二次分析的方案，并探讨如何通过这种方法促进创新研究设计的发展。

Abstract: This paper draws attention to the potential of computational methods in
reworking data generated in past qualitative studies. While qualitative
inquiries often produce rich data through rigorous and resource-intensive
processes, much of this data usually remains unused. In this paper, we first
make a general case for secondary analysis of qualitative data by discussing
its benefits, distinctions, and epistemological aspects. We then argue for
opportunities with computationally intensive secondary analysis, highlighting
the possibility of drawing on data assemblages spanning multiple contexts and
timeframes to address cross-contextual and longitudinal research phenomena and
questions. We propose a scheme to perform computationally intensive secondary
analysis and advance ideas on how this approach can help facilitate the
development of innovative research designs. Finally, we enumerate some key
challenges and ongoing concerns associated with qualitative data sharing and
reuse.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [132] [Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams](https://arxiv.org/abs/2506.05265)
*Mohammed Almutairi*

Main category: cs.HC

TL;DR: 研究开发AI增强的团队优化系统，包括团队组建框架和即时反馈系统，提升团队满意度和表现。


<details>
  <summary>Details</summary>
Motivation: 现有的团队优化工具无法有效处理个体偏好和团队目标之间的动态平衡，导致成员不满意及团队表现下降。研究动机是开发能够提高团队满意度、凝聚力和绩效的AI增强团队优化框架。

Method: 提出了一个多臂赌博算法用于团队组成，并引入了名为tAIfa的系统，利用LLMs提供即时反馈。此外，使用PuppeteerLLM模拟多代理团队的动态。

Result: 构建了AI增强的团队优化框架，通过动态调整团队组成和大语言模型的应用，提升了团队的整体表现和成员满意度。

Conclusion: 通过引入多臂赌博算法和大型语言模型(LLMs)技术，该研究提出了新的团队优化框架，以提升团队满意度、凝聚力和表现。

Abstract: Effective teamwork is essential across diverse domains. During the team
formation stage, a key challenge is forming teams that effectively balance user
preferences with task objectives to enhance overall team satisfaction. In the
team performing stage, maintaining cohesion and engagement is critical for
sustaining high team performance. However, existing computational tools and
algorithms for team optimization often rely on static data inputs, narrow
algorithmic objectives, or solutions tailored for specific contexts, failing to
account for the dynamic interplay of team members personalities, evolving
goals, and changing individual preferences. Therefore, teams may encounter
member dissatisfaction, as purely algorithmic assignments can reduce members
commitment to team goals or experience suboptimal engagement due to the absence
of timely, personalized guidance to help members adjust their behaviors and
interactions as team dynamics evolve. Ultimately, these challenges can lead to
reduced overall team performance. My Ph.D. dissertation aims to develop
AI-augmented team optimization frameworks and practical systems that enhance
team satisfaction, engagement, and performance. First, I propose a team
formation framework that leverages a multi-armed bandit algorithm to
iteratively refine team composition based on user preferences, ensuring
alignment between individual needs and collective team goals to enhance team
satisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an
AI-powered system that utilizes large language models (LLMs) to deliver
immediate, personalized feedback to both teams and individual members,
enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an
LLM-based simulation framework that simulates multi-agent teams to model
complex team dynamics within realistic environments, incorporating task-driven
collaboration and long-term coordination.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [133] [Benchmark for Antibody Binding Affinity Maturation and Design](https://arxiv.org/abs/2506.04235)
*Xinyan Zhao,Yi-Ching Tang,Akshita Singh,Victor J Cantu,KwanHo An,Junseok Lee,Adam E Stogsdill,Ashwin Kumar Ramesh,Zhiqiang An,Xiaoqian Jiang,Yejin Kim*

Main category: q-bio.QM

TL;DR: AbBiBench是一种用于抗体亲和力设计和优化的基准框架，比较了多种模型，发现结构条件逆折叠模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的抗体评估策略主要依赖于抗体本身及其与自然抗体的相似性，并未考虑抗体与抗原的实际结合能力。AbBiBench提出了一种新的衡量抗体设计潜力的方法，通过测量抗体抗原复合体来改进抗体亲和力筛选和设计。

Method: AbBiBench考虑抗体抗原复合物作为功能单元，测量蛋白模型在Ab-Ag复合体上的可能性，通过模型可能性与实验亲和力值的相关性来评估模型性能。

Result: 通过对14种蛋白模型的比较分析，结构条件逆折叠模型在亲和力相关性和抗体生成任务中表现最佳。

Conclusion: AbBiBench为抗体设计领域提供了一个综合的、生物学驱动的评价框架，有助于开发更有效的、具有功能意识的抗体设计模型。结构条件逆折叠模型在亲和力相关性和生成任务中表现优于其他模型。

Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking
framework for antibody binding affinity maturation and design. Unlike existing
antibody evaluation strategies that rely on antibody alone and its similarity
to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench
considers an antibody-antigen (Ab-Ag) complex as a functional unit and
evaluates the potential of an antibody design binding to given antigen by
measuring protein model's likelihood on the Ab-Ag complex. We first curate,
standardize, and share 9 datasets containing 9 antigens (involving influenza,
anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain
mutated antibodies. Using these datasets, we systematically compare 14 protein
models including masked language models, autoregressive language models,
inverse folding models, diffusion-based generative models, and geometric graph
models. The correlation between model likelihood and experimental affinity
values is used to evaluate model performance. Additionally, in a case study to
increase binding affinity of antibody F045-092 to antigen influenza H1N1, we
evaluate the generative power of the top-performing models by sampling a set of
new antibodies binding to the antigen and ranking them based on structural
integrity and biophysical properties of the Ab-Ag complex. As a result,
structure-conditioned inverse folding models outperform others in both affinity
correlation and generation tasks. Overall, AbBiBench provides a unified,
biologically grounded evaluation framework to facilitate the development of
more effective, function-aware antibody design models.

</details>
