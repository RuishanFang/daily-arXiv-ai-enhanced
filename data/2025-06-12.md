<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.LG](#cs.LG) [Total: 95]
- [math.OC](#math.OC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 48]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.NI](#cs.NI) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.SD](#cs.SD) [Total: 6]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.SE](#cs.SE) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: 本文提出了一种质性评估方法LLM-as-a-qualitative-judge，针对NLG系统输出的问题进行结构化报告，该方法能有效识别实例问题并生成详细报告。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估方法主要作为一种定量工具，本文提议的LLM质性评估方法旨在为NLG系统开发者提供改进方向的有意义见解。

Method: 提出了一种名为LLM-as-a-qualitative-judge的基于大语言模型的评估方法，包括开放式的单实例问题分析和使用直观的累积算法对发现的问题进行聚类。

Result: 结果表明，LLM-as-a-qualitative-judge在2/3的情况中能正确识别实例特定问题，并能够生成类似于人类标注者撰写的错误类型报告。

Conclusion: 本研究提出LLM作为质性评判者，以帮助开发者获得关于NLG系统改进的深入见解，其评估方法在大多数情况下能够识别特定实例问题，并生成类似于人工标注报告的错误类型报告。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型是否具有推理他人意图的能力，通过合作多智能体强化学习提高人机协作能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）仅基于大量文本进行训练，没有明确监督作者意图，它们似乎能够推断文本交互的潜在意义。研究其是否具有心理理论是至关重要的，因为理解他人的意图对于合作互动以及人类与自主系统之间的协作至关重要。

Method: 采用合作多智能体强化学习（MARL）的框架，使用大型语言模型（LLM）进行自然语言互动，使智能体通过反复交互学习合作。

Result: 提出了一种新的方法，以增强人工智能体适应和与人工和人类伙伴合作的能力，推动打造能够促进无缝协作的混合人机系统。

Conclusion: 本研究表明，通过合作的多智能体强化学习（MARL），大型语言模型（LLM）可以模拟和推理他人的意图，推动与人类和人工智能伙伴的无缝协作。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [3] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: 提出了短语字典偏置方法，在语音翻译和多模态语言模型中显著提高了短语翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 在会话中理解核心概念的短语在训练数据中较为罕见，因此在语音翻译任务中正确翻译短语是一个挑战。

Method: 我们提出了一种短语字典偏置方法，该方法利用从源语言到目标语言的短语映射对。并将此方法应用于基于转换器的流式语音翻译模型和多模态大型语言模型。

Result: 实验结果显示，短语字典偏置方法在流式语音翻译模型中相较于短语列表偏置相对提高了21%。此外，该方法使多模态大型语言模型能够使用外部短语信息，实现了短语召回率85%的相对提升。

Conclusion: 我们提出的短语字典偏置方法在流式语音翻译模型和多模态大型语言模型中表现显著优于现有方法。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [4] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: 该研究探讨了卷积神经网络在音素学上的词汇不变概括能力，并提出了一种新的探测技术，表明卷积层可以超越完全连接层的词汇约束动态推广音素依赖。


<details>
  <summary>Details</summary>
Motivation: 探讨深度神经网络在词汇学习中推导出的音素学概括能力，特别是其词汇不变的概括能力。

Method: 研究采用生成性卷积神经网络（CNN），在词汇项目的原始音频波形上进行训练，并通过缩小完全连接层（FC）瓶颈从1024通道到8来探索其影响。

Result: 在缩小了完全连接层瓶颈的情况下，提出了一种新的技术来探索模型的词汇独立概括。结果表明，这种在卷积层上生成的音频输出在训练中同样受音素学限制的偏倚影响。

Conclusion: 卷积层可以动态地推广音素依赖关系，超越完全连接层学习的词汇约束配置。

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [5] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: ReasonMed is a large medical reasoning dataset used to create ReasonMed-7B, a model excelling in medical question answering.


<details>
  <summary>Details</summary>
Motivation: To explore and improve the capabilities of large language models in knowledge-intensive medical question answering, an area underexplored compared to mathematics and programming.

Method: A multi-agent verification and refinement process is used, involving an Error Refiner to enhance reasoning paths by identifying and correcting errors, combined with the best practice of detailed Chain-of-Thought reasoning and concise answer summaries for training.

Result: ReasonMed-7B outperforms the previous best model by 4.17% and exceeds LLaMA3.1-70B by 4.60% on PubMedQA.

Conclusion: ReasonMed-7B, trained on the ReasonMed dataset, outperforms previous models in medical reasoning and sets a new benchmark for sub-10B models.

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [6] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 该论文探讨了变压器语言模型的长度泛化能力，并展示了这种能力在相关任务之间可以转移。


<details>
  <summary>Details</summary>
Motivation: 理解变压器语言模型的泛化能力是如何产生的，特别是如何从较短的输入推断到更长的输入。

Method: 通过任务关联的视角研究长度泛化，并通过长而相关的辅助任务训练模型，观察其在其他目标任务中泛化到未见过的更长输入。

Result: 变压器模型能够从类似任务继承泛化能力，且预训练语言模型展示类似的转移效应，使得在下游环境中能更好地推理。

Conclusion: 变压器语言模型可以通过将相关任务一起训练，继承并转移长度泛化能力，以便在下游环境中进行推理。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [7] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 研究开发了一种用于识别在线游戏聊天中亲社会行为的新模型，展示了如何在低资源环境下促进积极互动。


<details>
  <summary>Details</summary>
Motivation: 在线游戏中存在大量玩家互动，通过聊天进行交流。现有研究主要关注检测有害内容。然而，识别和促进亲社会行为同样重要，且现有的数据集和模型较为有限。

Method: 采用无监督发现策略结合游戏领域专家的合作，提出了一种新的自锚定注意力模型（SAAM），利用整个训练集作为“锚点”改善模型性能。

Result: 在游戏聊天中分类亲社会行为的自动化系统首次开发，尤其适用于数据稀缺的情况，并应用于知名的在线游戏《使命召唤：现代战争II》，展示了其有效性。

Conclusion: 将NLP技术应用于亲社会行为的发现和分类，帮助转变对在线平台中内容从惩罚有害行为到鼓励积极互动的关注。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [8] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: 本文提出一种新框架，用于定量测量LLM生成的自然语言解释的忠实性，通过直接比较解释与模型内部状态，推进了对解释忠实性的理解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的发展，它们可以生成自由文本的自然语言解释（self-NLE）来解释其答案。然而，这些解释虽然表面上逻辑合理，但不一定反映模型的实际决策过程，因此可能是不忠实的。现有用于测量self-NLE忠实性的技术大多依赖行为测试或计算模块识别，但没有从模型推理的神经活动角度进行分析。因此提出一种新框架直接比较解释与模型内部隐含状态，以评估self-NLE忠实性。

Method: 引入了一种灵活的框架，通过直接比较LLM生成的self-NLE与模型内部隐含状态的解释，以定量测量self-NLE的忠实性。

Result: 该框架提供了关于self-NLE忠实性的重要洞察，并通过在self-NLE与模型推理之间建立直接联系来推进对其忠实性的理解。

Conclusion: 该框架为生成更忠实的self-NLE提供了构建模块，促进了有关self-NLE忠实性的理解。

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [9] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 引入一种新的$(RSA)^2框架，通过考虑修辞策略，无需建模说话者动机，有效解释比喻语言，与LLMs结合后在讽刺数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的RSA框架在处理比喻表达时要么不能有效解释，要么需要特定情景下对使用比喻语言的内在动机进行建模。因此，需要一种能够无须解释说话者使用比喻语言动机的框架来进行更广泛的解读。

Method: 引入Rhetorical-Strategy-Aware RSA $(RSA)^2框架，此框架通过考虑说话者的修辞策略来解释比喻语言的不匹配，通过与大规模语言模型结合，进行讽刺理解的实验。

Result: $(RSA)^2框架无需解释说话者的动机即实现了人类兼容的非字面表达解读。在新讽刺理解数据集PragMega+中，结合大规模语言模型后实现了最先进的性能。

Conclusion: $(RSA)^2框架通过考虑说话者使用的修辞策略，有效解释了非字面表达，而无需解释说话者不使用字面表达的动机。结合大规模语言模型，该框架在新引入的PragMega+数据集中的讽刺任务中实现了最先进的性能。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [10] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: 研究利用Mistral-7B扩展了检测阿尔茨海默病的方法，提高检测准确性，并展示了模型的清晰决策界限和对阿尔茨海默病语言模式的学习能力。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用先进的语言模型提高检测阿尔茨海默病的准确性。

Method: 使用 Mistral-7B 的指令跟随版本的大语言模型来扩展检测阿尔茨海默病的方法。

Result: 新方法平均提高了检测准确性3.33%，在ADReSS 2020挑战基准中提高6.35%。

Conclusion: 大语言模型（LLM）可以有效地检测阿尔茨海默病，并较目前的方法提供更明确的决策界限。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [11] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: 论文提出了先进方法来提升大型语言模型的对齐效果，着重于数据收集、训练和评估，从而改善模型的推理能力和约束遵循能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在各种任务中表现出色，但有效地将其与人类期望对齐仍是一个重要的挑战。本文旨在通过引入新的方法来提升LLM的对齐能力。

Method: 本文采用了Lion框架进行数据收集，通过对抗性蒸馏来迭代优化数据。此外，WebR框架自动合成指令微调数据以提高数据的多样性和可扩展性。在训练方面，开发了LTE框架和BMC方法以优化知识更新和偏好数据对齐。在评估方面，引入了FollowBench基准以检查模型的约束遵循能力。

Result: 实验结果揭示了现有模型在约束遵循方面的关键弱点，并提供了未来改进的方向。

Conclusion: 本文提出了一系列创新的方法来改善大规模语言模型的对齐，包括数据收集、训练和评估。这些方法显著提升了模型的推理能力和约束遵循能力，为未来的改进提供了宝贵的见解。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [12] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: RePO通过利用重放策略提高了LLM的优化效率，相较于GRPO性能更优，且计算成本增加有限。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法高计算成本和数据效率低的问题。

Method: 使用多样的重放策略来获取重放缓冲区中的离政策样本，并基于更广泛和多样化的样本集进行策略优化。

Result: 在五个LLM的七个数学推理基准上，RePO相较于GRPO分别获得了18.4和4.1点的性能提升。

Conclusion: RePO展示了显著的性能提升，并在计算成本和优化步数上取得了良好的平衡。

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [13] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 本文研究了潜在多头注意力（MLA）在小型语言模型中的效率质量权衡，发现MLA结合旋转位置嵌入（RoPE）在内存节省和质量保持之间实现了优势。


<details>
  <summary>Details</summary>
Motivation: 研究潜在多头注意力（MLA）在小型语言模型中的效率质量权衡，以优化内存使用和提高模型性能。

Method: 本文比较了三种架构变种：标准多头注意力（MHA）、潜在多头注意力（MLA）和与旋转位置嵌入结合的MLA（MLA+RoPE），并评估了它们在GPT模型上的表现。

Result: MLA + RoPE以半秩潜在维度（r = d/2）实现了45%的KV缓存内存减少，仅增加0.3%的验证损失，且相较标准注意力机制带来了速度提升和最佳质量分数。

Conclusion: MLA + RoPE在内存受限的情况下提供了有效的效率质量权衡，其以较小的内存开销实现了与MHA相匹配的质量。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [14] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了OmniDRCA，一种新的并行语音文本生成模型，实现了最先进的性能，并探讨了在全双工对话中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要分为两类, 方法(1)在生成离散语音令牌时不考虑LLM的自回归过程，而方法(2)通过联合自回归模型生成交错或并行的语音文本令牌，从而在生成过程中实现模态之间的互相感知。为了改善这一问题，论文提出了一种新的模型。

Method: 本论文提出了OmniDRCA，一种基于联合自回归建模的并行语音文本基础模型，具有双分辨率语音表示和对比跨模态对齐。

Result: 实验结果表明，OmniDRCA在Spoken Question Answering基准测试中，实现了新的SOTA表现。

Conclusion: OmniDRCA在并行语音文本建模中达到新的SOTA性能，并在交错模型中表现出竞争力，同时探索了该框架在全双工对话场景中的潜力。

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [15] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: 介绍了一种名为RuleReasoner的高效规则推理方法，通过动态采样提高了领域泛化能力，显著优于大型推理模型且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 在推理领域中，规则推理被认为是一个基本问题，而现实应用中规则格式、类型和复杂性的变化带来了严峻挑战。最近的研究表明，大型推理模型在推理能力方面表现出色，并且通过强化学习显著提升了其性能。但是，小型推理模型是否能够在各种任务和领域中有效地学习规则推理并具备强大的泛化能力仍然是一个开放性问题。

Method: 本文引入了一种称为RuleReasoner的增强规则推理方法，通过大量精心设计的任务和一种新颖的领域感知动态采样方法实现规则推理。具体而言，RuleReasoner通过根据历史奖励更新不同领域的采样权重来重新采样每个训练批次。这促进了领域增强和灵活的在线学习计划，从而不需要现有方法中使用的预先设计的人为混合训练方式。

Result: 实证评估在组内（ID）和组外（OOD）基准上表明，RuleReasoner在性能上显著超越了前沿的大型推理模型，在八个ID任务上平均提高了4.1%，在三个OOD任务上平均提高了10.4%。而且，与先前用于强化学习的动态采样方法相比，我们的方法还表现出了更高的计算效率。

Conclusion: RuleReasoner证明了一种高效的小型推理模型，可以通过动态采样在多个任务和领域上实现出色的规则推理性能，并超越了大型推理模型。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [16] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: DIVE通过增强专家多样性实现了LLM训练效率的提升。在实验中，DIVE优于现有方法，维持了较低的准确性损失。


<details>
  <summary>Details</summary>
Motivation: 现有的重建方法往往忽视了专家之间的多样性，导致潜在的冗余。本文观察到，特定LLM在不同校准数据集上修剪后表现出显著的多样性，因此提出了DIVE这一多样性增强重建方法。

Method: DIVE的配方包括领域亲和力挖掘、基于剪枝的专家重建以及高效的再训练。特别是重建包括喂入前馈网络（FFN）模块的剪枝和重组。在重建后，模型将针对路由器、专家和归一化模块高效地进行再训练。

Result: DIVE使LLM的训练效率得到提升，且准确性上的妥协最小，实验结果表明DIVE的表现优于现有的剪枝和MoE重建方法，并且激活参数数量相同。

Conclusion: DIVE有效地提高了训练效率，同时保持了模型的准确性，对现有方法进行了性能上的超越。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [17] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: 论文探讨了使用大语言模型评估生成SQL的语义等价性，分析了SQL等价性和不等价性的模式，以及评估中的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于用户查询的模糊性和多个有效SQL解释，生成SQL的语义等价性评估成为挑战，特别是在NL2SQL系统中。

Method: 探索使用大语言模型（LLM）来评估生成的SQL在语义上和更实用的“弱”语义等价性。

Result: 分析了SQL等价和不等价的常见模式，讨论了基于LLM评估中的挑战。

Conclusion: 评估生成SQL的语义等价性仍然具有挑战性，使用LLM可以提供新的视角和方法。

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [18] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: The study proposes a framework called COGENT for generating grade-appropriate educational content using Generative AI, addressing challenges like curriculum alignment and readability. It shows superior performance compared to human references in evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of using Generative AI in educational contexts, specifically its alignment with curriculum standards and maintaining appropriate reading levels.

Method: The method involves creating a curriculum-oriented framework named COGENT that integrates curriculum components, controls readability, and uses a "wonder-based" approach to boost student engagement. Evaluation is through a combination of LLM-as-a-judge and human expert analysis.

Result: Experimental results indicate that COGENT effectively generates grade-appropriate educational content, achieving results that meet or exceed human references.

Conclusion: COGENT can consistently produce educational content that is grade-appropriate and comparable or superior to human references.

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [19] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: CoLMbo addresses limitations in speaker recognition by generating detailed descriptions of speaker characteristics and performing well in zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing speaker recognition systems struggle to generate detailed speaker characteristics and lack structured capture of demographic attributes like dialect, gender, and age.

Method: The paper introduces CoLMbo, a Speaker Language Model (SLM), by integrating a speaker encoder with prompt-based conditioning.

Result: CoLMbo can create detailed captions based on speaker embeddings and adapt dynamically to new speaker characteristics using user-defined prompts, including regional dialect and age.

Conclusion: CoLMbo significantly advances the field of speaker recognition by successfully generating detailed speaker characteristics descriptions and performing well in zero-shot scenarios across diverse datasets.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [20] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: 研究显示机器学习可有效区分新闻质量，优化的深度学习模型达到了最高的准确率，但训练时间较长。


<details>
  <summary>Details</summary>
Motivation: 随着在线新闻的激增，低质量新闻标题和链接可能会广泛传播，有必要寻找方法自动区分这些低质量内容与高质量内容。

Method: 研究评估了12种机器学习模型，对一个全球新闻网站链接/标题的二元平衡数据集进行分析，该数据集涵盖2018年至2024年期间的57,544,214个样本。根据领域质量专家共识的评分，每个文本被贴上二元标签。使用传统集成方法和优化的DistilBERT进行分类。

Result: 基于传统的集成方法，尤其是bagging分类器，表现出良好的性能（准确率88.1%，F1分数88.3%）。而经过微调的DistilBERT达到了最高的准确率（90.3%），但需要更长的训练时间。

Conclusion: 结果表明，无论是使用自然语言处理特征结合传统分类器，还是深度学习模型，都可以有效区分新闻标题和链接的质量，但是在预测性能和训练时间之间存在权衡。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [21] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 大型语言模型在礼貌策略上表现卓越，但在积极场景中过度依赖于消极礼貌策略，可能导致误解。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够像人类一样，在平衡信息和社交目标时，使用丰富多样的语言策略。

Method: 通过比较人类和大型语言模型在受限和开放式生产任务中的反应，来调查大型语言模型是否使用类似于人类的语境敏感策略。

Result: 较大的模型（参数大于等于70B）成功地复制了计算语用学文献中的关键偏好，人类评估者在开放式场景中令人惊讶地更喜欢LLM生成的反应。然而，进一步的语言分析显示，模型在积极场景中过多地依赖消极礼貌策略。

Conclusion: 现代大型语言模型在礼貌策略上表现出令人瞩目的能力，但过度依赖于消极礼貌策略，即使在积极的上下文中，也可能导致误解，提出了关于AI系统语用对齐的重要问题。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [22] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识追踪模型KT$^2$，利用分层知识概念信息和隐马尔科夫树模型，在数据稀疏的情况下实现优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 在数据匮乏的情况下恢复良好性能，需要借助许多教室场景中通常可用的分层知识概念信息，该信息在数据稀疏时提供强有力的先验。

Method: 使用基于知识树的概率KT框架，通过隐马尔科夫树模型对知识概念的树状层次结构进行学生理解建模，采用EM算法估计学生掌握情况，并通过增量更新机制实现个性化预测。

Result: KT$^2$在现实在线低资源环境中始终优于强基线。

Conclusion: KT$^2$ consistently outperforms strong baselines in realistic online, low-resource settings.

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [23] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: Token Constraint Decoding (TCD) improves LLM robustness against input noise, achieving significant performance gains and enhancing reasoning stability.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the robustness and stability of Large Language Models (LLMs) in noisy environments, specifically for multiple-choice question answering benchmarks.

Method: The paper introduces Token Constraint Decoding (TCD), an inference-time algorithm that aligns token-level predictions to improve robustness against input noise.

Result: TCD, especially when paired with prompt engineering, significantly restores performance degraded by input noise. For instance, it provides up to +39% absolute improvement on models like Gemma3 1B.

Conclusion: Token Constraint Decoding (TCD) demonstrates significant improvements in reasoning stability and robustness for LLMs in noisy settings, making them more reliable for deployment in critical applications.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [24] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: PGDA-KGQA是一种提示引导的生成框架，通过多种数据增强策略提升知识图谱问答任务的表现，显著超过现有技术。


<details>
  <summary>Details</summary>
Motivation: 知识图谱问答任务在处理自然语言问题时需要对知识图谱进行推理。当前利用大语言模型的方法虽然具有显著的语义解析能力，但由于缺乏多样化的标注数据和多跳推理样本而受到限制。此外，传统的数据增强方法主要集中在单跳问题，容易导致语义失真，而基于LLM的方法虽然能解决语义失真问题，但通常忽略了多跳推理，从而限制了数据的多样性。缺乏多跳样本进一步削弱了模型的泛化能力。

Method: 提出一种基于提示引导的生成框架PGDA-KGQA，通过多种数据增强策略来解决KGQA问题。主要通过设计精巧的提示来结合给定的文本内容，利用LLMs生成大规模的问题和逻辑形式对来进行模型训练。具体而言，通过生成单跳伪问题、应用语义保留的问题改写和采用答案引导的反向路径探索等方式来丰富训练集。

Result: PGDA-KGQA在标准KGQA数据集上的表现超过了最先进的方法。具体来说，在WebQSP数据集上，F1提高了2.8%，Hits@1提高了1.2%，准确率提高了3.1%；在ComplexWebQuestions数据集上，F1提高了1.8%，Hits@1提高了1.1%，准确率提高了2.4%。

Conclusion: PGDA-KGQA通过采用提示引导的生成框架，有效解决了当前KGQA任务中数据多样性不足和多跳推理能力弱的问题。通过增强数据的多样性和提高逻辑形式生成的准确性，显著提升了回答检索的表现。

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [25] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: 微调的LLMs在文本欺骗检测表现优异，但LMMs在跨模态线索利用上有局限。


<details>
  <summary>Details</summary>
Motivation: 在日益数字化的世界中，检测欺骗是关键且具有挑战性的任务。鉴于此，本研究评估了大型语言模型（LLMs）和大型多模态模型（LMMs）在不同领域中自动欺骗检测的能力。

Method: 研究采用了零样本和少样本方法，并通过随机或基于相似性的上下文示例选择进行实验。采用了不同的提示策略，如直接标签生成和逐步思考推理。

Result: 微调的LLMs在文本欺骗检测中达到了技术最先进的表现，而LMMs在充分利用跨模态线索上仍面临挑战。研究还分析了辅助特征（如非语言手势和视频摘录）的影响。

Conclusion: 本研究表明，经过微调的LLMs在文本欺诈检测任务中表现出色，但LMMs在利用跨模态线索方面仍存在困难。

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [26] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: 提出了一种新的SFT方法，可以在没有原始数据的情况下降低灾难性遗忘风险，同时提升任务特定性能。


<details>
  <summary>Details</summary>
Motivation: 在实施SFT时，由于原始预训练数据不可访问，第三方实施者可能加剧灾难性遗忘问题。因此，研究动机是寻求一种能够在不访问原始SFT数据的情况下，降低灾难性遗忘风险的方法。

Method: 研究首先重构基础模型可能的SFT指令分布，然后进行多模型筛选过程，以选择最佳数据，并将其与新数据混合进行SFT。

Result: 实验结果表明，提出的方法能够在保留LLMs在一般领域的泛化能力的同时，提升其在特定任务上的性能表现。

Conclusion: 研究提出了一种新的、更经济的监督微调（SFT）方法，该方法能够在没有原始数据的情况下有效降低灾难性遗忘的风险。实验结果表明，该方法在保留一般领域的泛化能力的同时，提升了任务特定的性能。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [27] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: 本文介绍了专为俄语设计的GigaChat大型语言模型系列，提供了开放访问以促进NLP研究和工业发展。


<details>
  <summary>Details</summary>
Motivation: 生成性大型语言模型已成为现代自然语言处理研究和应用的关键，但针对俄语专门设计的基础模型的发展有限。

Method: 我们提供了详尽的模型架构、预训练过程以及实验报告，以指导设计选择，并评估了这些模型在俄语和英语基准上的性能，与多语言类比进行了比较。

Result: GigaChat系列在俄罗斯和英语基准测试中表现良好，并可通过API、电报机器人和Web界面进行访问。

Conclusion: 本文发布了三种开源的GigaChat模型，以拓展俄语自然语言处理（NLP）研究机会，并支持俄语工业解决方案的发展。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [28] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: 引入UniToMBench评估并提高LLM的ToM能力，发现其在情感和信念任务中准确性高，但在知识任务中变异性大。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在预测人类心理状态方面存在挑战，需要改进其心智理论（ToM）能力。

Method: 引入UniToMBench，与SimToM和TOMBENCH结合，通过多互动任务设计和不断发展的故事场景提高和评估LLM的ToM能力。使用包含超过1000个手写场景的自定义数据集，结合视角转变技术和多样化评估指标。

Result: 评估显示，GPT-4o和GPT-4o Mini在处理情感和信念相关场景的任务中表现出高准确性（通常超过80%），但在知识相关任务上表现出显著的变异性。

Conclusion: 当前LLM在ToM相关任务中表现出强项和局限性，UniToMBench是未来发展的综合工具。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [29] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了POET方法来改进直接对齐算法(DAAs)的表现，解决奖励生成差距问题，并在多项实验中取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 目前直接对齐算法(DAAs)在训练中的优化目标与推理阶段的生成性能之间存在奖励生成差距，作者希望解决这一问题。

Method: 使用Prefix-Oriented Equal-length Training (POET)方法，通过截断响应以使其长度相等来改善DAAs中的优化目标，使得优化过程更加关注前缀token。

Result: 实验表明，在代表性DAAs如DPO和SimPO中使用POET可以比标准方法提高性能，具体在AlpacaEval 2中提升了至多15.6分，并在下游任务中整体提升。

Conclusion: POET方法可以有效解决DAAs中奖励生成的差距问题，并提高模型的生成性能。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [30] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 研究通过分析社交媒体上的自杀行为，结合计算机和专家的方法，揭示了自杀行为的数字表现及其与传统临床知识的差异。


<details>
  <summary>Details</summary>
Motivation: 在西方国家，自杀仍是导致死亡的重要原因，这突显了新的研究方法的必要性。随着社交媒体成为日常生活的核心，数字足迹可以为理解自杀行为提供宝贵的见解。

Method: 研究采用了三个方法：计算机自下而上、混合方法以及专家引导的自上而下的方法。通过一个包含181个有自杀未遂个体的YouTube频道和134个对照频道的新型纵向数据集，这些方法被用于探讨自杀行为的体现及与专家知识的差异。

Result: 研究发现，在计算机自下而上的方法中，有五个主题与自杀未遂相关，其中两个显示出时间上的变化，分别是心理健康问题和YouTube参与。在混合方法中，临床专家评估了LLM衍生的话题，并标记出19个与自杀有关的话题，但没有发现超过自下而上识别的显著时间效应。而在自上而下的心理评价中，唯一显著的差异在于分享经历的动机：前者旨在帮助他人，而后者将其作为个人恢复的一部分。

Conclusion: 通过整合多种研究方法，研究揭示了自杀行为在数字行为和临床视角之间的联系，尤其强调了社交媒体平台上自杀行为独特的表现形式。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [31] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

TL;DR: 研究浮点精度如何影响LLM推理的可重复性，并开发了LayerCast管道以提高数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 展示在评估批量大小、GPU数量和版本变化等系统配置中，LLM性能的可重复性是脆弱的，特别是对推理模型影响较大。

Method: 通过在不同的硬件、软件和精度设置下进行精心控制的实验来量化何时以及如何发生模型输出的差异。

Result: 发现浮点数精度对可重复性至关重要，但在评估实践中过于忽视。

Conclusion: 传递了浮点数精度对大规模语言模型推理中可重复性影响的重要性，并提出了轻量级的推理管道LayerCast，以平衡内存效率和数值稳定性。

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [32] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出了统一旋转位置嵌入方法，成功整合Transformer和SSM架构，使训练和推理速度大幅提升，并提高了精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer和SSM架构在其各自位置编码机制方面存在的基本不一致性问题，尤其是在长序列建模中。

Method: 提出了一种新的统一旋转位置嵌入（\textbf{\ourRoPE})方法，将Transformer和SSM层在同一位置编码框架下整合。

Result: 在4K序列长度下，\model的训练和推理速度分别比标准Transformer模型快42.3%和29.5%。在同等条件下，它在语言建模基准测试中比Transformer基线高出4%以上的准确性，\model-1.3B在平均准确率上比其320M版本提高了7.22%。

Conclusion: 提出了一种统一旋转位置嵌入的方法，成功解决了Transformer和SSM模型在位置编码方面的不兼容性，使得混合模型能够进行高效和高性能的长上下文建模。

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [33] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KG-Infused RAG通过整合知识图谱和模型学习提高了RAG的性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法通常依赖单一的数据来源，缺乏认知启发的机制来激活相关知识。本文旨在通过整合知识图谱，解决这些问题。

Method: 提出了KG-Infused RAG框架，通过在RAG系统中整合知识图谱(KGs)实现扩散激活认知过程，进行多源检索并结合模型学习优化。

Result: 在五个问答基准测试中，KG-Infused RAG比原始RAG表现提升了3.8%到13.8%。

Conclusion: KG-Infused RAG显著提升了RAG的性能和多功能性，尤其在与Self-RAG结合后表现更加优秀。

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [34] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

TL;DR: MEDUSA是一个多模态框架，通过四阶段训练流程处理情感识别中的挑战，在2025年Interspeech比赛中获胜。


<details>
  <summary>Details</summary>
Motivation: 人类情感的主观性质和自然条件下的不均匀表示，使得情感识别成为一项具有挑战性的任务。

Method: 提出了MEDUSA，一个具有四阶段训练流程的多模态框架。前两个阶段训练分类器集成，使用DeepSER深度交叉模态 transformer结合预训练的自监督声学和语言学表示。最后两个阶段优化可训练的元分类器以结合集成预测。此外，采用Manifold MixUp进一步正则化。训练过程中使用人类注释分数作为软目标，并结合平衡数据抽样和多任务学习。

Result: MEDUSA在2025年的Interspeech情感识别比赛中获得了第一名。

Conclusion: MEDUSA是一个有效的多模态框架，能够处理情感识别中的类不平衡和情感模糊性问题，并在2025年Interspeech挑战中获得了第一名。

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [35] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

TL;DR: 本研究调查了谷歌翻译和DeepL在英语到希腊语翻译中的性别偏见，并评估了GPT-4o作为偏见缓解工具的潜力。发现两者在性别不明确的情况下难以实现性别中立翻译。


<details>
  <summary>Details</summary>
Motivation: 随着对包容性语言的需求增加，关注点转向了机器翻译系统可能会加剧性别刻板印象的问题。

Method: 研究了谷歌翻译和DeepL这两个商业机器翻译系统的性别偏见，特别是以前研究较少的英语到希腊语的翻译对。引入了GendEL，一个手动制作的双语数据集，包括240个具有刻板印象的职业名词和形容词的性别模糊和明确句子。

Result: 发现两种机器翻译系统在翻译中的性别偏见仍然存在；当性别明确时，它们表现良好，其中DeepL在女性性别明确句子上表现优于谷歌翻译和GPT-4o，但当性别不限时，距离产生性别包容或中性翻译仍有差距。GPT-4o在多数模糊情况下能生成适当的性别和中性替代方案，但仍有残存偏见。

Conclusion: 机器翻译系统在性别不明确的情况下难以提供性别中立的翻译。GPT-4o在性别偏见消除方面表现出一定的潜力。

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [36] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

TL;DR: 研究者创建了一个用于马其顿语的8B参数模型，提供开源数据和资源，解决低资源语言在大型语言模型中的应用限制。


<details>
  <summary>Details</summary>
Motivation: 考虑到低资源语言在大型语言模型应用中的局限性，作者旨在支持马其顿语的技术采用和研究进展。

Method: 作者收集了马其顿语语料库和指令数据集，并构建了评估基准，然后训练和评估了一个8B参数的模型。

Result: 训练出的模型在性能上超越了同参数范围内的所有现有模型，并在多个评估基准上表现优异，受到当地使用者的偏好。

Conclusion: 作者发布了一套用于马其顿语的大型语言模型资源，包括数据集、代码和模型权重，以促进该语言的研究发展。

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [37] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: 调查知识图谱与大型语言模型整合的现状，提出了增强推理能力和减少幻象现象的关键方法，并提出未来研究方向以发展智能系统以支持复杂知识任务。


<details>
  <summary>Details</summary>
Motivation: 整合知识图谱中的结构化知识到大型语言模型中可以增强其事实基础和推理能力，因此亟需对此领域的系统性研究，以揭示实现此整合的有效方法及其带来的优越性。

Method: 本文采用系统调查的方式，分类并审查了现有方法，将其分为两大类：增强型LLMs和扩展型KGs。

Result: 本文识别了当前研究中的关键空白，并提出了未来研究方向，包括神经符号整合、动态知识图谱更新、数据可靠性和伦理问题，以支持管理更复杂的现实世界知识任务的智能系统发展。

Conclusion: 本研究通过分析发现，知识图谱与大型语言模型整合的关键在于改善推理、减少幻象现象以及增强复杂问题的回答能力。同时，也促进了知识图谱的构建、补全和查询。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [38] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: 研究发现语言模型的内在维度（ID）影响记忆行为，高ID序列较难被记住，特别是在过参数化模型下。


<details>
  <summary>Details</summary>
Motivation: 语言模型在训练过程中容易记住部分数据，并在生成时无意中输出这些信息，导致隐私泄露和知识产权的问题。虽然之前的研究识别出了一些导致非自愿记忆的因素，但关于潜在结构如何影响记忆率知之甚少。

Method: 研究内在维度（ID）作为序列在潜在空间中的结构复杂性的几何代理，如何影响记忆率。

Result: 高内在维度序列比低内在维度序列更不容易被记住，特别是在过参数化模型和稀疏曝光下。

Conclusion: 内在维度作为记忆的抑制信号，影响了语言模型的记忆行为，揭示了规模、曝光和复杂性在记忆中的相互作用。

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [39] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

TL;DR: 研究发现，DSL和PPI能在大数据集下减少偏差，但DSL在不同数据集上表现不一，强调了偏差与方差的权衡。建议开发量化其效率的指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能够以低成本标注文本，但与专家相比常存在不一致性，这会导致下游对群体参数的估计出现偏差。研究者希望通过开发去偏方法来缓解这种偏差。例如，借助有限但成本高昂的专家标注，设计基于监督学习（DSL）与预测能力推理（PPI）合并LLM标注以实现有效估计。

Method: 文章研究了去偏方法DSL和PPI在不同专家标注数量下的性能扩展情况，并比较这两种方法在多个任务中的表现，通过实验揭示出在数据量大的情况下，两者都能达到低偏差，但DSL在偏差降低和经验效率上通常优于PPI。

Result: 尽管DSL在大数据集下表现良好，但其在不同数据集上的表现不够稳定。研究揭示了去偏方法在偏差与方差层面上的权衡，并呼吁开发量化有限样本效率的指标。

Conclusion: DSL和PPI在理论假设下能产生一致的估计，而在研究实践中其有限样本表现方面还有待进一步探索。当前结果表明两种方法各有优势，但在偏差和方差之间存在一定权衡。

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [40] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Main category: cs.CL

TL;DR: 该研究对比了信息论概率预测器与NDL预测器在音频词持续时间建模中的效果，发现N-gram模型表现最佳，但也指出结合信息理论公式能提升NDL模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索基于信息论的概率预测器与朴素区分学习（NDL）预测器在建模音频词持续时间方面的效果，尤其关注概率减少现象。

Method: 本研究使用Buckeye语料库对三种模型进行对比分析：一种是使用信息理论公式的NDL导出预测器，一种是传统的NDL预测器，另一种是N-gram概率预测器。

Result: N-gram模型在建模音频词持续时间中表现优于两个NDL模型。然而，将信息理论公式引入NDL模型后，其性能有所提升。

Conclusion: N-gram模型在建模音频词持续时间时表现优于NDL模型，反驳了仅凭认知动机就认为NDL更有效的假设。然而，将信息理论公式引入NDL可以提高模型性能。研究强调了结合频率、情境可预测性以及平均情境可预测性的必要性，同时强调了将信息理论的可预测性指标与辨别学习中提取的信息结合起来的重要性。

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [41] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: 利用手语生成技术提升手语翻译模型性能，最高可达19%。


<details>
  <summary>Details</summary>
Motivation: 由于手语数据集相较于发声语言数据集要小得多，为手语翻译系统提供高质量的数据是一项挑战。因此，研究旨在通过手语生成技术来增强数据集规模和翻译模型的性能。

Method: 采用骨架驱动的方法、符号拼接技术以及SignGAN和SignSplat两种生成模型来增加手语数据集的多样性。

Result: 结果表明，所提出的方法能够有效增加现有数据集，并使手语翻译模型的性能提高最多达到19%。

Conclusion: 研究表明，通过使用骨架驱动、符号拼接以及SignGAN和SignSplat两种生成模型的方法，可以有效扩大现有手语数据集，并提高手语翻译模型的性能。

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [42] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: RAPL 提出了一种高效的知识图谱检索框架，通过结构化推理增强语言模型的表现，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成能够通过外部知识缓解语言模型的知识更新滞后和幻觉问题，但现有方法普遍使用非结构化文本，限制了可解释性和结构化推理。知识图谱提供了一种结构化、紧凑的替代方案。

Method: 1. 两阶段标注策略：结合启发式信号和参数模型提供因果监督；2. 模型无关的图转换方法：捕捉关系三元组间的交互，增强表示能力；3. 基于路径的推理策略：支持从注入的知识中进行学习，并通过结构化输入支持后续推理。

Result: 在实验中，RAPL的性能优于现有最先进的方法，提升幅度为2.66%-20.34%，且显著缩小了较小和较大语言模型推理器间的性能差距，以及在跨数据集设置下的性能差距，突显了其卓越的检索能力和广泛适用性。

Conclusion: RAPL 在知识图谱问答中表现优异，超越现有最先进的方法，并缩小了不同规模的语言模型间的性能差距，以及在不同数据集上的性能差距。

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [43] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Main category: cs.CL

TL;DR: 本文开发了一个用于SemEval 2025任务中的表格数据问答系统，通过整合多模块和使用大型语言模型，显著提高开放源码模型准确性，适用于通过表格进行问答的任务。


<details>
  <summary>Details</summary>
Motivation: 开发一个集成系统，提高开放源码模型在表格数据上的问答准确性，并使性能与专有模型相当。

Method: 该系统整合了文本到SQL和文本到代码生成模块、自纠正机制以及检索增强生成（RAG），此外还包含一个端到端（E2E）模块，由大型语言模型（LLM）进行协调。

Result: 我们的方案在比赛评估阶段实现了80%的准确性，在38支参赛队伍中排名前13。

Conclusion: 本文提出了一种用于表格数据问答系统的集成解决方案，展示了开放源代码模型在准确性方面的显著提升，且其性能与专有大型语言模型在表格问答任务中相当。

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [44] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Main category: cs.CL

TL;DR: 提出了一种无需训练的内部置信度方法，用于检测知识边界，提升大语言模型的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要识别已知和未知查询的能力，这种能力有助于模型执行高效和可信的自适应推理，从而推动高效和可信AI的发展。

Method: 引入了一种名为“内部置信度”的新颖且无需训练的方法，通过跨层和跨令牌的自我评估来实现查询级别的不确定性检测。

Result: 在事实问答和数学推理任务的实证结果显示，内部置信度在多个基准上表现优异。此外，该方法可用于有效的RAG和模型级联，能够在降低推理成本的同时保持性能。

Conclusion: 提出了一种基于查询级别不确定性检测知识边界的方法，能够在不生成任何令牌的情况下确定模型能否处理给定查询。

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [45] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: 提出了一种有效的微调方法FT-UKE，以解决知识编辑中的局部性及性能问题，实验结果显示该方法显著优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法缺乏局部性评估和微调方法在知识编辑中的异常失败问题。提出系统评估局部性的方法以及训练处于最佳设定的FT方法。

Method: 构建了两个数据集UnKEBench-Loc和AKEW-Loc (CF)，用于系统评估模型编辑后的局部性，并进行实验找出影响FT方法性能的因素，为未来的训练提供方法指南。

Result: 基于最佳设定的FT-UKE方法在性能上优于当前的SOTA方法，特别是在批量编辑场景中，其优势随着批量大小的增长显著增加。

Conclusion: FT-UKE显著优于现有的SOTA方法，尤其是在批量编辑场景中表现出色，其优势随着批量大小的增加而扩大。

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [46] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Main category: cs.CL

TL;DR: 本文为大语言模型的不确定性量化提供了理论和方法支持，引入了基于逆模型的框架及新算法GAAP，实验结果表明其优于现有UQ方法。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化方法缺乏概率基础，本文旨在为LLM的不确定性量化提供理论依据，并开发可靠的UQ方法。

Method: 采用双重随机游走视角，将输入输出对建模为两个马尔可夫链，并通过语义相似性定义过渡概率，以此构建一个基于逆模型的完全概率框架，并引入遗传算法的扰动算法GAAP。

Result: 实验表明，Inv-Entropy在语义不确定性量化方面优于现有方法。

Conclusion: 本文提出了一种基于逆模型的完全概率框架，通过系统性扰动评估给定输出条件下输入空间的多样性来量化不确定性。

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [47] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 研究提出ComfyUI-R1模型来自动化生成AI内容工作流，表现优于当前领先封闭源码模型。


<details>
  <summary>Details</summary>
Motivation: 为了应对当前用户在创建AI生成内容的模块化工作流时所面临的陡峭学习曲线，提出了ComfyUI-R1以简化工作流生成。

Method: 论文使用了两阶段框架：（1）CoT微调以适应ComfyUI领域；（2）通过细粒度规则指标混合奖励引导的强化学习，确保格式有效性、结构完整性和节点级保真度。

Result: 实验显示，ComfyUI-R1模型在格式有效性、高通过率、节点级和图形级F1分数方面显著超过了使用封闭源码模型的方法。

Conclusion: ComfyUI-R1能够自动生成工作流，具有高格式有效性, 高通过率以及节点级和图形级F1分数，显著超越之前利用封闭源码模型的方法。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [48] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: The study examines whether LLMs can serve as human-like participants in test development. Results indicate larger LLMs may mimic human responses better, especially in reading, but are not yet suitable for zero-shot educational assessment piloting.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can be used as pilot participants in educational assessments to expedite test development and item quality evaluation.

Method: Evaluation of the human-likeness of 18 instruction-tuned LLMs using classical test theory and item response theory methodologies on reading, U.S. history, and economics test items.

Result: Larger models, though excessively confident, show more human-like response distributions with temperature scaling. LLMs correlate better with humans in reading comprehension items compared to other subjects.

Conclusion: LLMs correlate better with humans in reading comprehension but not strongly enough for piloting educational assessments in a zero-shot setting.

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [49] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: paper介绍了CoRT框架，用于优化大型推理模型与代码解释器的交互，提高数学推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型推理模型在处理复杂数学计算时效率和准确性较差。

Method: 引入了Hint-Engineering方法，通过策略性插入不同的提示来优化LRM-CI的互动，并进行后期训练。

Result: 通过Hint-Engineering方法，模型在数学推理任务上获得了显著的性能提升，同时减少了模型生成内容时使用的tokens数量。

Conclusion: CoRT框架能够有效和高效地利用代码解释器（CI），解决LRM在复杂数学操作中的不准确和低效问题。

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [50] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: 本文介绍了一个新的语音情感检测数据集和模型，依托合成音频和专家验证，实现了高精度情感识别，尤其在处理高唤醒情感方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的语音情感识别数据集在情感的细粒度上存在不足，隐私问题以及对表演情感的依赖。

Method: 利用最新的语音生成技术，创造出合成的音频片段，并由心理学专家验证，打上感知强度标签。

Result: 开发了Empathic Insight Voice模型，在情感识别方面与人类专家的高度一致性设立了新标准；结果发现高唤醒情感如愤怒比低唤醒状态如专注更易于检测。

Conclusion: 本文引入了一个新的语音情感检测资源EmoNet-Voice，开发了新的基准数据集和模型，并证明其在情感处理方面优越的表现。

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [51] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文介绍了错误引导姿态增强（EGPA）方法，通过模拟运动错误生成合成数据，并结合注意力图卷积网络，提高了康复评估的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 有效的康复评估对于监测患者进展至关重要，尤其是在家庭环境中。然而，现有系统通常面临数据不平衡和难以检测细微运动错误的挑战。

Method: 该论文引入了错误引导姿态增强（EGPA）的方法，这是一种通过模拟临床相关的运动错误来生成合成骨架数据的方法。EGPA结合了一种基于注意力的图卷积网络，以提高多个评估指标的性能。

Result: 实验表明，平均绝对误差减少了27.6％，错误分类准确性提高了45.8％。注意力可视化显示该模型能够学习关注临床重要的关节和运动阶段，增强了准确性和可解释性。

Conclusion: EGPA提供了一种有前途的方法，可以在临床和家庭康复环境中改善自动化运动质量评估。

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [52] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: 引入新的数据集和任务以解决图像与文本叙述不匹配的问题，提供基准结果并指出性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测图像与文本不匹配时不够准确，因此需要更有效的解决方案。

Method: 引入包含起源标记图像的新闻文章数据集，并设计了两个任务：起源位置相关性（LOR）和起源日期和时间相关性（DTOR）。

Result: 在六个大型语言模型上进行了基线测试，发现LOR的零样本性能有潜力，但DTOR的性能较低，需要进一步研究。

Conclusion: 该论文通过引入一个新的数据集来解决图像与文本叙述不匹配的问题，并在两个任务上提供基准结果。

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [53] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: 新因果框架优化了链式推理，提高了大型语言模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决CoT推理中的两个基本挑战：充要性和必要性，以提高复杂推理的效率和准确性。

Method: 提出一个因果框架，利用因果概率来识别并量化推理步骤的充要性，并在不同干预场景下优化推理过程。

Result: 在各种数学和常识推理基准上实验表明，提高了推理效率并减少了令牌使用，同时保持了准确性。

Conclusion: 通过引入因果框架，利用充要因果概率来改进CoT推理的效率和准确性，同时减少了不必要的推理步骤。

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [54] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出了一种检测幻觉的方法，利用分布距离作评分，无需外部知识或模型，表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 发现幻觉的响应比以事实为基础的响应与其提示的偏离更小，提出此方法以消除需外部知识或辅助模型的幻觉检测方法。

Method: 通过分析提示和响应隐藏状态分布之间的概率差异，使用分布距离作为幻觉评分的方法模型内在检测方法。引入了深度可学习核来自动调节，以捕捉分布之间细微的几何差异。

Result: 方法超过现有基准，在多个测试中表现出色。

Conclusion: 提出了一种新颖的方法来检测大语言模型中的幻觉，通过分析提示和响应隐藏状态分布之间的概率差异。这种方法在多个基准测试中表现出色，即使在没有进行核训练的情况下，仍然表现出竞争力，提供了一个稳健且可扩展的幻觉检测解决方案。

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [55] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: The paper explores how LLMs achieve multilingual capabilities through a core set of language-agnostic parameters, suggesting neuron-specific training as a promising strategy.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs achieve effective multilingual capabilities and challenge the assumption that they 'think' in English by exploring the role of core language-agnostic parameters.

Method: The paper identifies language-related neurons and categorizes them as shared or exclusive, analyzing their activation patterns during processing of various languages.

Result: Neuron-specific training strategies are proposed and supported by experiments across diverse LLM families, showing improved multilingual performance and abstract thinking abilities.

Conclusion: LLMs progressively develop a core language-agnostic parameter space, vital for their multilingual capacity. Shared neurons are key to this space, enabling abstract thinking beyond specific languages.

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [56] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 引入PersonaLens测试套件，评估AI助手在任务导向情景中的个性化能力，揭示了现有LLM助手的个性化性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化评估基准主要集中在闲聊、非对话任务或狭窄领域，无法捕捉个性化任务导向助手的复杂性。因此，有必要开发一个综合性的评估工具，以有效评估任务导向AI助手的个性化能力。

Method: 引入PersonaLens，一个用于评估个性化任务导向AI助手的综合性基准。该基准包括丰富的用户档案和互动历史，使用两个专门的基于LLM的代理：一个用户代理，用于与AI助手进行现实的任务导向对话；一个评估代理，采用LLM-as-a-Judge范例，评估个性化、响应质量和任务成功率。

Result: 借助丰富的实验，揭示了当前LLM助手在不同任务中个性化能力上的显著差异，为提升对话式AI系统提供了关键洞察。

Conclusion: PersonaLens提供了一种有效的方法来评估任务导向AI助手的个性化能力，揭示了现有LLM助手在个性化功能上的差距，为未来的研究指明了方向。

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [57] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Main category: cs.CL

TL;DR: 提出了一种新颖的总结系统ASESUM，能够自动总结评论中的主要观点并适应不同领域，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于大量的在线购物评论使得手动提取主要观点变得不切实际，因此需要自动化的观点总结系统。

Method: 使用ASESUM框架，通过提取与产品关键方面相关的论点，并评估其显着性和有效性来总结观点。

Result: 实验结果表明，ASESUM能够比现有的方法更好地捕捉评论中关于产品的多样化观点。

Conclusion: ASESUM框架在总结产品评论方面表现优越，能够结合支持性证据，自动生成基于方面的总结。

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [58] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 该研究提出了VerIF验证方法，通过强化学习提升指令跟随模型性能，并达到了最新水平，验证工程对模型提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 目前在指令跟随任务中，强化学习的最佳实践仍未充分探索。验证工程是强化学习中一个关键的挑战，本文试图通过提出有效的验证方法来解决这一问题。

Method: 本文提出了一种名为VerIF的验证方法，将基于规则的代码验证与来自大型推理模型（例如QwQ-32B）的LLM验证结合起来。为了支持这一方法，构建了一个名为VerInstruct的高质量指令跟随数据集，包含约22,000个带有关联验证信号的实例。

Result: 训练出的模型在多个指令跟随基准测试中达到了与其大小相当的模型的最新性能，并很好地泛化到未见约束。

Conclusion: 通过使用VerIF方法进行强化学习训练，模型在多个指令跟随基准测试中取得了显著的改进，并在未见约束下很好地泛化，表现出色。他们的总体能力保持不变，表明VerIF可以集成到现有的强化学习方法中以提高整体模型性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [59] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Main category: cs.CL

TL;DR: 本文提出QRHEAD和QR-RETRIEVER，使用改进的注意力机制提高长文本语言模型的检索性能，在多个任务上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高长文本模型的检索性能，特别是在处理复杂的多跳推理任务时，更准确地选择相关信息。

Method: 通过聚合与输入查询相关的注意力得分，识别出QRHEAD；通过累积QRHEAD的注意力质量，QR-RETRIEVER用作检索分数，以实现高效长文本推理。

Result: 在长文本推理任务LongMemEval和CLIPPER上，QR-RETRIEVER使用QRHEAD注意力质量作为检索分数，实现了超过10%的性能提升，并优于使用全上下文的推理模型。QR-RETRIEVER在BEIR基准测试中还表现出强大的零样本性能，超越其他基于LLM的重排模型。

Conclusion: QRHEAD和QR-RETRIEVER方法显著提高了长文本模型的检索性能，特别是在多跳推理任务中表现优异，并提供了关于长文本语言模型能力的解释性见解。

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [60] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: 研究提出一种名为Resa的新方法，通过稀疏自动编码器微调训练模型，以低成本、高效地提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 根据语言模型中的潜在表示，如何以成本效益高的方式激发其强大的推理能力。

Method: 使用一种新颖且高效的稀疏自动编码器微调(SAE-Tuning)过程，首先训练一个SAE从源模型中捕获推理能力，然后使用训练好的SAE指导标准的监督微调过程。

Result: 在减少训练成本超过2000倍至约1美元，并且训练时间减少超过450倍至约20分钟的情况下，SAE-Tuning保留了>97%其RL训练对手的推理性能。

Conclusion: 通过使用经过验证的问题回答数据并指导标准的监督微调过程，SAE-Tuning在保持高推理性能的同时大幅降低了训练成本和时间。

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [61] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 研究显示微调的LLM使得AI生成文本在社交媒体上更难被检测，尤其当攻击者不公开其生成模型时，检测的难度显著增加。


<details>
  <summary>Details</summary>
Motivation: 社交媒体可能被利用作为线上影响活动的重要攻击途径，AI生成文本可以大规模支持或反对特定政策或事件的活动。

Method: 通过利用开源、闭源和微调的LLM创建数据集，并进行消融实验和人类研究验证。

Result: 实验表明当攻击者不公开其微调模型时，AI生成文本的检测性大幅下降，人类研究也验证了这一结果。

Conclusion: 在缺乏对生成模型的充分了解和访问权限下，检测AI生成文本的可行性显著下降。

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [62] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Main category: cs.CL

TL;DR: 本文提出了一种改进大型语言模型依存句法分析性能的新方法，通过分步指令和多语言微调，在17种语言的依存数据集上实现了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 标准提示在生成结构有效且准确的输出时常常面临困难，特别是在依存句法分析中。我们希望能够提高此任务的模型性能。

Method: 我们提出了一种分步指令策略：先进行通用词性标注，然后预测句法头和依存关系标签，使用简化的类似CoNLL-U的输出格式。

Result: 我们的方法在17种语言的通用依存数据集上实现了最先进的准确性。同时，经过多语言微调后，跨语言的泛化性能得到改善。

Conclusion: 我们的研究表明，通过分步指令策略以及格式一致的方法，可以有效提升大型语言模型在句法分析任务中的表现。多语言微调能够增强跨语言的泛化能力。

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [63] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: 研究评估大型语言模型在处理塞尔维亚、克罗地亚和波斯尼亚语有毒评论中的表现，发现上下文增强可以显著提高检测效果，尤其是Gemini模型在精度和召回率之间实现了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 在线有害语言在受限的监管工具地区造成严重伤害，作者着眼于如何在塞尔维亚语、克罗地亚语和波斯尼亚语等标签数据稀缺的语言中处理这些问题。

Method: 作者构建并手动标注了一个包含4500条来自YouTube和TikTok视频评论的数据集，测试了四个语言模型，分别在零样本和上下文增强模式下进行评估。

Result: 在上下文增强模式中，Gemini模型在平衡精度和召回率方面表现最佳，F1得分和准确率均达到0.82；而零样本模式下的GPT-4.1在精度方面领先，并且误报率最低。

Conclusion: 通过添加最少的上下文可以提高低资源环境中的有毒语言检测效果，并且仅通过提示设计就能在巴尔干地区语言社区中实现显著提升。

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [64] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Main category: cs.CL

TL;DR: 提出了一种支持部分检测的SCM方法，使用FineHarm数据集训练，可以较早判断生成内容的有害性，实现较低延迟且效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于现有的内容审核大多采用完整检测方法来判断生成内容的有害性，这会导致较高的服务延迟。近期研究更多地关注部分检测，即在生成过程中中途监控，并在检测到有害性时提早终止输出，但直接将完整检测训练的审核器应用于不完整输出，导致训练和推理之间存在差距，降低了性能。

Method: 1. 构建FineHarm数据集，包含29K个带有细粒度注释的提示-响应对，用于令牌级训练。2. 提出流媒体内容监控器(SCM)，并通过对响应和令牌级别标签的双重监督进行训练，可以在生成过程中实时判断有害性。

Result: 实验显示，SCM在宏观F1分数上获得了超过0.95的分数，这与完整检测相当，而平均只观察了响应的前18%的令牌。此外，SCM还能作为伪有害性注释器来提升安全对齐，导致无害性评分高于DPO。

Conclusion: 提出了一种名为流媒体内容监控器(SCM)的方法，通过对响应和令牌级别标签进行双重监督训练，能够实时判断输出的有害性。实验结果表明，SCM在宏观F1评分上能够达到与完整检测相当的0.95分，仅需观察响应的前18%令牌即可。此外，SCM还可用作伪有害性注释器来提高安全对齐，且在无害性评分上超过DPO。

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.AI

TL;DR: 提出了自适应干预机制（AIM），一种新的互动模仿学习算法，可以减少专家监控负担，提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 目前的互动模仿学习方法对人类监督员的认知需求较高。我们希望开发一种可以减轻人类监督员认知负担的算法。

Method: 我们提出了一种名为自适应干预机制（AIM）的新型机器人引导的互动模仿学习算法，该算法通过一个代理Q函数模仿人类干预规则，并基于代理和人类行为的一致性来调整干预请求。

Result: AIM 在专家参与的实验中显著减少了人工监控的工作量，还能更有效地识别需要专家协助的安全关键状态，从而收集更高质量的专家演示数据，减少所需的总体专家数据和环境交互。

Conclusion: AIM 显著减少了专家在连续和离散控制任务中的监控工作量，并比基于不确定性的方法 Thrifty-DAgger 提高了40%的人工接管成本和学习效率。

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [66] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Main category: cs.AI

TL;DR: 本文指出，Shojaee等人对于大规模推理模型准确性崩溃的研究主要反映了实验设计的问题而非推理的根本失误。


<details>
  <summary>Details</summary>
Motivation: 质疑Shojaee等人关于大规模推理模型在复杂规划难题上表现不佳的实验结果。

Method: 通过控制实验设计偏差，重新进行实验，发现较大的准确性。

Result: 通过纠正实验设计中的失误，表明模型在之前被报告为完全失败的汉诺塔实例上可以达到高准确性。

Conclusion: 本文强调了在评估AI推理能力时，实验设计的严谨性的重要性。

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [67] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 论文提出LLM-HAS模型，强调人机协作优于全自治AI，认为AI的价值在于增强而非取代人类能力。


<details>
  <summary>Details</summary>
Motivation: 质疑构建全自治AI代理的做法，提出人类与AI协作的LLM-HAS模型，以解决当前自治系统在可靠性、透明性及理解人类需求上存在的问题。

Method: 以人机协作系统为研究对象，提出LLM-HAS模型，提供实际解决方案，并在医疗、金融、和软件开发领域进行案例分析。

Result: 人机协作模式比AI单独工作能更好地应对复杂任务，该方法提高了系统的可信度和适应性。

Conclusion: 通过人机协作系统增强人类的能力而非取代人类角色，是AI未来最有前途的发展方向。

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [68] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni是一种能够处理图像、文本、音频和视频的统一多模态模型，支持音频和图像生成，开源以促进社区进一步研究。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个能够同时处理图像、文本、音频及视频，并在语音与图像生成方面表现优异的统一多模态模型。

Method: Ming-Omni采用专门的编码器从不同模态中提取标记，这些标记由科学架构Ling处理，Ling配备了新提出的模态特定路由器。

Result: 实验结果显示，Ming-Omni在所有模态上提供了强大的跨模态解决方案，支持音频和图像生成，并能进行上下文聊天、文本到语音转换和多种图像编辑。

Conclusion: Ming-Omni作为一个开源模型，提供了一种强大的跨模态统一感知和生成解决方案，支持多种任务而无需单独模型或任务特定微调。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [69] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng,Jinfeng Zhou,Hongning Wang*

Main category: cs.AI

TL;DR: 大语言模型表现出部分人类有限理性特征，但需要改进训练方法以增强灵活的对手建模和环境动态感知能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型与人类在战略决策中的表现，以了解它们是否表现出类似于人类的有限理性特征。

Method: 通过在相同的实验条件下将大语言模型放入与行为博弈论研究直接适配的实验范式中进行比较分析。

Result: 大语言模型再现了人类常见的启发式策略，如基于结果的策略转换和在未来互动可能性下增加的合作，但它们更严格地应用这些规则，对游戏环境的动态变化敏感性较弱。

Conclusion: 当前的大语言模型仅捕获了部分的人类有限理性特征。

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [70] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon,Hyeonseo Cho,Yoshua Bengio,Sungjin Ahn*

Main category: cs.AI

TL;DR: 提出了Fast-MCTD，通过并行和稀疏优化大幅提高了MCTD的速度和可扩展性，在保持甚至提升性能的同时实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 尽管MCTD在复杂规划问题中表现优异，但其由于树搜索的顺序性和迭代去噪的计算成本而导致了显著的计算开销。

Method: Fast-MCTD结合了两种技术：Parallel MCTD，实现了通过延迟树更新和冗余感知选择的并行滚动；Sparse MCTD，通过轨迹粗化减少了滚动长度。

Result: Fast-MCTD在某些任务中推理速度甚至超过了无需搜索的Diffuser，同时提供了更强的解决方案。

Conclusion: Fast-MCTD在保持或提高规划性能的同时，实现了对标准MCTD的多达100倍的加速。

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [71] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.AI

TL;DR: DipLLM 是一种用于外交游戏的微调大型语言模型，通过减少数据需求并采用自回归因式分解框架，能够在多人游戏中实现复杂战略决策。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型提供了一种很有前景的替代方案，可以利用预训练知识，通过相对小规模的微调实现强大性能。

Method: DipLLM 使用自回归因式分解框架，将多单元动作分配任务简化为单元级决策的序列。通过在该框架内定义平衡政策作为学习目标进行微调。

Result: 微调模型仅使用 Cicero 模型所需数据的 1.5%，就超越了其性能。

Conclusion: 微调后的大型语言模型可以在多人游戏中实现复杂的战略决策。

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [72] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng,Hengshu Zhu,Chuan Qin,Han Wu,Yihang Cheng,Sirui Zhang,Xiaowei Jin,Yinuo Shen,Zhenxing Wang,Feimin Zhong,Hui Xiong*

Main category: cs.AI

TL;DR: 本文综述AI代理系统的价值对齐问题，探讨了价值原则、应用场景及价值对齐评估方法，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术和应用场景的复杂化，特别是在多代理决策和任务协作中，出现了越来越多的情境性和系统性风险。因此，关注AI代理的价值对齐，确保其行为符合人类价值和社会规范，成为了一项重要的研究任务。

Method: 本文通过文献综述的方法，结合大型模型驱动的AI进展与社会治理需求，系统性地审查了代理系统的价值原则、应用场景以及价值对齐评估，并探索了多代理系统中的价值协调。

Result: 本文将价值原则从宏观、中观到微观进行层次化组织，对代理系统应用场景从一般到具体进行分类和回顾，并对代理价值对齐评估进行了系统性的审查，探讨多代理系统中的价值协调问题，并提出多个未来的研究方向。

Conclusion: 随着人工智能技术的进步，特别是大型语言模型的应用，AI代理的价值对齐成为研究重点，旨在确保AI代理的目标、偏好和行为与人类价值和社会规范一致。本文综述了在特定应用场景中的代理系统价值对齐问题，并探讨了代理系统价值原则、应用场景以及价值对齐评估方法，同时提出了多个未来研究方向。

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [73] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: 这项研究提出了意图分解生成方法，提高了大型语言模型在多样性和质量方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的增加多样性的方法往往只在token级别操作，导致在推理问题上的探索性差以及对话代理的重复性。

Method: 提出的意图分解生成（IFG）方法将采样过程分为两个阶段：首先采样语义密集的意图，其次在原始提示和第一阶段的意图下采样最终响应。

Result: 该方法在数学和代码任务上提高了pass@k和基于验证器反馈的强化学习效果，同时结合直接偏好优化增加了对话多样性而不影响奖励，并在一般语言建模任务中维持生成质量的同时实现更高多样性。

Conclusion: 提出了一种简单方法，可以在不影响表现的情况下提高语言模型的采样多样性。

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [74] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou,Antonio Rago,Maria Vanina Martinez,William Yeoh*

Main category: cs.AI

TL;DR: 研究发现，人们偏爱解释导向的信念修订，对AI系统设计有影响。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够有效模拟并与人类推理相一致的AI系统，理解人类在面对新的信息时如何修改其信念是至关重要的。认知心理学的实证证据表明，人们在面对冲突信息时可能遵循不同的模式，而不是依赖理论信念修正框架中所依赖的一组原则。

Method: 通过三个综合用户研究，系统地调查了人们如何在面对不一致时使用解释来修改信念，包括他们是被提供了解释还是自己进行解释。

Result: 研究结果表明，人们在不同情境下表现出一种对于解释引导的信念修订的持续偏好，这种偏好导致信念系统的改变不一定被经典信念变更理论所捕捉。

Conclusion: 发现表明设计用于模拟人类推理或与人类互动的AI系统应该能够适应基于解释的、可能是非最小化的信念修正操作，以更好地与人类认知过程保持一致。

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [75] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: 该研究利用互联网视频数据和少量机器人交互数据，通过自监督学习开发出一个用于物理世界规划的AI模型，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代AI面临的主要挑战是通过观察来理解世界和学习行动。

Method: 首先在由超过100万小时的互联网视频组成的视频和图像数据集上进行预训练，然后与大型语言模型进行对齐，并在机器人任务上应用潜在的动作条件世界模型进行后训练。

Result: 在运动理解和人类动作预测上表现强劲，并在多个视频问答任务中达到最先进的性能。在机器人规划任务中成功实现零样本部署。

Conclusion: 这项研究证明了通过网络规模的数据和少量的机器人交互数据进行自监督学习，可以开发出能够在物理世界中进行规划的世界模型。

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [76] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 提出了基于元学习的大规模多模态模型增强方法，通过注意力映射模块和软提示来提高模型在低数据条件下的任务适应性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于上下文学习（ICL）的模型性能不稳定，尤其是在较小的大规模多模态模型中，增加示例数量并不能保证性能持续提升。本文认为这是因为模型被图像嵌入中的多余信息所淹没，这些信息与后续任务无关。

Method: 本文引入了一种注意力映射模块，该模块可以与流行的大规模多模态模型架构如LLaVA v1.5集成，帮助从图像特征中提取任务相关的软提示。

Result: 在VL-ICL Bench上的评估表明，即使在图像扰动的情况下，本文的方法在任务诱导和推理能力上显著优于ICL和相关的提示微调方法。

Conclusion: 本文提出了一种基于元学习的方法，通过从任务相关的图像特征中提取软提示（prompts），可以在低数据的情况下增强大规模多模态模型的稳健性和任务适应性。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [77] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Main category: cs.MA

TL;DR: ISEK系统利用去中心化网络和区块链技术，推动人类与AI代理平等协作，实现自组织大型认知系统的进化。


<details>
  <summary>Details</summary>
Motivation: ISEK旨在创建一个去中心化的网络，促进人类和人工智能代理的平等协作，以实现自组织的认知生态系统。

Method: ISEK系统通过创新的协调协议和六阶段工作流（发布、发现、招募、执行、结算、反馈）来动态分配任务，利用原生$ISEK代币进行经济激励，并通过NFT身份管理维护代理自主权。

Result: ISEK结合区块链技术、人工智能和激励机制，提供了一种基础结构，该结构可以积极促进智能的自然涌现与发展。

Conclusion: ISEK的系统基础设施实现了一种新的智能进化模式，使自治代理能够在去中心化的网络中自主协同发展，超越传统中心化平台的限制。

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [78] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: 研究奖励机制与异质性团队的关系，通过MARL验证异质性团队在特定情境下的表现优势。


<details>
  <summary>Details</summary>
Motivation: 探索在何种情况下异质性团队会优于同质性团队，特别是在多代理任务分配的问题中，从奖励设计的角度研究适合异质性团队的目标类型。

Method: 本文采用了多代理强化学习（MARL）作为计算范式，并引入了异质环境设计（HED），这是一种基于梯度的算法，用于优化未指定的MARL环境的参数空间，以发现异质性有利的情景。实验在矩阵游戏和具体现多目标捕获环境中进行。

Result: 通过理论分析和实验验证，发现异质性团队可以在某些奖励机制下表现更优。特别地，曲率可以决定异质性是否能够增加团队的全球奖励，而广泛奖励族中这只需进行简单的凸性测试。

Conclusion: 研究表明，通过设计合适的奖励机制可以使异质性团队在多代理任务分配中表现得比同质性团队更好。本文的成果验证了理论对奖励机制的预测，并建立了与MARL奖励设计的联系。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


### [79] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: 本文提出了一种新的威胁模型和CRAFT系统，以评估和提高遵循政策的代理的稳健性，发现现有防御策略尚需加强。


<details>
  <summary>Details</summary>
Motivation: 确保遵循严格政策的代理在面对恶意用户时能够抵御攻击，同时保持自然互动。

Method: 我们提出了一种新的威胁模型，并介绍了CRAFT系统，通过使用策略感知的劝说策略对抗遵循策略的代理。同时引入了tau-break基准测试以评估代理的稳健性。

Result: CRAFT系统超越了传统的越狱方法，并通过tau-break基准测试评估了代理的稳健性，发现现有防御不足以提供全面保护。

Conclusion: 现有的简单防御策略无法完全保护遵循策略的代理不受恶意攻击，强调了需要更强大的研究驱动的保障措施。

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Main category: cs.LG

TL;DR: 本研究提出LlamaAffinity模型，在抗体-抗原结合亲和力预测方面优于现有方法，且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 由于传统的亲和力测量实验方法耗时且昂贵，人工智能的到来特别是大语言模型在抗体表示中的应用，开辟了AI设计和亲和力预测的新途径。

Method: 本研究提出了一种先进的抗体-抗原结合亲和力预测模型LlamaAffinity，利用开源的Llama 3框架以及Observed Antibody Space (OAS)数据库中的抗体序列数据。

Result: 该模型在多个评估指标上优于现有的最先进方法，取得了准确率0.9640，F1-score为0.9643，精度为0.9702，召回率为0.9586，AUC-ROC为0.9936。此外，该策略展示了更高的计算效率，平均约五倍的累计训练时间仅为0.46小时。

Conclusion: 研究表明，基于Llama 3骨架的LlamaAffinity模型在抗体-抗原结合亲和力预测方面表现优异，相较于现有的AntiFormer、AntiBERTa、AntiBERTy方法取得了显著提高。

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [81] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Main category: cs.LG

TL;DR: FinHEAR提升了语言模型在金融决策中的表现，优于传统基准方法。


<details>
  <summary>Details</summary>
Motivation: 强大的语言模型通常难以准确捕捉人类金融决策中的行为模式。

Method: 通过多代理框架FinHEAR进行分析和预测，使用专家指导的检索、信心调整的位置大小及基于结果的优化。

Result: FinHEAR在趋势预测和交易任务中表现优异，不仅精确度更高且风险调整收益更好。

Conclusion: FinHEAR在金融领域的决策中展现出提升的解释能力与稳健性。

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [82] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Main category: cs.LG

TL;DR: PageLLM uses reward-based fine-tuning to optimize search results, showing a 0.44% GMV boost in user tests.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of fine-tuning large language models for Whole Page Optimization without relying on expensive human-annotated data, which is necessary to overcome issues like hallucinations and model instability.

Method: The paper proposes a reward-based fine-tuning approach called PageLLM, which uses mixed-grained rewards combining page-level and item-level evaluations to fine-tune large language models for better search and recommendation presentation.

Result: PageLLM outperforms existing baselines and shows a 0.44% increase in GMV during an online A/B test with over 10 million users, indicating its practical effectiveness.

Conclusion: PageLLM demonstrates improved performance in Whole Page Optimization by effectively utilizing a reward-based fine-tuning mechanism, resulting in a significant 0.44% GMV increase.

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [83] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出结合LLM和ML的合作框架解决特征转化问题，提高性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在特征转化中有潜力，但面临稳定生成和有效生成的挑战，当前传统机器学习和大型语言模型各有不足。

Method: 提出一种结合大型语言模型（LLM）符号生成和机器学习（ML）梯度优化的合作框架，包括四个步骤：黄金实例生成、特征转化序列嵌入与搜索、学生LLM特征转化、LLM-ML解码合作。

Result: 在各种数据集的实验中，该方法实现了5%的下游性能提升，同时错误率减少近半，证明了合作策略的效率和鲁棒性。

Conclusion: 合作框架在各种数据集的实验中表现出了有效性和鲁棒性，能提升下游性能并显著减少错误率。

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [84] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Main category: cs.LG

TL;DR: 研究提出一种结合学习机制的尖峰神经网络模型，弥合认知与生物模型差距，揭示神经活动与行为之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有的漂移扩散模型和泊松计数模型虽然能有效描述决策任务中的证据积累过程，但缺少学习机制且局限于参与者已有类别知识的任务。为弥合认知模型及生物学模型之间的差距，提出集合学习机制的生物学上合理的SNN模型。

Method: 研究提出了一种尖峰神经网络（SNN）模型，该模型中的神经元活动通过一个多变量霍克斯过程来建模。模型还设计了一项在线分类任务来评估预测结果。

Result: 研究展示了DDM和泊松计数模型间的耦合结果，指出这两个模型在分类及反应时间上表现相似，并且DDM可由尖峰泊松神经元来近似。同时，揭示了一个带相关噪声的DDM可通过一个受局部学习规则支配的霍克斯尖峰神经网络导出。

Conclusion: 这项研究通过整合生物学和认知模型，提出了一种生物学上合理的尖峰神经网络模型，该模型结合了学习机制，为神经活动与行为之间的关系提供了更深入的理解。

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [85] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Main category: cs.LG

TL;DR: 这篇论文分析了一种增强的异步AdaBoost框架在联邦学习中的应用，显著提高了通信效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于加强异步AdaBoost框架在五个不同领域中的应用能力，包括边缘设备上的计算机视觉、基于区块链的模型透明性、移动设备个性化、物联网异常检测和联邦医疗诊断。

Method: 文章提出了一种增强的异步AdaBoost框架，集成自适应通信调度和延迟权重补偿，以降低同步频率和通信开销，同时保持或提高模型准确性。

Result: 经验结果显示，与基线AdaBoost相比，训练时间减少20-35%，通信开销减少30-40%，并在显著更少的迭代中实现收敛。

Conclusion: 整体而言，改进型AdaBoost在不同的联邦学习场景中表现出显著提高的效率和鲁棒性，表明该方法具有广泛的适用性。

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [86] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架用于变分推断，通过一种耦合变分自编码器（CVAE），显著提高了模型处理重尾分布的能力，在图像重构任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决当前变分推断技术在面对重尾分布时缺乏有效性的问题。

Method: 本文使用耦合自由能等于耦合证据下界（ELBO）的方法，结合Fisher信息度量的耦合广义和仿射连接，设计了一种耦合变分自编码器（CVAE）。

Result: 在CelebA图像上的重构实验表明，CVAE在经历5个训练周期后比传统VAE有3%的改进。

Conclusion: 本文提出的CVAE能够更有效地处理尾部分布，从而提高了模型的准确性和稳健性。

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [87] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Main category: cs.LG

TL;DR: FSR框架使LLMs能生成高效、正确的CUDA代码，显著提升GPU内核性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面展现了强大的能力，但是对于深度硬件特定、架构感知和性能关键的代码生成仍然是个复杂的挑战，特别是在大规模并行GPU上。

Method: 提出了一种名为“特征搜索与强化（FSR）”的新框架，该框架联合优化编译和功能正确性，以及通过在目标GPU上测量的实际内核执行延迟来优化运行时性能。

Result: 使用FSR评估代表性的CUDA内核，涵盖了AI工作负载和计算密集型算法。结果表明，FSR增强的LLM可以保证正确性率，自动生成的内核在执行速度上可以比人类编写的代码快高达179倍。

Conclusion: 结合LLMs与性能强化的潜力，可以自动化面向硬件特定、架构敏感和性能关键的GPU编程。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [88] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Main category: cs.LG

TL;DR: 提出了LwPTV方法，通过层次剪枝显著提高了OOD任务性能，同时保持了ID任务能力。


<details>
  <summary>Details</summary>
Motivation: 当前多任务学习（MTL）中的模型合并方法主要关注于提升在域内（ID）数据集上的性能，对于域外（OOD）数据集的效果关注较少。因此，研究一种在保证ID任务能力的同时显著提升OOD任务性能的方法具有重要意义。

Method: 提出了一种称为LwPTV（Layer-wise Pruning Task Vector）的方法，通过构建显著性评分来衡量任务向量中参数的冗余度。使用该评分，每个任务生成掩码向量，从而在任务向量上执行层次剪枝，仅保留合并模型中对应层的预训练模型参数。该方法能够与现有的大多数模型合并方法无缝集成，从而提高其在OOD任务上的性能。

Result: 广泛的实验表明，应用LwPTV方法后，在保持ID任务能力的同时能够显著提升OOD任务性能。

Conclusion: LwPTV方法解决了当前多任务学习模型合并中对OOD数据集关注不足的问题，实现了性能上的提升。

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [89] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出一种利用生成概率的正则化策略以提高奖励模型的细粒度信号，从而提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前的奖励模型依赖于对整体响应的评分来学习奖励，而这些评分往往是粗粒度的，无法明确出与评分相关的具体部分，导致模型在未见过的响应上泛化效果差。因此，研究的动机在于提供细粒度的信号以优化奖励学习。

Method: 本文在Bayesian框架下分析，开发了一种轨迹内一致性正则化方法。此方法确保相邻的生成过程具有更高的一致性，从而推测在更高的下一个token生成概率时应维持一致性奖励，将这个正则化应用于先进的结果奖励模型中。

Result: 文中方法提高了奖励模型在RewardBench上的性能，并且体现出更好的DPO对齐策略和最佳N选优推理时间验证结果。

Conclusion: 文中提出的方法通过在响应轨迹中利用生成概率来建立奖励一致性，从而改进现有的奖励模型的性能，并提高在RewardBench上的效果。此外，通过该方法训练的奖励模型能生成更好的DPO对齐策略，并在N选优推理验证中表现更佳。

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [90] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: 研究揭示大型语言模型中，记忆与泛化之间存在权衡关系。小模型擅长泛化，而大模型擅长记忆。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型中记忆与泛化之间的关系，这在研究领域中仍未完全理解。

Method: 预训练一系列容量受限的Transformer模型，通过设计两个合成字符级任务分别探测模型的记忆与泛化能力。

Result: 小模型可以泛化未见的算术情况但无法记忆事实，而大模型可以记忆但无法泛化。容量中等的模型表现出类似的记忆偏向。联合训练两种任务时，没有模型在算术泛化上取得成功。

Conclusion: 预训练可能本质上更偏向某一种学习模式，模型的容量影响学习行为。研究对小型语言模型的设计和部署有更广泛的影响。

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [91] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: 提出了一个新的威胁模型以对比分析越狱攻击方法，发现成功率低于预期并且离散优化方法效果显著。


<details>
  <summary>Details</summary>
Motivation: 目前存在的越狱攻击方法虽成功获取目标输出，但在流利性和计算成本上却存在较大差异。为了对这些方法进行原则性的比较，需要提出一个统一的威胁模型。

Method: 建立一个基于1兆标记的N-gram语言模型，用于在文本分布中评估越狱攻击的可能性。该模型是LLM不可知的、非参数化的，且本质上是可解释的。然后将现有的攻击方法适配到该威胁模型中进行比较。

Result: 攻击对当前安全调优的模型的成功率低于之前预期，且基于离散优化的攻击显著优于最近的LLM攻击。有效攻击倾向于利用和滥用不频繁的二元组。

Conclusion: 越狱攻击成功率不如预期，离散优化攻击效果更好，并且对攻击进行了可解释和综合的分析。

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [92] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 本研究提出了FSL-Net神经网络，可以准确定位大型高维数据中的特征转移，改善数据分析质量，代码已开放。


<details>
  <summary>Details</summary>
Motivation: 在涉及医疗、生命科学、社会经济、金融、调查、多传感器数据等应用中，数据源之间的特征转移时常出现，这是由于数据源不统一、数据测量噪音或处理标准化流程不一致导致的。这种特征转移可能导致数据特征错误，从而影响下游分析效果，定位这些转移的特征对于解决转移的根本原因和纠正或过滤数据以避免损害后续分析是至关重要的。

Method: 本文提出了一种称为特征转移定位网络（FSL-Net）的神经网络，用于快速准确地在大型高维数据集中定位特征转移。该网络经过大量数据集训练，学习提取数据的统计属性，并且能够在不需要重新训练的情况下从以前未见过的数据集和转移中定位特征转移。

Result: FSL-Net经过大量数据集训练，可以在之前未见过的数据集和特征转移中准确定位特征，而无需重新训练。

Conclusion: FSL-Net能够有效地解决大型高维数据集中的特征转移定位问题，提供快速且准确的方法来识别特征移位的起源，从而改善后续数据分析的质量。

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [93] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Main category: cs.LG

TL;DR: UPQ框架成功实现了指令调优LLM的2-bit量化，并在MMLU和IFEval基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLM)的快速扩展，对资源有限设备的部署带来了显著挑战，因此对极低比特量化的兴趣在增长，如2-bit。然而现有工作仅限于预训练LLM，未扩展到指令调优模型。

Method: 提出了一种名为Unified Progressive Quantization (UPQ)的量化框架，通过逐渐量化（FP16→INT4→INT2）结合块级后训练量化(PTQ)与基于蒸馏的量化感知训练(Distill-QAT)，用于INT2指令调优LLM量化。

Result: UPQ成功将指令调优的LLM从FP16量化到INT4，并通过Distill-QAT达到INT2的量化，使生成的响应与原始FP16模型一致，同时在MMLU和IFEval基准上达到先进性能。

Conclusion: UPQ可以在不依赖专有后训练数据的情况下，将开源指令调优LLM量化到INT2，并在MMLU和IFEval两个最具代表性的基准上实现了最先进的性能。

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [94] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: MetaTT是一种新的Tensor Train适配器框架，通过减少参数量实现与LoRA相似的精度，并且优于其他张量微调方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在微调预训练Transformer时独立调整每个权重矩阵带来的参数膨胀问题，提出一种能够压缩参数且维持精度的统一适配器框架。

Method: MetaTT使用一种共享的张量列车（TT）框架，通过索引结构轴（如层和矩阵类型）来因式分解所有Transformer子模块（查询、键、值、投影和前馈层），并可选择性地考虑头和任务。与LoRA相比，MetaTT根据模的总和而不是模的乘积来增加参数，从而显著压缩最终适配器。

Result: 在标准语言建模基准测试中，MetaTT能显著减少参数量，同时与LoRA保持相似的准确性，并且能超过其他张量方法。

Conclusion: MetaTT为预训练Transformer的低秩微调提供了一种有效的工具，能够在很多任务中共享适配器而无需重新设计核心张量。

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [95] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM是一种新的传感器-语言基础模型，改善了穿戴式传感器数据与自然语言的理解和应用。


<details>
  <summary>Details</summary>
Motivation: 穿戴式传感器数据与语言的对齐和解释具有挑战性，因为缺乏配对的、丰富注释的传感器文本描述。

Method: 引入分层字幕生成流水线，捕捉传感器数据的统计、结构和语义信息，并扩展了重要的多模态预训练架构。

Result: SensorLM显示出扩展行为、标签效率、传感器字幕生成以及对未见任务的零样本泛化能力。

Conclusion: 实验表明，SensorLM在零样本识别、少样本学习和跨模态检索方面的表现优于现有技术。

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [96] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: CodeBrain通过引入TFDual-Tokenizer和EEGSSM以提高EEG建模的可推广性和解释性，为神经科学研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 针对传统任务特定模型的局限性和现有EFM模型的缺陷，提出CodeBrain以提高可转移性和多尺度脑依赖捕获效率。

Method: 提出了一种称为CodeBrain的高效EFM模型，该模型与大脑结构对齐，分两阶段训练。

Result: 通过对10个公共EEG数据集的全面实验，证明了CodeBrain在线性探测中的可推广性。

Conclusion: CodeBrain通过生物启发和可解释的EEG建模，为未来的神经科学研究奠定了基础。

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [97] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: TRACE通过将时间序列嵌入与文本对齐实现跨模态检索，具有信息丰富的上下文 强化了下游模型，提升了绩效。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列数据检索方法缺乏语义基础，难以对齐异构模态，且处理多通道信号的能力有限，因此需要一种新方法来解决这些局限性。

Method: 提出TRACE，一种多模态检索系统，通过将时间序列嵌入与文本语境对齐，支持多种跨模态检索模式，并利用强负采样促进语义关联。

Result: TRACE在下游预测和分类任务中表现出先进的性能，提升了模型的解释性和准确性。

Conclusion: TRACE是一种有效的多模态检索工具，通过结合时间序列与文本情境，实现了跨模态检索的语义关联，在多领域下提升了预测准确性和可解释性。

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [98] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Main category: cs.LG

TL;DR: 本文提出的BSA-TNP架构实现了高效的训练和推理，可以在不牺牲精度的情况下提高现代神经过程的可扩展性，特别针对平移不变过程。


<details>
  <summary>Details</summary>
Motivation: 面对应用领域不断增加的复杂性和对数据饥渴的问题，现代的神经过程在追求可扩展性的同时常常损失精度。该研究旨在证明对于某些过程，这种取舍并非必要。

Method: 提出了一种新的架构-Biased Scan Attention Transformer Neural Process (BSA-TNP)，该架构引入了Kernel Regression Blocks (KRBlocks)、群不变注意力偏差和内存高效的Biased Scan Attention (BSA)。

Result: BSA-TNP架构在保持或超越最佳模型精度的情况下，通常能在更短的时间内完成训练，并能够：1）表现出平移不变性，可以同时以多种分辨率进行学习，2）透明地模拟在空间和时间中演化的过程，3）支持高维固定效应，4）可以良好地扩展，在单个24GB GPU上仅需不到一分钟即可用逾100万个测试点和10万上下文点进行推理。

Conclusion: BSA-TNP提供了一种高效的解决方案，可以在不牺牲精度的情况下实现可扩展性，特别是在完全或部分平移不变的过程中。

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [99] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出了一种新的LLM代理框架，通过上下文学习、原子事实增强和递归前瞻搜索来增强计划能力，使得代理在复杂交互环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能难以适应新信息或在不进行微调的情况下有效利用过去的经验进行多步骤推理。

Method: 通过上下文学习增强计划能力，使用原子事实增强和递归前瞻搜索。我们的代理从交互轨迹中提取任务关键的“原子事实”，这些事实动态增强了对负责行动建议、潜在世界模型模拟和状态价值评估的LLM组件提供的提示。

Result: 我们的方法使得代理能够在线改进其理解和决策能力，利用其经验优化行为而无需更新权重。

Conclusion: 我们的代理在具有挑战性的交互任务中表现出色，随着经验的积累，实现了更优的行为。

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [100] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet is an open-source benchmark and ecosystem for evaluating vision-language-action models with a large dataset and standardized protocols, aiding research on model generalization.


<details>
  <summary>Details</summary>
Motivation: To support the development and evaluation of general-purpose multimodal agentic systems by combining vision, language, and action capabilities.

Method: Design and implementation of MultiNet, a benchmark and software ecosystem, including standardized evaluation protocols and an extensive composite dataset.

Result: Creation of a comprehensive benchmark, framework, and toolkit that supports research into the generalization limitations of vision-language-action models.

Conclusion: MultiNet represents a significant advancement in evaluating and adapting multimodal action models, providing a fully open-source benchmark for vision-language-action domains.

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [101] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Main category: cs.LG

TL;DR: 介绍了一种通过贪婪树搜索进行信息采集的策略CuriosiTree，能在临床诊断模拟中以低成本更有效地选择诊断动作序列。


<details>
  <summary>Details</summary>
Motivation: 决策者通常缺乏足够的信息来做出自信的决策，并且需要通过咨询权威人士或进行实验来获取必要的信息，但信息获取的成本不同，从而需要选择既有信息价值又经济的行动。

Method: 提出了一种名为CuriosiTree的启发式工具，通过在大语言模型（LLMs）中进行零样本信息采集，使用贪婪树搜索来估计每个动作的预期信息增益，并战略性地选择动作以平衡信息增益与相关成本。

Result: 实验证明，CuriosiTree在临床诊断模拟中能够实现成本效益高的异质信息整合，并在选择有效诊断的动作序列方面优于基线动作选择策略。

Conclusion: CuriosiTree可以在信息获取过程中提供一种有效的方法，帮助决策者在众多选择中找到平衡点，从而提高诊断的准确性。

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [102] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Main category: cs.LG

TL;DR: 提出一种新的FNF主干网络和DBD架构，实现了在时空建模中的卓越性能，不需要辅助技术，在多个领域的数据集上取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有多元长时间序列预测方法主要使用来自自然语言处理或计算机视觉的通用主干网络，未能充分考虑时间序列的独特性质，尤其是时间序列的周期性。因此需要设计具有时间特定归纳偏差的专用主干网络。

Method: 引入了FNF作为主干网络，能够统一处理局部时域和全局频域信息，并能自然扩展到空间建模。采用信息瓶颈理论证明，DBD架构在梯度流动和表现力方面优于现有架构。

Result: 新方法在五大领域的11个公开基准数据集上取得了优异的性能，且保持一致的超参数设置，无需辅助技术。

Conclusion: 提出了FNF作为主干网络和DBD作为架构，大大提升了时间序列建模的能力，特别是在处理时空建模时展现出卓越性能。

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [103] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Main category: cs.LG

TL;DR: 提出一种新的强化学习方法，使用人类评分推断奖励函数，并在若干实验中取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法通常通过孤立的任务来简化人类推理的模型化，而人类在做决策时会整合多种策略。因此，我们希望开发一种能够模仿人类决策过程的强化学习方法。

Method: 我们提出了一种新的强化学习方法，通过在无需奖励的环境中利用人类评分来推断奖励函数，并引入可学习的权重，平衡分类和回归模型的贡献。

Result: 实验结果表明，我们的方法始终优于现有的基于评分的强化学习方法，甚至在某些情况下超越了传统的强化学习方法。

Conclusion: 我们提出的方法在一系列实验中表现优异，尤其是在一些情况下优于传统的强化学习方法。

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [104] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Main category: cs.LG

TL;DR: LaDCast是一个新型的全球潜伏扩散框架，用于中期集合天气预测，成功改进了预测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的数值天气预报和机器学习方法在准确性和不确定性量化方面面临挑战。

Method: 系统采用了基于自动编码器的压缩和基于变换器的扩散模型。

Result: LaDCast在跟踪稀有极端天气事件（如气旋）上表现出色，其性能接近IFS-ENS。

Conclusion: LaDCast能够在实时情况下以公里级分辨率进行预测，是一种实用的路径。

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [105] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: FLoRIST通过奇异值分解本地适配器并进行全局低秩适配器的构建，以达到在联邦学习中高效微调大语言模型，解决了通讯效率和计算成本的挑战，并在各数据集上展示了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦LoRA方法在通讯效率、模型准确性和计算成本的平衡上存在显著挑战，尤其是在异构客户端中。为了解决这些问题，提出了一种在不产生高通信或计算开销的情况下实现精确聚合的联邦微调框架FLoRIST。

Method: FLoRIST采用了一种有效的分解流程，通过对堆叠的本地适配器分别执行奇异值分解，代替在服务器上构建完整的全局权重更新矩阵。该方法在一个紧凑的中间空间内操作，以表示从本地LoRA累积的信息。此外，还引入了可调奇异值阈值进行服务器端的最佳秩选择，以构建一对由所有客户端共享的全局低秩适配器。

Result: FLoRIST在实现数学上准确的聚合的同时，未产生高通信和计算开销，并在多数据集和大语言模型上进行的广泛实验证明，其在通信效率和性能上具备优越的平衡性。

Conclusion: FLoRIST在多数据集和多种LLM的广泛实验证明，在同质和异质设置中，FLoRIST始终在通信效率和竞争性能之间达到了最佳平衡。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [106] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Main category: cs.LG

TL;DR: FedRAG为RAG系统微调提供了一种新框架，提高了其在集中和联邦架构中的性能，并与现有RAG生态系统整合。


<details>
  <summary>Details</summary>
Motivation: 解决依赖大语言模型参数内存的缺陷，增强RAG系统。

Method: 引入FedRAG框架，用于集中式和联邦式架构的RAG系统微调。

Result: FedRAG能够提供从集中到联邦训练任务的无缝转换，并与现代RAG生态系统深度整合。

Conclusion: FedRAG填补了现有工具的关键空白，并改进了RAG系统的微调能力。

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [107] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 提出一种聚类离线强化学习数据集轨迹的新方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 我们研究了离线强化学习数据集中轨迹聚类的新任务，其中每个聚类中心代表生成其轨迹的策略。

Method: 我们提出了政策引导的K均值（PG-Kmeans）和吸引中心自编码器（CAAE）的两种方法。PG-Kmeans 通过迭代训练行为克隆（BC）策略，根据策略生成概率分配轨迹，而CAAE通过引导轨迹的潜在表示接近特定码本入口，以实现聚类。

Result: 在通用的D4RL数据集和自定义的GridWorld环境中，实验验证了我们的方法，其中PG-Kmeans和CAAE都能够有效地将轨迹分配到有意义的集群中。

Conclusion: PG-Kmeans和CAAE能够有效地将轨迹分成有意义的簇，在离线强化学习及其他领域应用广泛。

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [108] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: mLaSDI通过多级autoencoder序列提高了预测和重构精度，且训练时间更短。


<details>
  <summary>Details</summary>
Motivation: LaSDI在高频或复杂模型下重构误差较大，需要改进模型以提高精度和效率。

Method: 通过多级的autoencoder序列训练，每个autoencoder在当前阶段纠正之前的误差。

Result: mLaSDI成功地降低了预测和重构误差，并缩短了训练时间。

Conclusion: mLaSDI比单个autoencoder的方法在预测和重构误差上更出色，同时训练时间更短。

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [109] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: 本文提出一种注意力自适应池化方法，改善了在输入信噪比波动情况下transformer模型的性能，经过实验验证其鲁棒性优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 作者主要受到强化学习和视觉应用的驱动，研究用于总结transformer嵌入模型输出的池化方法设计，尤其在输入信号和噪声的比率波动情况下。

Method: 该论文通过理论分析、监督实验及标准基准测试（包括关系推理、多智能体强化学习和视觉任务）验证了注意力自适应池化方法。

Result: 注意力自适应池化方法在一系列任务中显示出优越的鲁棒性，证明了其在处理输入信噪比波动问题上的有效性。

Conclusion: 通过将池化方法框架化为向量量化，并尝试将信号损失最小化，他们展示了传统的池化方法在输入信号与噪声比率波动时性能容易崩溃。使用注意力机制的自适应池化方法可在任意的信噪比情况下近似信号最佳向量量化器，并在实验中表现出卓越的鲁棒性。

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [110] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Main category: cs.LG

TL;DR: 本文探讨语言模型卸载问题，提出了一种新分类法，并提供框架支持卸载方法的发展和政策指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的影响力日益增长，导致对某些训练数据或特定知识的消除要求增多，因此需要进行模型卸载研究，以避免从头再培训模型。

Method: 提出了一种基于意图的新分类法，将卸载方法按是否真正移除内部知识或仅抑制其行为影响进行分类。

Result: 提出了三个主要贡献：重新审视现有卸载方法表现出的抑制特点，调查现有评估策略并建议改进方向，突出实际应用挑战以支持卸载方法的推广。

Conclusion: 本文提供了一种理解及推进生成式AI卸载的全面框架，旨在支持未来研究和指导数据移除及隐私政策的决策。

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [111] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Main category: cs.LG

TL;DR: 本文提出MindRAG框架，通过融合大型语言模型和多模态RAG技术，以解决CM系统中的误报、决策支持以及数据解释问题，获得了积极的初步实验结果。


<details>
  <summary>Details</summary>
Motivation: 当前系统的自动分析和决策表现出相当的不确定性和高误报率，导致工作负担增加、效率下降；因此，需要一种能够减少误报、提升故障严重性估计、改进决策支持并提供可解释界面的解决方案。

Method: 本文提出MindRAG，一个模块化框架，结合多模态RAG和专门为CM数据设计的新型向量存储结构，并利用现有标注和维护工单作为监督学习协议中的标记替代品。

Result: 初步结果表明，MindRAG为报警管理提供了有意义的决策支持，提高了CM系统的解释性。

Conclusion: MindRAG能有效支持决策，提高警报管理的效率，增强CM系统的可解释性。

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [112] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Main category: cs.LG

TL;DR: CFMI是一种创新的数据插补方法，与现有方法相比，在性能上具有明显优势，特别是在处理高维数据时具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 开发一种通用方法以弥补遗失数据，克服传统多重插补方法的缺陷。

Method: 将连续归一化流、流匹配和共享条件建模结合，以处理传统多重插补的难以处理的问题。

Result: 在24个小到中等维度的表格数据集上的比较显示，CFMI能够匹配或超越传统和现代技术。在时间序列数据的零样本插补中，其精度与相关的基于扩散的方法相匹配，而计算效率更高。

Conclusion: CFMI至少在低维数据上与传统方法的表现相当，同时具有可扩展性，在高维环境中能与其他深度学习方法相媲美或超越。

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [113] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Main category: cs.LG

TL;DR: 基于认知不确定性优先级的重放策略有效减少噪声影响，并在实验中性能超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于时间差错误的优先级方法容易因噪声而优先选择不相关的过渡，导致价值估计偏差，因此需要一种减少噪声干扰的方法。

Method: 使用认知不确定性估计来引导从重放缓冲区中选择优先过渡。

Result: 在Atari游戏实验中，相较于量化回归深度Q学习基准，该方法表现更佳。

Conclusion: 使用不确定性优先重放策略可有效提高强化学习中的过渡优先级，从而改善结果。

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [114] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 介绍了G-Sim，一个结合LLM驱动设计与经验校准的模拟器构建框架，提升了模拟器可靠性与决策支持能力。


<details>
  <summary>Details</summary>
Motivation: 在关键领域构建健壮的模拟器至关重要，现有方法在泛化与准确性上存在不足。

Method: G-Sim采用混合框架，以LLM驱动的结构设计结合实证校准，融合领域知识，采用无梯度与无似然的方法进行参数估计，如无梯度优化或基于模拟推理。

Result: G-Sim通过整合领域先验与实证证据，生成可靠且因果关系明确的模拟器，能够处理非微分与随机模拟器，支持复杂决策。

Conclusion: G-Sim能够结合领域知识与经验证据，成功构建具备因果关系的可靠模拟器，解决了数据效率低下的问题，为复杂的决策提供有力支持。

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [115] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Main category: cs.LG

TL;DR: A state representation framework for MDPs is proposed, learning minimum action distance (MAD) to enable tasks like reinforcement learning. It doesn't require reward signals/actions and outperforms current methods.


<details>
  <summary>Details</summary>
Motivation: To represent states in a Markov decision process only from state trajectories without needing reward signals or actions, facilitating tasks like goal-conditioned reinforcement learning and reward shaping.

Method: The approach involves self-supervised learning to create an embedding space where distances between state pairs match their minimum action distance (MAD). It accommodates both symmetric and asymmetric distances.

Result: Empirical results show that the proposed method efficiently learns MAD representations across different environments and outperforms existing methods in representation quality.

Conclusion: The proposed approach effectively learns accurate MAD representations across various settings, significantly outperforming existing state representation methods in terms of representation quality.

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [116] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Main category: cs.LG

TL;DR: 此研究应用自然语言处理技术从电子健康记录中提取与HIV相关的污名、社会及行为主题，发现不同年龄群体间的差异，优化了传统问卷评估方式。


<details>
  <summary>Details</summary>
Motivation: 利用自然语言处理方法分析电子健康记录中的临床笔记，以识别和理解在寻求护理的HIV感染者中存在的污名维度、社会关系及相关行为特点。

Method: 使用潜在狄利克雷分配（LDA）进行主题建模分析，以揭示有关HIV污名维度的主题。领域专家创建了HIV相关污名关键词的种子列表，并通过滚雪球策略迭代回顾笔记以寻找其他术语。采用三种基于关键词的过滤策略以识别更多目标主题，并由领域专家手动审查检测到的主题。进行了词频分析和主题变化分析。

Result: 主题建模揭示了与HIV相关的污名、社会和行为情况的一系列主题，包括“心理健康问题及污名”、“社会支持与参与”、“有限的医疗保健途径与严重疾病”、“拒绝治疗与隔离”等。跨年龄子组的主题变化分析显示存在差异。

Conclusion: 通过自然语言处理技术分析电子健康记录临床笔记，可以有效提取和理解与HIV相关的污名维度、社会和相关行为情况，从而实现可扩展、高效的评估，克服传统问卷调查的局限性，并改善患者的健康结果。

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [117] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Main category: cs.LG

TL;DR: 研究提出一种方法，通过答案集编程处理子采样效应，提高从时间序列数据中推导因果图的准确性，验证结果优于已有方法，并增加了F1分数。


<details>
  <summary>Details</summary>
Motivation: 从时间序列数据中学习因果结构面临挑战，特别是当测量频率与系统的因果时间尺度不匹配时。由于子采样造成的信息损失，这往往导致一组同样可能的潜在因果图。我们的研究旨在通过考虑子采样效应，提高因果图推导的准确性和直观性。

Method: 我们使用约束优化方法，特别是答案集编程（ASP），来找到最优答案集。ASP 不仅识别出最可能的底层图，还为专家选择提供一组等效的可能图。同时，ASP 通过利用图论进一步减少可能解集的规模。

Result: 我们在模拟数据和实际结构脑连接性数据上验证了我们的方法，显著优于传统方法，并能够在已建立的方法基础上提升12%的F1分数。我们的结果达到了现有技术在子采样时间序列数据因果图重构方面的最优水平，并且对于较高子采样率表现出更好的稳健性。

Conclusion: 我们的研究提出了一种方法，能够在从子采样时间序列数据中重构因果图时，提升精度和召回率，同时对各种采样率表现出稳健性。

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [118] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: 提出DSLA和Serve，通过双状态线性注意力和在线自适应蒸馏提高大语言模型的长文本处理效率，显著加快推理速度且性能表现良好。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在处理长文本时的计算和内存成本高的问题，提出了一种能够提高效率而不牺牲准确性的解决方案。

Method: 引入双状态线性注意力（DSLA）和在线自适应蒸馏框架Serve，通过链式微调策略和敏感性排序替换Transformer层，以在不损失精度的情况下提升效率。

Result: 进行广泛评估表明，Serve在保持与现有模型可比性能的同时，实现了2.3倍于Llama2-7B和3.0倍于Zamba-7B的推理速度提升。

Conclusion: 提出的DSLA和Serve架构在大幅提高推理速度的同时，能够保持与现有模型相当的性能，并成功解决了线性注意力结构中历史信息表现不足的问题。

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [119] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: InstructPro designs ligand-binding proteins using human language instructions, outperforming existing models with high docking success rates and low RMSD values.


<details>
  <summary>Details</summary>
Motivation: To overcome the scarcity of protein-ligand complex data and leverage the abundant human-curated text descriptions about protein-ligand interactions for designing proteins that bind to specific ligands.

Method: Developed a family of protein generative models named InstructPro and created a large-scale dataset InstructProBench. Trained two variants: InstructPro-1B and InstructPro-3B, using a large dataset of human-curated text descriptions, ligand formulas, and protein sequences.

Result: InstructPro-1B achieves the highest docking success rate at 81.52% and the lowest RMSD of 4.026Å in moderate confidence settings. InstructPro-3B improves the average RMSD to 2.527Å, demonstrating superior alignment with functional specifications.

Conclusion: InstructPro can successfully design proteins that bind to specific ligands by following human language instructions, achieving better results than existing models.

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [120] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Main category: cs.LG

TL;DR: ErrorEraser improves continual learning by correcting biases, enhancing accuracy, and reducing forgetting rates.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning methods fail to address biases in real-world data, resulting in spurious correlations that hamper knowledge retention and transfer. Therefore, there is a need for CL methods that can also intentionally forget biased information.

Method: ErrorEraser is a plugin consisting of Error Identification and Error Erasure modules. It identifies biased samples by learning task data's probability density distribution. Erroneous knowledge is then selectively erased by altering the decision space of outlier samples.

Result: ErrorEraser enhances continual learning by significantly mitigating negative impacts of data biases, achieving higher accuracy and lower forgetting rates across various CL methods.

Conclusion: ErrorEraser effectively addresses data bias in continual learning, leading to improved task performance and reduced forgetting.

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [121] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: 探讨了对抗性训练的收敛率并提供了替代风险的收敛界定。


<details>
  <summary>Details</summary>
Motivation: 现有研究已知对抗性替代风险最小化序列能否作为对抗性分类风险最小化序列（即对抗性一致性）的条件，但未探讨该风险收敛至其最优值的速率。

Method: 本文通过给出替代风险边界，量化了对抗性分类风险对于最小化序列的收敛速度。

Result: 本文为二分类问题中的对抗性替代风险最小化序列提供了收敛速度的界定。同时，在非对抗学习的背景下，推导出依赖于分布的替代风险界。

Conclusion: 本文探讨了对抗性训练在提升分类器对抗攻击鲁棒性方面的应用，并通过提供替代风险边界量化了对抗性风险收敛率。

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [122] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Main category: cs.LG

TL;DR: 该综述探讨了扩散模型在异常检测与生成中的应用，提出通过结合生成与检测技术解决数据稀缺问题，并提高生成效果。


<details>
  <summary>Details</summary>
Motivation: 由于异常数据在实际应用中的稀缺性，结合生成技术与检测技术能够更好地应对这一挑战。

Method: 使用扩散模型进行异常检测和生成，结合检测与生成技术形成相互增强的循环。

Result: 通过详细分析扩散模型在异常检测和生成中的应用，分类及评价了不同方法的优缺点，并提出了未来可能的发展方向。

Conclusion: 该论文综述了扩散模型在异常检测和生成中的应用，提出了一种将二者相结合的方法，以解决异常数据稀缺性，并提高生成样本的质量和相关性。

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [123] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: 提出了一个名为LPO的新方法，以优化交互位置的偏好，显著提升了GUI代理的空间定位能力。


<details>
  <summary>Details</summary>
Motivation: 尽管当前GUI代理中监督微调方法在实现空间定位方面占主导地位，但由于其有限的准确感知位置数据的能力，这些方法面临巨大挑战。现有策略，例如强化学习，常常无法有效评估位置准确性，从而限制了其实用性。

Method: LPO使用信息熵预测交互位置，关注信息丰富的区域，并进一步引入基于物理距离的动态位置奖励函数。

Result: LPO通过优化交互偏好显著增强了交互精度。

Conclusion: 综合实验表明，LPO在离线基准测试和实际在线评估中均表现出色，达到了最新的结果。

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [124] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Main category: cs.LG

TL;DR: 本研究提出将扩散训练视作生成性预训练过程，通过轻量级GAN微调实现高效一步生成。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散蒸馏技术虽然能够降低扩散模型的采样成本，但常需大量训练，且学生模型性能常下降。近年来的研究表明，结合GAN目标可能缓解这些问题，但机制尚不明确。

Method: 针对扩散蒸馏的限制：教师和学生模型的步长和参数数量不匹配。提出一个单独的GAN目标，无需依赖蒸馏损失来解决这一限制。通过对预训练模型进行轻量级GAN微调，生成一款一步生成模型。

Result: 通过对85%的参数冻结的预训练模型进行微调，仅使用0.2M图像即可达到强大的性能，并在5M图像上获得接近最先进的结果。

Conclusion: 扩散训练作为一种生成性预训练过程，可以通过轻量级GAN微调解锁模型的潜力，构建高效的一步生成模型。频域分析可能解释扩散训练中一步生成能力的获得。

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [125] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: QHNetV2是一种高效的网络，可在不使用SO(3)张量积的情况下实现全局SO(3)等变，提升电子结构计算速度，并在各类数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于Hamiltonian矩阵的非对角块与SO(2)局部框架之间的内在关系，目标是加速电子结构计算。

Method: 提出了一种新颖且高效的网络QHNetV2，通过引入一套高效有力的SO(2)等变操作，以及在SO(2)局部框架中进行特征更新和信息传递，避免了耗费资源的SO(3) Clebsch-Gordan张量积，从而实现了全局SO(3)等变。

Result: 充分的实验表明模型在QH9和MD17数据集上取得了卓越的性能，证明了其广泛的泛化能力。

Conclusion: 我们的模型在各类分子结构和轨迹上表现出了优异的性能，展示了其强大的泛化能力。

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [126] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种新的演化增强机制（EAM），将深度强化学习与遗传算法结合，提高了解决组合优化问题的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题由于其离散结构和指数级大的解空间而具有挑战性。近年来，深度强化学习（DRL）使得从数据中直接学习启发式解法成为可能，但通常受到有限探索和局部最优解的影响。而遗传算法（GA）表现出强大的全局探索能力，但通常在样本效率方面存在不足且计算量大。

Method: 提出了一种称为演化增强机制（EAM）的框架，将深度强化学习（DRL）的学习效率与遗传算法（GA）的全局搜索能力结合。具体做法是从学习的策略中生成解决方案，并通过领域特定的遗传操作进行优化，然后将优化后的解决方案重新引入策略训练循环，增强探索并加速收敛。

Result: 理论分析确定了演化解分布与政策分布之间的KL散度上界，确保了政策更新的稳定性和有效性。实验结果表明，在旅行商问题（TSP）、车辆路径问题（CVRP）、概率集配送问题（PCTSP）和面向对象编程问题（OP）等基准问题上，EAM显著提高了解决方案质量和训练效率。

Conclusion: EAM展示出对比基准方法，具有显著改进求解质量和训练效率。

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [127] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Main category: cs.LG

TL;DR: CAPT通过因果关系感知的后训练减轻LLM预训练的偏差，增强泛化能力，并在内外分布任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理分布外样本时表现不佳，原因是预训练期间获得的虚假相关性。该研究旨在通过因果关系感知的后训练来减轻这种虚假相关性。

Method: 将偏置预测分解为两个无偏步骤：事件估计和事件干预。

Result: 实验表明，使用CAPT微调的3B模型能够在内分布和外分布任务上表现优于传统SFT和较大模型，仅使用100个内分布微调样本，展示了CAPT的有效性和样本效率。

Conclusion: 通过因果关系感知的后训练（CAPT），可以在不增加额外微调偏差的情况下减少LLM预训练的偏差，从而增强模型的泛化能力。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [128] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: 该论文分析了去中心化学习中泛化误差的问题，提出了在异构数据和无攻击或拜占庭攻击情况下的细粒度误差分析，揭示了多种因素对泛化误差的影响并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习在信号和信息处理领域受到了广泛关注，其允许跨地域的代理进行联合模型训练。然而，以往研究主要集中于去中心化学习算法的优化误差，对其泛化误差的研究相对较少。然而，泛化误差反映了训练模型在未见数据上的可扩展性，是决定模型在现实应用中性能的关键，因此理解去中心化学习的泛化误差非常重要。

Method: 本文针对无攻击和拜占庭容错的去中心化学习，在异构数据和温和假设下，提出了细粒度的泛化误差分析。相较于以往考虑同质数据和依赖严格有界随机梯度假设的研究，本研究更加灵活。

Result: 研究揭示了数据异质性、模型初始化和随机梯度噪声对去中心化学习的泛化误差的影响，这些因素在过去未被深入研究。此外，本文还揭示了恶意代理进行的拜占庭攻击对泛化误差有显著影响，其负面影响与数据异质性密切相关，同时与样本规模独立。

Conclusion: 文章通过对比不同类型任务的数值实验，验证了理论发现。

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [129] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Main category: cs.LG

TL;DR: 研究者提出了一种新的筛选规则来提高Group SLOPE的计算效率和内存使用。


<details>
  <summary>Details</summary>
Motivation: Group SLOPE在选择预测变量组时表现出色，但由于块非分离组效应使现有方法无效或效率不高，通常在实际高维场景中会带来显著的计算成本和内存使用。

Method: 研究者提出了一种适用于Group SLOPE模型的安全筛选规则，该规则通过处理块非分离组效应来高效识别具有零系数的不活跃组。通过排除这些不活跃组，在训练过程中实现了计算效率和内存使用的显著提高。

Result: 通过排除不活跃组，即具有零系数的组，实现了计算效率和内存使用的显著提高，并且提出的筛选规则可以无缝集成到现有的批处理和随机算法求解器中。

Conclusion: 实验结果证明该方法能够有效检测出不活跃的特征组，并大幅提高计算效率，同时不损失准确性。

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [130] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: The paper proposes a transformation method, Stained Glass Transform, ensuring privacy for data inputs in LLM operations, effectively balancing data utility and privacy.


<details>
  <summary>Details</summary>
Motivation: The need to protect enterprise data privacy in shared AI compute environments while still utilizing large language models drives the research of a method to transform data such that it remains private yet useful in LLM operations.

Method: The paper introduces Stained Glass Transform, a learned, stochastic, sequence-dependent transformation of word embeddings that theoretically provides privacy based on mutual information, relating to Gaussian Mixture Models. The authors calculate privacy estimates and verify utility through benchmarks.

Result: The study confirms that Stained Glass Transform can provide privacy for input data in LLMs, demonstrated through privacy estimates and utility verification using token-level privacy metrics and standard performance benchmarks.

Conclusion: Stained Glass Transform is effective in maintaining privacy while preserving model utility in LLM deployments.

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [131] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Main category: cs.LG

TL;DR: 提出了一种新的损失函数，用于解决SM Loss在大规模数据中的计算和扩展性问题，结果表明其在实际应用中提高了效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决SM Loss在大规模对象空间中存在的计算开销大及可扩展性受限的问题。

Method: 通过对SM Loss的泰勒展开，提出新的损失函数RG² Loss和RG× Loss，并结合交替最小二乘法(ALS)优化方法进行评估。

Result: 实证评估表明，该方法在实际数据集上实现了与SM Loss相当或更好的排名性能，并显著加快了收敛速度。

Conclusion: 提出的RG损失函数在保留排名性能的同时，显著加速了收敛速度，相较于SM Loss，具有更好的计算效率和理论保证。

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [132] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Main category: cs.LG

TL;DR: 论文指出了在LLM的RL训练中，KL散度梯度估计的一些实现错误，并展示了这些错误的影响，提供了正确的实现方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型（LLM）在强化学习（RL）中的训练效果，研究人员尝试使用KL散度梯度估计。但在实现过程中发现了一些潜在的问题，这些问题会影响KL梯度的准确性。由于这些问题在开源项目和一些论文中普遍存在，提出了明确解决方案的必要性。

Method: 论文中演示了一系列表格和LLM实验，以显示这些实现问题的影响，并提示正确的KL梯度实现方法。

Result: 作者发现了一些开源项目和论文中的实现问题，并提出了正确的方法来改善KL梯度的估计。

Conclusion: 不正确的KL梯度估计可能会对LLM在RL训练中的效果产生负面影响。论文展示了这些实现问题的影响，同时提供了正确的实现方案。

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [133] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Main category: cs.LG

TL;DR: 提出了EnerBridge-DPO框架，通过能量优化生成低能量、高稳定性的蛋白质序列，并保持较高的序列恢复率。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在设计蛋白质序列时主要训练以最大化序列恢复率，但通常忽略生成序列的能量。本研究旨在克服这一局限性，开发一个能够直接生成低能量、稳定蛋白质序列的模型。

Method: 我们提出了EnerBridge-DPO框架，该框架综合了Markov Bridges和直接偏好优化（DPO）。通过能量偏好微调Markov Bridge模型，Markov Bridge从信息丰富的先验序列中开始优化，为DPO提供结构上可行的候选序列池。此外，还引入了一个显式能量约束损失，以增强DPO的能量驱动特性，使模型能够从丰富的先验知识中有效学习能量表示，并直接预测序列能量值。

Result: 我们的评估表明，EnerBridge-DPO能生成低能量的蛋白质复杂序列，并保持与当前最先进模型相当的序列恢复率。同时，它还能准确预测不同序列间的ΔΔG值。

Conclusion: EnerBridge-DPO可以在保持与当前最先进模型相同的序列恢复率的同时，设计出能量更低的蛋白质复杂序列，并准确预测不同序列间的ΔΔG值。

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [134] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Main category: cs.LG

TL;DR: OKBEs引入了无奖励马尔可夫决策过程中的状态时间选项核，促进目标达成和约束处理，实现灵活的长远规划。


<details>
  <summary>Details</summary>
Motivation: 介绍了一种新的无奖励冻结的马尔可夫决策过程，旨在解决高维状态转换模型的难以处理的规划问题。

Method: OKBEs直接构建和优化称为状态时间选项核（STOK）的预测映射，以最大化完成目标而避免违反约束的概率。

Result: 通过局部STOKs和目标条件策略的分解，我们可以在高维目标级别进行前向规划，解决问题。STOKs支持快速合成元策略，并在多个任务中重用规划表示。

Conclusion: OKBEs通过支持可验证的长时间规划和可以扩展到动态高维世界模型的内在动机，促进了组合性、模块化和可解释性的特性。

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [135] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 提出了一种改进的基于人类反馈的强化学习算法，利用随机探索和优化实验设计来选择信息性偏好查询，既保证了理论性能又减少了查询数量，在实践中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在广义的马尔可夫决策过程中进行基于人类反馈的强化学习时，面临的主要挑战是设计出选择信息性偏好查询的算法，以识别潜在的奖励并保证理论上的可靠性。

Method: 提出了一种基于随机探索的元算法，避免了与乐观方法相关的计算挑战，并且保持了可处理性。为了提高查询复杂度，介绍并分析了一种改进的算法，该算法收集成对的轨迹并应用最优实验设计来选择信息性比较查询。

Result: 证明了在温和的强化学习假设下，该方法在后悔值和最后迭代上都具有保证。实证评估显示，该方法在偏好查询数量很少的情况下，能够与基于奖励的强化学习竞争。

Conclusion: 本文的算法不仅理论上有效，而且在实践中也能以较少的偏好查询数量实现与传统奖励方法相当的学习效果。

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [136] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Main category: cs.LG

TL;DR: 提出了一种网络架构，通过提取周期模式以改善信号表示，从而提高模型的泛化能力和外推性能。


<details>
  <summary>Details</summary>
Motivation: 坐标型多层感知机（MLP）在学习多种信号类型时取得了显著成功，但存在过拟合和训练区域以外的一般化能力有限的问题，导致其外推性能不佳。

Method: 提出了一种新颖的网络架构，用于提取测量中的周期模式以改进信号的表示。通过综合实验验证了该方法的有效性。

Result: 提出的方法在学习微分方程的周期解，以及在实际数据集上的时间序列插补（插值）和预测（外推）方面显示了有效性。

Conclusion: 该研究表明，通过提取测量中的周期模式并利用这些信息来表达信号，可以提高模型的泛化能力和外推性能。

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [137] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM is a multimodal process reward model that efficiently generates high-quality process-labeled data and shows superior performance across various benchmarks, setting new state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a high-performance process reward model efficiently, with reduced time and financial investment, while overcoming the limitations of traditional automated labeling methods.

Method: Athena-PRM evaluates reward scores using prediction consistency between weak and strong completers. Two strategies, ORM initialization and up-sampling for negative data, are introduced to improve PRM performance.

Result: Athena-PRM achieves state-of-the-art results in several benchmarks, improving test time scaling performance on WeMath by 10.2 points and MathVista by 7.1 points. It also outperforms previous state-of-the-art results in VisualProcessBench by 3.9 F1-score.

Conclusion: Athena-PRM exhibits robust capabilities in accurately assessing reasoning step correctness and significantly enhances performance when integrated with other models.

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [138] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Main category: cs.LG

TL;DR: 该论文提出了用于空间-时间因果时间序列概率预测的新框架STOAT，展示出在预测中的高效性，尤其在强空间依赖的场景中优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立地模型化空间和时间动态，忽略了因果驱动的概率预测，限制了预测能力。因此提出了STOAT，以提高空间-时间因果时间序列的预测能力。

Method: STOAT通过构建空间关系矩阵编码区域间的依赖关系，并处理潜在系列进行参数估计，实现校准的不确定性建模。该方法探索多种输出分布以捕获区域特异性变化。

Result: 实验证明，STOAT在COVID-19数据中的表现优于DeepAR、DeepVAR和Deep State Space Model等先进模型，特别是在具有强空间依赖的地区。

Conclusion: STOAT框架通过结合因果推理和地理空间概率预测，为复杂的时空任务，如流行病管理，提供了一个通用的解决方案。实验表明STOAT在关键指标上优于现有的最先进概率预测模型。

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [139] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Main category: cs.LG

TL;DR: MOORL结合离线和在线RL的优点进行高效学习，实验表明性能优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 为了解决离线RL中存在的分布外动作问题及使策略性能和泛化受到限制的问题，提出了一种新的方法MOORL，使得学习更加高效和可扩展。

Method: 引入了一种元策略，能够在离线和在线轨迹中无缝适应，结合离线数据进行初始化和在线数据进行探索，形成一个混合框架。

Result: 理论分析表明，该混合方法通过结合离线和在线数据的互补优势增强了探索能力，且无需增加复杂性即可学到稳定的Q函数。

Conclusion: MOORL在28个任务上的实验中显示出其效果优于现有的离线和混合RL基准，表明其在真实应用中的潜力。

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [140] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Main category: cs.LG

TL;DR: 本研究探讨基础模型的校准行为，发现其在原始分布下取得较低信心但在变化分布下校准改善；后处理技术在改变分布条件下不可靠。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在高风险应用中的校准可靠性问题需要解决，尤其是在分布转移情况下。尽管基础模型在预测性能上已有显著提升，其校准特性仍未被充分研究。

Method: 通过实验分析，研究基础模型在不同数据分布下的校准行为，并评估后处理校正技术的有效性。

Result: 基础模型在原始分布下倾向于低信心，而在分布转移下校准有所改善。后处理校正技术在原始分布下有效，但在严重分布转移时效果不佳，甚至可能产生反效果。

Conclusion: 基础模型在预测校准方面表现出复杂的非线性效应，挑战了持续改进的既定叙述。

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [141] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Main category: cs.LG

TL;DR: 提出了一种快速准确的随机算法和新型非凸建模框架，用于解决大规模张量恢复中的计算成本问题，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的张量恢复方法未能识别张量尺度变化对其结构特征的影响，处理大规模高阶张量数据时面临高昂的计算成本。

Method: 提出两种快速准确的随机算法用于低秩张量近似问题，并建立了逼近误差估计的理论界限。同时开发了一种适用于大规模张量恢复的新型广义非凸建模框架，并研究新的统一非凸模型和高效优化算法。

Result: 通过将随机LRTA方案集成到核心和时间密集型计算中，所提出的方法在各种大规模张量实验中显示出卓越的性能。

Conclusion: 实验结果表明，所提出的方法在大规模张量的数据处理中表现出良好的实用性、有效性和优越性，与一些最先进的方法相比具有更好的性能。

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [142] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Main category: cs.LG

TL;DR: SparseSSM prunes 50% weights in Mamba models without accuracy loss, extending OBS to state-space architectures.


<details>
  <summary>Details</summary>
Motivation: Although state-space language models offer linear complexity inference while maintaining quality, their large parameter size hinders deployment. Current pruning methods don't effectively apply to the unique characteristics of state-space architectures.

Method: The paper introduces a training-free pruning framework called SparseSSM, which extends the optimal brain surgeon (OBS) method to state-space architectures. Their algorithm calculates a second-order saliency score using Hessian-trace information and performs component sensitivity analysis to guide pruning. It is adaptable to various sparsity structures.

Result: The proposed SparseSSM method prunes 50% of the weights in the selective state-space module while maintaining zero-shot accuracy, demonstrating its efficacy as the best current pruning algorithm for Mamba-based large language models.

Conclusion: SparseSSM successfully prunes 50% of the weights in state-space language models like Mamba without any fine-tuning, achieving state-of-the-art performance in pruning algorithms for such architectures.

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [143] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Main category: cs.LG

TL;DR: 文章提出了一种基于几何代数的新型等变神经网络架构，具有较轻的参数化，减少了过拟合现象，并在多个基准测试任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有等变神经网络模型在处理伪正交变换（包括旋转、反射）时存在过拟合倾向，需开发轻参数化的解决方案。

Method: 论文提出了基于几何（Clifford）代数的新型等变神经网络架构，并采用权重共享参数化技术。

Result: GLGENN架构在等变任务上表现出色，在使用更少的可优化参数情况下仍超越或匹敌其他模型。

Conclusion: GLGENN能够在多个基准测试任务中超越或匹敌竞争模型，且优化参数显著减少。

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [144] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Main category: cs.LG

TL;DR: 研究揭示了使用大语言模型生成合成数据的新风险：上下文偏倚会导致下游任务的不公平性，特别是在数据稀缺和敏感领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）常用于合成表格数据生成，可以在数据稀缺情况下提供数据增强的实用解决方案。然而，过去的研究假设可以获取一部分无偏的上下文示例来提升下游任务表现，但在现实世界中数据常常是有噪声或人口偏倚的。本文旨在研究这些上下文示例中的统计偏倚如何影响合成表格数据的分布。

Method: 本文系统研究了上下文示例中的统计偏倚如何传播到合成表格数据的分布，并展示了即便是轻微的上下文偏倚也会导致全局统计扭曲。同时引入了一种对抗场景，其中恶意贡献者可以通过一组上下文示例将偏倚注入合成数据集中，从而最终影响下游分类器对特定群体的公平性。

Result: 研究结果展示了基于LLM的数据生成管线的新漏洞，特别是依赖于在敏感领域的上下文提示的管线。

Conclusion: 大语言模型在使用上下文学习生成合成数据时存在注入偏倚的风险，特别是在敏感领域中可能会影响下游分类器的公平性。

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [145] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 提出了FedVLMBench，一个用于评估视觉语言模型联邦微调的系统基准，揭示了架构、策略和数据异质性对FL结果的重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前VLM的微调方法面临部署在隐私要求严格领域的挑战，例如医疗保健，因此需要通过引入联邦学习来解决隐私问题，并评估其效果。

Method: 整合了两种主流的VLM架构、四种微调策略、五种FL算法、六个多模态数据集，涵盖多个任务设置和下游任务类别，进行实验评估。

Result: 发现采用2层多层感知器连接器并同时调优连接器和LLM的配置在联邦学习中效果最佳。此外，现有的FL方法在视觉任务中对数据异质性的敏感度显著高于文本任务。

Conclusion: FedVLMBench是一种系统的基准，用于评估视觉语言模型在隐私保护领域进行联邦微调的效果，提供了重要的工具和指导，以推动多模态基础模型的隐私保护和联邦训练。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [146] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: 引入SyncFed框架，通过时间同步和加权改善联邦学习模型的准确性和信息新鲜度。


<details>
  <summary>Details</summary>
Motivation: 针对联邦学习中网络延迟、时钟不同步和客户端更新多样性导致的训练不一致性，现有方法难以量化时延，特别是在对延迟敏感和跨区域的部署中。

Method: 引入SyncFed框架，利用显式同步和时间戳技术，基于NTP协议计算延迟，将时间信息纳入模型更新的加权过程中。

Result: 在地理分布的测试环境中，SyncFed使全局模型在稳定的时间背景下演化，提高了模型的准确性和信息的新鲜度。

Conclusion: SyncFed通过同步机制提高了全局模型的准确性和信息新鲜度。

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [147] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Main category: cs.LG

TL;DR: 提出WAFFLE算法，通过WST或傅里叶变换进行检测，实验表明其在不增加通信和计算开销情况下有效提升FL异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 在不访问原始数据的情况下检测异常或损坏的客户端仍然是一个关键挑战。

Method: 提出了一种名为WAFFLE（Wavelet和Fourier表示用于联邦学习）的检测算法，利用从小波散射变换（WST）或傅里叶变换得出的本地计算压缩表示来标记恶意客户端。

Result: 相较于现有的联邦学习异常检测算法，我们的方法提升了检测准确性和下游分类性能。

Conclusion: WST提供了理论优势，如不可逆性和对局部变形的稳定性，使其特别适合于联邦学习场景。

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [148] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Main category: cs.LG

TL;DR: Wasserstein超图神经网络通过切片Wasserstein池化更好地聚合信息，在超图节点分类上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络多基于两阶段集成框架，主要从传统图模型中获取灵感，通常简化为基础的池化操作。本文希望通过引入更复杂的聚合方式来更好地表示高阶关系。

Method: 在模型中，节点和超边邻域被视为分布，并通过切片Wasserstein池化聚合信息，其能够保留分布的几何特征。

Result: 实验结果表明，将Wasserstein池化应用于超图设置中，可以在多个真实数据集上的节点分类任务中取得最佳表现。

Conclusion: 本文提出了一种新型的超图神经网络模型，即Wasserstein超图神经网络，能够在超图设置中显著提高节点分类任务的性能。

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [149] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Main category: cs.LG

TL;DR: TRIDENT是一种在推理阶段确保输出满足LTLf约束的算法，不需要重新训练，能提高效率和输出质量。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型和神经架构难以保证输出符合时间约束。

Method: TRIDENT在推理阶段使用了一种通用且与模型无关的算法，通过将LTLf公式编译为确定性有限自动机（DFA），并以此指导一种受约束的beam search。每个解码步骤中，会屏蔽可能导致约束违反的转移，并根据模型概率和DFA接受结构动态重新排名剩余路径。

Result: 实验表明，TRIDENT在时间约束图像流分类和受控文本生成任务中实现了完美的约束满足，同时提高了输出质量。

Conclusion: TRIDENT能够完美遵循LTLf约束，同时在效率和质量指标上优于现有方法。

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [150] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.LG

TL;DR: Auto-Compressing Networks offer efficient, robust learning by replacing residual connections with additive long feedforward connections, minimizing redundancy and resource use, and enhancing performance across deep learning models.


<details>
  <summary>Details</summary>
Motivation: Overcome computational redundancy and representation quality issues in deep networks with traditional short residual connections.

Method: Introduced Auto-Compressing Networks (ACNs) using long feedforward connections from each layer to the output instead of traditional short residual connections. This design allows for 'auto-compression' during training.

Result: Achieved up to 18% reduction in catastrophic forgetting, 30-80% architectural compression, and maintained accuracy. ACNs outperform in noise robustness, low-data settings, and transfer learning capabilities, proving to be better than residual networks.

Conclusion: ACNs prove to be an efficient neural architecture approach that adapts computational demands to task complexity while maintaining robust learning.

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [151] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: 研究通过AtmosMJ模型展示，标准网格上可实现稳定的长期天气预测，而无需非常规数据表示，且训练资源需求低。


<details>
  <summary>Details</summary>
Motivation: 研究挑战了传统观点，探讨在标准纬度-经度网格上是否可以实现与非标准空间域相似的长期预测性能。

Method: 引入了AtmosMJ，这是一种基于ERA5数据且不需要球面重映射的深度卷积网络，其稳定性由新颖的门控残差融合机制（GRF）支持。

Result: AtmosMJ能够生成大约500天的稳定、物理合理的预测。在量化评估中，与Pangu-Weather和GraphCast等模型相比，10天的预测准确性竞争力强，并且只需5.7天的V100 GPU训练时间。

Conclusion: 本研究表明，通过有效的网络架构设计可以获得稳定和计算上高效的长期天气预测，而不需要依赖于非常规的数据表示。

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [152] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Main category: cs.LG

TL;DR: 论文探讨了开发多模态图大型语言模型以泛化和统一多种图数据和任务，并提出了五个关键特点和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态图学习方法难以在不同的数据和任务间普遍化，因此需要开发一个可以统一和泛化的多模态图大型语言模型。

Method: 提出一个统一框架，包含多模态图数据、任务和模型，揭示多模态图中的多粒度和多尺度特征，并设定达到五大特点的挑战和方向。

Result: 提出了一个能够统一和泛化多模态图数据和任务的框架，及其五个关键特点，并且对现有研究进行了回顾和讨论未解挑战。并且为这些特点的实现指明了方向。

Conclusion: 该论文旨在推进研究，以实现通用化的多模态图数据和任务。提出的框架和特点为未来的研究提供了重要的方向，并总结现有数据集以支持模型训练。

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [153] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Main category: cs.LG

TL;DR: 结合特征工程和LLM的认知架构方法提高了ML监控输出的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型监控方法输出冗长且可解释性低，影响了有效决策。本研究旨在通过提高输出的可解释性来改善这一问题。

Method: 使用特征工程原则，设计了一种认知架构，通过三个关键步骤——重构、分解和编译，模拟特征工程，来提高监控输出的可解释性。

Result: 该方法显著提高了解释效果，在多个领域的实验中，相较于各种基线模型，获得了显著更高的准确性。

Conclusion: 提出的方法通过特征工程驱动规划和选择性地利用大型语言模型（LLM），显著提高了监控输出的可解释性，形成了一个强大的决策支持系统。

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [154] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Main category: cs.LG

TL;DR: 本文提出Load-aware Tram-FL，通过优化调度机制减少分散式联邦学习的训练时间，实验结果表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在分散式联邦学习中，处理计算和通信负载以最小化总训练时间。

Method: 提出了一种名为Load-aware Tram-FL的训练调度机制，通过将原始问题分解为节点级子问题，实现全局优化任务。

Result: Load-aware Tram-FL在MNIST和CIFAR-10模拟实验中显著减少了训练时间并加速了收敛。

Conclusion: Load-aware Tram-FL能有效减少训练时间，促进数据利用平衡，加速模型的收敛。

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [155] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Main category: cs.LG

TL;DR: 本文提出了一种基于余弦相似度分析的统一对比学习框架，解决了全批量和小批量设置下的对齐问题，通过引入辅助损失项提高了小批量训练的性能。


<details>
  <summary>Details</summary>
Motivation: 在对比学习领域，已有的方法缺乏解释广泛目标的系统性框架。本研究旨在通过分析正负样本对的余弦相似度，为对比学习提供一个统一的理解框架。

Method: 本文通过分析正负样本对嵌入的余弦相似度来研究对比学习。在全批量设置中，研究正样本对齐的不可能情况，并引入视图内负样本对解决失配问题。在小批量设置中，通过引入辅助损失项，降低负样本对相似性的方差。

Result: 结合本文提出的损失项后，CL方法在小批量训练中的性能得到了一致的改善。

Conclusion: 本文提出了一种统一的对比学习(CL)框架，通过分析正负样本对嵌入的余弦相似度来理解对比学习。在全批量设置下，证明了当负样本对的相似度低于某一阈值时，正样本对无法完美对齐。该研究通过引入视图内负样本对来减轻这种失配。在小批量设置下，研究表明更小的批量会导致负样本对之间更强的分离，从而导致负样本对相似度的更高方差。为解决这一问题，提出了一种辅助损失项，旨在降低对比学习中负样本对相似度的方差。实证结果表明，结合该损失的CL方法在小批量训练中的性能得到了一致改善。

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [156] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出了一种针对连续相关数据的对比自监督学习方法，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的对比自监督学习方法假设样本间语义独立性，但这对于复杂相关性数据并不适用。因此需要一种新的理论框架来处理连续相关数据的对比自监督学习。

Method: 提出一个新的对比自监督学习理论框架，该框架适应连续相关数据，并允许样本间存在语义上的接近。引入硬和软两种接近度来反映样本间的真实相似性，并推导出包含这些接近度类型的估计相似矩阵，从而引入依赖感知的损失函数。

Result: 在时间和时空领域的问题上验证了该方法，证明在存在数据依赖模式的情况下，该方法优于现代处理相关数据的方法。在标准UEA和UCR基准上分别提高了4.17%和2.08%的准确性。在复杂的时空模式的干旱分类任务中，该方法ROC-AUC得分提高了7%。

Conclusion: 提出的依赖TS2Vec方法能够有效捕捉时空数据中的依赖模式，并取得优于现有方法的性能提升，验证了其理论性损失函数的有效性。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [157] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Main category: cs.LG

TL;DR: 研究介绍了一种针对局部隐私图学习协议的数据投毒攻击，并强调需更有效的防御措施。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的图包含敏感个人信息，使用图神经网络进行图学习时会带来隐私问题。此外，局部隐私图学习协议可能会受到数据投毒攻击，威胁图学习的鲁棒性与安全性。

Method: 攻击者注入假用户，操控这些假用户与真实用户建立联系，并发送精心设计的数据至服务器。

Result: 该研究从理论和实证方面展示了攻击的有效性，同时探索了几种防御策略，但其有限效力突出了需进一步加强防御措施的必要性。

Conclusion: 本研究介绍了第一种针对局部隐私图学习协议的数据投毒攻击，并提出了需要更强大的防御措施。

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [158] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Main category: cs.LG

TL;DR: 该论文引入ProjNCE，统一监督和自监督对比学习目标，并在多种数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探讨SupCon与互信息的关系，并通过引入ProjNCE来弥补当前的研究缺口。

Method: 引入ProjNCE，一个InfoNCE损失的泛化形式，通过结合投影函数和负样本调整项来统一监督和自监督对比学习目标。

Result: ProjNCE在多个数据集和环境下持续优于SupCon和标准交叉熵训练方法。

Conclusion: ProjNCE保持了一个有效的互信息界限，并提供了更大的灵活性来选择类嵌入的投影策略。通过研究多种投影方法，我们证明ProjNCE可以在各种数据集和环境中持续超过SupCon和标准交叉熵训练。

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [159] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Main category: cs.LG

TL;DR: 论文应用社会选择理论定义评估指标选择的表示方法，提出了定位表示和比例性，并在LLM评估和医院质量评估中进行实证研究，确认其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了从可能的一整套指标中选择一个子集，目的是在效率或可解释性上取得平衡，并选取有代表性的指标集。

Method: 论文引入了社会选择理论中的概念：定位表示和定位比例性，用于定义和选择评估指标的代表性方法。

Result: 通过理论推导，证明了在最坏情况下保证任一性质所需的指标最小数目，并在实践中通过案例研究验证了理论应用的有效性。

Conclusion: 该论文通过将社会选择理论应用于评估指标子集的选择，提出了定位表示和定位比例性两种表示法，以帮助选出有效的指标集。

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [160] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Main category: cs.LG

TL;DR: 研究稀疏线性ODE的可识别性，发现其在常用稀疏条件下不可识别，并总结了现有方法对其估计的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管线性ODE通常被认为从单一轨迹可识别，但在稀疏矩阵情况下尚未解决不可识别性，稀疏性在生物、社会和物理系统中非常常见，因此有必要填补这一空白。

Method: 对稀疏线性常微分方程（ODE）进行可识别性特征研究，并通过实验证明理论不可识别性在现有线性ODE估计方法中的体现。

Result: 稀疏系统在常见的稀疏性条件下具有正概率的不可识别性，为此概率提供了下界，并发现这种理论不可识别性在最新的线性ODE估计方法中得以表现。

Conclusion: 稀疏线性ODE系统在与当前方法的结合下实质上是不可识别的，需要重新思考数据驱动动态系统建模的期望。

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [161] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Main category: cs.LG

TL;DR: 提出了一种新的梯度加权损失方法——WoLA，用于增强拜占庭弹性联邦学习方法在异质环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，参与者可以通过贡献恶意梯度来影响模型收敛，从而引发安全问题，而不是隐私问题。

Method: 引入了Worker Label Alignement Loss (WoLA)，一种加权损失，通过协调诚实工人的梯度来应对数据异质性，以便更好地区分恶意梯度。

Result: 在异质性环境中，该方法显著优于现有的最先进方法。

Conclusion: 该研究提供了理论和实践证据，证明WoLA在异质性环境中对抗拜占庭参与者的有效性。

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [162] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Main category: cs.LG

TL;DR: 该论文提出引导图压缩（GGC）框架，利用图自编码器在图数据分类任务中提高性能，优于现有算法并支持量子图神经网络在真实数据中的应用。


<details>
  <summary>Details</summary>
Motivation: 处理大规模图结构数据时，图神经网络面临内存需求高和GPU上稀疏矩阵运算效率低的问题。量子计算提供了新的算法可能性，但受到当前量子硬件限制，量子图神经网络需要有效编码的数据维度有限。

Method: 介绍了一种引导图压缩（GGC）框架，利用图自编码器减少节点数量和节点特征的维度，并指导压缩以提升后续分类任务的性能。该框架可用于量子或经典分类器。

Result: GGC在Jet Tagging任务中进行了评估，并与仅使用自编码器作为预处理步骤以及基准经典GNN分类器进行了比较。数值结果显示GGC的性能优于其它方案，促使对真实数据集上的新颖QGNN框架进行测试。

Conclusion: 引导图压缩框架提升了大型图数据的处理效率，并为量子图神经网络在实际数据集上的应用提供了可行性。

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [163] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: 该研究通过微波谐振传感器捕捉油样的介电性质变化，利用机器学习分类器对油样进行高精度分类，随机森林分类器实现了99.41%高精度。


<details>
  <summary>Details</summary>
Motivation: 利用油的介电性质进行分类，可以实现非破坏性、低功耗的实时工业应用，因此提出这种基于机器学习的方法。

Method: 通过微波谐振传感器捕捉油样的谐振频率和振幅响应的变化，从中提取特征，然后输入到多个机器学习分类器中进行训练和评估。

Result: 随机森林分类器实现了99.41%的高分类准确率，表明该方法在油识别方面具有很强的潜力。

Conclusion: 该研究提出了一种基于机器学习的方法，通过微波谐振传感器根据油样的介电性质进行分类，随机森林分类器实现了99.41%的高分类准确率，展示了自动化油识别的强大潜力。

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [164] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出了一种新的多阶段方法，通过结合多种安全技术，在异质数据环境下确保联邦学习的隐私和弹性，同时减小通信开销。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，为了保持客户端数据的隐私，确保拜占庭弹性是一个基本挑战。当客户端数据同质时，已有的对策使用了安全聚合技术。然而，当数据异质时，这些对策失效。最近，适当的预处理技术在异质环境中提升了这些对策的性能，但无法与引入的隐私保护机制一起使用。

Method: 该方法结合了可验证秘密共享、安全聚合和定制对称私有信息检索技术。还调查了零阶估计方法与这些技术的结合以降低通信成本。

Result: 在各种攻击情况下评估了该方案的有效性，并显示其性能优于之前已知的技术。

Conclusion: 本文提出了一种多阶段方法，通过可验证的秘密共享、安全聚合和定制的对称私有信息检索方案的精细协同设计，在数据异质性下实现信息论隐私保障和拜占庭弹性。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [165] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文重新定义单指标模型的数学基础，提出基于球谐函数的方法，并展示了其在任意球对称输入下的复杂性分析，介绍了两类优化估计器。


<details>
  <summary>Details</summary>
Motivation: 研究单指标模型的学习问题，探索不同数学基础对问题的影响。

Method: 提出了两类估计器：基于张量展开和在线SGD的估计器，分别在样本复杂度和运行时间上达到最优。

Result: 理论不仅恢复了已有结果并进行了澄清，还揭示了之前被忽视的新现象。

Conclusion: 本文通过引入球谐函数作为单指标模型的学习基础，提出了一种新的视角，对任意球对称输入分布下学习单指标模型的复杂性进行了刻画。

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [166] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Main category: cs.LG

TL;DR: 开发了一种基于因果关系学习的气候模型模拟器，能够在不牺牲物理完整性的情况下，提高预测气候变化的效率。


<details>
  <summary>Details</summary>
Motivation: 传统气候变化模型运算复杂且计算资源消耗巨大，限制了对气候变化的预测与其成因和影响的分析。机器学习虽可快速模拟气候模型数据，但当前的方法无法整合物理信息的因果关系。

Method: 运用因果关系学习，推导出一种包含贝叶斯滤波器的物理信息方法，以实现稳定的长期自回归模拟。

Result: 我们的模拟器在真实和合成数据集上展示了其组件的重要性，并且可以精确模拟气候动态。

Conclusion: 我们开发了一种基于因果关系学习的可解释气候模型模拟器，并证明该模拟器能够准确学习气候动态。

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [167] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: 研究VQVAE对抗攻击的抑制性能，发现其有效降低了攻击效果，并在离散空间中观察到有助于检测攻击的特性。


<details>
  <summary>Details</summary>
Motivation: 为了分析VQVAE在高信噪比射频数据中的对抗攻击抑制性能。

Method: 设计了一个VQVAE，将数字无线电波形映射到离散潜在空间，并对其重建效果进行分类分析。

Result: 通过比较不同类型的对抗攻击以及在攻击后的分类准确率，证明VQVAE有效地降低了对抗攻击的效果，且在离散空间中观察到一些可能有助于检测攻击的有趣特性。

Conclusion: VQVAE可以显著降低对高信噪比射频数据的对抗攻击的有效性。

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [168] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Main category: cs.LG

TL;DR: DNA方法提供可解释的多样性近最优轨迹规划选项，采用奖励塑造和Q学习实现策略多样性。


<details>
  <summary>Details</summary>
Motivation: 提供一种新的可解释的强化学习方法，使得代理提供给人类用户的轨迹规划有更多可选项。从而提升RL中的探索和自适应规划能力。

Method: 使用局部修改的Q学习中的奖励塑造来获得具有保证ε-最优性的不同策略。

Result: 在模拟中展示了不同的政策，包括与质量多样性领域中的相关方法进行简要比较。

Conclusion: DNA成功为轨迹规划代理返回具有质的不同政策，展示不同的可选方案。

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [169] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 研究了大语言模型在采样上的不足，提出了语言化拒绝采样（VRS），通过自然语言适配显著降低了采样偏差，提高了模型的随机性任务可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在描述概率分布时表现出色，但在从中生成样本时存在困难，限制了其在需要可靠随机性任务中的应用。

Method: 引入了语言化拒绝采样（VRS）方法，将经典的拒绝采样通过自然语言适配到大语言模型中。

Result: 在无需访问模型内部或进行复杂提示设计的情况下，VRS 提升了直接采样的效果，理论分析表明在一定的假设下，VRS 的改进得益于算法和提示设计。

Conclusion: 本文提出的 VRS 方法可以有效改善 LLM 在伯努利分布采样中的偏差，从而提高大语言模型在涉及随机性任务中的可靠性。

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


### [170] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Main category: cs.LG

TL;DR: 提出了一种新的隐私攻击方法Apollo，可以在严格的条件下进行高精度的未学习样本成员状态推断。


<details>
  <summary>Details</summary>
Motivation: 现有的针对机器学习的隐私推理攻击假设攻击者可以访问未学习模型和原始模型，但这在现实场景中不太可行，因此需要一种不依赖于这种假设的攻击方法。

Method: 提出一种名为Apollo的隐私攻击方法，这种方法在严格的威胁模型下，通过只访问未学习模型的标签输出，推断一个数据样本是否被遗忘。

Result: Apollo在访问目标模型信息较少的情况下，能够较为精确地推断未学习样本的成员状态。

Conclusion: 提出了一种名为Apollo的隐私攻击方法，这种攻击在严格的威胁模型下只需访问未学习模型的标签输出，显示出较高的推断精度。

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [171] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Main category: cs.LG

TL;DR: 研究使用MCMC和VI方法近似后验分布，VI收敛更快，MCMC更精准。


<details>
  <summary>Details</summary>
Motivation: 概率矩阵分解（PMF）通过在潜在因子上引入概率分布来扩展传统矩阵分解，以便进行不确定性量化。然而，计算后验分布由于高维积分而难以进行。

Method: 采用两种贝叶斯推理方法：马尔可夫链蒙特卡罗（MCMC）和变分推断（VI）来近似后验分布。

Result: 在MovieLens数据集上评估它们的性能，并比较其收敛速度、预测准确性和计算效率。实验显示VI收敛速度更快，但MCMC提供了更准确的后验估计。

Conclusion: 实验结果表明，变分推断（VI）收敛速度更快，而马尔可夫链蒙特卡罗（MCMC）提供更准确的后验估计。

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [172] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 研究开发了一种样本高效的算法，能在信息不对称环境中识别系统动态，处理知识转移，用于在线学习策略。


<details>
  <summary>Details</summary>
Motivation: 多代理系统中普遍存在信息不对称现象，尤其在经济学和社会科学中。参与者基于私人信息调整行为以最大化奖励，这种策略行为常引入复杂性。此外，在目标环境中进行实验证明困难，使得知识转移成为一项重大挑战。本研究旨在解决这些问题。

Method: 提出了一种在线学习算法，该算法在一个策略互动模型中，通过样本效率来准确识别系统动态，并处理信息不对称和知识转移问题。

Result: 该算法能够在信息不对称和需要知识转移的情况下，通过非独立同分布的行动，达到一个接近最优的策略学习，且样本复杂度为$O(1/\epsilon^2)$。

Conclusion: 这项研究开发了一种算法，可以在信息不对称的环境中，通过非独立同分布的行动来识别系统动态，并有效应对知识转移的挑战。

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [173] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: 研究引入CLAReps，用于提取稳健、可解释的表示，并通过CaDistill将核心类别知识传递给学生模型，显著提高模型鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然CDMs在生成任务中表现优异，但它们在建模数据分布时会将类别定义特征与无关的背景混在一起，难以提取稳健且可解释的表示。为了解决这一问题，研究引入了CLAReps。

Method: 研究提出了一种新的基于扩散的特征蒸馏范式，称为CaDistill。该方法通过CLAReps将CDM的核心类别知识转移给学生模型，CLAReps仅占训练数据的10%。

Result: 学生模型在训练后展现出强大的对抗性稳健性和推广能力，更专注于类信号而非虚假的背景线索。

Conclusion: 研究表明，CDMs不仅可用于图像生成，还能作为紧凑、可解释的教师推动稳健的表示学习。

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [174] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Main category: cs.LG

TL;DR: Multiverse模型通过并行生成方法，提高了生成效率和性能，并开源提供了完整的生态系统。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归大型语言模型在顺序生成时常表现出隐性并行性，因此作者希望通过引入一种新的生成模型来实现本机并行生成。

Method: 作者介绍了Multiverse模型，采用MapReduce范式进行三阶段生成：自适应任务分解阶段、并行子任务执行阶段和无损结果合成阶段。此外，设计了Multiverse Attention以分隔并行推理步骤，并保持与因果注意力的兼容性。

Result: Multiverse-32B模型在经过3小时的微调后，性能达到与同规模顶尖自回归模型相当的水平，并实现了显著的效率提升，超出传统模型1.87%的表现。而且，能够达到高达2倍的速度提升。

Conclusion: Multiverse-32B成功证明了非自回归模型在处理复杂推理任务上的潜力，并同时开放了完整的生态系统以供进一步研究和应用。

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [175] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Main category: math.OC

TL;DR: 本文评估了三类一阶优化方法在梯度计算相对不准确性时的鲁棒性，发现加速法比预期强，长步法通过缩短因子得到改进。


<details>
  <summary>Details</summary>
Motivation: 在使用较少位数的信息压缩梯度时（例如在GPU上处理大规模问题），可能导致梯度计算的相对不准确性。本文旨在评估不同一阶优化方法在面对这种不准确性时的鲁棒性。

Method: 本文使用性能估计方法从理论和实验两个方面评估一阶优化方法在其梯度计算存在相对不准确性时的鲁棒性。分析了三大类算法：常数步长梯度下降、长步法、加速法，引入半启发式的缩短因子以改善其理论保证。

Result: 所有方法在具体的不准确问题中进行了测试，结果表明加速法比预期更加鲁棒，而缩短因子显著帮助了长步法。缩短方法在不准确环境中表现出色。

Conclusion: 尽管理论上加速方法和长步法对相对不准确性不够稳健，但实际上加速方法表现出了比预期更强的鲁棒性，而引入的缩短因子显著提升了长步法的理论保证，所有缩短的算法在无精确环境中展现出有希望的表现。

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [176] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: STREAMINGGS通过创新的渲染方式，显著提高了3D Gaussian Splatting在移动设备上的速度和能效。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting因其效率和稀疏高斯基础的表示而受到关注，但在资源受限的移动设备上难以实现90 FPS的实时需求，且现有加速器忽视了内存效率导致冗余DRAM流量。

Method: 提出了一种流式的3DGS算法架构联合设计，通过从基于瓦片的渲染转变为基于内存的渲染，实现精细的流水线处理。

Result: STREAMINGGS设计取得了高达45.7倍的加速和62.9倍的能量节省，相较于移动Ampere GPU。

Conclusion: STREAMINGGS通过算法架构联合设计，实现了精细的流水处理，并通过转变为以内存为中心的渲染方式，显著减少了DRAM流量。该设计在移动设备上实现了巨大的性能提升和能量节省。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [177] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 研究引入一个基于Transformer的简单框架，发现数据建模选择显著影响动作插值性能，挑战了模型复杂性至上的假设。


<details>
  <summary>Details</summary>
Motivation: 当前的动作过渡解决方案依赖复杂的模型架构，本研究旨在通过简化模型结构探讨如何只使用简单的单一步骤模型实现高质量的动作插值。

Method: 该研究采用了一个简单但有效的基于Transformer的框架，即使用单个Transformer编码器来合成逼真的运动过渡。

Result: 数据建模的选择显著影响动作插值性能；增加数据量、正确选择姿态表示和引入速度特征均能提高动画性能。

Conclusion: 数据建模方法的重要性可能超过模型复杂性。增强数据量、选择合适的姿态表示和加入速度特征，可提升动作过渡性能。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [178] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: DGS-LRM是一种用于动态场景重建的前馈方法，通过引入增强的数据集和创新的3D表示及大型变压器网络，实现实时的高质量场景重建，优于现有的动态重建方法。


<details>
  <summary>Details</summary>
Motivation: 在动态场景重建方面，现有的前馈场景重建方法通常局限于静态场景，并未能重构动态物体的运动。因此，开发面向动态场景重建的前馈模型面临重大挑战，包括训练数据稀缺、合适的3D表示及培训范式的需求。

Method: 该方法提出了一种增强的大规模合成数据集，包含地面实况的多视角视频和密集的3D场景流监督；采用逐像素可变形的3D高斯表示，支持高质量动态视图合成，并实现长期3D追踪；以及一个大型变压器网络用于实时、普遍化的动态场景重建。

Result: 该模型在动态场景重建的质量上与基于优化的方法相当，同时显著超越了现有的预测动态重建方法。其物理上的3D变形预测准确，并适应长期的3D追踪任务，达到单目视频3D追踪方法的顶尖性能。

Conclusion: DGS-LRM能够有效地重建动态场景，并在真实世界例子中极大地优于现有的预测动态重建方法。它的物理上有根据的3D变形预测准确，能够适应长期的3D追踪任务，性能媲美最先进的单目视频3D追踪方法。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [179] [A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project](https://arxiv.org/abs/2506.09204)
*Xiaotian Chen,Hongyun Liu,Seyed Sahand Mohammadi Ziabari*

Main category: cs.NE

TL;DR: 本文研究了通过结构优化提高稀疏进化训练应用于多层感知机的性能，结果表明性能下降小于4%的情况下可以实现超过40%的效率提升。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络模型复杂性的增加，降低计算成本和内存开销的需求变得越来越迫切。稀疏性已成为一种主要方法，用于减少计算成本而不牺牲准确性。本研究旨在探索应用结构优化的Sparse Evolutionary Training（SET）技术能否在不显著降低性能的情况下提高多层感知机（MLP）的效率。

Method: 采用结构优化方法来提高Sparse Evolutionary Training应用于多层感知机（SET-MLP）的性能，其中使用了称为motif-based optimization的结构优化技术。

Result: 通过结构优化方法，SET-MLP有可能在性能下降不超过4%的条件下实现超过40%的效率提升。

Conclusion: 结构优化技术可以显著提高稀疏多层感知机的性能，同时显著降低计算成本。采用SET-MLP 的稀疏进化训练有望实现性能改进。

Abstract: Deep Neural Networks (DNNs) have been proven to be exceptionally effective
and have been applied across diverse domains within deep learning. However, as
DNN models increase in complexity, the demand for reduced computational costs
and memory overheads has become increasingly urgent. Sparsity has emerged as a
leading approach in this area. The robustness of sparse Multi-layer Perceptrons
(MLPs) for supervised feature selection, along with the application of Sparse
Evolutionary Training (SET), illustrates the feasibility of reducing
computational costs without compromising accuracy. Moreover, it is believed
that the SET algorithm can still be improved through a structural optimization
method called motif-based optimization, with potential efficiency gains
exceeding 40% and a performance decline of under 4%. This research investigates
whether the structural optimization of Sparse Evolutionary Training applied to
Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent
this improvement can be achieved.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [180] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet通过缝合两个预训练模型，动态构建混合网络，能在变动的资源约束下提供灵活的准确率与效率权衡，显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 面对不同设备的异构计算和存储资源，很难跨所有平台部署单一模型，传统压缩方法不够灵活，难以适应不断变动的资源限制。

Method: 通过计算层级相似性来确定缝合点；保留大容量模型的前层并追加小容量模型的深层；只对缝合层进行微调；支持同构和异构模型的缝合。

Result: ReStNet实现了动态构建混合网络，在不同资源限制下展示了灵活的运行时准确率与效率的权衡。

Conclusion: ReStNet实现了灵活的准确率与效率的权衡，并大幅降低了训练成本。

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [181] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 提出了一种合成临床示范的防御策略，提升Med-VLMs模型的安全性，且不会显著影响性能，兼顾安全与过度防御的问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗视觉语言模型（Med-VLMs）在生成复杂文本信息时存在安全漏洞。因此，我们需要设计方法来确保模型能够拒绝有害查询，而不损害模型对正常临床查询的响应能力。

Method: 我们提出了一种基于合成临床示范的推理时间防御策略，同时使用混合示范策略来在安全与性能之间找到平衡，以缓解过度防御问题。

Result: 我们的防御策略在使用从九种模态收集的多样化医学成像数据集时，提升了模型的安全性，并且随着示范预算的增加，过度防御问题得到缓解。

Conclusion: 我们提出了一种新的推理时间防御策略，通过合成临床示范来进行防御，有效地减缓了有害查询的问题，并能够在不显著损害模型性能的前提下提升模型安全性。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [182] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于多模态语义指导的建筑立面墙壁和窗户自动分割模型SAAF，具有高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着建筑数字化发展，墙壁和窗户的自动分割是提高建筑信息模型和计算机辅助设计效率的关键步骤。

Method: 该研究提出了一种自动分割建筑立面墙壁和窗户的模型，称为SAAF（Segment Any Architectural Facades），采用多模态语义指导。该模型具有多模态语义协作特征提取机制，结合自然语言处理技术，将文本描述中的语义信息与图像特征融合，以增强建筑立面组件的语义理解能力。此外，开发了一种端到端的训练框架，使模型能够自主学习从文本描述到图像分割的映射关系，减少人为干预对分割结果的影响，并提高模型的自动化和鲁棒性。

Result: SAAF模型在多立面数据集上进行了广泛的实验，分割结果在mIoU指标上优于现有方法，表明在应对多样化数据集时，SAAF模型能够保持高精度的分割能力。

Conclusion: 该模型在提高墙壁和窗户分割任务的准确性和泛化能力方面取得了进展，有望为建筑计算机视觉技术的发展提供参考，并为多模态学习在建筑领域的应用探索新思路和技术路径。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [183] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 引入专为视频理解和推理设计的数据集，提出VersaVid-R1模型，在多项视频任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视频推理方面仍然发展不足，主要由于缺乏高质量的推理数据和有效的训练方法。

Method: 利用精心设计的DarkEventInfer和MixVidQA数据集进行强化学习训练，开发VersaVid-R1模型，支持多项选择和开放式问答，以及视频字幕生成。

Result: 提出了VersaVid-R1模型，在视频理解、认知推理和字幕生成任务中表现出色，显著超越现有模型。

Conclusion: VersaVid-R1在视频理解和推理任务中显著优于现有模型。

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [184] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM 是一个开源框架，用于评估多模态模型的表现，具有高效的评估速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 设计一个全面评估多模态模型的开源框架，以应对视觉语言理解与生成任务的多样化需求。

Method: FlagEvalMM 利用先进的推理加速工具（如 vLLM、SGLang）和异步数据加载，从而显著提高评估效率。

Result: 通过大量实验表明，FlagEvalMM 能够有效评估模型的强项和弱项。

Conclusion: FlagEvalMM 能够提供有关模型优缺点的准确、高效的见解，是促进多模态研究的宝贵工具。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [185] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: AVA-Bench is a new benchmark that evaluates specific visual abilities of VFMs, improving upon existing VQA benchmarks by offering precise performance insights and cost-efficient testing.


<details>
  <summary>Details</summary>
Motivation: Current VQA benchmarks often misrepresent VFM performance due to misaligned instruction tuning data and complex multi-ability requirements, necessitating a clearer evaluation framework like AVA-Bench.

Method: The paper introduces AVA-Bench, a benchmark that decouples 14 Atomic Visual Abilities and aligns training and test distributions, facilitating targeted evaluation of VFMs.

Result: Applying AVA-Bench highlights VFMs' specific ability fingerprints and shows a smaller LLM can perform as well as a larger one in ranking VFMs, greatly reducing evaluation costs.

Conclusion: AVA-Bench provides a systematic way to evaluate VFMs by explicitly testing 14 Atomic Visual Abilities (AVAs), allowing for precise identification of strengths and weaknesses.

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [186] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: BakuFlow is a semi-automatic tool that streamlines data labeling in computer vision, enhancing efficiency and scalability with features like a modified YOLOE framework and interactive modules.


<details>
  <summary>Details</summary>
Motivation: Labeling large-scale data in computer vision is time-consuming and prone to errors, making it necessary to develop tools that streamline and enhance the efficiency of the annotation process.

Method: BakuFlow provides a semi-automatic label generation process, incorporating tools like a live adjustable magnifier, interactive data augmentation, label propagation, and an automatic labeling module based on a modified YOLOE framework.

Result: The introduction of various features in BakuFlow, such as the ability to add new object classes and multiple visual prompts during annotation, enables a more flexible and scalable labeling process.

Conclusion: BakuFlow significantly reduces the labeling workload and improves efficiency for object detection and tracking in practical computer vision and industrial applications.

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [187] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为AAPT的方法，通过自回归对抗训练，实现了实时交互的视频生成，并在实验中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算密集，无法用于实时交互应用。本文旨在通过新的方法改善这种情况。

Method: 通过自回归对抗性后训练（AAPT），利用单一神经功能评估逐帧生成潜在视频帧，同时允许实时流式传输和用户交互。

Result: 实验显示8B模型在单个H100上能够实时生成24fps的736x416分辨率视频，或在8台H100上生成1280x720分辨率视频，最长可达一分钟。

Conclusion: 提出的AAPT方法成功将预训练的视频扩散模型转化为实时交互式视频生成器，能够在单个NFE中逐帧生成视频并流式传输结果。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [188] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: CAIRe是一个新的评估指标，能够评估图像的文化相关性，在多个数据集上的表现与人类判断高度一致，超越所有基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型的普及，确保其在不同文化背景下的公平性变得至关重要。然而，目前的努力由于性能下降、事实不准确或冒犯性的输出等权衡问题而受阻。通过可靠地衡量这些偏见的能力受限，进展停滞。

Method: 提出了一种新的评估指标叫做CAIRe，通过将图像中的实体和概念连结到知识库，并利用事实信息来为每个文化标签做出独立的分级判断。

Result: CAIRe在一个手动编制的具有文化意义的稀有物品数据集上，超越了所有基线28%的F1分数。在构建的两个文化通用概念数据集上，CAIRe与人类评分的皮尔森相关系数分别达到0.56和0.66。

Conclusion: CAIRe评估指标在评估图像的文化相关性方面表现出色，能够有效地解决跨文化偏见的问题，在多个数据集上表现与人类判断高度一致。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [189] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: SAGE通过语义增强擦除和全局-局部合作保留机制实现安全生成，解决了现有方法中"词概念深渊"问题，在确保去除不安全概念的同时保留无关概念。


<details>
  <summary>Details</summary>
Motivation: 在文本生成图像的扩散模型训练中，使用了含有敏感信息的预训练，带来了诸如不安全内容生成和版权侵权等安全风险。

Method: 提出语义增强擦除，通过循环自检和自擦除，将概念词擦除转化为概念域擦除，并通过语义空间关系在原始和训练扩散模型间高效探索和消除概念域的边界表示，无需额外的预处理数据。

Result: SAGE方法在生成安全内容的性能上，比其他方法表现出明显的优势，且其代码和模型权重将开源。

Conclusion: 通过有效探索和消除概念域边界以及增强无关概念的保留范围，SAGE方法在提升文本生成图像的安全性方面有显著的优势。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [190] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: 本文提出的姿势迁移生成合成人体动作视频数据方法改善了动作识别的效果，并通过可扩展少样本数据集弥补了训练数据的不足。


<details>
  <summary>Details</summary>
Motivation: 在视频理解任务中，尤其是涉及到人类运动时，合成数据生成常常因出现怪异特征而降低其训练有效性，未能充分发挥合成数据的潜力。

Method: 本文提出了一种利用姿势迁移生成合成人体动作视频数据的方法，具体采用可控的三维高斯头像模型。

Result: 我们在Toyota Smarthome和NTU RGB+D数据集上评估了这一方法，证明其能够提高动作识别任务的性能，还能有效扩展少样本数据集，从而补充训练数据中代表性不足的群体，并增加不同背景。

Conclusion: 合成数据由于不真实感，往往在任务训练中效果有限。但本文所提出的方法能够提升动作识别的性能。

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [191] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 本文介绍了 InterSyn 数据集与 SynJudge 评估模型，为构建高质量的多模态模型提供了创新性方法与工具。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在生成紧密交织的图文输出时表现不佳，主要是因为现有训练数据集的规模、质量和指令丰富性不足。

Method: 采用自我评估与迭代细化（SEIR）方法构建 InterSyn 数据集，同时开发了 SynJudge 评价模型。

Result: 实验研究表明，SEIR 方法生成的数据集质量显著高于无细化过程的数据集，基于 InterSyn 训练的 LMMs 在所有评估指标上均显示出性能提升。

Conclusion: InterSyn 为训练下一代遵循指令的大规模多模态模型提供了有力的数据支持，并通过 SynJudge 实现了对生成输出的可靠评估。

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [192] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: ReVisiT is a decoding method for LVLMs that enhances visual grounding by utilizing vision tokens, achieving better results and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs struggle with visually ungrounded responses due to failure in utilizing visual information effectively. Existing solutions typically require more training or computational resources.

Method: The paper introduces ReVisiT, a decoding method that references vision tokens to guide the text generation process in LVLMs. It projects vision tokens into the text token distribution space and dynamically selects the most relevant vision token at each decoding step via constrained divergence minimization.

Result: ReVisiT improves visual grounding as demonstrated in experiments on three LVLM hallucination benchmarks using two recent LVLM models. It also reduces computational costs by up to 2 times.

Conclusion: ReVisiT significantly enhances the visual grounding of LVLMs with minimal computational overhead, achieving competitive or superior results compared to state-of-the-art baselines.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [193] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: 本研究开发了TOGA模型，实现了在弱监督条件下的视频问答任务中的时间定位并达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在视频问答任务中，传统方法需要精确的时间标注，这在实际应用中代价高昂且难以获取。因此，本研究目标是开发一种无需时间注释即可实现时间定位的视频问答模型。

Method: 本研究提出了一种名为TOGA的视觉语言模型，该模型通过指令微调来联合生成答案和时间定位。在弱监督情境下，该模型利用伪标签生成时间定位，并使用一致性约束验证这些标签的有效性，从而提高了问答和时间定位的性能。

Result: TOGA模型在NExT-GQA、MSVD-QA和ActivityNet-QA等基准测试上展示了其在弱监督条件下的问答和时间定位的最先进性能。

Conclusion: 该研究成功开发了一个在弱监督条件下进行时间定位的开放式视频问答模型，TOGA，实现了在无时间注释下的高效视频问答和时间定位，并在多个基准上达到了最先进的性能。

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [194] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 通过在推理阶段对图像进行裁剪并与文本匹配，提高视觉语言模型的组合性能，尤其在属性-对象绑定上效果显著。


<details>
  <summary>Details</summary>
Motivation: 目前大多数提升视觉语言模型组合能力的工作集中于训练阶段，而推理阶段的技术探索较少。本文提出在推理过程中添加简单结构，以提高模型的组合性能。

Method: 在推理过程中对图像进行裁剪，将其与文本段落匹配，并通过聚合匹配的相似度来计算最终的图像文本相似度，以此提高视觉语言模型的组合能力。

Result: 提出的方法在不需要训练的情况下显著提升了不同视觉语言模型的性能，特别是在属性-对象绑定方面的表现优异。

Conclusion: 添加简单的结构到推理过程中可以提高视觉语言模型在图像文本检索任务中的性能，尤其在属性-对象绑定方面表现优异。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [195] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: 本研究探讨了一种新的视频基础对话问答任务，提出了一个数据集，并评估了基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉问答只能处理静态图像，而本研究旨在扩展到基于视频的对话场景，其中需要结合时间维度的信息识别及外部知识来回答问题。

Method: 研究通过构建包含2017个视频和5986段人类标注对话的数据集，在这些对话中有40954个交替的对话轮次。此外，研究评估了几个基于该数据集的基线模型。

Result: 模型成功地识别了视频中相关的部分并结合外部知识进行对话，但在处理对话情境和没有视觉信息的问题时仍面临挑战。

Conclusion: 本研究引入了一个新的数据集和若干基线模型，以探索视频基础上的对话式视觉问答任务，并展示了该任务所面临的未来挑战。

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [196] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 本文提出Symbolic Graph Ranker (SGR)，结合文本和图结构的方法，提升大语言模型（LLMs）在会话搜索中的效果，并在AOL和Tiangong-ST数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的会话搜索策略通常侧重于深度语义理解的序列建模，而忽视了交互中的图结构。尽管一些方法侧重于捕获结构信息，但它们对文档使用的是广义表示，忽略了词级语义建模。

Method: 我们首先引入一套符号语法规则，将会话图转化为文本。这使得可以将会话历史、交互过程和任务指令无缝地整合为LLM的输入。此外，我们引入了一系列自监督符号学习任务，包括链接预测、节点内容生成和生成对比学习，以使LLMs能够从粗粒到细粒捕获拓扑信息。

Result: 我们的方法能够提升LLMs在文本格式中捕获图结构的能力。

Conclusion: 实验结果和全面分析表明，SGR方法在两个基准数据集AOL和Tiangong-ST上表现出色。该方法为传统搜索策略与现代LLMs之间开辟了一条新颖且有效的方法论。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [197] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: 引入AD^2-Bench，首个针对自动驾驶的链式推理基准测试，填补了在复杂环境下评估多模态模型推理能力的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试在恶劣天气和复杂交通环境中对链式推理的评估存在缺失，亟需一种严谨的评估方法来填补这一空白，推动多模态大型模型在自动驾驶领域的推理能力发展。

Method: AD^2-Bench通过收集超过5400个高质量的手动标注的链式推理实例，并对每个中间推理步骤进行细粒度分析，以评估多模态大型模型的推理过程。

Result: 对最先进的多模态大型模型进行全面评估，发现其在AD^2-Bench上的准确率低于60%，这表明测试难度很大，需要进一步提升自动驾驶系统的稳健性和可解释性。

Conclusion: AD^2-Bench为链式推理在自动驾驶领域的研究提供了一个标准化的评估平台，推动多模态大型模型在复杂环境中的推理能力的提高。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [198] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: BG-HOP通过扩展单手先验来模拟双手交互，展示了生成双手与物体交互的能力。


<details>
  <summary>Details</summary>
Motivation: 解决有限的双手交互数据问题。

Method: 通过扩展现有单手生成先验来建模双手交互。

Result: 模型能够生成双手交互并合成特定物体的抓取姿态。代码和模型已经公开。

Conclusion: BG-HOP是一个生成先验模型，可以有效建模3D双手交互。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [199] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: HSENet提供了一种新的方法来提高3D医学图像的视觉-语言理解，提升了诊断准确性和效率，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中于2D医学图像，限制了在复杂3D解剖结构中的表现，导致病理误解和误诊。

Method: 提出了Hybrid Spatial Encoding Network (HSENet)，使用双3D视觉编码器以及空间打包器，通过体积上下文和解剖细节的双阶段对齐实现视觉与语言的有效感知和投射。

Result: 实验表明，该方法在3D语言视觉检索中表现出色，3D医学报告生成和3D视觉问答任务中，分别取得了39.85%、24.01%和73.60%的出色成绩。

Conclusion: HSENet能够感知并融合3D视觉信息，提高诊断文本生成的准确性，验证了其在医学成像中的有效性。

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [200] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: DGAE通过改进解码器和利用扩散模型，在高压缩率下提高性能并缩小潜在空间尺寸。


<details>
  <summary>Details</summary>
Motivation: 提升自动编码器在高压缩率条件下的性能，并解决因GAN训练不稳定性的问题。

Method: 提出DGAE，通过利用扩散模型指导解码器恢复未完全解码的潜在表示中的信息信号。

Result: DGAE在高空间压缩率下有效缓解性能下降，并在整合扩散模型时显示出在ImageNet-1K上的竞争性能。此外，DGAE实现了2倍更小的潜在空间，促进了扩散模型的更快收敛。

Conclusion: DGAE通过改善解码器的表现力应对高空间压缩率下的性能挑战，同时保持紧凑高效的潜在表示。

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [201] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 推理模型在面对误导性输入时表现出稳健性不足，甚至顶级模型在“气灯效应”下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然推理模型的链式思维提示和测试时缩放等机制提高了稳健性，但其面对误导性用户输入的能力仍需进一步探索。

Method: 进行了系统评估，使用三个多模态基准测试对三个最先进的推理模型进行实验。提出了新诊断基准GaslightingBench-R来进一步探测这种漏洞。

Result: 通过新提出的GaslightingBench-R基准测试，发现显著的模型准确率下降，超过53%。

Conclusion: 当前顶级推理模型在应对误导性用户输入时表现出显著的准确率下降，这意味着这些模型在策略性用户反馈下难以保持正确答案。

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [202] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: FasterSNN offers an efficient, stable solution for AD diagnosis by leveraging spiking neural networks, overcoming existing limitations.


<details>
  <summary>Details</summary>
Motivation: Address limitations of deep learning in AD diagnosis: energy inefficiency, computational demands, and lack of interpretability.

Method: Proposed FasterSNN, a hybrid architecture combining LIF neurons, adaptive convolution, and multi-scale spiking attention.

Result: Experiments show FasterSNN achieves high performance with enhanced efficiency and stability in AD screening tasks.

Conclusion: FasterSNN demonstrates competitive diagnostic performance with improved efficiency and stability for AD screening.

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [203] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: 研究生成模型中的偏见，并发现属性偏移小但对属性分类器敏感，尤其在高密度区域。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能模型的广泛采用引发了对表征性伤害和潜在歧视性结果的日益关注，但有关偏见产生机制的文献仍不完善，尤其是在无条件生成方面。

Method: 我们训练了一组无条件的图像生成模型，并采用常用的偏差评估框架来研究训练和生成分布之间的偏差转移。

Result: 实验显示检测到的属性偏移较小。属性偏移对用于标记生成图像的属性分类器极为敏感，尤其是当其决策边界位于高密度区域时。对于位于谱系上的属性值，这种分类器灵敏度尤为明显。

Conclusion: 分析表明，属性偏移的检测值较小，属性偏移对用于标记生成图像的属性分类器极为敏感，特别是当其决策边界位于高密度区域时。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [204] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: 研究提出了一个用于远程光电容积图（rPPG）监测的长时间数据集LADH，结合RGB和IR输入及多任务学习，改善了生理信号监测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 寻求解决在高海拔环境中进行长期个人护理监测的挑战，如环境光变化、手部动作遮挡和动态面部姿势。

Method: 提出并使用了LADH长时间rPPG数据集，结合多任务学习，进行包括RGB和IR视频输入的实验。

Result: 实验表明，结合RGB和IR视频输入在心率估计中实现了4.99 BPM的平均绝对误差，增强了非接触式生理监测的准确性和鲁棒性。

Conclusion: 结合RGB和IR视频输入可以提高非接触式生理监测的准确性和鲁棒性，多任务学习可以同时提升多种生理指标的性能。

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [205] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: 提出了一种视觉扰动框架，提升多模态模型的数学推理能力，研究显示视觉处理的核心作用。


<details>
  <summary>Details</summary>
Motivation: 研究发现语言模型通过图像标题可以达到与直接使用视觉输入的多模态模型相比较好的表现，因此提出了一个框架来增强视觉处理的鲁棒性。

Method: 提出了一种简单的视觉扰动框架，包括干扰物拼接、保留主导性的混合和随机旋转，可以轻松集成到现有的后期训练管道中。

Result: 通过大量实验，在多个数据集上，展示了在数学推理性能上的一致改进，且这种改进与通过算法变化获得的效果相当。

Conclusion: 视觉扰动对多模态数学推理中的角色很关键：更好的推理始于更好的视觉处理。

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [206] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 研究提出了一种名为PatchGuard的基于ViT的异常检测与定位方法，通过利用伪异常样本显著提升了其对抗性鲁棒性及性能。


<details>
  <summary>Details</summary>
Motivation: 当前的异常检测和定位方法容易受到对抗性攻击，主要是因为训练数据的限制，它们通常只包含正常的、未标记的样本。

Method: 研究引入了一种基于视觉Transformer架构的PatchGuard方法，通过局部化掩码结合伪异常来增强对抗性鲁棒性。

Result: PatchGuard在实验上对工业和医疗数据集的评估显示，在对抗性环境中比现有方法表现出显著提升，在异常检测方面提升了53.2%，在异常定位方面提升了68.5%。

Conclusion: PatchGuard方法在提高模型鲁棒性方面展示了显著表现，并在对抗性和非对抗性环境中保持竞争性准确率。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [207] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: 我们提出ELBO-T2IAlign方法，校正扩散模型中的像素文本对齐偏误，无需训练，对多种架构有效。


<details>
  <summary>Details</summary>
Motivation: 当前方法假设扩散模型中的文本图像对齐是完美的，但实际上并非如此，因此需要一个新的方法来评估和校正这种对齐。

Method: 我们使用零样本引用图像分割作为代理任务，评估流行扩散模型的像素级图像和类别级文本对齐。我们提出了一种简单而有效的方法ELBO-T2IAlign，以校准基于证据下界(ELBO)的像素文本对齐。

Result: 在常用的图像分割和生成基准数据集上的广泛实验验证了我们提出的方法的有效性。

Conclusion: 我们提出的ELBO-T2IAlign方法能够有效校正扩散模型中的像素文本对齐问题，特别是在小尺寸、遮挡和稀有物体类别的图像中。

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [208] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: UFM is a unified model for dense image correspondence, significantly improving accuracy and speed over specialized methods using a simple transformer architecture.


<details>
  <summary>Details</summary>
Motivation: The authors aim to bridge the separate treatment of wide-baseline scenarios and optical flow estimation in dense image correspondence by developing a unified model.

Method: UFM employs a simple, generic transformer architecture to directly regress the (u,v) flow from unified training data for pixels visible in both source and target images.

Result: UFM achieves 28% higher accuracy compared to state-of-the-art flow methods, reduces error by 62%, and is 6.7 times faster than dense wide-baseline matchers.

Conclusion: UFM demonstrates that unified training can outperform specialized approaches in dense image correspondence, improving both accuracy and efficiency.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [209] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: DarkerBB通过相似度分数在特征脸空间重建彩色人脸，实现了领先的验证准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别模型存在隐私威胁，特别是面临嵌入访问限制时的模型反演问题。

Method: 本文提出DarkerBB，通过PCA导出的特征脸空间进行零阶优化以重建彩色人脸图像。

Result: 在LFW、AgeDB-30和CFP-FP基准测试中，DarkerBB在仅使用相似度的情况下达到了最先进的验证准确率，并且查询效率具有竞争力。

Conclusion: DarkerBB在极少的信息条件下，通过相似度实现了有效的人脸图像重建，表现出色的验证准确率和查询效率。

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [210] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 本文提出了一种轻量高效的航空影像物体检测方法，即量化后的YOLOv4-Tiny模型，具备较小的模型大小和较快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 紧急救援场景中的航空影像物体检测需要轻量且高效的解决方案。

Method: 使用后训练量化到INT8精度优化的YOLOv4-Tiny模型，并在定制的航空紧急数据集上进行训练和测试。

Result: 量化后的YOLOv4-Tiny在保持检测效果的同时，实现了模型大小的显著减小和推理速度的提升。

Conclusion: 证明量化YOLOv4-Tiny对实时低功耗设备紧急检测的适用性。

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [211] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: Q-SAM2通过低位量化和训练技术显著提高了资源受限场景中的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决高计算成本和内存消耗问题，以支持资源受限场景中的SAM2应用。

Method: 提出了一种准确的低位量化方法Q-SAM2，通过线性层校准和量化感知训练（QAT）来提高SAM2的量化效率。

Result: 实验表明，Q-SAM2不仅在量化过程中获得高精度，还能在现有的量化方案中表现优异，尤其是在超低2-bit量化中。

Conclusion: Q-SAM2展示了显著的量化性能，尤其是在2-bit量化中，显著提高了效率和精度。

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [212] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat advances dynamic scene reconstruction with superior accuracy and realism by using Gaussian Splatting with motion modeling and opacity estimation.


<details>
  <summary>Details</summary>
Motivation: Improve the ability to reconstruct complex, dynamic environments in computer vision, addressing shortcomings in existing methods.

Method: DynaSplat uses Gaussian Splatting extended to dynamic scenes with dynamic-static separation, hierarchical motion modeling, and physically-based opacity estimation.

Result: DynaSplat successfully reconstructs dynamic scenes with higher accuracy and realism than state-of-the-art methods, handling complex motions and occlusions effectively.

Conclusion: DynaSplat exceeds existing methods in accuracy and realism for reconstructing dynamic scenes, offering a more intuitive, compact, and efficient approach.

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [213] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: 研究提出了一种新方法OctoNav-R1，通过思考前行动提高了导航代理的推理能力，取得了优异的任务表现。


<details>
  <summary>Details</summary>
Motivation: 寻求通用化的导航代理，能够遵循包括任意多模态和多能力组合的自由形式指令，提升在多种导航任务中的适应性和性能。

Method: 提出一个大规模基准OctoNav-Bench和对应的方法OctoNav-R1，设计了Hybrid Training Paradigm (HTP)，包括三个阶段：Action-/TBA-SFT、Nav-GPRO和Online RL。使用TBA-CoT数据集进行模型微调以增强推理能力。

Result: OctoNav-R1在与之前方法的比较中表现出更优的性能，成功实现了思考前行动的能力。

Conclusion: OctoNav-R1在多个导航任务中展示了优异的性能，支持多模态和多能力的自由指令导航。

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [214] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: 提出了一种通过字符分布对齐增强手写文本识别模型的新损失函数，实验结果证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 手写文本识别面临的问题是字符集和频率分布随着时间和地域变化而变动，导致训练在广泛异质语料上的模型在特定子集上的表现不佳。

Method: 提出了一种新的损失函数，利用Wasserstein距离对字符频率分布进行惩罚，并在推理阶段作为打分函数集成到引导解码方案中。

Result: 新的方法提高了模型的准确性和鲁棒性，特别是在数据集内的时空和上下文变化条件下。实验结果验证了该方法在多个数据集和架构上的有效性。

Conclusion: 我们提出了一种新的损失函数，通过计算预测文本的字符频率分布与目标分布的Wasserstein距离来优化手写文本识别模型的性能。实验结果表明，这提高了模型在多个数据集和架构下的泛化能力和表现。

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [215] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出了一种几何蒸馏框架，通过蒸馏三维模型中的几何线索，提升视觉-语言模型对三维空间的理解，取得了优于前人的结果且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在视觉和语言任务上表现优异，但在理解三维空间结构方面存在局限。

Method: 提出了一种几何蒸馏方法，通过从现成的三维基础模型中蒸馏稀疏对应关系、相对深度关系和密集代价量而不修改模型架构。

Result: 在三维视觉语言推理和三维感知基准测试中，该方法优于之前的方法，实现了更好的三维空间推理，计算成本显著降低。

Conclusion: 展示了一种将二维训练的视觉-语言模型与三维理解结合的高效途径，促进其在空间多模态任务中的应用。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [216] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: 提出了一种名为HadaNorm的新方法，通过更有效的激活量化和规范化处理改善图像生成中的量化误差和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的扩散模型在图像生成方面表现出色，但高内存和计算需求限制了其在资源受限设备上的部署。PTQ作为一种解决方案在减少矩阵操作的位宽方面表现较好，但在处理离群值时存在困难。

Method: 提出HadaNorm，一种新颖的线性变换方法，能够在应用Hadamard变换之前通过规范化激活特征通道，有效缓解离群值的问题，从而实现更积极的激活量化。

Result: HadaNorm能够在变压器块的不同组件上一致地减少量化误差，与现有的先进方法相比，具有更优越的效率性能权衡。

Conclusion: HadaNorm提供了一种解决现有PTQ方法在处理离群值时遇到困难的新解决方案，并能够在实现较高压缩的同时保持性能。

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [217] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: CausalVQA是一个视频问答基准数据集，测试模型在现实世界中的因果理解和预测能力，现有多模态模型显著低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有的VQA基准测试要么侧重于对真实视频的表面感知理解，要么通过模拟环境创建狭窄的物理推理问题，CausalVQA填补了现实场景中推理理解能力测试的空白。

Method: 设计了一套质量控制机制，防止模型利用简单捷径，要求模型基于深刻的视觉理解而不是语言线索做出回答。

Result: 展示出当前系统在利用时空推理、理解物理原则和理解可能的替代方案以在现实世界中做出准确预测方面的挑战。

Conclusion: 当前前沿的多模态模型在CausalVQA基准测试中表现明显低于人类水平，尤其在预测和假设问题上。

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [218] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: UniPre3D是一种统一的预训练方法，通过高斯预测和2D特征整合来处理任意规模的点云和3D模型，实现精准的像素级监督。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏能够同时有效处理对象级和场景级点云的统一3D模型及预训练方法，因此需要开发一种能够适用于点云的统一表示学习技术。

Method: 使用可微高斯喷溅技术进行图像渲染，实现精确的像素级监督和端到端优化，并整合预训练图像模型中的2D特征。

Result: 通过广泛的实验验证UniPre3D在多种对象级和场景级任务中的通用有效性。

Conclusion: UniPre3D是一种首创的统一预训练方法，能够同时应用于任何规模的点云和3D模型架构，显著提升对象级和场景级任务的效果。

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [219] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文综述了视觉通才模型及其在计算机视觉任务中的表现，探讨了相关技术、应用场景和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于通才模型在自然语言处理中的成功，越来越多的研究人员开始尝试将这些模型应用于计算机视觉任务。但视觉任务的输入输出更加多样化，难以总结为统一的表示。

Method: 综述方法，包括对数据集、任务和基准的回顾，分析现有研究中提出的框架设计及用于提升性能的技术，探讨相关领域的联系和潜在协同作用。

Result: 提供视觉通才模型的背景、框架设计、增强性能的技术、实际应用场景及面临的挑战，并对未来的研究方向提供指导。

Conclusion: 对视觉通才模型进行了全面综述，强调了其在该领域的特点和能力，并提供了实际应用场景、挑战分析及未来研究方向的见解。

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [220] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: 提出VILASR，通过绘图操作提升视觉语言模型在空间推理任务中的表现，平均改善18.4%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间推理任务中存在根本性限制，难以实现精确的几何理解和连续空间跟踪，通过“在空间中绘图推理”以增强LVLMs的空间推理能力。

Method: 提出了一种名为“在空间中绘图推理”的新范式，通过基本绘图操作来引导LVLMs进行空间推理，并开发了三阶段训练框架，包括冷启动训练、反射性拒绝采样和强化学习。

Result: VILASR模型在各种空间推理基准测试中表现优异，平均提升18.4%。

Conclusion: VILASR模型通过在视觉空间中进行基本绘图操作，提升了LVLMs在空间推理任务中的表现，从而超越了现有方法。

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [221] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: 引入了一种新框架，实现了对多概念视频的精确控制，验证了较其他现有方法更为有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多只支持单一主体的动画，并以全局方式注入条件，该假设限制了对多个概念的精确控制，影响了应用效果。

Method: 通过区域特异性绑定条件，将来自多种模态的信息应用于每个身份的时空足迹，并利用遮罩预测器匹配去噪视频与参考外观之间的外观线索。

Result: 实验结果和消融研究证实，提出的方法在多模态条件下的显式布局控制相比于现有方法更有效。

Conclusion: 提出了一种新的框架，可以实现对多个概念的精确控制，提高了多概念以人为中心的视频生成质量。

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [222] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出HopaDIFF框架，用于解决多人场景中基于文本指导的动作分段问题，在RHAS133数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要解决固定动作序列的单人活动，忽略多人场景，需要开发一种能够处理多人场景中基于文本指导的动作分段的新方法。

Method: 提出了一种称为HopaDIFF的整体-部分意识的傅里叶条件扩散框架，利用一种新的交叉输入门控注意xLSTM来增强整体-部分的长程推理，并通过新的傅里叶条件引入更精细的控制，以改进动作分段生成。

Result: HopaDIFF在RHAS133数据集的多种评估设置中达到了最新的技术水平。

Conclusion: HopaDIFF achieves state-of-the-art results on the RHAS133 dataset for multi-person action segmentation guided by textual references.

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [223] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: 提出EditInspector框架评估文本指导图像编辑，发现现有模型在评估时的不足，并开发了两种新方法提高评估性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的进步，文本指导的图像编辑越来越普及，需要一个全面的框架来验证这些编辑并评估其质量。

Method: 创新提出了一个基准测试——EditInspector，用于评估文本指导的图像编辑，通过一个详细的编辑验证模板收集的人类注释进行评估。另外，提出了两种新的方法提升性能。

Result: 新方法在伪影检测和生成差异标题方面胜过当前最先进的模型。

Conclusion: 当前模型在全面评估编辑方面表现不佳，常常在描述变化时产生幻觉。新的方法在检测伪影和生成差异标题方面表现优于当前最先进的模型。

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [224] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出一种新的图像恢复任务TAIR及其方法TeReDiff，可提高文本恢复的准确性，实验结果优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散基础的图像恢复方法在恢复文本区域时往往难以保真，因此引入TAIR任务以提高文本区域的恢复质量。

Method: 提出了一种名为TeReDiff的多任务扩散框架，将扩散模型的内部特征整合到文本识别模块中，通过联合训练提高文本区域的恢复质量。

Result: 实验结果显示该方法在文本识别准确性上超过了现有的最先进恢复方法。

Conclusion: TAIR方法在图像恢复特别是文本恢复方面表现优异，显著提高了文本区域的识别准确性。

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [225] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出了一种新框架CINeMA，用于创建低数据环境下的新生儿和胎儿大脑高分辨率多模态图谱，显著提升了构建效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统大脑图谱依赖于大规模数据集，但在病理学条件下数据稀缺，研究面临挑战。该论文旨在解决这一问题，创造适用于低数据环境的高分辨率图谱。

Method: CINeMA是一种条件隐式神经多模态图谱框架，它通过在潜空间中操作，避免了计算密集型的图像配准过程，并显著减少了图谱构建时间。

Result: CINeMA在准确性、效率和灵活性上超越了现有技术，并能够支持组织分割、年龄预测、数据合成和数据扩充任务。

Conclusion: CINeMA提供了一种高效、准确和多功能的方法来创建新生儿和胎儿大脑的多模态高分辨率时空图谱，尤其适用于数据稀缺环境。

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [226] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: Kvasir-VQA-x1是一个大规模的GI内视镜数据集，增加了复杂的问答对和视觉增强，以支持临床AI的发展。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持系统的开发因缺乏临床复杂性和视觉多样性的数据集而受限。

Method: 使用大型语言模型系统地生成问题，并通过复杂度分层来更好地评估模型的推理能力。

Result: 引入Kvasir-VQA-x1，一个大规模胃肠内视镜数据集，包含159,549个新的问答对，旨在测试更深层次的临床推理能力，并包含多种视觉增强以模拟常见图像伪影。

Conclusion: Kvasir-VQA-x1提供了一个更具挑战性和临床相关性的基准，旨在加速更可靠和有效的多模态AI系统在临床环境中的发展。

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [227] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: The paper introduces the MVP benchmark to better test video-language models' physical understanding by using video pairs that counter shortcut solution reliance. Models perform poorly compared to humans under this benchmark.


<details>
  <summary>Details</summary>
Motivation: To provide a more accurate assessment of the spatio-temporal understanding and reasoning abilities of video language models by addressing existing benchmarks' susceptibility to shortcut solutions.

Method: The MVP benchmark uses minimal-change pairs to test video language models, requiring them to answer both videos correctly to counteract superficial cue reliance.

Result: Human performance on MVP is significantly higher than models, with humans scoring 92.9% compared to the best model's 40.2%.

Conclusion: MVP benchmark is effective in assessing the physical understanding of video language models by eliminating shortcut solutions.

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [228] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Main category: physics.med-ph

TL;DR: 这项研究使用强化学习在HDR前列腺治疗中生成针位置方案，减少针数量并实现相同或更好的计划质量，具有标准化治疗计划的潜力。


<details>
  <summary>Details</summary>
Motivation: 在HDR前列腺近距离放射治疗过程中，针的位置完全依赖于医生的经验。因此研究使用强化学习在术前规划阶段根据患者解剖提供针的位置和驻留时间的可行性，以减少手术时间并确保一致的计划质量。

Method: 通过训练一个强化学习代理，在观察环境后调整选择的针的位置及其所有驻留时间，以最大化预定义的奖励函数。代理会依次调整所有针的位置，进行多轮操作直到达到限制的轮数。本研究包含11例基于前列腺HDR增强治疗患者的数据，其中1例用于训练，10例用于测试，然后比较RL计划和临床结果的剂量学指标和使用的针数量。

Result: 平均而言，强化学习计划和临床计划具有非常相似的前列腺覆盖率（Prostate V100）和直肠D2cc（无统计学意义），而RL计划在前列腺热点（Prostate V150）和尿道D20%计划上具有统计学意义。此外，RL计划比临床计划平均少使用2个针。

Conclusion: 研究展示了使用强化学习自动生成临床实用的HDR前列腺近距离放射治疗计划的可行性。与传统临床方法相比，该RL方法能够实现相同或更好的计划质量，同时使用更少的针。此方法数据需求低且通用性强，具有标准化近距离放射治疗计划、减少临床变异性和改善患者结果的巨大潜力。

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [229] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Main category: cs.LO

TL;DR: 文章引入了支持正式证明的抽象基础DNN验证框架，解决了工具在扩展性和可证性保证之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 随着DNN验证结果可靠性需求的增加，现行验证工具面临着在扩展性和可证性保证方面的差距。本文旨在通过引入支持证据生成的抽象基础DNN验证框架来填补这一差距。

Method: 将验证任务模块化地分为两个部分：一是证明抽象网络的正确性；二是证明该抽象相对于原始DNN的有效性。

Result: 提出了第一个生成正式证明的方法，支持常见的抽象技术使验证更具可扩展性和可信度。

Conclusion: 该研究提出了一种新的框架，用于生成抽象基础深度神经网络验证的证明，从而在可扩展性和可证明保证之间架起了一座桥梁。

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [230] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: 提出了一种新颖的对抗性文本攻击方法DCP，能在保持语义一致和流利的同时，动态改变文本上下文，并在众多NLP模型中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常专注于词级或局部文本片段的改变，忽视了更广泛的背景，从而导致可检测或语义不一致的扰动。

Method: 我们提出了一种新颖的对抗性文本攻击方案，称为动态语境扰动（DCP）。DCP在句子、段落和文档中动态生成上下文感知的扰动，确保语义保真度和流利性。利用预训练语言模型的能力，DCP通过一个对抗性目标函数迭代地优化扰动，该目标函数平衡了引发模型误分类和保持文本自然性的双重目标。

Result: 我们的实验结果表明，在各种NLP模型和数据集上，DCP在挑战最先进NLP系统的鲁棒性方面非常有效。通过整合动态上下文分析，DCP显著增强了对抗性攻击的微妙性和影响力。

Conclusion: 这项研究强调了上下文在对抗性攻击中的重要性，为创建能够抵御复杂对抗性策略的更强大NLP系统奠定了基础。

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [231] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Main category: cs.CR

TL;DR: 本研究分析了深度学习在恶意软件检测中对虚假关联的依赖，并比较了两种端到端模型的适用性，提升生产选择模型的理解。


<details>
  <summary>Details</summary>
Motivation: 深度学习在恶意软件检测中显示出无与伦比的性能，但是这种性能的实现是通过利用虚假关联——即在推理过程中具有高相关性但通过领域知识被认为无用的特征。

Method: 进行详细分析，揭示模型在检测恶意软件时对编译器留下的空白区的依赖程度，并通过小规模平衡数据集进行开创性分析，介绍两个端到端模型的排名。

Result: 分析展示深度学习在恶意软件检测中的虚假关联现象，并提出两个模型的适用性比较，为实际生产中的模型选择提供依据。

Conclusion: 深度学习在恶意软件检测中的性能受虚假关联影响，应关注模型在实际应用中的选择。

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [232] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: 研究探讨了为深度学习生成模型提供差分隐私保证的实现方法及其效用影响，在大数据集和有限使用下可能实现形式化隐私保证。


<details>
  <summary>Details</summary>
Motivation: 探索在深度学习生成模型中实现差分隐私的实用性，以及分析在此过程中模型效用和隐私之间的权衡。

Method: 应用DP-SGD方法及提出新的差分隐私机制来保障生成模型的隐私性，并通过不同的模型（如Diffusion、VAE、GAN）评估其效用性。

Result: DP-SGD方法对性能影响显著，但在数据集较大时仍能保持一定效用。新的DP机制提高了训练稳定性，尤其是对于不稳定模型如GAN和较小数据集。Diffusion模型在无保证的情况下效用最佳，但在DP-SGD环境下GAN表现最佳。

Conclusion: 差分隐私轨迹生成仍然是一项具有挑战性的任务，目前只有在大数据集和有限的使用情况下，才能实现形式化的隐私保证。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [233] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: TooBadRL框架通过优化触发器设置，提高了深度强化学习后门攻击的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 为了填补当前深度强化学习后门攻击方案在触发器优化方面的不足。

Method: 引入TooBadRL框架，通过触发器优化来提升后门攻击效果。它通过性能感知自适应冻结机制优化注入时机，利用Shapley值分析选择注入维度最具影响力的状态变量，并提出基于梯度的对抗程序优化注入幅度。

Result: TooBadRL在三个主流DRL算法和九个基准任务上的评估表明，它显著提高了攻击成功率，同时确保正常任务性能的降级最小化。

Conclusion: 原则性的触发器优化在DRL后门攻击中是非常重要的。TooBadRL显著提高了攻击效果。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [234] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Main category: cs.CR

TL;DR: LLMail-Inject挑战模拟了间接提示注入攻击，通过大量攻击尝试的数据集提供了对指令与数据分离问题的新见解。


<details>
  <summary>Details</summary>
Motivation: 许多实际的基于LLM的应用易受间接提示注入攻击的影响，尽管有许多防御提议，但系统性评估针对自适应对手的研究仍然有限。

Method: 进行了LLMail-Inject公开挑战，模拟真实场景，参与者尝试将恶意指令注入电子邮件，以触发LLM电子邮件助手中的未授权工具调用。挑战涵盖了多种防御策略、LLM架构和检索配置。

Result: 收集到了从839名参与者提交的208,095个独特的攻击企图的数据集，并发布了挑战代码、完整的提交数据集以及分析结果。

Conclusion: 这项研究为理解指令与数据分离问题提供了新的见解，并希望能成为未来研究的基础，以开发实用的结构性解决方案来应对提示注入问题。

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [235] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: 提出了一种基于MAB和BCOMD算法的在线优化框架，以优化TN-NTN网络中的重要系统参数，实现了在高峰期降低不满意用户比例及最多19%吞吐量提升和5%节能。


<details>
  <summary>Details</summary>
Motivation: 非地面网络（NTN）在传统网络设置中的作用较少被关注，特别是在减轻地面网络负载和提高能效方面，本研究旨在探讨NTN在支持可持续网络中的潜力。

Method: 采用多臂匪徒（MAB）公式和反馈约束的在线镜像下降（BCOMD）算法来实现优化框架。

Result: 通过24小时的广泛系统级模拟，研究表明该框架在网络高峰期显著减少了不满意用户，并在低流量时期实现了最多19%的吞吐量提升和5%的节能。

Conclusion: 这项研究提出了一种新颖的在线优化框架，通过多臂匪徒公式和BCOMD算法来优化TN-NTN架构，以实现实时的网络容量和能效平衡。

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [236] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: 本文提出了一种生成模型方法，用于处理缺失数据情况下的实时网络流量预测，并实现了高效准确的预测。


<details>
  <summary>Details</summary>
Motivation: 现有的网络流量预测方法假设数据是完全观测到的。然而，在实际中，由于各种人为和自然因素，收集到的数据往往是不完整的。

Method: 我们首先将网络流量预测建模为一个张量完成问题，然后结合预训练的生成模型以实现通常与张量完成相关的低秩结构。最后，通过优化潜在表示简化优化过程，实现实时预测。

Result: 实验表明，我们的方法在100毫秒内实现了精确的网络流量预测，平均绝对误差(MAE)低于0.002。

Conclusion: 本文提出的生成模型方法能够解决在存在缺失数据的情况下的实时网络流量预测问题，并在实验中得到了准确的预测效果。

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [237] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究探讨了使用代理模型优化高纵横比特征中的PEALD过程，使用人工神经网络预测饱和时间，结果显示有高达99%的准确率，可加速微电子领域PEALD工艺优化。


<details>
  <summary>Details</summary>
Motivation: 在高纵横比结构中，为优化等离子体增强原子层沉积(PEALD)过程而探索代理模型。

Method: 使用基于PEALD模拟的合成数据集，训练人工神经网络，以基于部分涂层条件下获得的横截面厚度数据预测饱和时间。

Result: 通过未饱和条件下进行的两个实验可预测饱和时间，误差在10%以内。一个训练出来的代理模型用于确定表面复合是否主导PEALD过程中的等离子体与表面相互作用，达到了99%的准确率。

Conclusion: 该研究证明了机器学习可以加速PEALD工艺优化，特别是在微电子领域。这种方法也可以轻松扩展到原子层刻蚀和更复杂的结构。

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [238] [How attention simplifies mental representations for planning](https://arxiv.org/abs/2506.09520)
*Jason da Silva Castanheira,Nicholas Shea,Stephen M. Fleming*

Main category: q-bio.NC

TL;DR: 研究虚拟迷宫中空间注意力对计划的影响，发现注意力机制影响任务表示的构建，并解释了个体间行为差异。


<details>
  <summary>Details</summary>
Motivation: 探索人类在计划时如何通过感知机制构建任务表示，进而影响计划过程。

Method: 使用虚拟迷宫导航研究空间注意力对任务表示的影响。

Result: 发现空间注意力的分布影响了任务表示的构建，人们依据注意力的自然轮廓更容易构建简化的迷宫表示。

Conclusion: 研究表明个体之间的注意力机制差异导致任务表征和行为的不同，并将视觉空间注意力纳入现有的计算模型以优化计划过程。

Abstract: Human planning is efficient -- it frugally deploys limited cognitive
resources to accomplish difficult tasks -- and flexible -- adapting to novel
problems and environments. Computational approaches suggest that people
construct simplified mental representations of their environment, balancing the
complexity of a task representation with its utility. These models imply a
nested optimisation in which planning shapes perception, and perception shapes
planning -- but the perceptual and attentional mechanisms governing how this
interaction unfolds remain unknown. Here, we harness virtual maze navigation to
characterise how spatial attention controls which aspects of a task
representation enter subjective awareness and are available for planning. We
find that spatial proximity governs which aspects of a maze are available for
planning, and that when task-relevant information follows natural (lateralised)
contours of attention, people can more easily construct simplified and useful
maze representations. This influence of attention varies considerably across
individuals, explaining differences in people's task representations and
behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the
effects of visuospatial attention into existing computational accounts of
value-guided construal. Together, our work bridges computational perspectives
on perception and decision-making to better understand how individuals
represent their environments in aid of planning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [239] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Main category: stat.ML

TL;DR: 提出一种PRM校准方法和实例自适应缩放框架，通过校准PRM输出改善成功概率估计，并动态调整推理预算以减少推理成本，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的过程奖励模型（PRM）校准较差且常常过高估计成功概率，因此需要对PRM进行校准以改善与真实成功几率的对齐。

Method: 通过量化回归校准PRM输出，并引入实例自适应缩放框架来动态调整推理预算，根据部分推理轨迹产生正确答案的可能性来适应每个实例和推理步骤。

Result: 实验表明该校准方法有效降低了校准误差，且IAS策略在减少推理成本的同时保持了最终答案的准确性，能够在更自信的问题上利用更少的计算资源。

Conclusion: 提出的PRM校准方法通过量化回归有效降低了校准误差，IAS策略减少了推理成本并保持了最终答案的准确性，表明校准对实现有效的自适应缩放至关重要。

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [240] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Main category: stat.ML

TL;DR: 我们引入了一个混合跟踪框架，结合transformer和贝叶斯滤波，提高了在复杂场景中的多粒子跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 虽然transformer架构在面对高组合负担方面表现出显著改善，但在呈现减少的轨迹假设集的场景中，其性能仍低于传统的贝叶斯滤波方法。

Method: 我们的方法通过解决一个标签预测问题进行轨迹到检测的关联，使用transformer编码器推断跨帧检测之间的软关联。这一过程修剪了假设集，使得在贝叶斯滤波框架中能够有效地进行多粒子跟踪。

Result: 我们的方法在高杂乱的多粒子跟踪场景中表现出改进的跟踪准确性和增强的鲁棒性。

Conclusion: 我们提出的混合跟踪框架通过结合自注意力机制学习粒子行为的潜在表示，和贝叶斯滤波的可靠性与可解释性，提高了跟踪精度和应对虚假检测的能力。

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [241] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Main category: stat.ML

TL;DR: 使用LLM分析高频在线文本以改进CPI预测的潜力，通过模拟和实证验证显示出其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的CPI预测方法主要依赖于低频的调查数据，而大型语言模型的进步提供了利用高频在线文本数据提高CPI预测潜力的机会。

Method: 利用大型语言模型（LLM）如ChatGPT和BERT，通过LDA和BERT提取在线文本嵌入，并结合ARX和VARX结构进行时间序列建模，预测CPI。

Result: 通过模拟和实际数据验证，LLM-CPI方法在有限样本中表现良好，并展示出实际应用优势。

Conclusion: LLM-CPI预测方法通过结合在线文本数据与传统CPI数据，展示了有效预测CPI的潜力。

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [242] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: 该研究探索了贝叶斯预测模型的对抗性攻击，提出基于梯度的新型攻击方法以优化逃逸攻击的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性机器学习研究主要集中在经典预测模型的薄弱环节，而贝叶斯预测模型的易受攻击性仍未被充分探索。

Method: 该论文采用了一种通用的方法来设计针对贝叶斯预测模型的最优逃逸攻击，并提出了基于梯度的新型攻击方法。

Result: 提出了新颖的基于梯度的攻击方法，并研究了其在不同计算环境中的实施及特性。

Conclusion: 该研究提出的方法能够有效设计针对贝叶斯预测模型的逃逸攻击，并通过实验验证了不同攻击策略在不同计算环境中的表现。

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [243] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Main category: stat.ML

TL;DR: 本文通过实验验证了预测不确定性与数据集及模型大小有关的缩放定律，同时支持了贝叶斯方法在处理不确定性时的应用价值。


<details>
  <summary>Details</summary>
Motivation: 受深度学习中揭示的缩放定律启发，研究是否存在类似的缩放定律来管理深度学习中的预测不确定性。

Method: 通过对视觉和语言任务的实验，使用流行的近似贝叶斯推理和集成方法来估计分布内和分布外的预测不确定性。

Result: 实验证明了与数据集和模型大小有关的预测不确定性缩放定律的存在。这项工作还为贝叶斯方法的实用性提供了有力的支持。

Conclusion: 得出结论，在许多深度学习应用中，大量的数据并不足以使认知不确定性变得可以忽略不计，因此贝叶斯方法仍然有其价值。

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [244] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Main category: stat.ML

TL;DR: This paper demonstrates that DDPMs are robust to noise and achieve optimal convergence rates, with finite-sample guarantees in Wasserstein-2 distance.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the robustness of DDPMs to noise in the score evaluations and improve convergence rates.

Method: The paper utilizes DDPMs and evaluates their performance through empirical evidence and finite-sample guarantees in Wasserstein-2 distance.

Result: The results show that DDPMs are robust to noisy score estimates and achieve faster, optimal convergence rates similar to those in the Gaussian case.

Conclusion: DDPMs exhibit robustness to constant-variance noise and achieve finite-sample guarantees with optimal convergence rates in Wasserstein-2 distance.

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [245] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Main category: stat.ML

TL;DR: 本文提出了一种新方法，使用GraphRNN和G-DDPM模型，以图的形式模拟喀斯特网络，生成的网络与真实数据非常相似。


<details>
  <summary>Details</summary>
Motivation: 由于不同地质条件下物理化学过程的复杂性导致了喀斯特网络多种多样的模式，模拟这些网络具有挑战性。

Method: 该方法利用图递归神经网络（GraphRNN）和去噪扩散概率模型（G-DDPM）来模拟喀斯特网络。

Result: 通过几何和拓扑指标测试，此方法能够生成逼近真实数据的喀斯特网络子图。

Conclusion: 该研究提出了一种利用图生成模型模拟离散喀斯特网络的新方法。

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [246] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: 通过使用游戏引擎合成课堂噪音并创建SimClass数据集，解决了缺乏课堂语音数据的问题，促进了AI语音模型在教育中的应用。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模课堂语音数据限制了AI驱动的教学用语音模型的发展，公共课堂数据集有限，缺乏专门的课堂噪音语料库。

Method: 利用游戏引擎合成课堂噪音，结合公共儿童语音语料库和YouTube讲座视频生成模拟课堂语音数据集。

Result: SimClass数据集在干净和嘈杂语音上的实验表明其能够很好地模拟真实课堂语音，为开发稳健的语音识别和增强模型提供了有价值的资源。

Conclusion: 本文提出了一种可扩展的合成课堂噪音的方法，并发布了一个包含合成课堂噪音和模拟课堂语音数据集的SimClass数据集，对声音识别和增强模型开发具有重要意义。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [247] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Main category: cs.SD

TL;DR: 该论文提出了一种通过整合上下文偏置方法和语音基础模型来改善罕见词识别的方法。实验结果显示明显的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管语音基础模型在大规模数据集上训练以实现准确的自动语音识别，但这些模型在识别罕见和未见过的词汇时仍然面临困难。

Method: 整合现有的上下文偏置方法与OWSM v3.1，并冻结其预训练参数。

Result: 实验结果显示，所提出的方法使偏置词错误率（B-WER）提高了11.6点，使整体错误率（WER）提高了0.9点，同时与无偏置基线相比，在LibriSpeech 100测试集上降低了7.5%的实时因数。

Conclusion: 通过利用语音基础模型中嵌入的知识，该方法使上下文偏置更有效，同时保留了语音基础模型的优势，即便使用较小的数据集。

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


### [248] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: BemaGANv2是一个先进的GAN声码器，具有音频生成和建筑创新，改善了声音周期结构的建模和长时音频生成的保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的GAN架构在高保真和长时音频生成方面存在局限性，因此作者提出了BemaGANv2以解决这些问题，通过一系列创新架构提高生成效果。

Method: BemaGANv2采用了一种新的GAN架构，其中生成器中使用AMP模块替代传统ResBlocks，并在辨别器中结合MED和MRD架构。这些创新提高了模型对长时音频的生成和周期结构的检测能力。

Result: 在实验中，各种辨别器配置经过系统评估，并通过FAD、SSIM、PLCC、MCD等客观指标以及MOS、SMOS等主观评价验证了BemaGANv2的优异性能。模型展示了在长时依赖和音频周期性结构的准确建模能力。

Conclusion: BemaGANv2在声音生成方面展示了卓越的成果。通过将AMP模块和MED架构结合，增强了对长程依赖和音频周期结构的建模能力，有助于更准确地生成高保真音频。

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [249] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: MKL-VC is a training-free voice conversion algorithm that improves upon kNN-VC by using a factorized transport map, enhancing cross-lingual conversion quality with short audio references and achieving results comparable to FACodec.


<details>
  <summary>Details</summary>
Motivation: To develop a high quality, training-free, any-to-any cross-lingual voice conversion algorithm using only a short reference audio.

Method: MKL-VC employs a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution, addressing non-uniform variance and ensuring effective feature transformation.

Result: Experiments on LibriSpeech and FLEURS datasets demonstrate that MKL-VC substantially enhances content preservation and robustness compared to kNN-VC, and its performance is comparable to that of FACodec, particularly in the cross-lingual voice conversion domain.

Conclusion: MKL-VC significantly improves content preservation and robustness in cross-lingual voice conversion tasks using short reference audio, outperforming kNN-VC and achieving performance comparable to FACodec.

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [250] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 本文提出在音频-视觉目标语音提取模型中引入语言模型作为辅助监督信号，以提升语音质量和可懂度，且在多语言和视觉受损情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类在言语感知中利用语言知识（如句法和语义）。

Method: 将预训练语音-语言模型（PSLMs）和预训练语言模型（PLMs）的语言学约束融入到AV-TSE模型中，作为额外的监督信号。

Result: 在多语言设置和视觉线索受损的情形中，方法显示出稳健的性能提升。

Conclusion: 引入的语言模型约束可以有效提升语音质量和可懂度，而不会增加推理时的计算成本。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [251] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Main category: cs.SD

TL;DR: UmbraTTS是一种新的TTS模型，可在文本和声学环境的条件下生成语音和环境音，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 在TTS中整合复杂背景环境声音仍然具有挑战性，需要解决缺乏对齐的语音和背景音频数据的问题。

Method: 提出了一种自监督框架，从未标注的录音中提取语音、背景音频和文本记录，并使用flow-matching技术构建TTS模型。

Result: 通过广泛的评估，表明UmbraTTS在生成自然、高质量且对环境感知的音频方面显著优于现有基线。

Conclusion: UmbraTTS显著优于现有基线，能生成自然、高质量且对环境感知的音频。

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [252] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Main category: q-bio.GN

TL;DR: 一种用于推测未测序病例遗传距离的概率框架，提高了基因组数据集的可扩展性和不确定性处理。


<details>
  <summary>Details</summary>
Motivation: 完整的测序覆盖率有限，限制了病原体基因组数据在空间模型中的应用。

Method: 提出了一种概率框架，用于推测未测序病例与已知序列之间的遗传距离，使用时间敏感的进化距离模型。

Result: 该方法应用于美国野生鸟类的高致病性禽流感A/H5病例，支持可扩展的不确定性感知的基因组数据集增强。

Conclusion: 这种方法增加了演化信息在时空建模工作流中的整合。

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [253] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Main category: math.PR

TL;DR: 递归训练生成模型中，外部源对样本的贡献会导致不同的渐近行为。


<details>
  <summary>Details</summary>
Motivation: 研究递归训练生成模型时模拟概率分布的"崩溃"现象及其影响因素。

Method: 分析生成模型在递归训练过程中的渐近行为。

Result: 根据是否有外部源贡献样本，生成模型的递归训练有两种不同的渐近行为。

Conclusion: 当有外部源（即使是微小的）贡献样本时，递归训练生成模型可能会导致模拟概率分布的不同渐近行为。

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [254] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Main category: cs.HC

TL;DR: 文章讨论了在中文外语教学中，教师如何利用ChatGPT设计口语课程任务，以及这种协作对课程设计的影响。


<details>
  <summary>Details</summary>
Motivation: 为提高中文作为外语教学中学生的口语互动技能，教师希望通过创新的任务设计来促进学习者的积极互动。

Method: 教师通过设计基于冲突的沟通任务来增强学生的口语互动能力，并在此过程中使用ChatGPT协助计划的制定。

Result: 研究揭示了在特定教学背景下，ChatGPT的使用对课程设计的积极影响，并为今后类似课程的开发提供了参考和经验。

Conclusion: 本文总结了教师在与ChatGPT协同开发教学计划时的互动特点，并评估了ChatGPT在口语表达课程设计中的作用及其影响。

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [255] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 研究评估了一种AI支持系统在提高同伴支持质量上的潜力，但揭示了当前培训的不足和需要标准化培训的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康问题的全球化增加，AI提供的解决方案可以扩展心理支持的获取，同伴支持在专业护理中起到补充作用，但当前的培训和效果不一致性提出质量和安全性问题。

Method: 设计并评估了一个由大语言模型模拟受困客户、情景感知生成建议和实时情感可视化支持的AI系统，通过混合方法研究分析效果。

Result: 研究发现，同伴支持者和专家都认可系统在提高互动质量和培训效果上的潜力，但也发现了支持者在回应时存在失误和过早给建议的问题。

Conclusion: AI支持的系统有潜力提高同伴支持的互动质量和培训效果，但目前的同伴支持培训在情感丰富的情境下存在关键问题，需要标准化心理基础的培训。

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [256] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 研究探索了新加坡同行支持，揭示其动机和实践，并提出设计方向以增强文化回应的数字工具及人工智能在精神健康中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台越来越多地介入同行支持，这些技术的设计和影响尚未得到充分研究，特别是在亚洲背景下。本研究的动机是填补这一研究空白，并为文化响应的数字工具设计提供基础。

Method: 采用主题分析法，通过采访20位在新加坡从事同行支持的参与者，揭示其动机、情感劳动及其实践的社会文化维度。

Result: 通过采访和分析，研究揭示了参与者的同行支持实践，提出了设计方向以增强文化回应的数字工具，并探讨了人工智能如何负责任地增强同行支持。

Conclusion: 本研究强调了设计面向文化回应的数字工具以支持同行支持的重要性，提出了人工智能如何负责任地增强同行支持的设计方向。

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [257] [Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms](https://arxiv.org/abs/2506.09195)
*Haoran Peng,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: 研究提出了一种新方法（GADC），利用图注意网络及双策略优化多无人机系统的覆盖率与电池寿命，测试结果优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 在多无人机系统中，如何最大化服务覆盖率的同时延长电池寿命是一个具有挑战性的任务，其优化具有重要实际意义。

Method: 提出了一种基于图注意力的去中心化Actor-Critic方法（GADC），通过图注意力网络处理无人机的局部观测数据，并使用演员-双评论者网络管理双重策略，以优化联合目标。

Result: 通过理论和实验分析，GADC在多无人机系统的覆盖性能与电池寿命之间实现平衡，并在理想与仿真环境中表现出优越的性能。

Conclusion: GADC在优化多无人机系统的覆盖率和电池寿命方面表现出色，并在多个基准测试中比现有方法更具优势。

Abstract: This research focuses on optimizing multi-UAV systems with dual objectives:
maximizing service coverage as the primary goal while extending battery
lifetime as the secondary objective. We propose a Graph Attention-based
Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed
approach leverages a graph attention network to process UAVs' limited local
observation and reduce the dimension of the environment states. Subsequently,
an actor-double-critic network is developed to manage dual policies for joint
objective optimization. The proposed GADC uses a Kullback-Leibler (KL)
divergence factor to balance the tradeoff between coverage performance and
battery lifetime in the multi-UAV system. We assess the scalability and
efficiency of GADC through comprehensive benchmarking against state-of-the-art
methods, considering both theory and experimental aspects. Extensive testing in
both ideal settings and NVIDIA Sionna's realistic ray tracing environment
demonstrates GADC's superior performance.

</details>


### [258] [Estimating Visceral Adiposity from Wrist-Worn Accelerometry](https://arxiv.org/abs/2506.09167)
*James R. Williamson,Andrew Alini,Brian A. Telfer,Adam W. Potter,Karl E. Friedl*

Main category: eess.SP

TL;DR: 该研究使用NHANES数据，结合机器学习方法估算体力活动与内脏脂肪组织(VAT)之间的关系，结果显示相关性为r=0.86，表明两者与代谢健康风险紧密相关。


<details>
  <summary>Details</summary>
Motivation: 研究内脏脂肪组织(VAT)作为代谢健康和习惯性体力活动的关键标志，探讨过量VAT与2型糖尿病和胰岛素抵抗的高度相关性及其病理生理机制。

Method: 使用两种方法估算体力活动与内脏脂肪组织(VAT)的关系。第一种方法使用基于步态和睡眠期间运动的工程特征，再通过岭回归将这些特征的汇总统计转换为VAT估算；第二种方法使用深度神经网络，首先通过基础模型将每10秒帧映射为高维特征向量，随后变压器模型将每天的特征向量时间序列映射为VAT估计，并通过多天平均获得结果。

Result: 结合两种方法进行VAT估算，结果显示体力活动和VAT估算的相关性为r=0.86。

Conclusion: 该研究表明，体力活动与内脏脂肪组织(VAT)之间存在强烈关系，从而表明体力活动与代谢健康风险之间的紧密联系。

Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and
habitual physical activity (PA). Excess VAT is highly correlated with type 2
diabetes and insulin resistance. The mechanistic basis for this pathophysiology
relates to overloading the liver with fatty acids. VAT is also a highly labile
fat depot, with increased turnover stimulated by catecholamines during
exercise. VAT can be measured with sophisticated imaging technologies, but can
also be inferred directly from PA. We tested this relationship using National
Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for
individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;
2,427 women) [1]. Two approaches were used for estimating VAT from activity.
The first used engineered features based on movements during gait and sleep,
and then ridge regression to map summary statistics of these features into a
VAT estimate. The second approach used deep neural networks trained on 24 hours
of continuous accelerometry. A foundation model first mapped each 10s frame
into a high-dimensional feature vector. A transformer model then mapped each
day's feature vector time series into a VAT estimate, which were averaged over
multiple days. For both approaches, the most accurate estimates were obtained
with the addition of covariate information about subject demographics and body
measurements. The best performance was obtained by combining the two
approaches, resulting in VAT estimates with correlations of r=0.86. These
findings demonstrate a strong relationship between PA and VAT and, by
extension, between PA and metabolic health risks.

</details>


### [259] [Integration of Contrastive Predictive Coding and Spiking Neural Networks](https://arxiv.org/abs/2506.09194)
*Emirhan Bilgiç,Neslihan Serap Şengör,Namık Berk Yalabık,Yavuz Selim İşler,Aykut Görkem Gelen,Rahmi Elibol*

Main category: eess.SP

TL;DR: This paper integrates CPC with SNNs to enhance biological plausibility and demonstrates effectiveness and high classification performance on the MNIST dataset.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive coding model with greater biological plausibility by simulating the computational processes of biological neural systems over time.

Method: The study integrates Contrastive Predictive Coding (CPC) with Spiking Neural Networks (SNN) to process inputs and outputs in a spike-based system.

Result: The proposed model was tested on the MNIST dataset and achieved a high classification rate in distinguishing positive sequential samples from non-sequential negative samples.

Conclusion: CPC can be effectively integrated with SNNs, showing that SNNs trained for classification can function as encoding mechanisms.

Abstract: This study examines the integration of Contrastive Predictive Coding (CPC)
with Spiking Neural Networks (SNN). While CPC learns the predictive structure
of data to generate meaningful representations, SNN mimics the computational
processes of biological neural systems over time. In this study, the goal is to
develop a predictive coding model with greater biological plausibility by
processing inputs and outputs in a spike-based system. The proposed model was
tested on the MNIST dataset and achieved a high classification rate in
distinguishing positive sequential samples from non-sequential negative
samples. The study demonstrates that CPC can be effectively combined with SNN,
showing that an SNN trained for classification tasks can also function as an
encoding mechanism. Project codes and detailed results can be accessed on our
GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN

</details>


### [260] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Main category: eess.SP

TL;DR: 提出了一种结合临床选择和计算分析的机器学习方法，提高SEEG通道的分析效率。


<details>
  <summary>Details</summary>
Motivation: 目前通过目视检查进行通道选择效率低下，因此需要一种更高效的方法来识别关键的SEEG通道。

Method: 通过XGBoost训练分类模型识别每个通道的特征，利用SHAP评分对通道进行排名，并结合通道扩展策略识别潜在发作区域。

Result: 对五位患者的数据进行验证，结果显示该方法在准确性、一致性和可解释性上具有良好的表现。

Conclusion: 机器学习方法可以有效地对SEEG通道进行排序，并对癫痫发作的贡献进行分析和解释。

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [261] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Main category: eess.SP

TL;DR: 提出了一种处理跨通道无标签感知问题的新方法，适用于子空间信号的联合。通过应用于全脑钙成像验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，例如全脑钙成像和多目标追踪，样本和通道之间的错配是常见问题。需要一种能处理更复杂信号结构的框架来解决这一问题。

Method: 扩展跨通道无标签感知框架，使其适用于位于子空间联合的信号，通过推导更紧的样本数量界限来支持更一般的信号类型。

Result: 验证了新的跨通道无标签感知框架能够在样本通道关联不精确的情况下实现信号准确恢复，并且该方法在全脑钙成像应用中表现良好。

Conclusion: 改进了现有模型，提供了更严格的样本数量要求，从而支持更广泛的信号类型和应用任务。

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [262] [Large Language Models for Design Structure Matrix Optimization](https://arxiv.org/abs/2506.09749)
*Shuo Jiang,Min Xie,Jianxi Luo*

Main category: cs.CE

TL;DR: 研究将大语言模型应用于工程组合优化问题，结合语义和数学推理，提高优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以捕捉复杂工程系统中的上下文细微差别，LLM有潜力解决这些难题。

Method: 提出了一种结合网络拓扑和领域知识的LLM框架，以对DSM元素序列进行迭代优化。

Result: 通过多种DSM案例的实验，该方法在收敛速度和解决方案质量上优于随机和确定性基线。

Conclusion: 将LLM结合到DSM组合优化问题中能够提高性能，无论使用何种LLM，都能显著增强优化效果。

Abstract: In complex engineering systems, the interdependencies among components or
development activities are often modeled and analyzed using Design Structure
Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and
enhance modularity or process efficiency constitutes a challenging
combinatorial optimization (CO) problem in engineering design and operations.
As problem sizes increase and dependency networks become more intricate,
traditional optimization methods that solely use mathematical heuristics often
fail to capture the contextual nuances and struggle to deliver effective
solutions. In this study, we explore the potential of Large Language Models
(LLMs) for helping solve such CO problems by leveraging their capabilities for
advanced reasoning and contextual understanding. We propose a novel LLM-based
framework that integrates network topology with contextual domain knowledge for
iterative optimization of DSM element sequencing - a common CO problem.
Experiments on various DSM cases show that our method consistently achieves
faster convergence and superior solution quality compared to both stochastic
and deterministic baselines. Notably, we find that incorporating contextual
domain knowledge significantly enhances optimization performance regardless of
the chosen LLM backbone. These findings highlight the potential of LLMs to
solve complex engineering CO problems by combining semantic and mathematical
reasoning. This approach paves the way towards a new paradigm in LLM-based
engineering design optimization.

</details>


### [263] [Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era](https://arxiv.org/abs/2506.09755)
*Shuo Jiang,Min Xie,Frank Youhua Chen,Jian Ma,Jianxi Luo*

Main category: cs.CE

TL;DR: 本文提出了智能设计4.0的新框架，通过多代理系统实现工程设计过程的自动化，并探讨其未来的发展潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型展示了基于知识的推理能力，为工程设计的进一步转型开辟了新途径。借助这种变革动力，本文提出了ID 4.0的概念。

Method: 首先回顾了智能设计（ID）的历史演变，提出了ID 4.0的概念框架，并讨论其如何通过协调的自主多代理系统支持工程设计的全面自动化。

Result: ID 4.0能够实现更高的适应性和自主性，提高解决复杂设计挑战的有效性。

Conclusion: 引入了智能设计4.0（ID 4.0）这一新兴范式，该范式由代理AI系统支撑，通过多代理协作支持工程设计过程的端到端自动化。

Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced
engineering innovation, efficiency, quality, and productivity over recent
decades, fundamentally reshaping how engineering designers think, behave, and
interact with design processes. The recent emergence of Foundation Models
(FMs), particularly Large Language Models (LLMs), has demonstrated general
knowledge-based reasoning capabilities, and open new paths and avenues for
further transformation in engineering design. In this context, this paper
introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by
agentic AI systems. We review the historical evolution of ID across four
distinct stages: rule-based expert systems, task-specific machine learning
models, large-scale foundation AI models, and the recent emerging paradigm of
multi-agent collaboration. We propose a conceptual framework for ID 4.0 and
discuss its potential to support end-to-end automation of engineering design
processes through coordinated, autonomous multi-agent-based systems.
Furthermore, we discuss future perspectives to enhance and fully realize ID
4.0's potential, including more complex design scenarios, more practical design
implementations, novel agent coordination mechanisms, and autonomous design
goal-setting with better human value alignment. In sum, these insights lay a
foundation for advancing Intelligent Design toward greater adaptivity,
autonomy, and effectiveness in addressing increasingly complex design
challenges.

</details>


### [264] [Superstudent intelligence in thermodynamics](https://arxiv.org/abs/2506.09822)
*Rebecca Loubet,Pascal Zittlau,Marco Hoffmann,Luisa Vollmer,Sophie Fellenz,Heike Leitte,Fabian Jirasek,Johannes Lenhard,Hans Hasse*

Main category: cs.CE

TL;DR: OpenAI 的 o3 模型在大学热力学考试中全对，表现超过所有学生，引发对机器在复杂任务上超越人类及其带来的教育影响的讨论。


<details>
  <summary>Details</summary>
Motivation: 考试通常要求学生对热力学的基本原理进行创意和知识性结合，因而失败率很高，获得A等级的情况很少，这被视为学生在智力方面的卓越证明。o3模型被用来测试其在这一复杂任务中的能力。

Method: 研究人员将大学热力学考试题目同样用于o3模型，并将其答案与学生的答案进行相同的评估。o3在零样本模式下解决了所有问题，其总分达到了自1985年以来超过1万次类似考试中的最佳范围。

Result: OpenAI 的 o3 模型在考试中所有题目全部答对，成绩超过所有学生，并达到了近年来考试的最高分数范围。

Conclusion: OpenAI 的 o3 模型在一次大学热力学考试中表现出色，超过了所有学生。这表明机器在复杂任务中的表现优于人类，这可能对工程师的工作以及未来工程师的教育产生影响。

Abstract: In this short note, we report and analyze a striking event: OpenAI's large
language model o3 has outwitted all students in a university exam on
thermodynamics. The thermodynamics exam is a difficult hurdle for most
students, where they must show that they have mastered the fundamentals of this
important topic. Consequently, the failure rates are very high, A-grades are
rare - and they are considered proof of the students' exceptional intellectual
abilities. This is because pattern learning does not help in the exam. The
problems can only be solved by knowledgeably and creatively combining
principles of thermodynamics. We have given our latest thermodynamics exam not
only to the students but also to OpenAI's most powerful reasoning model, o3,
and have assessed the answers of o3 exactly the same way as those of the
students. In zero-shot mode, the model o3 solved all problems correctly, better
than all students who took the exam; its overall score was in the range of the
best scores we have seen in more than 10,000 similar exams since 1985. This is
a turning point: machines now excel in complex tasks, usually taken as proof of
human intellectual capabilities. We discuss the consequences this has for the
work of engineers and the education of future engineers.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [265] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Main category: cs.SI

TL;DR: 本文引入了新的虚拟模型，通过保留双分图联合度矩阵来改进二元事务和序列数据集的结果评估。Alice算法验证了这些模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有虚拟模型的局限性，我们提出了新的虚拟模型，更好地维护观察到的数据集的固有属性，尤其是双分图联合度矩阵。

Method: 我们描述了一套名为Alice的马尔可夫链蒙特卡洛算法，用于从我们的虚拟模型中对数据集进行采样。该算法基于一组精心定义的状态及有效的转换操作。

Result: 实验评估显示，Alice算法混合得快且扩展性良好，并且我们的虚拟模型能找到与先前文献中不同的显著结果。

Conclusion: Alice的实验结果显示，其混合速度较快且具良好的扩展性。此外，与之前文献中的模型相比，我们的虚拟模型能发现不同的显著结果。

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [266] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: 使用LSTM和GBC进行USD/BDT汇率预测，深度学习方法表现优异，显示出应用潜力。


<details>
  <summary>Details</summary>
Motivation: 外汇预测在全球金融市场中非常重要，影响贸易、投资和经济稳定。本研究旨在通过机器学习模型提高外汇预测的准确性。

Method: 使用长短期记忆（LSTM）神经网络进行预测，并采用梯度提升分类器（GBC）进行方向预测。

Result: LSTM模型预测准确率达到99.449%，显著优于传统方法如ARIMA。方向预测通过GBC显示40.82%的盈利交易率，但整体上亏损。

Conclusion: 深度学习在外汇预测中的潜力巨大，虽有挑战但为交易者和政策制定者提供了强大工具。

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [267] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: ThinkQE is a framework for query expansion that promotes exploration and result diversity, outperforming previous methods on web search benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of narrowly focused expansions in LLM-based methods for web search query expansion, which often miss multiple interpretations and facets of a query.

Method: ThinkQE comprises two main components: a thinking-based expansion process for deeper semantic exploration and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback.

Result: Experiments on web search benchmarks DL19, DL20, and BRIGHT demonstrate that ThinkQE achieves superior performance compared to traditional dense retrievers and rerankers.

Conclusion: ThinkQE consistently outperforms prior web search query expansion methods, including dense retrievers and rerankers, by addressing the limitations of narrowly focused expansions.

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>


### [268] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Main category: cs.IR

TL;DR: 研究提出了一种基于图结构的简单有效方法改善互补产品推荐，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升客户体验和零售销售，通过推荐合适的互补产品解决用户-商品交互中的噪声和稀疏问题。

Method: 利用用户-商品二部图投影结构的有向加权图，从历史用户-商品交互中推断互补关系。

Result: 该方法较最近文献中的其他方法平均提高了43%和38%的推荐性能。

Conclusion: 提出了一种结合用户-商品二部图和有向加权图的新方法预测互补产品，实验显示该方法比其他现有方法具有显著优势。

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [269] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: 引入了一种利用AI进行ASD诊断的新技术，增强诊断便利性和保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 近年来，孤独症谱系障碍（ASD）的患病率快速增加，对受影响的个体在沟通、行为和注意力方面带来了显著挑战。目前的诊断技术虽然有效，但耗时较长，导致社会和经济成本较高。

Method: 本文介绍了一种基于人工智能的辅助技术，通过整合迁移学习和源自眼动变量的图像变换来诊断ASD。

Result: 该系统支持在家定期诊断，减少个人和护理人员的压力，同时通过图像变换保护用户隐私。

Conclusion: 所提出的方法确保了及时、可访问的诊断，保护受试者隐私并改善ASD个体的结果。

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [270] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: 综述了基础模型在医学图像分析中的应用，提出了方法的核心组成部分以及未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 改变医学图像分析方式，利用大量未标记数据进行学习。

Method: 核心组件包括模型架构、自监督学习方法以及下游适应策略。

Result: 综述了FMs在病理学、放射学和眼科中的发展与应用，比较了设计选择，并讨论了关键挑战和开放问题。

Conclusion: 揭示了FM如何在医学图像分析中发挥作用并指导未来研究方向。

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [271] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: 本文提出了一种新型无监督框架LoREIN，集成双重先验，显著提高了3D MP-qMRI的重建精度和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的qMRI重建方法通常依赖于单一的先验或物理模型，导致了次优结果，因此需要更稳健的方法来从高度欠采样的高维测量中重建高质量的qMRI。

Method: 本文提出了一种名为LoREIN的无监督框架，结合了低秩先验和连续性先验，以低秩表示和隐式神经表示来增强3D MP-qMRI的重建精度。

Result: LoREIN框架在高保真重建加权图像的同时，通过多对比加权图像对结构和定量参数图的重建提供指导，最终提高了定量参数图重建的准确性。

Conclusion: 机器人通过引入低秩先验和连续性先验的双重集成框架，提高了3D MP-qMRI重建的准确性和效率。

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [272] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: 提出了一种用于戴瓦纳加里手写数字识别的混合量子-经典模型，显著优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 戴瓦纳加里这类区域手写数字识别在多语言文档数字化、教育工具以及文化遗产保护中具有重要意义。然而，复杂的结构和有限的标注数据集对传统模型构成了巨大挑战。

Method: 本文引入了一种首次用于戴瓦纳加里手写数字识别的混合量子-经典架构，结合卷积神经网络（CNN）用于空间特征提取和10量子比特变分量子电路（VQC）用于量子增强分类。

Result: 在戴瓦纳加里手写字符数据集（DHCD）上训练和评估，该模型实现了量子实现的最先进测试准确率99.80%及测试损失为0.2893，平均每类别F1-得分为0.9980。

Conclusion: 本文中的混合量子-经典架构在戴瓦纳加里手写数字识别中表现出色，取得了99.80%的测试准确率和优于传统CNN模型的性能。

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [273] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Main category: eess.AS

TL;DR: 使用BERT模型评估说话者匿名化系统隐私性，发现语音隐私数据集需重新设计以确保评估公平性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估语音匿名化系统的隐私保护效果，并挑战现有评估方法的公正性。

Method: 研究利用BERT语言模型适配为自动语音验证（ASV）系统来评估语音隐私系统对攻击的抵抗力。

Result: 在VoicePrivacy攻击者挑战数据集上，研究方法取得了平均等错误率（EER）35%的成绩，有些说话者的EER低至2%。

Conclusion: 这项研究发现语音隐私数据集的公平性存在问题，建议重新设计数据集以确保更公正的评估。

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [274] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: 研究提出了一种利用LoRA技术微调音频语言模型Qwen2-Audio的方法，实现对PE治疗保真度的自动时间定位，并在实际数据集上取得了显著的性能，展示了其在临床应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于需要人工审查会话录音，评估治疗师PE治疗中的保真度是劳动密集型的，因此提出了一种自动化方法来提高这一过程的效率。

Method: 本研究利用Low-Rank Adaptation (LoRA)对大型预训练的音频语言模型Qwen2-Audio进行微调，从会话音频和转录中自动时间定位PE保真度的关键元素。模型处理30秒音频转录输入窗口，核心匹配标签通过基于LLM的提示生成，并由训练过的评审员验证。

Result: 在313个真实PE会话数据集上，最佳配置（LoRA等级为8，30秒输入窗口）在各任务上实现了5.3秒的平均绝对误差（MAE）。研究还分析了窗口大小和LoRA等级的影响，强调了上下文粒度和模型适应性的重要性。

Conclusion: 本研究提出了一种可扩展的框架来跟踪PE治疗中的治疗师保真度，有潜力支持临床医生培训、监督和质量保证。

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [275] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 本文探讨了训练带有可学习特征提取前端的ASR模型的正则化方法，通过改进SpecAugment和音频扰动方法，成功缩小了与传统方法的性能差距。


<details>
  <summary>Details</summary>
Motivation: 神经前端与传统的自动语音识别系统特征提取方法相比具有吸引力，但因更容易过拟合，导致性能不足。

Method: 研究音频扰动方法，并针对标准的SpecAugment使用方式进行了优化，提出在短时傅里叶变换（STFT）域进行掩蔽。

Result: 发现更大的相对提升可在可学习特征中获得，同时提出的STFT域掩蔽能够有效解决标准SpecAugment使用中的两个局限。

Conclusion: 通过有效整合两种正则化方法，可以弥合传统特征和可学习特征之间的性能差距。

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [276] [Delegations as Adaptive Representation Patterns: Rethinking Influence in Liquid Democracy](https://arxiv.org/abs/2506.09789)
*Davide Grossi,Andreas Nitsche*

Main category: cs.CY

TL;DR: 这篇论文提出了一种新模型来分析基于LiquidFeedback软件实施的液体民主如何通过跨地域委托来调节决策影响力。


<details>
  <summary>Details</summary>
Motivation: 目前的研究将液体民主中的委托结构视为静态的，这可能导致权力过度积累。本文旨在分析液体民主的适应性特性及其影响，以解决这一问题。

Method: 该论文引入了一个关于液体民主委托的新模型，分析投票节点在跨地域委托图中的影响力，解释了投票和委托作为独立活动的关系。

Result: 研究表明，跨地域委托有助于有效调节讨论影响力和决策权力，同时保持“一人一票”原则。其影响力因跨地域委托而呈指数下降。

Conclusion: 液体民主可以作为一种适应性的民主代表过程进行深入分析，这种适应性特点尚未充分研究。

Abstract: Liquid democracy is a mechanism for the division of labor in decision-making
through the transitive delegation of influence. In essence, all individuals
possess the autonomy to determine the issues with which they will engage
directly, while for other matters, they may appoint a representative of their
choosing. So far, the literature has studied the delegation structures emerging
in liquid democracy as static. As a result, transitivity defined as the
capacity to transfer acquired authority to another entity, has been identified
as a concern as it would be conducive to unrestrained accumulation of power.
  Focusing on the implementation of liquid democracy supported by the
LiquidFeedback software, we propose a novel approach to assessing the influence
of voting nodes in a transitive delegation graph, taking into account the
process nature of real-world liquid democracy in which delegation and voting
are distinct and increasingly independent activities. By introducing a novel
model of delegations in liquid democracy, we show how transitivity may in fact
contribute to an effective regulation of deliberation influence and
decision-making power. While maintaining the one-person, one-vote paradigm for
all votes cast, the anticipated influence of an agent, to the extent it is
stemming from transitivity, experiences a precipitous decline following an
exponential trajectory.
  In general, it is our objective to move the first steps towards a rigorous
analysis of liquid democracy as an adaptive democratic representation process.
The adaptivity aspect of liquid democracy has not yet been explored within the
existing academic literature despite it being, we believe, one of its most
important features. We therefore also outline a research agenda focusing on
this aspect of liquid democracy.

</details>


### [277] [Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation](https://arxiv.org/abs/2506.09102)
*Mihaela van der Schaar,Richard Peck,Eoin McKinney,Jim Weatherall,Stuart Bailey,Justine Rochon,Chris Anagnostopoulos,Pierre Marquet,Anthony Wood,Nicky Best,Harry Amad,Julianna Piskorz,Krzysztof Kacprzyk,Rafik Salama,Christina Gunther,Francesca Frau,Antoine Pugeat,Ramon Hernandez*

Main category: cs.CY

TL;DR: 此宣言提出使用因果推断和数字孪生技术，通过AI改善临床试验，提供更快、更安全的个性化患者结果。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术改进临床试验，以实现更快速、更安全和个性化的患者结果。

Method: 将因果推断和数字孪生技术与现有监管框架整合，以改革临床研究。

Result: 提出了一种利用AI革新临床试验的新标准。

Conclusion: 通过因果推断和数字孪生技术的集成，AI可以重新定义临床试验的标准。

Abstract: This manifesto represents a collaborative vision forged by leaders in
pharmaceuticals, consulting firms, clinical research, and AI. It outlines a
roadmap for two AI technologies - causal inference and digital twins - to
transform clinical trials, delivering faster, safer, and more personalized
outcomes for patients. By focusing on actionable integration within existing
regulatory frameworks, we propose a way forward to revolutionize clinical
research and redefine the gold standard for clinical trials using AI.

</details>


### [278] [FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines](https://arxiv.org/abs/2506.09107)
*Athena Vakali,Ilias Dimitriadis*

Main category: cs.CY

TL;DR: 本文提出利用代理技术来监控和改善AI决策过程中的公平性问题，通过三层架构的FAIRTOPIA框架实现这一目标。


<details>
  <summary>Details</summary>
Motivation: AI技术的快速发展已经导致了一些伤害个体和社会的事件，且AI的不公平性备受批评，因此需要一种解决方案来使AI决策过程更加符合人类原则。

Method: 本文采用多角色代理嵌入的方式，通过三层架构监控AI管道的各个阶段，以实现公平性。

Result: 通过实施多代理工作流程，我们可以在AI管道的所有阶段实现公平性监控，并激发新的公平性研究假设、启发式方法和基于人类的原则。

Conclusion: 本文提出了一种名为FAIRTOPIA的框架，通过多角色代理的嵌入来保证AI系统的公平性。

Abstract: AI models have become active decision makers, often acting without human
supervision. The rapid advancement of AI technology has already caused harmful
incidents that have hurt individuals and societies and AI unfairness in heavily
criticized. It is urgent to disrupt AI pipelines which largely neglect human
principles and focus on computational biases exploration at the data (pre),
model(in), and deployment (post) processing stages. We claim that by exploiting
the advances of agents technology, we will introduce cautious, prompt, and
ongoing fairness watch schemes, under realistic, systematic, and human-centric
fairness expectations. We envision agents as fairness guardians, since agents
learn from their environment, adapt to new information, and solve complex
problems by interacting with external tools and other systems. To set the
proper fairness guardrails in the overall AI pipeline, we introduce a
fairness-by-design approach which embeds multi-role agents in an end-to-end
(human to AI) synergetic scheme. Our position is that we may design adaptive
and realistic AI fairness frameworks, and we introduce a generalized algorithm
which can be customized to the requirements and goals of each AI decision
making scenario. Our proposed, so called FAIRTOPIA framework, is structured
over a three-layered architecture, which encapsulates the AI pipeline inside an
agentic guardian and a knowledge-based, self-refining layered scheme. Based on
our proposition, we enact fairness watch in all of the AI pipeline stages,
under robust multi-agent workflows, which will inspire new fairness research
hypothesis, heuristics, and methods grounded in human-centric, systematic,
interdisciplinary, socio-technical principles.

</details>


### [279] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Main category: cs.CY

TL;DR: 研究探讨人类与系统式信任对学生对AI聊天机器人的信任影响，提出了新的信任理论框架以适应人机信任。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在教育中的普及，学生如何信任这些系统存在模糊性。现有的信任模型不适用于具有人性化特征的AI系统，需要开发新的理论框架来理解人机信任。

Method: 研究采用偏最小二乘结构方程模型分析不同信任类型对学生感知的影响。通过比较人类信任和系统式信任对不同教育结果的预测能力，评估其影响。

Result: 研究发现，人类式信任更能预测信任意图，而系统式信任更能预测行为意图和感知有用性。在感知享受方面，两者作用相似。这表明需要新的理论框架来解释人机信任。

Conclusion: 学生对AI聊天机器人的信任是一种独特的信任形式，称为人机信任。这种信任不同于人际信任和技术信任模型。研究显示，人类与系统式信任显著影响学生的感知，但效果不同。

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [280] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: UAD通过无监督学习自动标注数据，训练出具有广泛泛化能力的视觉可供性模型，支持机器人在开放环境中的任务执行。


<details>
  <summary>Details</summary>
Motivation: 机器人在非结构化环境中执行任务时，需要能够理解复杂的物体可供性。目前的方法需要人工数据标注或仅限于预定义任务，因此需要无监督的跨任务模型。

Method: UAD结合了大型视觉模型和视觉-语言模型，通过自动标注详细的通知和视觉可供性数据对，利用轻量级任务解码器进行训练。

Result: 经过渲染对象模拟训练后，UAD在真实环境中展示了出色的泛化能力。利用UAD的可供性作为观察空间，模仿学习策略对未见过的实例和任务指令变化表现出良好的适应性。

Conclusion: UAD方法证明了通过大规模基础模型的无监督学习能够生成具有广泛泛化能力的视觉可供性预测系统，对不同的场景和任务显示出良好的适应性。

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [281] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 研究通过模拟人类全身骨骼肌系统，揭示平衡控制的动态，将有助于平衡障碍的干预和人形机器人的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管动态平衡在运动中受到了相当多的关注，但对于静态平衡和跌倒的定量理解仍然有限，因此需要更加深入的研究。

Method: 研究采用了一个分层控制管道，通过全面的全身骨骼肌系统来模拟人类的平衡。

Result: 该研究识别了稳定站立期间的时空平衡动力学，揭示了肌肉损伤对平衡行为的影响。

Conclusion: 该研究揭示了肌肉损伤对平衡行为的影响，并生成了与临床数据一致的跌倒接触模式。此外，模拟的髋部外骨骼辅助显示出在干扰下提高平衡维持能力和降低肌肉努力的效果。

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [282] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: Adv-BMT框架，不需要碰撞数据预训练，可以生成逼真的碰撞场景，训练后可以将碰撞率降低20%。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中安全关键场景稀缺，限制了自动驾驶系统性能验证。

Method: 本文提出了Adv-BMT框架，通过双向运动变换器（BMT）模型进行逆向交通运动预测，增强真实世界场景中的多样化和逼真的对抗性互动。

Result: 在增强数据集中训练，Adv-BMT可以将碰撞率降低20%。

Conclusion: Adv-BMT框架生成的碰撞场景可以显著降低自动驾驶系统的碰撞率。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [283] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了一种结合LiDAR-IMU和腿部运动学的里程计方法，通过神经模型和在线训练提高适应性，在挑战环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有技术在特征缺乏和可变形地形的环境中表现差，为解决此问题，作者提出了新的里程计方案。

Method: 该研究开发了一种在线学习的腿部运动学模型（神经腿运动学模型），结合触觉信息进行非线性动力学表达，并通过在线训练来增强适应性。

Result: 实验结果表明，该方法在沙滩和校园环境下的表现优于现有的尖端技术。

Conclusion: 该论文提出了一种名为紧耦合LiDAR-IMU-腿部里程计的新方法，实验结果表明该方法在极具挑战性的情况下一表现优异。

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [284] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: 引入SkillBlender，一个分层强化学习框架，通过融合预训练的技能，增强人形机器人的任务多样性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人全身控制和操作方法需要繁琐的任务特定调优，限制了其在各种日常任务中的通用性和可扩展性。

Method: 引入了SkillBlender，一个新颖的分层强化学习框架，通过动态融合预训练的目标导向任务无关基础技能，实现复杂的四足操控任务。

Result: 大量模拟实验表明，SkillBlender 方法在实现准确和可行的运动方面优于所有基线。

Conclusion: SkillBlender 显著优于所有基线，并自然正则化行为以避免奖励机制被滥用，适用于日常场景下的多样化四足操控任务。

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [285] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: 四足机器人通过在腿上安装铲子，提高其在动态操控任务中的能力，展示了只使用腿来操控物体的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管四足机器人在运动方面取得了显著进展，并开始应用于真实世界的场景，但其在使用腿进行操控任务方面的研究多集中于相对静态的任务。本研究旨在探索四足机器人在动态操控任务中的潜力。

Method: 该方法采用分层策略结构，包括两个专业策略（用于铲取和抛掷以及接近物体位置），以及一个用于在这两个策略之间动态切换的元策略。专业策略分别训练，随后进行元策略训练以协调多物体收集。

Result: 该框架展示了四足机器人单靠腿部的灵活性在动态物体操控方面的有效性，拓展了其在运动之外的角色。

Conclusion: 通过在四足机器人腿上增加一个简单的铲子状附加装置，机器人可以有效地收集并抛掷物体到其背部的收集盘中，实现动态物体操控。

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [286] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: 提出了一种时间统一扩散策略(TUDP)，通过时间统一去噪过程和动作识别训练，提升机器人操作的实时性和动作精度，并在RLBench上取得了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略方法在迭代去噪机器人动作时耗时较长，影响了机器人在复杂场景中的实时反应能力，且时间变化动作去噪过程增加了模型训练的复杂性，导致动作精度不理想。

Method: 本文提出了时间统一扩散策略(TUDP)，通过构建时间统一去噪过程，利用额外的动作识别信息建立动作空间的时间统一速度场，并提出动作识别分支进行动作识别训练，以提高去噪准确性。

Result: TUDP在RLBench上取得了最优的性能，多视角设置下的成功率达到82.6%，单视角设置下成功率达到83.8%。使用较少去噪迭代时，成功率有显著提升，并能在广泛的真实任务中生成准确动作。

Conclusion: 通过统一动作去噪的时间步骤，TUDP减少了策略学习的困难，加快了动作生成速度，提升了机器人操作的实时性和动作精度。

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [287] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: 本文介绍了一种通用的故障检测器SAFE，可以增强视觉-语言-行动模型在新任务中的表现，具备检测任务失败的最高性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-行动模型（VLAs）在多样化的操控任务中展示了有前途的机器人行为，但在新任务中部署时，成功率有限。为了让这些策略安全地与环境交互，需要一种故障检测器及时发出警报，以便机器人可以停止、回退或请求帮助。

Method: 该论文提出了多任务故障检测问题，并提出了SAFE，一种用于通用机器人策略的故障检测器。SAFE通过学习VLA的内部特征，预测指示任务失败可能性的单一标量。

Result: SAFE在未见过的任务上进行了测试，与多种基线进行了比较，显示出高级别的故障检测性能，并在准确性和检测时间之间实现了最佳平衡。

Conclusion: SAFE是一种兼容不同策略架构的故障检测器，具备通用检测失败的能力，可应用于模拟和现实环境中的广泛任务。

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [288] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Main category: cs.RO

TL;DR: eFlesh是一种可定制的低成本磁性触觉传感器，能提升机器人操作性能，可用于复杂环境，开放源码可用。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人操作中缺乏通用、易于定制的触觉传感器的问题，尤其在非结构化环境下的应用需求。

Method: 引入eFlesh磁性触觉传感器，通过3D打印、廉价磁性材料和磁力计电路板，实现传感器的自定义和多功能性。开放源码设计工具将3D模型转换为可打印格式，供应用定制。

Result: eFlesh传感器在接触定位和力预测方面表现优异。接触定位的均方根误差为0.5毫米，法向力预测误差为0.27牛顿，剪切力预测误差为0.12牛顿。滑动检测模型对未见物体的识别准确率达到95%。在四个需要毫米级精度的任务中，使用触视觉和控制策略的操作成功率提升40%。

Conclusion: eFlesh提供了一种低成本、高度可定制的触觉传感器解决方案，能够显著提升机器人在复杂环境中的操作能力。

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>


### [289] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 本研究提出了一种基于机器学习的框架，通过优化变换调整和吸盘选择，显著降低了大规模仓库自动化中的抓取失败率。


<details>
  <summary>Details</summary>
Motivation: 以往的研究虽然展示了机器学习模型在提高大规模机器人队列的抓取成功率方面的潜力，但主要集中在利用启发式方法预测抓取的成功概率，缺乏数据驱动的方法来优化抽样抓取以提高性能。因此，本文提出了一种基于机器学习的框架，旨在直接优化抽样抓取，以在大规模操作中取得更好的性能。

Method: 提出了一种基于机器学习的框架，用于预测变换调整以及改进多吸盘末端执行器的选择，以提高抽样抓取的成功概率。

Result: 在相似于亚马逊机器人‘Robin’队列中进行评估，该框架在超过200万次的抓取中，比基于启发式方法的抓取抽样基准减少了20%的失败率。

Conclusion: 本文提出了一种基于机器学习的框架，在选择多吸盘末端执行器并预测其调整变换方面提高抽样抓取的成功率，从而显著降低了抓取失败率。

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [290] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: 研究提出了Chain-of-Action（CoA），一种基于Trajectal Autoregressive Modeling的新颖视觉-运动策略，通过反向推理生成完整的动作轨迹，实现了RLBench任务和实际操作任务中的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法的局限性体现在前向预测未来步骤，而CoA通过任务专题目标向后推理生成整个轨迹。

Method: CoA通过Trajectal Autoregressive Modeling实现，通过一系列反向推理生成整个动作轨迹。

Result: CoA在60个RLBench任务和8个真实操作任务中达到了最先进的性能。

Conclusion: CoA提供强大的空间泛化能力，同时保持视觉-运动策略的灵活性和简单性。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [291] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 提出了一种改进SWE-Bench测试案例的方案UTGenerator和UTBoost，发现并修正了排行榜的错误排名。


<details>
  <summary>Details</summary>
Motivation: 解决当前SWE-Bench使用的手动书写测试案例不足的问题，以确保生成的代码补丁确实解决所述问题。

Method: 提出UTGenerator和UTBoost，利用LLM自动生成Python项目的测试案例，并增强测试案例的覆盖率。

Result: 在评估中，识别出了36个测试案例不足的任务实例，并发现了345个错误标记为通过的补丁，从而对排行榜结果产生显著影响。

Conclusion: 通过引入UTGenerator和UTBoost，有效地改进了SWE-Bench中测试案例的准确性及其排行榜结果。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [292] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: 该论文主张在代码生成模型中将推理深度作为可控资源进行管理，以优化模型性能，提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 首次提出将推理深度作为一个可控资源，而不是仅仅作为提示的衍生结果，从而在准确性、延迟和成本之间实现更优的权衡。

Method: 通过对推理深度进行自适应控制，优化在模型生命周期中的推理预算，从而丰富监督信号、推动新的多维标杆测试，并指导成本感知和安全部署策略。

Result: 展示了如何通过合理管理推理深度提升编码代理的智能性和效率。

Conclusion: 通过将快速和慢速思考视为互补模式，并进行合理的调度，编码代理能够在必要时进行深入思考，在可能时快速行动。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [293] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: 商业软件开发中的利益相关者参与实践未能推动负责任的AI发展，需要新的干预措施。


<details>
  <summary>Details</summary>
Motivation: 为了推动AI开发过程中的利益相关者参与，以促进负责任的AI（rAI）的发展。

Method: 分析56份负责任的AI指导文件，并进行在线调查和半结构化访谈，以了解商业环境中利益相关者参与的实践情况。

Result: 研究发现，当前商业环境中的利益相关者参与主要受到商业优先事项的驱动，如客户价值和合规性，与rAI努力的目标并不一致。

Conclusion: 现有的利益相关者参与实践未能有效促进负责任的AI的发展，因此需要针对性措施来解决这一断层。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [294] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Main category: eess.SY

TL;DR: 本文综述了最近人工智能和机器学习在6G-V2X通信中的应用，分析挑战和未来研究方向，旨在推动智能V2X生态系统发展。


<details>
  <summary>Details</summary>
Motivation: 随着V2X通信的快速发展和6G网络的预期能力，需要对人工智能在优化V2X通信中的角色进行系统综述，以指导未来研究方向。

Method: 本文采用综述方法，全面回顾了人工智能和机器学习模型在6G-V2X通信中的应用，尤其是深度学习、强化学习、生成学习和联邦学习等技术。

Result: 研究分析了人工智能在6G-V2X应用中的作用和面临的技术挑战，并识别了未来的研究方向，为推动智能、AI驱动的6G-V2X生态系统提供了洞见。

Conclusion: 本文综述了人工智能和机器学习模型在6G-V2X通信中的应用，特别关注了过去两年的进展。尽管研究取得了显著成绩，但系统性总结仍然缺乏，本文试图解决这一问题。

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [295] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: EdgeProfiler是一种快速剖析轻量级LLM在边缘设备上性能的框架，通过4-bit量化使存储和能耗大幅减少，同时保持较高的准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言理解和生成方面表现出色，但其高计算量、内存和功耗要求限制了其在云环境的使用。因此，需要一种高效的方法来评估LLM在资源受限的边缘系统上的性能。

Method: 该论文通过对如TinyLLaMA、Gemma3.1B等紧凑型LLM应用积极的量化技术和严格的内存限制，采用分析建模来估算延迟、FLOPs和能耗。同时进行4-bit量化来优化模型性能。

Result: 4-bit量化技术使模型内存使用减少了约60-70%，准确率仅降低2-5%。推理速度比FP16基线提高2-3倍，能耗估计降低35-50%，可实现在Raspberry Pi 4/5和Jetson Orin Nano Super等硬件上的实际部署。

Conclusion: EdgeProfiler提供了一种高效的方法来分析轻量级LLM在边缘设备上的性能。通过使用4-bit量化技术，模型内存使用减少了约60-70%，推理速度提高了2-3倍，能耗降低了35-50%。

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [296] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: 提出SLED方法，通过协调异构设备的计算，提高边缘计算大语言模型推理效率，实验证明其在不减少精度的情况下能显著降低延迟和提高能效。


<details>
  <summary>Details</summary>
Motivation: 现有策略如量化、剪枝或远程推理在效率与准确性之间存在权衡，或导致巨大成本。在边缘设备高效推理大型语言模型仍具挑战性，因此提出新的方法。

Method: 引入SLED方法，允许轻量级边缘设备使用多种草稿模型本地生成多个候选标记，而单个共享边缘服务器则利用更精确的目标模型高效地批量验证标记。

Result: 初步实验表明，与Jetson Orin Nano、Raspberry Pi 5和RTX 6000边缘服务器一起使用的SLED方法显示出显著优势：显著降低延迟、改善能量效率，并增加并发推理会话。

Conclusion: SLED方法能够在不牺牲模型精度的情况下，实现降低延迟、提高能效和增加并发推理会话。

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [297] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: TTrace系统能检测和定位分布式训练中的无声错误，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 分布式训练程序复杂，容易产生无声错误，这些错误在训练过程中没有显式的错误信号，但导致错误的训练结果。现有的调试方法在定位这种错误时效率低下。

Method: 收集分布式训练的中间张量并与可信单设备参考实现进行比较，通过创新的数学分析来设置阈值，以区分错误诱发的错误与浮点舍入误差。

Result: 实验结果表明，TTrace有效地检测到Megatron-LM框架中的11个现有错误和3个新错误，仅需少于10行代码改变。

Conclusion: TTrace系统能够检测和定位分布式训练中的无声错误，且在各种训练配方中表现有效。

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [298] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: ScalableHD是一种在多核CPU上实现高吞吐量和可扩展性的HDC推理方法，显著提高了任务处理速度，并保留准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的HDC方法在噪声环境下表现出色，但往往精度较低且训练过程为单次无参数化。尽管推理轻量且适合实时执行，但大多集中在专用硬件上，缺乏在多核CPU上的研究。

Method: ScalableHD采用了两阶段流水线执行模型，并根据工作负载特性设计了两种适合不同批量大小的执行变体，以充分利用计算并行性。

Result: 在多核CPU上，ScalableHD实现了在任务吞吐量上比最先进的基线加速10倍的速度提升，同时保持准确性。在增加核心数量时，吞吐量近乎线性增长。

Conclusion: ScalableHD在各种任务中提供了显著的吞吐量提高，同时保持任务准确性，并且具有良好的可扩展性。

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [299] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: 提出一种新的使用层次高斯混合模型的方法来处理具有变形和差异的分子结构，并在实验中证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在具有非刚性构象灵活性和组成差异的粒子时，进行分子结构的建模。

Method: 引入一种新的3D重构框架，采用层次高斯混合模型，灵感部分来源于4D场景重构的高斯斑点法。

Result: CryoSPIRE框架揭示了复杂实验数据集中具有生物学意义的结构，并在CryoBench中建立了新的技术标杆。

Conclusion: 这种新的3D重构框架在处理复杂的实验数据集时表现出色，证明了其在生物学结构分析中的价值。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


### [300] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Main category: q-bio.QM

TL;DR: 本研究提出了一种基于Signature理论和ctDNA动态的癌症检测方法，在数据稀疏条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 近年来，监测ctDNA水平的动态已被证明可能足以实现多癌症的早期检测。我们希望通过结合Signature理论克服由于每位患者的血样极少而导致的数据稀疏性问题。

Method: 我们的方法结合了连续时间马尔可夫模型以分析血液中ctDNA水平变化的动态，利用Signature理论构建高效的检测程序。

Result: 通过广泛的数值实验验证了我们提出的检测方案的有效性，表明它能够成功处理数据稀疏性的挑战。

Conclusion: 我们的研究成功表明，利用Signature理论结合ctDNA水平动态的马尔可夫模型是一种有效的癌症检测工具，即使在患者血样极少的情况下也表现出较强的检测能力。

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>
