<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
*Dimitri Schreiter*

Main category: cs.CL

TL;DR: 增加提示词具体性对LLM影响不大，但存在一个最佳范围，可优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 探讨在STEM、医学和法律等领域中，增加提示词汇的具体性是否能够提高LLM在特定领域的问题解答和推理任务中的性能。

Method: 开发了一种同义词替换框架，系统地替换名词、动词和形容词，并测量其对四个LLM（Llama-3.1-70B-Instruct、Granite-13B-Instruct-V2、Flan-T5-XL和Mistral-Large 2）的影响。

Result: 研究表明，虽然一般来说提升提示具体性没有显著影响，但有一个具体性范围可提高模型性能。识别最优具体性范围对提示设计有重要意义，能优化LLM性能并提升在特定领域的应用效率。

Conclusion: 增加提示词汇的具体性对LLM性能没有显著影响，但存在一个最佳具体性范围。在这一范围内，LLM表现最好。识别这一范围对提示设计具有重要意义，可以优化特定领域的LLM应用。

Abstract: Prompt engineering has emerged as a critical component in optimizing large
language models (LLMs) for domain-specific tasks. However, the role of prompt
specificity, especially in domains like STEM (physics, chemistry, biology,
computer science and mathematics), medicine, and law, remains underexplored.
This thesis addresses the problem of whether increasing the specificity of
vocabulary in prompts improves LLM performance in domain-specific
question-answering and reasoning tasks. We developed a synonymization framework
to systematically substitute nouns, verbs, and adjectives with varying
specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,
Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in
STEM, law, and medicine. Our results reveal that while generally increasing the
specificity of prompts does not have a significant impact, there appears to be
a specificity range, across all considered models, where the LLM performs the
best. Identifying this optimal specificity range offers a key insight for
prompt design, suggesting that manipulating prompts within this range could
maximize LLM performance and lead to more efficient applications in specialized
domains.

</details>


### [2] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
*Xian Gong,Paul X. McCarthy,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 论文提出了一种结合社交媒体分析和调查提交的新方法，通过人工智能改进灾难响应。


<details>
  <summary>Details</summary>
Motivation: 政府在灾难响应中愈发依赖于庞大且多样化的网络数据，尤其是在极端天气事件期间的公众行为分析，这在2022年澳大利亚新南威尔士州（NSW）的洪水中尤为体现。

Method: 研究采用了主题模型的潜在狄利克雷分布（LDA）和大语言模型（LLMs），以增强语义理解。使用公共调查提交作为参考来过滤推文，提高相关性指数，通过减少噪音和优先显示可操作内容来改善应急响应的情境感知。

Result: LDA揭示了社交媒体帖子中反映的不同意见和地理模式，而LLMs则通过识别与洪水相关的推文，改进了过滤过程，提高了应急响应的情境感知。

Conclusion: 我们的研究展示了一种新的人工智能驱动方法，结合社交媒体和公共调查数据流，能够改进危机相关内容的精炼，提高实时灾害响应，并为长期韧性规划提供信息。

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [3] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
*Diego Bonatto*

Main category: cs.CL

TL;DR: 研究通过分析6万多份啤酒配方，使用自组织映射法和统计分析，建立了一种新颖的啤酒分类系统，揭示了不同风格间的原料使用模式和历史传统，为啤酒分析和开发提供了一个新的框架工具。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种新的啤酒分类系统，以超越传统的基于感官的分类方法，并为啤酒配方分析和开发提供工具。

Method: 通过数据挖掘和分析6万多份啤酒配方，结合自组织映射（SOMs）和统计分析，开发了新的啤酒分类系统。

Result: 识别出四个超级簇群，展示了不同的麦芽和啤酒花使用模式、风格特征和历史酿造传统。冷发酵风格展现了保守的粮食和啤酒花组成，而热发酵啤酒表现出高度异质性，反映了地区的偏好和创新。

Conclusion: 该研究提供了一种新颖的分类体系，可以重现并客观地为啤酒配方分析和开发提供基础工具。

Abstract: A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.

</details>


### [4] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Main category: cs.CL

TL;DR: 提出了一种多模态VLM框架，用于生成放射学知识图谱，克服了现有单模态方法的局限性，并取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方案主要是单模态的，只从放射学报告生成知识图谱，未利用影像资料，并且无法有效处理长格式的放射学数据。我们需要一个能克服这些局限性的解决方案。

Method: 提出一种新颖的多模态VLM框架，用于生成放射学中的知识图谱，综合利用文本和影像数据。

Result: 我们的多模态VLM框架超越了之前的方法，成功生成了放射学知识图谱。

Conclusion: 我们的方法优于以往的方法，并首次引入放射学知识图谱生成的多模态解决方案。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [5] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
*Anya Belz*

Main category: cs.CL

TL;DR: QRA++ 是一种用于评估 NLP 可重复性的量化方法，使评估结果更具可比性和信息性。


<details>
  <summary>Details</summary>
Motivation: 由于个别再现研究难以提供明确的结论，本研究提出 QRA++，以便能够进行可比较和可解释的可重复性评估。

Method: 提出了一种称为 QRA++ 的定量方法，实现了三个粒度水平的连续值可重复性评估。

Result: 通过应用 QRA++，揭示了可重复性程度不仅与实验属性的相似性有关，还与系统类型和评估方法有关。

Conclusion: QRA++ 能够实现更信息化的可重复性评估，并帮助确定影响可重复性的因素。

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [6] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
*Afifah Kashif,Heer Patel*

Main category: cs.CL

TL;DR: 该研究揭示了GPT-3.5/4/4o模型在处理特定国籍和心理健康问题时的偏见，特别是对北朝鲜人的负面偏见，强调了改善模型公平性和情感一致性设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 近期研究显示，语言模型中存在对某些国籍和社会群体的偏见。本文研究这些偏见在GPT-3.5/4/4o模型输出中的伦理影响。

Method: 通过结构化的提示系列，评估模型在涉及美国和朝鲜国籍以及多种心理健康问题场景中的响应。

Result: 结果表明，与美国人相比，北朝鲜人尤其在心理健康问题情境下遭遇了更大的消极偏见。这反映了模型在处理不同群体时的共情能力差异。

Conclusion: 本研究强调，LLMs在处理多重身份交叉问题上仍存在显著偏见，尤其是在涉及到特定国籍和心理健康问题时。这需要在模型设计中引入更细致的身份楔入机制，确保同情心的一致性和公平性。

Abstract: Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.

</details>


### [7] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
*Erin Palm,Astrit Manikantan,Mark E. Pepin,Herprit Mahal,Srikanth Subramanya Belwadi*

Main category: cs.CL

TL;DR: The study developed a method to compare AI-generated clinical notes with human experts' notes, finding slightly better quality in human notes but supporting the use of AI in documentation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of established methods for evaluating the quality of AI-generated clinical notes used as scribes, given their increasing use in medical practices.

Method: A blinded study comparing large language model (LLM) generated clinical notes with those drafted by field experts, using the PDQI9 assessment tool, involving clinical experts from five medical specialties. Notes from 97 patient visits were evaluated.

Result: There is a modest yet significant difference in quality between AI-generated notes and human-created notes, with human notes slightly scoring higher on average. High inter-rater agreement was found among evaluators in most specialties, supporting the method's reliability.

Conclusion: The study supports the use of the PDQI9 instrument as a viable method to evaluate the quality of notes generated by LLMs compared to human-written notes.

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [8] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
*Agam Shah,Siddhant Sukhani,Huzaifa Pardawala,Saketh Budideti,Riya Bhadani,Rudra Gopal,Siddhartha Somani,Michael Galarnyk,Soungmin Lee,Arnav Hiray,Akshar Ravichandran,Eric Kim,Pranav Aluru,Joshua Zhang,Sebastian Jaskowski,Veer Guda,Meghaj Tarte,Liqin Ye,Spencer Gosden,Rutwik Routu,Rachel Yuh,Sloka Chava,Sahasra Chava,Dylan Patrick Kelly,Aiden Chiang,Harsit Mittal,Sudheer Chava*

Main category: cs.CL

TL;DR: 引入WCB数据集测试语言模型，发现整合训练效果优于单独训练，验证了框架经济效用。


<details>
  <summary>Details</summary>
Motivation: 解析中央银行通信中的政策含义，以避免错误解读对脆弱人群的不利影响。

Method: 引入了一个名为WCB的数据集，包含380k句子，并进行了详细的双人标注和专家审核。定义了立场检测、时间分类和不确定性估计三个任务。使用多个预训练语言模型和大语言模型进行基准测试。

Result: 模型训练结果表明，在集成多个银行的数据训练模型的效果显著优于单一银行数据模型，并经过严格的人类评估、错误分析及预测任务验证了框架的经济实用性。

Conclusion: 模型在整合的数据上训练效果优于单独银行数据，证实了整体优于部分之和的原理。

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [9] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/abs/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: 研究发现，LLMs在职业候选人选择中展现出对女性名字的偏好，尽管具备相同资历，而加入性别字段及性别代词后，这种偏好更强。此外，位置偏差显着，提示在高风险决策中使用需要谨慎。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在根据简历评估职业候选人时的行为，特别是分析性别偏见的存在。

Method: 进行实验，涉及22个领先的大型语言模型（LLMs），每个模型被系统地给出一个职位描述和一对职业匹配的简历（一个男性名字，一个女性名字），要求选择更合适的候选人。

Result: 所有LLMs在70个不同职业中一致偏好选取女性名字的候选人，尽管候选人具备相同的专业资历。在加入性别字段后，这种对女性候选人的偏好进一步增加。使用性别中立标识符时，部分模型倾向选择“候选人A”。不同性别赋予性别中立标识符可达到性别平衡。独立评分时，女性简历得分略高但影响较小。加入性别代词后，候选人被选中的几率略微增加。大多数模型显示出选取首先列出候选人的位置偏差。

Conclusion: 该研究表明，LLMs在选择职业候选人时倾向于选择具有女性名字的候选人，这对在高风险的自动化决策环境中使用LLMs提出了警示。

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [10] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
*Yanhao Jia,Xinyi Wu,Qinglin Zhang,Yiran Qin,Luwei Xiao,Shuai Zhao*

Main category: cs.CL

TL;DR: 引入了PBLBench对象基准以测试多模态大语言模型的复杂推理能力，发现现有模型面临较大挑战。


<details>
  <summary>Details</summary>
Motivation: 提升多模态大语言模型在教育任务中的应用效果，解决现有评估标准不足的问题。

Method: 使用AHP进行专家驱动的成对比较来建立结构化和加权的评估标准。

Result: 经过测试，最先进的模型在PBLBench中仅表现出59%的排名准确性，表明该基准带来的挑战。

Conclusion: PBLBench挑战现有的多模态大语言模型，目标是在教育环境中提升其表现以减轻教师负担。

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [11] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
*Bernd Huber,Ghazal Fazelnia,Andreas Damianou,Sebastian Peleato,Max Lefarov,Praveen Ravichandran,Marco De Nadai,Mounia Lalmas-Roellke,Paul N. Bennett*

Main category: cs.CL

TL;DR: Embedding-to-Prefix (E2P)方法通过注入上下文嵌入实现了高效的个性化，而不需要昂贵的适配技术。


<details>
  <summary>Details</summary>
Motivation: 当前利用LLM进行个性化的方法需要昂贵的微调或大量的提示，存在挑战。

Method: 提出了一种名为Embedding-to-Prefix (E2P)的方法，通过学习的投射将预计算的上下文嵌入注入到LLM的隐藏表示空间中，以实现个性化。

Result: E2P在保持上下文信号的同时实现了强劲的性能，并具有最小的计算开销，提供了一种可扩展且有效的方案。

Conclusion: 通过将预计算的上下文嵌入注入到LLM的隐藏表示空间，可以实现在保持骨干模型不变的情况下进行有效的个性化。

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [12] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
*Jinwoo Park,Seunggeun Cho,Dongsu Han*

Main category: cs.CL

TL;DR: SpecEdge通过边缘和服务器协作，引入了一种可扩展的推理框架，提高了LLM的服务成本效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前以服务器为中心的系统忽视了边缘的消费级GPU资源，而在边缘参与的情况下，以更经济的方式进行大语言模型的推理是值得探索的方向。

Method: SpecEdge利用了一种推测解码方案，将边缘和服务器GPUs进行负载分担，并使用管道感知调度来交错处理多个用户请求。

Result: 实验表明，SpecEdge通过实现2.22倍的服务器吞吐量，提高了1.91倍的整体成本效率，并将每个token的延迟降低了11.24%。

Conclusion: SpecEdge在整体成本效率和延迟方面都优于仅服务器的基线，证明了其为LLM服务引入了一种可扩展且经济高效的范式。

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [13] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
*Ou Jiamin,Eikmans Emile,Buskens Vincent,Pankowska Paulina,Shan Yuli*

Main category: cs.CL

TL;DR: 研究探索了大型语言模型如何在经济信任游戏中模拟人类的互动行为，发现LLMs在游戏中能表现出信任和互惠。这种能力在没有提示特定人设时已经显现，且在被提示后差异更大。


<details>
  <summary>Details</summary>
Motivation: 这项研究的动机是探索大型语言模型如何将其语言理解能力转化为社会交互语境，特别是模拟人类在经济信任游戏中的行为。研究旨在揭示LLMs在处理信任和互惠时的社会偏好和互动推理能力。

Method: 研究使用三种大型语言模型——ChatGPT-4、Claude和Bard——在经济信任游戏中进行测试。游戏中，玩家需要在自我利益与信任和互惠之间取得平衡，做出能够揭示其社会偏好和交互推理能力的决策。

Result: 研究发现，LLMs在游戏中偏离纯粹的自我利益，表现出信任和互惠。在简单的单次交互中，LLMs能够模拟人类玩家在游戏开始时的信任行为。在涉及信任回报或多轮交互的场景中，LLMs的决策受社会偏好和互动推理的影响，表现出更大的与人类的差异。当被提示采用自私或无私人设时，ChatGPT-4在无私或中立人设下的信任和互惠水平最高，超过了人类和其他模型。而在被赋予自私人设时，所有LLMs的信任和互惠水平均低于人类。在应对对手行动或游戏机制变化时，LLMs的互动推理表现出随机而非稳定再现的特征，尽管ChatGPT-4在自私或无私人设下有一些改善。

Conclusion: 研究得出结论，大型语言模型在经济信任游戏中表现出信任和互惠，而不仅仅是出于自身利益的考虑。即使在未被提示采用特定人设时，LLMs也能模拟人类在信任博弈中的行为，但在信任回报或多轮交互的情况下，与人类的行为差异更大。此外，当被提示采用特定人设时，LLMs的反应差异显著，超过了不同模型或游戏类型之间的差异。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [14] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
*Linglong Qian,Zina Ibrahim*

Main category: cs.CL

TL;DR: The paper introduces \METHOD~, a novel transformer architecture that outperforms existing models in clinical sequence modelling and maintains clinical relevance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The unique challenges in healthcare domains such as irregular sampling, variable temporal dependencies, and complex contextual relationships in patient timelines necessitate a specialized transformer architecture for clinical sequence modelling in electronic health records.

Method: Introduces \METHOD~ (Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture with three key innovations: a patient-aware attention mechanism, an adaptive sliding window attention scheme, and a U-Net inspired architecture with dynamic skip connections.

Result: \METHOD~ consistently outperforms the state-of-the-art \ETHOS~ model, especially in predicting high-severity cases requiring urgent clinical intervention. It shows stable performance across varying inference lengths and better preserves clinical hierarchies and relationships between medical concepts.

Conclusion: \METHOD~ represents a significant advancement in transformer architectures optimized for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [15] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
*Fidaa khandaqji,Huthaifa I. Ashqar,Abdelrahem Atawnih*

Main category: cs.CL

TL;DR: 论文开发了一个97.59%准确率的巴勒斯坦手语识别系统，以促进听障学生的数学教育，提高他们的学习可访问性。


<details>
  <summary>Details</summary>
Motivation: 由于巴勒斯坦手语的数字资源缺乏，研究的动机是通过先进的人工智能技术来增强听障学生的数学教育可访问性。

Method: 使用Vision Transformer ViTModel进行手势分类，该模型经过微调以提高其对数学手势的识别能力。

Result: 模型实现了97.59%的准确率，证明了其在识别数学手势方面的高精度与可靠性。

Conclusion: 这项研究通过开发一种准确的巴勒斯坦手语识别系统，利用深度学习技术来增强听障学生的数学教育。该系统取得高达97.59%的分类准确率，展示了其可靠性和精确性。研究表明，智能教育工具可以通过互动解决方案来缩小听障学生的学习差距。

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [16] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
*Luoxi Tang,Tharunya Sundar,Shuai Yang,Ankita Patra,Manohar Chippada,Giqi Zhao,Yi Li,Riteng Zhang,Tunan Zhao,Ting Yang,Yuqiao Meng,Weicheng Ma,Zhaohan Xi*

Main category: cs.CL

TL;DR: 研究评估了大语言模型在支持英语标准化考试准备中的潜力，引入了一个综合基准ESTBOOK用于评估其解决问题的能力，并提出分解分析框架进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 调查大型语言模型支持英语标准化考试准备的潜力，特别是评估其在生成准确和上下文适当的解决方案方面的能力。

Method: 提出了一个名为ESTBOOK的综合基准，用于评估大语言模型在解决英语标准化考试（EST）问题上的能力。ESTBOOK汇集了五个广泛认可的测试，涵盖29种问题类型和超过10576个问题，包括文本、图像、音频、表格和数学符号等多种形式。研究还提出了一种分解分析框架，将复杂的EST问题拆分为特定任务的解决步骤。

Result: 通过使用ESTBOOK系统地评估了大语言模型的准确性和推断效率，并通过分解分析框架评估其在各个推理环节中的表现。

Conclusion: 评估结果表明，在教育环境中，大型语言模型具有潜在的能力，但也需要采取针对性的策略来提高其作为智能辅导系统的可靠性。

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [17] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
*David Osei Opoku,Ming Sheng,Yong Zhang*

Main category: cs.CL

TL;DR: DO-RAG, a scalable hybrid QA system, enhances factual accuracy by integrating knowledge graphs and vector retrieval, achieving high recall and relevancy.


<details>
  <summary>Details</summary>
Motivation: Existing RAG frameworks struggle to integrate heterogeneous data while maintaining consistency in reasoning. There is a need for systems with high factual accuracy grounded in structured knowledge.

Method: Proposes a hybrid QA framework called DO-RAG that integrates multi-level knowledge graph construction with semantic vector retrieval, employing an agentic chain-of-thought architecture for extracting structured relationships and constructing dynamic knowledge graphs.

Result: Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%.

Conclusion: DO-RAG represents a significant improvement in the field of domain-specific QA systems by enhancing recall and answer relevancy through the integration of knowledge graphs and semantic vector retrieval, outperforming existing frameworks.

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [18] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
*Van-Tinh Nguyen,Hoang-Duong Pham,Thanh-Hai To,Cong-Tuan Hung Do,Thi-Thu-Trang Dong,Vu-Trung Duong Le,Van-Phuc Hoang*

Main category: cs.CL

TL;DR: Medalyze, an application using FLAN-T5-Large models, excels in summarizing medical texts and improves healthcare information access.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in understanding complex medical texts due to terminology and context-specific language.

Method: This paper utilizes three specialized FLAN-T5-Large models, fine-tuned for different tasks related to medical texts. The system is deployed on web and mobile platforms using a scalable API and YugabyteDB for real-time inference.

Result: The system shows superior performance in summarization tasks over GPT-4, supported by BLEU, ROUGE-L, BERTScore, and SpaCy Similarity metrics.

Conclusion: Medalyze provides an effective solution for accessing healthcare information, with superior performance over GPT-4 in specific tasks.

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [19] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
*Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang*

Main category: cs.CL

TL;DR: Introduces SALMONN-omni, a full-duplex speech LLM improving conversational AI efficiency, achieving notable performance gains without extensive training data.


<details>
  <summary>Details</summary>
Motivation: To address error accumulation, context-dependent barge-in, and echo cancellation challenges in modular conversational systems by creating a more efficient single-model solution.

Method: Introduces a standalone full-duplex speech LLM with a novel dynamic thinking mechanism, avoiding audio codecs in the token space and improving through reinforcement learning.

Result: SALMONN-omni achieves at least 30% relative performance improvement over existing models in benchmarks and excels in complex conversational scenarios.

Conclusion: SALMONN-omni significantly improves full-duplex conversational systems by integrating a dynamic thinking mechanism, outperforming existing models with less training data.

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [20] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Main category: cs.CL

TL;DR: 大型视觉语言模型中的幻觉问题仍然存在。为了解决这个问题，提出了一种新的解码策略MoD，通过动态适应解码策略来评估图像token的正确性，从而显著改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLM在各种视觉任务中表现出色，但幻觉问题仍然存在。为了解决这一关键问题，提出了一种新的解码方法。

Method: Mixture of Decoding（MoD）是一种动态调整解码策略的新方法，通过评估模型对图像token注意力的正确性来缓解幻觉问题。

Result: 大量实验表明，在多个主流基准上，MoD显著优于现有的解码方法并有效缓解LVLM中的幻觉问题。

Conclusion: MoD显著优于现有解码方法，能够有效减轻LVLMs中的幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 研究通过HistVis数据集评估了TTI系统在描绘历史时的表现，发现其在风格、时代和人口统计上存在不准确性。提出了一种评估方法以提高历史再现的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成图像(TTI)扩散模型在内容创作中变得越来越有影响力，人们对其社会和文化影响的关注也在增加。然而，尽管此前的研究主要考察了人口和文化偏见，但这些模型在准确再现历史背景方面的能力却仍未被充分探索。

Method: 研究提出了一种系统且可重现的方法，以评估TTI系统如何描绘不同历史时期。为此目的，引入了HistVis数据集——一个包含三种先进扩散模型使用精心设计的提示生成的30000幅合成图像的精选集合。研究评估了生成的图像在三个关键方面的表现：隐含风格关联、历史一致性和人口统计代表性。

Result: 研究发现，在历史主题的生成图像中存在系统性不准确性，TTI模型常通过未经声明的风格线索刻板化过去的时代，引入了现代背景下不应出现的物体，并未能反映出合理的种族和性别分布。

Conclusion: 通过这项工作，研究者揭示出当前TTI扩散模型在生成历史主题图像时存在的系统性不准确性。这些模型常常以未经声明的风格线索刻板化过去的时代，引入时代错置现象，并未能反映出合理的人口统计模式。该研究提供了一种可扩展的方法和基准，以评估生成图像中的历史再现，旨在推动构建更具历史准确性和文化对齐的TTI模型。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


### [22] [EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language](https://arxiv.org/abs/2505.17090)
*Phoebe Chua,Cathy Mengying Fang,Takehiko Ohkawa,Raja Kushalnagar,Suranga Nanayakkara,Pattie Maes*

Main category: cs.CV

TL;DR: EmoSign 数据集解决了手语情感识别研究的空白，为多模态情感识别提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 由于手语中的情感指标尚未得到充分理解，造成了批判性环境中的沟通障碍，故需建立这一 Emosign 数据集以解决这一问题。

Method: 通过包含情感和情绪标签的美国手语视频数据集，以及由专业解译经验的三位聋人手语用户进行的开放式情感提示描述注释；并提供情感分类基线模型。

Result: 为美国手语视频提供情感和情绪标签的全面数据集，并包含基线情感分类模型。

Conclusion: EmoSign 数据集填补了手语情感研究的空白，并为多模态情感识别模型能力设立了新基准。

Abstract: Unlike spoken languages where the use of prosodic features to convey emotion
is well studied, indicators of emotion in sign language remain poorly
understood, creating communication barriers in critical settings. Sign
languages present unique challenges as facial expressions and hand movements
simultaneously serve both grammatical and emotional functions. To address this
gap, we introduce EmoSign, the first sign video dataset containing sentiment
and emotion labels for 200 American Sign Language (ASL) videos. We also collect
open-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL
signers with professional interpretation experience. Alongside the annotations,
we include baseline models for sentiment and emotion classification. This
dataset not only addresses a critical gap in existing sign language research
but also establishes a new benchmark for understanding model capabilities in
multimodal emotion recognition for sign languages. The dataset is made
available at https://huggingface.co/datasets/catfang/emosign.

</details>


### [23] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
*Yanshu Li,JianJiang Yang,Bozheng Li,Ruixiang Tang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练即可应用于多模态学习的新方法CAMA，解决了注意力机制的内在问题。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态的ICL支持大量的实际应用，但其仍然不稳定，目前的研究主要集中在优化序列配置，而忽略了LVLM的内在机制。本文旨在通过理论分析发现标准注意力影响ICL表现的三个核心限制，并提出解决方法。

Method: 提出了一个名为Context-Aware Modulated Attention (CAMA)的方法，直接校准LVLM的注意力控制，这是一种简单而有效的即插即用方法。

Result: 在四个LVLM上进行评估，通过六个基准测试，证明了CAMA的有效性和普适性。

Conclusion: 本文提出的CAMA方法可以无缝应用于各种开源LVLM，评估结果表明其有效性和普适性。

Abstract: Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.

</details>


### [24] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/abs/2505.17127)
*Michal Golovanevsky,William Rudman,Michael Lepori,Amir Bar,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 研究引入Visual CounterFact数据集来测试多模态模型在直觉知识与视觉证据冲突中的决策，并提出PvP机制以控制输出，发现视觉证据往往最终占据主导。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨多模态大型语言模型在视觉问答任务中的推理是更多依赖于记忆的世界知识还是输入图像中的视觉信息。

Method: 引入Visual CounterFact数据集，包含视觉真实的反事实，以检验模型在直觉知识特性与视觉信息的冲突中如何做出决策。研究使用了激活层次的干预技术——PvP导向向量，以控制模型输出。

Result: 研究发现，在模型预测过程中，会首先反映出记忆的直觉知识，但在中后期层次则逐渐倾向于视觉证据。PvP机制能将92.5%的颜色预测和74.6%的尺寸预测从直觉转向反事实。

Conclusion: 研究表明，在多模态大型语言模型中，通过在输入层次的干预，可以在输出结果中更倾向于世界知识或视觉输入。PvP机制能够在大多数情况下成功实现这一点。

Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.

</details>


### [25] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
*Tanqiu Jiang,Jiacheng Liang,Rongyi Zhu,Jiawei Zhou,Fenglong Ma,Ting Wang*

Main category: cs.CV

TL;DR: 该研究开发了DTR，一种通过优化KV缓存减轻视觉-语言模型越狱攻击的推理时间防御机制。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型容易受到利用视觉文本交互的越狱攻击的影响。该研究旨在开发一种防御机制来缓解此类攻击。

Method: 采用了一种新的视觉模态引发的安全相关分布变化公式，通过动态调整视觉标记权重，最小化对抗性视觉输入的影响。

Result: DTR在攻击鲁棒性和对良性任务的性能方面均优于现有防御，成功应用KV缓存优化增强了多模态基础模型的安全性。

Conclusion: DTR通过优化模型的关键值(KV)缓存实现了推理时间防御，以减轻多模态越狱攻击的影响，同时保持模型的通用能力和推理效率。

Abstract: Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)

</details>


### [26] [A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data](https://arxiv.org/abs/2505.17201)
*Chaim Chai Elchik,Fatemeh Karimi Nejadasl,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.CV

TL;DR: 论文开发了一个多视图框架，利用立体视频提高水下鱼类的检测和跟踪精度，显示出比单视图方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 传统单视图MOT模型难以应对水下鱼类由于复杂3D运动和数据噪声带来的挑战，因此需要一种改进的跟踪方法。

Method: 通过适应最先进的单视图MOT模型FairMOT和YOLOv8，以及开发多视图框架来进行水下鱼类检测和跟踪。利用立体视频输入增强跟踪精度，并使用立体匹配技术生成3D输出。

Result: 开发的框架在检测鱼类实体方面具有47%的相对准确性，并能生成新的3D输出，以更好地理解鱼类运动和互动。

Conclusion: 本研究开发了一种多视图框架，通过立体视频输入提高跟踪精度和鱼类行为模式识别。集成和评估这些模型在水下鱼类视频数据集上的表现，显示出与单视图方法相比在精度和可靠性方面有显著改善。

Abstract: Multi-object tracking (MOT) in computer vision has made significant
advancements, yet tracking small fish in underwater environments presents
unique challenges due to complex 3D motions and data noise. Traditional
single-view MOT models often fall short in these settings. This thesis
addresses these challenges by adapting state-of-the-art single-view MOT models,
FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological
studies. The core contribution of this research is the development of a
multi-view framework that utilizes stereo video inputs to enhance tracking
accuracy and fish behavior pattern recognition. By integrating and evaluating
these models on underwater fish video datasets, the study aims to demonstrate
significant improvements in precision and reliability compared to single-view
approaches. The proposed framework detects fish entities with a relative
accuracy of 47% and employs stereo-matching techniques to produce a novel 3D
output, providing a more comprehensive understanding of fish movements and
interactions

</details>


### [27] [REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge](https://arxiv.org/abs/2505.17223)
*Siyang Song,Micol Spitale,Xiangyu Kong,Hengde Zhu,Cheng Luo,Cristina Palmero,German Barquero,Sergio Escalera,Michel Valstar,Mohamed Daoudi,Tobias Baur,Fabien Ringeval,Andrew Howes,Elisabeth Andre,Hatice Gunes*

Main category: cs.CV

TL;DR: REACT 2025挑战旨在开发ML模型生成人类穿戴式面部反应，提供了MARS数据集用于基准测试。


<details>
  <summary>Details</summary>
Motivation: 为应对人类互动中由讲话者行为引起的多样化面部反应，提出REACT 2025挑战，旨在开发能够生成多样性和现实性面部反应的模型。

Method: 提出了REACT 2025挑战，鼓励开发机器学习模型，并提供大规模的MARS数据集用于基准测试。

Result: 论文展示了两个子挑战：离线MAFRG和在线MAFRG，并报告了基线模型的性能，其代码已公开。

Conclusion: 本论文通过提出REACT 2025挑战，促进了机器学习模型的发展，以生成符合人类听者在双人互动中对刺激做出反应时的适当面部表情。

Abstract: In dyadic interactions, a broad spectrum of human facial reactions might be
appropriate for responding to each human speaker behaviour. Following the
successful organisation of the REACT 2023 and REACT 2024 challenges, we are
proposing the REACT 2025 challenge encouraging the development and benchmarking
of Machine Learning (ML) models that can be used to generate multiple
appropriate, diverse, realistic and synchronised human-style facial reactions
expressed by human listeners in response to an input stimulus (i.e.,
audio-visual behaviours expressed by their corresponding speakers). As a key of
the challenge, we provide challenge participants with the first natural and
large-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human
dyadic interactions containing a total of 2856 interaction sessions covering
five different topics. In addition, this paper also presents the challenge
guidelines and the performance of our baselines on the two proposed
sub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge
baseline code is publicly available at
https://github.com/reactmultimodalchallenge/baseline_react2025

</details>


### [28] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
*Omar Moured,Yufan Chen,Ruiping Liu,Simon Reiß,Philip Torr,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文介绍了CHAOS，一个用于评估多模态大型语言模型在图表扰动下鲁棒性的基准测试。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大型语言模型可以进行图表分析和可视化，现实应用中图表常常具有挑战性的或噪声特征，对‘异常图表’的解读仍然是一个重大挑战。

Method: CHAOS基准测试包括五种文本和十种视觉扰动，每种扰动分为三个级别（简单，中等，困难），这些级别灵感来自于人类评估的研究结果。测试包括13种最先进的MLLMs分为三组（一般模型、文档模型和专门针对图表的模型），根据其训练范围和数据进行分类。

Result: 实验和案例研究提供了关于模型在图表扰动方面的鲁棒性的重要见解，并旨在指导未来图表理解领域的研究。

Conclusion: 我们引入了CHAOS作为一个鲁棒性基准，用于系统地评估MLLMs在面对图表扰动时的表现。

Abstract: Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, "outlier charts" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.

</details>


### [29] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/abs/2505.17245)
*Ryota Yagi*

Main category: cs.CV

TL;DR: 研究提出了针对对象检测任务的数据集修剪方法，通过新的评分机制有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 在更复杂的计算机视觉任务中应用数据集修剪技术，如对象检测任务。

Method: 提出了一种新的评分方法--基于方差的预测评分 (VPS)，结合了IoU和置信度评分，识别特定于检测任务的信息训练样本。

Result: 在PASCAL VOC和MS COCO数据集上的实验表明，该方法在平均平均精度(mAP)方面优于先前的数据集修剪方法。还发现，注释数量和类分布变化会影响检测性能，但选择信息示例比数据集大小或平衡性更为关键。

Conclusion: 成功将数据集修剪技术应用到了对象检测领域，为复杂视觉任务中的数据集修剪奠定了基础。

Abstract: Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.

</details>


### [30] [ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation](https://arxiv.org/abs/2505.17256)
*Liang Shi,Yun Fu*

Main category: cs.CV

TL;DR: 我们提出一个无需训练的框架ExpertGen，实现高精度和多专家协作控制的文本到面部生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要额外的模块训练以处理特定控制，如身份或年龄，使其不灵活且资源密集。

Method: 我们提出ExpertGen，一个无需训练的框架，利用预训练的专家模型引导生成，通过潜在一致性模型确保每个扩散步骤的预测真实且符合分布。

Result: 专家模型可以高精度地引导生成过程，多个专家可以协作实现对不同面部特征的同时控制。

Conclusion: 我们的方法允许预训练的专家模型直接集成，并能够作为可控面部生成的即插即用组件。

Abstract: Recent advances in diffusion models have significantly improved text-to-face
generation, but achieving fine-grained control over facial features remains a
challenge. Existing methods often require training additional modules to handle
specific controls such as identity, attributes, or age, making them inflexible
and resource-intensive. We propose ExpertGen, a training-free framework that
leverages pre-trained expert models such as face recognition, facial attribute
recognition, and age estimation networks to guide generation with fine control.
Our approach uses a latent consistency model to ensure realistic and
in-distribution predictions at each diffusion step, enabling accurate guidance
signals to effectively steer the diffusion process. We show qualitatively and
quantitatively that expert models can guide the generation process with high
precision, and multiple experts can collaborate to enable simultaneous control
over diverse facial aspects. By allowing direct integration of off-the-shelf
expert models, our method transforms any such model into a plug-and-play
component for controllable face generation.

</details>


### [31] [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/abs/2505.17280)
*Pushkar Shukla,Aditya Chinchure,Emily Diana,Alexander Tolbert,Kartik Hosanagar,Vineeth N Balasubramanian,Leonid Sigal,Matthew Turk*

Main category: cs.CV

TL;DR: BiasConnect通过量化偏见之间的互动关系帮助TTI模型设计更公平的生成；InterMit在较少步骤下实现更低的偏见且图像质量更优。


<details>
  <summary>Details</summary>
Motivation: 理解偏见之间的相互关系是设计更公平的生成模型的关键，而量化这些影响具有挑战性。

Method: BiasConnect使用反事实干预分析不同偏见轴上的偏见互动结构，并估计调节一个偏见轴对另一个产生的影响；InterMit则通过用户定义的目标分布及优先权重来引导偏见缓解。

Result: BiasConnect的估算与观察到的偏见缓解后的结果呈现出强相关性（+0.65），而InterMit在降低偏见方面表现优异（0.33对比0.52），需要的缓解步骤更少（平均步骤2.38对比3.15），且生成图像质量优于传统技术。

Conclusion: BiasConnect和InterMit为TTI模型提供了有效的偏见量化和调整解决方案，展示了较低的偏见水平和更高的图像质量。

Abstract: The biases exhibited by text-to-image (TTI) models are often treated as
independent, though in reality, they may be deeply interrelated. Addressing
bias along one dimension - such as ethnicity or age - can inadvertently affect
another, like gender, either mitigating or exacerbating existing disparities.
Understanding these interdependencies is crucial for designing fairer
generative models, yet measuring such effects quantitatively remains a
challenge. To address this, we introduce BiasConnect, a novel tool for
analyzing and quantifying bias interactions in TTI models. BiasConnect uses
counterfactual interventions along different bias axes to reveal the underlying
structure of these interactions and estimates the effect of mitigating one bias
axis on another. These estimates show strong correlation (+0.65) with observed
post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an
intersectional bias mitigation algorithm guided by user-defined target
distributions and priority weights. InterMit achieves lower bias (0.33 vs.
0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields
superior image quality compared to traditional techniques. Although our
implementation is training-free, InterMit is modular and can be integrated with
many existing debiasing approaches for TTI models, making it a flexible and
extensible solution.

</details>


### [32] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/abs/2505.17311)
*Harim Kim,Yuhan Wang,Minkyu Ahn,Heeyoul Choi,Yuyin Zhou,Charmgil Hong*

Main category: cs.CV

TL;DR: 我们提出了一种多模态扩散框架Diff3M，通过整合X光片和电子健康记录，提高了异常检测能力，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的UAD模型仅依赖于影像特征，无法有效区分正常的解剖变异和病理异常。本研究旨在解决这一问题。

Method: 我们提出了一种新的图像-EHR跨注意力模块，将结构化临床背景纳入图像生成过程，并采用静态掩码策略来增强从异常到正常样貌的图像重建。

Result: Diff3M在CheXpert和MIMIC-CXR/IV评估中超越了现有的UAD方法，实现了最先进的性能。

Conclusion: Diff3M框架通过整合胸部X光片和结构化电子健康记录，提高了异常检测性能，并在CheXpert和MIMIC-CXR/IV评估中实现了领先的表现。

Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M

</details>


### [33] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
*Jiachen Jiang,Jinxin Zhou,Bo Peng,Xia Ning,Zhihui Zhu*

Main category: cs.CV

TL;DR: 提出补丁对齐训练以增强视觉和语言模型的对齐，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 改善视觉嵌入与大型语言模型之间的对齐，以提升多模态语言模型的能力。

Method: 首先调查了投影器在压缩视觉嵌入和与词嵌入对齐中的作用，提出了补丁对齐训练以增强补丁级对齐。

Result: 补丁对齐训练提高了补丁级对齐，实现了更强的压缩能力以及更高质量的视觉内容生成，并在多项任务中提升了模型性能。

Conclusion: 提出一种名为补丁对齐训练的方法，可以有效增强视觉补丁与语义词之间的对齐，从而提升多模态语言模型的性能。

Abstract: Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.

</details>


### [34] [Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens](https://arxiv.org/abs/2505.17317)
*Alyson East,Elizabeth G. Campolongo,Luke Meyers,S M Rayeed,Samuel Stevens,Iuliia Zarubiieva,Isadora E. Fluck,Jennifer C. Girón,Maximiliane Jousse,Scott Lowe,Kayla I Perry,Isabelle Betancourt,Noah Charney,Evan Donoso,Nathan Fox,Kim J. Landsbergen,Ekaterina Nepovinnykh,Michelle Ramirez,Parkash Singh,Khum Thapa-Magar,Matthew Thompson,Evan Waite,Tanya Berger-Wolf,Hilmar Lapp,Paula Mabee,Graham Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本文提出了十个关键考虑因素，以优化生物标本图像用于计算机视觉应用，并强调详细记录方法选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 弥合现有成像实践与自动化分析潜力之间的差距，优化生物标本图像以适应计算机视觉应用。

Method: 本文通过跨学科的合作，制定了十个相互关联的考虑因素，为生物标本图像在计算机视觉管道中的成功整合提供了框架。

Result: 生成支持自动性状提取、物种识别及新颖生态和进化分析的图像。

Conclusion: 成功的生物标本图像创建需要详细记录方法选择，以支持大规模的自动性状提取、物种识别及新颖的生态和进化分析。

Abstract: Biological collections house millions of specimens documenting Earth's
biodiversity, with digital images increasingly available through open-access
platforms. Most imaging protocols were developed for human visual
interpretation without considering computational analysis requirements. This
paper aims to bridge the gap between current imaging practices and the
potential for automated analysis by presenting key considerations for creating
biological specimen images optimized for computer vision applications. We
provide conceptual computer vision topics for context, addressing fundamental
concerns including model generalization, data leakage, and comprehensive
metadata documentation, and outline practical guidance on specimen imagine, and
data storage. These recommendations were synthesized through interdisciplinary
collaboration between taxonomists, collection managers, ecologists, and
computer scientists. Through this synthesis, we have identified ten
interconnected considerations that form a framework for successfully
integrating biological specimen images into computer vision pipelines. The key
elements include: (1) comprehensive metadata documentation, (2) standardized
specimen positioning, (3) consistent size and color calibration, (4) protocols
for handling multiple specimens in one image, (5) uniform background selection,
(6) controlled lighting, (7) appropriate resolution and magnification, (8)
optimal file formats, (9) robust data archiving strategies, and (10) accessible
data sharing practices. By implementing these recommendations, collection
managers, taxonomists, and biodiversity informaticians can generate images that
support automated trait extraction, species identification, and novel
ecological and evolutionary analyses at unprecedented scales. Successful
implementation lies in thorough documentation of methodological choices.

</details>


### [35] [Game-invariant Features Through Contrastive and Domain-adversarial Learning](https://arxiv.org/abs/2505.17328)
*Dylan Kline*

Main category: cs.CV

TL;DR: 我们的方法结合对比学习和域对抗训练，成功学习了能够泛化到不同游戏的视觉特征，实现更好的跨游戏迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统的游戏图像编码器往往过拟合于特定游戏的视觉风格，限制了在新游戏中的下游任务表现。我们的目标是学习能够在多种游戏中通用的视觉特征，改善模型的泛化能力。

Method: 我们的方法结合了对比学习和域对抗训练，通过一个对抗性的域分类器同时鼓励相似内容聚类并抑制游戏特定的视觉线索。这种方法产生的嵌入在不同的游戏中表现出良好的泛化能力。

Result: 通过在Bingsu游戏图像数据集上的实验，我们的方法在短时间的训练后，特征不再按游戏聚类，说明实现了特征不变性，可以提升跨游戏任务表现。

Conclusion: 我们的模型能够成功实现跨游戏的特征不变性，显示出改进跨游戏迁移的潜力，例如在尽可能少的微调下进行故障检测。

Abstract: Foundational game-image encoders often overfit to game-specific visual
styles, undermining performance on downstream tasks when applied to new games.
We present a method that combines contrastive learning and domain-adversarial
training to learn game-invariant visual features. By simultaneously encouraging
similar content to cluster and discouraging game-specific cues via an
adversarial domain classifier, our approach produces embeddings that generalize
across diverse games. Experiments on the Bingsu game-image dataset (10,000
screenshots from 10 games) demonstrate that after only a few training epochs,
our model's features no longer cluster by game, indicating successful
invariance and potential for improved cross-game transfer (e.g., glitch
detection) with minimal fine-tuning. This capability paves the way for more
generalizable game vision models that require little to no retraining on new
games.

</details>


### [36] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
*Amit Agarwal,Srikant Panda,Kulbhushan Pachauri*

Main category: cs.CV

TL;DR: FS-DAG是一种用于视觉丰富文档理解的模型架构，在少样本环境中具有高度的性能和适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决在少样本环境中视觉丰富文档理解的问题，同时应对OCR错误、拼写错误和领域变化等实际挑战。

Method: FS-DAG使用模块化框架将领域特定和语言/视觉特定的骨干结合起来，以适应不同文档类型，并通过少量数据进行适应。

Result: FS-DAG通过广泛实验展示了其在信息提取任务上的快速收敛速度和性能提升，与最先进方法相比性能显著。

Conclusion: FS-DAG在信息提取任务上表现突出，并展示了与现有最先进方法相比的显著改进。

Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag

</details>


### [37] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/abs/2505.17333)
*Xin You,Minghui Zhang,Hanxiao Zhang,Jie Yang,Nassir Navab*

Main category: cs.CV

TL;DR: 我们引入了一种新的图像到视频合成框架，并采用时间微分扩散模型来改善时间运动的建模，以模拟4D视频。这种方法在不同数据集上显示了优越的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法模拟时间运动，除非同时存在高剂量成像扫描的起始和结束帧。在术前数据采集阶段，轻微的患者运动可能导致呼吸周期内的动态背景，从而影响时间建模，因此需要解决此限制。

Method: 我们引入了一个图像到视频合成框架，并设计了一种时间微分扩散模型用于生成时间微分场，采用提示注意层和场增强层来促进微分场与I2V框架的交互，提高合成视频的时间变化准确性。

Result: 在ACDC心脏和4D肺部数据集上，我们的方法能够沿着内在运动轨迹模拟4D视频，在感知相似性和时间一致性方面与其他竞争方法相当。

Conclusion: 我们的研究引入了一个创新的图像到视频合成框架，并利用时间微分扩散模型成功地改善了临时差异场之间的交互，从而在数据集上展示了优越的4D视频模拟效果，展示了其在感知相似性和时间一致性方面的竞争力。

Abstract: Temporal modeling on regular respiration-induced motions is crucial to
image-guided clinical applications. Existing methods cannot simulate temporal
motions unless high-dose imaging scans including starting and ending frames
exist simultaneously. However, in the preoperative data acquisition stage, the
slight movement of patients may result in dynamic backgrounds between the first
and last frames in a respiratory period. This additional deviation can hardly
be removed by image registration, thus affecting the temporal modeling. To
address that limitation, we pioneeringly simulate the regular motion process
via the image-to-video (I2V) synthesis framework, which animates with the first
frame to forecast future frames of a given length. Besides, to promote the
temporal consistency of animated videos, we devise the Temporal Differential
Diffusion Model to generate temporal differential fields, which measure the
relative differential representations between adjacent frames. The prompt
attention layer is devised for fine-grained differential fields, and the field
augmented layer is adopted to better interact these fields with the I2V
framework, promoting more accurate temporal variation of synthesized videos.
Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach
simulates 4D videos along the intrinsic motion trajectory, rivaling other
competitive methods on perceptual similarity and temporal consistency. Codes
will be available soon.

</details>
