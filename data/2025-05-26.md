<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 68]
- [cs.CV](#cs.CV) [Total: 61]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
*Dimitri Schreiter*

Main category: cs.CL

TL;DR: 增加提示词具体性对LLM影响不大，但存在一个最佳范围，可优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 探讨在STEM、医学和法律等领域中，增加提示词汇的具体性是否能够提高LLM在特定领域的问题解答和推理任务中的性能。

Method: 开发了一种同义词替换框架，系统地替换名词、动词和形容词，并测量其对四个LLM（Llama-3.1-70B-Instruct、Granite-13B-Instruct-V2、Flan-T5-XL和Mistral-Large 2）的影响。

Result: 研究表明，虽然一般来说提升提示具体性没有显著影响，但有一个具体性范围可提高模型性能。识别最优具体性范围对提示设计有重要意义，能优化LLM性能并提升在特定领域的应用效率。

Conclusion: 增加提示词汇的具体性对LLM性能没有显著影响，但存在一个最佳具体性范围。在这一范围内，LLM表现最好。识别这一范围对提示设计具有重要意义，可以优化特定领域的LLM应用。

Abstract: Prompt engineering has emerged as a critical component in optimizing large
language models (LLMs) for domain-specific tasks. However, the role of prompt
specificity, especially in domains like STEM (physics, chemistry, biology,
computer science and mathematics), medicine, and law, remains underexplored.
This thesis addresses the problem of whether increasing the specificity of
vocabulary in prompts improves LLM performance in domain-specific
question-answering and reasoning tasks. We developed a synonymization framework
to systematically substitute nouns, verbs, and adjectives with varying
specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,
Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in
STEM, law, and medicine. Our results reveal that while generally increasing the
specificity of prompts does not have a significant impact, there appears to be
a specificity range, across all considered models, where the LLM performs the
best. Identifying this optimal specificity range offers a key insight for
prompt design, suggesting that manipulating prompts within this range could
maximize LLM performance and lead to more efficient applications in specialized
domains.

</details>


### [2] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
*Xian Gong,Paul X. McCarthy,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 论文提出了一种结合社交媒体分析和调查提交的新方法，通过人工智能改进灾难响应。


<details>
  <summary>Details</summary>
Motivation: 政府在灾难响应中愈发依赖于庞大且多样化的网络数据，尤其是在极端天气事件期间的公众行为分析，这在2022年澳大利亚新南威尔士州（NSW）的洪水中尤为体现。

Method: 研究采用了主题模型的潜在狄利克雷分布（LDA）和大语言模型（LLMs），以增强语义理解。使用公共调查提交作为参考来过滤推文，提高相关性指数，通过减少噪音和优先显示可操作内容来改善应急响应的情境感知。

Result: LDA揭示了社交媒体帖子中反映的不同意见和地理模式，而LLMs则通过识别与洪水相关的推文，改进了过滤过程，提高了应急响应的情境感知。

Conclusion: 我们的研究展示了一种新的人工智能驱动方法，结合社交媒体和公共调查数据流，能够改进危机相关内容的精炼，提高实时灾害响应，并为长期韧性规划提供信息。

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [3] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
*Diego Bonatto*

Main category: cs.CL

TL;DR: 研究通过分析6万多份啤酒配方，使用自组织映射法和统计分析，建立了一种新颖的啤酒分类系统，揭示了不同风格间的原料使用模式和历史传统，为啤酒分析和开发提供了一个新的框架工具。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种新的啤酒分类系统，以超越传统的基于感官的分类方法，并为啤酒配方分析和开发提供工具。

Method: 通过数据挖掘和分析6万多份啤酒配方，结合自组织映射（SOMs）和统计分析，开发了新的啤酒分类系统。

Result: 识别出四个超级簇群，展示了不同的麦芽和啤酒花使用模式、风格特征和历史酿造传统。冷发酵风格展现了保守的粮食和啤酒花组成，而热发酵啤酒表现出高度异质性，反映了地区的偏好和创新。

Conclusion: 该研究提供了一种新颖的分类体系，可以重现并客观地为啤酒配方分析和开发提供基础工具。

Abstract: A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.

</details>


### [4] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Main category: cs.CL

TL;DR: 提出了一种多模态VLM框架，用于生成放射学知识图谱，克服了现有单模态方法的局限性，并取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方案主要是单模态的，只从放射学报告生成知识图谱，未利用影像资料，并且无法有效处理长格式的放射学数据。我们需要一个能克服这些局限性的解决方案。

Method: 提出一种新颖的多模态VLM框架，用于生成放射学中的知识图谱，综合利用文本和影像数据。

Result: 我们的多模态VLM框架超越了之前的方法，成功生成了放射学知识图谱。

Conclusion: 我们的方法优于以往的方法，并首次引入放射学知识图谱生成的多模态解决方案。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [5] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
*Anya Belz*

Main category: cs.CL

TL;DR: QRA++ 是一种用于评估 NLP 可重复性的量化方法，使评估结果更具可比性和信息性。


<details>
  <summary>Details</summary>
Motivation: 由于个别再现研究难以提供明确的结论，本研究提出 QRA++，以便能够进行可比较和可解释的可重复性评估。

Method: 提出了一种称为 QRA++ 的定量方法，实现了三个粒度水平的连续值可重复性评估。

Result: 通过应用 QRA++，揭示了可重复性程度不仅与实验属性的相似性有关，还与系统类型和评估方法有关。

Conclusion: QRA++ 能够实现更信息化的可重复性评估，并帮助确定影响可重复性的因素。

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [6] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
*Afifah Kashif,Heer Patel*

Main category: cs.CL

TL;DR: 该研究揭示了GPT-3.5/4/4o模型在处理特定国籍和心理健康问题时的偏见，特别是对北朝鲜人的负面偏见，强调了改善模型公平性和情感一致性设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 近期研究显示，语言模型中存在对某些国籍和社会群体的偏见。本文研究这些偏见在GPT-3.5/4/4o模型输出中的伦理影响。

Method: 通过结构化的提示系列，评估模型在涉及美国和朝鲜国籍以及多种心理健康问题场景中的响应。

Result: 结果表明，与美国人相比，北朝鲜人尤其在心理健康问题情境下遭遇了更大的消极偏见。这反映了模型在处理不同群体时的共情能力差异。

Conclusion: 本研究强调，LLMs在处理多重身份交叉问题上仍存在显著偏见，尤其是在涉及到特定国籍和心理健康问题时。这需要在模型设计中引入更细致的身份楔入机制，确保同情心的一致性和公平性。

Abstract: Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.

</details>


### [7] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
*Erin Palm,Astrit Manikantan,Mark E. Pepin,Herprit Mahal,Srikanth Subramanya Belwadi*

Main category: cs.CL

TL;DR: The study developed a method to compare AI-generated clinical notes with human experts' notes, finding slightly better quality in human notes but supporting the use of AI in documentation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of established methods for evaluating the quality of AI-generated clinical notes used as scribes, given their increasing use in medical practices.

Method: A blinded study comparing large language model (LLM) generated clinical notes with those drafted by field experts, using the PDQI9 assessment tool, involving clinical experts from five medical specialties. Notes from 97 patient visits were evaluated.

Result: There is a modest yet significant difference in quality between AI-generated notes and human-created notes, with human notes slightly scoring higher on average. High inter-rater agreement was found among evaluators in most specialties, supporting the method's reliability.

Conclusion: The study supports the use of the PDQI9 instrument as a viable method to evaluate the quality of notes generated by LLMs compared to human-written notes.

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [8] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
*Agam Shah,Siddhant Sukhani,Huzaifa Pardawala,Saketh Budideti,Riya Bhadani,Rudra Gopal,Siddhartha Somani,Michael Galarnyk,Soungmin Lee,Arnav Hiray,Akshar Ravichandran,Eric Kim,Pranav Aluru,Joshua Zhang,Sebastian Jaskowski,Veer Guda,Meghaj Tarte,Liqin Ye,Spencer Gosden,Rutwik Routu,Rachel Yuh,Sloka Chava,Sahasra Chava,Dylan Patrick Kelly,Aiden Chiang,Harsit Mittal,Sudheer Chava*

Main category: cs.CL

TL;DR: 引入WCB数据集测试语言模型，发现整合训练效果优于单独训练，验证了框架经济效用。


<details>
  <summary>Details</summary>
Motivation: 解析中央银行通信中的政策含义，以避免错误解读对脆弱人群的不利影响。

Method: 引入了一个名为WCB的数据集，包含380k句子，并进行了详细的双人标注和专家审核。定义了立场检测、时间分类和不确定性估计三个任务。使用多个预训练语言模型和大语言模型进行基准测试。

Result: 模型训练结果表明，在集成多个银行的数据训练模型的效果显著优于单一银行数据模型，并经过严格的人类评估、错误分析及预测任务验证了框架的经济实用性。

Conclusion: 模型在整合的数据上训练效果优于单独银行数据，证实了整体优于部分之和的原理。

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [9] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/abs/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: 研究发现，LLMs在职业候选人选择中展现出对女性名字的偏好，尽管具备相同资历，而加入性别字段及性别代词后，这种偏好更强。此外，位置偏差显着，提示在高风险决策中使用需要谨慎。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在根据简历评估职业候选人时的行为，特别是分析性别偏见的存在。

Method: 进行实验，涉及22个领先的大型语言模型（LLMs），每个模型被系统地给出一个职位描述和一对职业匹配的简历（一个男性名字，一个女性名字），要求选择更合适的候选人。

Result: 所有LLMs在70个不同职业中一致偏好选取女性名字的候选人，尽管候选人具备相同的专业资历。在加入性别字段后，这种对女性候选人的偏好进一步增加。使用性别中立标识符时，部分模型倾向选择“候选人A”。不同性别赋予性别中立标识符可达到性别平衡。独立评分时，女性简历得分略高但影响较小。加入性别代词后，候选人被选中的几率略微增加。大多数模型显示出选取首先列出候选人的位置偏差。

Conclusion: 该研究表明，LLMs在选择职业候选人时倾向于选择具有女性名字的候选人，这对在高风险的自动化决策环境中使用LLMs提出了警示。

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [10] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
*Yanhao Jia,Xinyi Wu,Qinglin Zhang,Yiran Qin,Luwei Xiao,Shuai Zhao*

Main category: cs.CL

TL;DR: 引入了PBLBench对象基准以测试多模态大语言模型的复杂推理能力，发现现有模型面临较大挑战。


<details>
  <summary>Details</summary>
Motivation: 提升多模态大语言模型在教育任务中的应用效果，解决现有评估标准不足的问题。

Method: 使用AHP进行专家驱动的成对比较来建立结构化和加权的评估标准。

Result: 经过测试，最先进的模型在PBLBench中仅表现出59%的排名准确性，表明该基准带来的挑战。

Conclusion: PBLBench挑战现有的多模态大语言模型，目标是在教育环境中提升其表现以减轻教师负担。

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [11] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
*Bernd Huber,Ghazal Fazelnia,Andreas Damianou,Sebastian Peleato,Max Lefarov,Praveen Ravichandran,Marco De Nadai,Mounia Lalmas-Roellke,Paul N. Bennett*

Main category: cs.CL

TL;DR: Embedding-to-Prefix (E2P)方法通过注入上下文嵌入实现了高效的个性化，而不需要昂贵的适配技术。


<details>
  <summary>Details</summary>
Motivation: 当前利用LLM进行个性化的方法需要昂贵的微调或大量的提示，存在挑战。

Method: 提出了一种名为Embedding-to-Prefix (E2P)的方法，通过学习的投射将预计算的上下文嵌入注入到LLM的隐藏表示空间中，以实现个性化。

Result: E2P在保持上下文信号的同时实现了强劲的性能，并具有最小的计算开销，提供了一种可扩展且有效的方案。

Conclusion: 通过将预计算的上下文嵌入注入到LLM的隐藏表示空间，可以实现在保持骨干模型不变的情况下进行有效的个性化。

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [12] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
*Jinwoo Park,Seunggeun Cho,Dongsu Han*

Main category: cs.CL

TL;DR: SpecEdge通过边缘和服务器协作，引入了一种可扩展的推理框架，提高了LLM的服务成本效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前以服务器为中心的系统忽视了边缘的消费级GPU资源，而在边缘参与的情况下，以更经济的方式进行大语言模型的推理是值得探索的方向。

Method: SpecEdge利用了一种推测解码方案，将边缘和服务器GPUs进行负载分担，并使用管道感知调度来交错处理多个用户请求。

Result: 实验表明，SpecEdge通过实现2.22倍的服务器吞吐量，提高了1.91倍的整体成本效率，并将每个token的延迟降低了11.24%。

Conclusion: SpecEdge在整体成本效率和延迟方面都优于仅服务器的基线，证明了其为LLM服务引入了一种可扩展且经济高效的范式。

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [13] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
*Ou Jiamin,Eikmans Emile,Buskens Vincent,Pankowska Paulina,Shan Yuli*

Main category: cs.CL

TL;DR: 研究探索了大型语言模型如何在经济信任游戏中模拟人类的互动行为，发现LLMs在游戏中能表现出信任和互惠。这种能力在没有提示特定人设时已经显现，且在被提示后差异更大。


<details>
  <summary>Details</summary>
Motivation: 这项研究的动机是探索大型语言模型如何将其语言理解能力转化为社会交互语境，特别是模拟人类在经济信任游戏中的行为。研究旨在揭示LLMs在处理信任和互惠时的社会偏好和互动推理能力。

Method: 研究使用三种大型语言模型——ChatGPT-4、Claude和Bard——在经济信任游戏中进行测试。游戏中，玩家需要在自我利益与信任和互惠之间取得平衡，做出能够揭示其社会偏好和交互推理能力的决策。

Result: 研究发现，LLMs在游戏中偏离纯粹的自我利益，表现出信任和互惠。在简单的单次交互中，LLMs能够模拟人类玩家在游戏开始时的信任行为。在涉及信任回报或多轮交互的场景中，LLMs的决策受社会偏好和互动推理的影响，表现出更大的与人类的差异。当被提示采用自私或无私人设时，ChatGPT-4在无私或中立人设下的信任和互惠水平最高，超过了人类和其他模型。而在被赋予自私人设时，所有LLMs的信任和互惠水平均低于人类。在应对对手行动或游戏机制变化时，LLMs的互动推理表现出随机而非稳定再现的特征，尽管ChatGPT-4在自私或无私人设下有一些改善。

Conclusion: 研究得出结论，大型语言模型在经济信任游戏中表现出信任和互惠，而不仅仅是出于自身利益的考虑。即使在未被提示采用特定人设时，LLMs也能模拟人类在信任博弈中的行为，但在信任回报或多轮交互的情况下，与人类的行为差异更大。此外，当被提示采用特定人设时，LLMs的反应差异显著，超过了不同模型或游戏类型之间的差异。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [14] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
*Linglong Qian,Zina Ibrahim*

Main category: cs.CL

TL;DR: The paper introduces \METHOD~, a novel transformer architecture that outperforms existing models in clinical sequence modelling and maintains clinical relevance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The unique challenges in healthcare domains such as irregular sampling, variable temporal dependencies, and complex contextual relationships in patient timelines necessitate a specialized transformer architecture for clinical sequence modelling in electronic health records.

Method: Introduces \METHOD~ (Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture with three key innovations: a patient-aware attention mechanism, an adaptive sliding window attention scheme, and a U-Net inspired architecture with dynamic skip connections.

Result: \METHOD~ consistently outperforms the state-of-the-art \ETHOS~ model, especially in predicting high-severity cases requiring urgent clinical intervention. It shows stable performance across varying inference lengths and better preserves clinical hierarchies and relationships between medical concepts.

Conclusion: \METHOD~ represents a significant advancement in transformer architectures optimized for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [15] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
*Fidaa khandaqji,Huthaifa I. Ashqar,Abdelrahem Atawnih*

Main category: cs.CL

TL;DR: 论文开发了一个97.59%准确率的巴勒斯坦手语识别系统，以促进听障学生的数学教育，提高他们的学习可访问性。


<details>
  <summary>Details</summary>
Motivation: 由于巴勒斯坦手语的数字资源缺乏，研究的动机是通过先进的人工智能技术来增强听障学生的数学教育可访问性。

Method: 使用Vision Transformer ViTModel进行手势分类，该模型经过微调以提高其对数学手势的识别能力。

Result: 模型实现了97.59%的准确率，证明了其在识别数学手势方面的高精度与可靠性。

Conclusion: 这项研究通过开发一种准确的巴勒斯坦手语识别系统，利用深度学习技术来增强听障学生的数学教育。该系统取得高达97.59%的分类准确率，展示了其可靠性和精确性。研究表明，智能教育工具可以通过互动解决方案来缩小听障学生的学习差距。

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [16] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
*Luoxi Tang,Tharunya Sundar,Shuai Yang,Ankita Patra,Manohar Chippada,Giqi Zhao,Yi Li,Riteng Zhang,Tunan Zhao,Ting Yang,Yuqiao Meng,Weicheng Ma,Zhaohan Xi*

Main category: cs.CL

TL;DR: 研究评估了大语言模型在支持英语标准化考试准备中的潜力，引入了一个综合基准ESTBOOK用于评估其解决问题的能力，并提出分解分析框架进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 调查大型语言模型支持英语标准化考试准备的潜力，特别是评估其在生成准确和上下文适当的解决方案方面的能力。

Method: 提出了一个名为ESTBOOK的综合基准，用于评估大语言模型在解决英语标准化考试（EST）问题上的能力。ESTBOOK汇集了五个广泛认可的测试，涵盖29种问题类型和超过10576个问题，包括文本、图像、音频、表格和数学符号等多种形式。研究还提出了一种分解分析框架，将复杂的EST问题拆分为特定任务的解决步骤。

Result: 通过使用ESTBOOK系统地评估了大语言模型的准确性和推断效率，并通过分解分析框架评估其在各个推理环节中的表现。

Conclusion: 评估结果表明，在教育环境中，大型语言模型具有潜在的能力，但也需要采取针对性的策略来提高其作为智能辅导系统的可靠性。

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [17] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
*David Osei Opoku,Ming Sheng,Yong Zhang*

Main category: cs.CL

TL;DR: DO-RAG, a scalable hybrid QA system, enhances factual accuracy by integrating knowledge graphs and vector retrieval, achieving high recall and relevancy.


<details>
  <summary>Details</summary>
Motivation: Existing RAG frameworks struggle to integrate heterogeneous data while maintaining consistency in reasoning. There is a need for systems with high factual accuracy grounded in structured knowledge.

Method: Proposes a hybrid QA framework called DO-RAG that integrates multi-level knowledge graph construction with semantic vector retrieval, employing an agentic chain-of-thought architecture for extracting structured relationships and constructing dynamic knowledge graphs.

Result: Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%.

Conclusion: DO-RAG represents a significant improvement in the field of domain-specific QA systems by enhancing recall and answer relevancy through the integration of knowledge graphs and semantic vector retrieval, outperforming existing frameworks.

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [18] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
*Van-Tinh Nguyen,Hoang-Duong Pham,Thanh-Hai To,Cong-Tuan Hung Do,Thi-Thu-Trang Dong,Vu-Trung Duong Le,Van-Phuc Hoang*

Main category: cs.CL

TL;DR: Medalyze, an application using FLAN-T5-Large models, excels in summarizing medical texts and improves healthcare information access.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in understanding complex medical texts due to terminology and context-specific language.

Method: This paper utilizes three specialized FLAN-T5-Large models, fine-tuned for different tasks related to medical texts. The system is deployed on web and mobile platforms using a scalable API and YugabyteDB for real-time inference.

Result: The system shows superior performance in summarization tasks over GPT-4, supported by BLEU, ROUGE-L, BERTScore, and SpaCy Similarity metrics.

Conclusion: Medalyze provides an effective solution for accessing healthcare information, with superior performance over GPT-4 in specific tasks.

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [19] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
*Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang*

Main category: cs.CL

TL;DR: Introduces SALMONN-omni, a full-duplex speech LLM improving conversational AI efficiency, achieving notable performance gains without extensive training data.


<details>
  <summary>Details</summary>
Motivation: To address error accumulation, context-dependent barge-in, and echo cancellation challenges in modular conversational systems by creating a more efficient single-model solution.

Method: Introduces a standalone full-duplex speech LLM with a novel dynamic thinking mechanism, avoiding audio codecs in the token space and improving through reinforcement learning.

Result: SALMONN-omni achieves at least 30% relative performance improvement over existing models in benchmarks and excels in complex conversational scenarios.

Conclusion: SALMONN-omni significantly improves full-duplex conversational systems by integrating a dynamic thinking mechanism, outperforming existing models with less training data.

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [20] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Main category: cs.CL

TL;DR: 大型视觉语言模型中的幻觉问题仍然存在。为了解决这个问题，提出了一种新的解码策略MoD，通过动态适应解码策略来评估图像token的正确性，从而显著改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLM在各种视觉任务中表现出色，但幻觉问题仍然存在。为了解决这一关键问题，提出了一种新的解码方法。

Method: Mixture of Decoding（MoD）是一种动态调整解码策略的新方法，通过评估模型对图像token注意力的正确性来缓解幻觉问题。

Result: 大量实验表明，在多个主流基准上，MoD显著优于现有的解码方法并有效缓解LVLM中的幻觉问题。

Conclusion: MoD显著优于现有解码方法，能够有效减轻LVLMs中的幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [21] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
*Dimitri Schreiter*

Main category: cs.CL

TL;DR: 研究提示专门性对LLM性能的影响，发现存在一个最佳专门性范围可提高性能。


<details>
  <summary>Details</summary>
Motivation: 探索增加提示词汇专门性是否能在领域特定问题回答和推理任务中提高LLM表现。

Method: 开发了一种同义化框架，将名词、动词和形容词替换为不同的专门性水平，并评估其对四个LLM的影响。

Result: 一般来说，增加提示的专门性不会显著影响性能，但在所有模型中存在一个最佳专门性范围。

Conclusion: 虽然增加提示词的专门性总体上对提高LLM性能影响不大，但在所有模型中存在一个最佳专门性范围，在该范围内LLM表现最佳。找到这个最佳范围为提示设计提供了关键的见解。

Abstract: Prompt engineering has emerged as a critical component in optimizing large
language models (LLMs) for domain-specific tasks. However, the role of prompt
specificity, especially in domains like STEM (physics, chemistry, biology,
computer science and mathematics), medicine, and law, remains underexplored.
This thesis addresses the problem of whether increasing the specificity of
vocabulary in prompts improves LLM performance in domain-specific
question-answering and reasoning tasks. We developed a synonymization framework
to systematically substitute nouns, verbs, and adjectives with varying
specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,
Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in
STEM, law, and medicine. Our results reveal that while generally increasing the
specificity of prompts does not have a significant impact, there appears to be
a specificity range, across all considered models, where the LLM performs the
best. Identifying this optimal specificity range offers a key insight for
prompt design, suggesting that manipulating prompts within this range could
maximize LLM performance and lead to more efficient applications in specialized
domains.

</details>


### [22] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
*Xian Gong,Paul X. McCarthy,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 研究通过分析社交媒体和查询提交数据，提出了一种采用LDA和LLMs的新颖AI驱动方法，以提高灾害响应和韧性规划。


<details>
  <summary>Details</summary>
Motivation: 政府在灾害应对过程中越来越需要分析大量多样化的网络数据。研究以2022年新南威尔士州的洪水为例，探讨社交媒体（例如X，前称Twitter）和公共查询如何提供危机期间的公众行为洞察。

Method: 该研究采用了Latent Dirichlet Allocation（LDA）进行主题建模，并结合大语言模型（LLMs）以增强语义理解。LDA用于揭示社交媒体上的不同观点和地理模式，而LLMs利用公共查询提交作为参考来过滤和识别与洪水相关的推文。此相关指数方法有效减少噪声，优先处理可操作内容。

Result: 研究分析55,000多条与洪水相关的推文和1,450份提交，成功识别出极端天气事件期间的行为模式。通过使用相关指数方法增强应急响应人员的态势感知。

Conclusion: 通过结合社交媒体数据和公共查询提交，该研究提出了一种新的人工智能驱动方法，以改善与危机相关的社交媒体内容，并帮助改进实时灾害响应和长期韧性规划。

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [23] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
*Diego Bonatto*

Main category: cs.CL

TL;DR: 该研究通过数据驱动的方法开发了一种新的啤酒分类系统，识别出了四个主要超级簇，提供了一种可重复和客观的分类框架，有助于理解啤酒的多样性。


<details>
  <summary>Details</summary>
Motivation: 传统的以感官为基础的啤酒分类体系存在局限性，该研究旨在通过数据驱动的方法开发一种新的分类系统，以客观评估啤酒类别和风格。

Method: 使用数据驱动的定量分析方法，通过对62,121个啤酒配方进行挖掘和分析，结合同步组织映射（SOMs）进行统计分析。

Result: 识别出四个主要超级簇，它们展示了不同的麦芽和啤酒花使用模式、风格特征和历史酿造传统。冷发酵风格表现出保守的谷物和啤酒花组成，而热发酵啤酒表现出高度异质性，反映了地区偏好和创新。

Conclusion: 该研究提出了一种新的啤酒分类系统，基于数据驱动的方法，这一贡献提供了一个可以重复和客观评估的框架，有助于酿酒师、研究人员和教育者进行配方分析和啤酒开发。

Abstract: A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.

</details>


### [24] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Main category: cs.CL

TL;DR: 本文提出了一种多模态VLM框架，结合图像和文本生成放射学知识图谱，性能优于现有方法，是首个多模态放射学图谱生成解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱生成方法主要依赖于文本信息，缺乏对于放射图像的处理能力，并且在解析长篇幅放射学数据时存在困难。为了提高放射学知识图谱的生成质量，本文提出了一种多模态的解决方案，以整合文本和图像信息。

Method: 提出了一种基于多模态VLM的放射学知识图谱生成框架，以克服现有解决方案的局限性。这种方法结合了语言和图像信息，更有效地处理长篇幅的放射学数据。

Result: 该多模态VLM框架在放射学知识图谱生成方面，表现出优于现有方法的性能，首次结合了图像与文本信息进行更全面的知识提取。

Conclusion: 本研究开发出一种新的多模态VLM框架，用于生成放射学知识图谱，并引入了第一个放射学知识图谱生成的多模态解决方案。这种方法在生成效果上优于之前的方法。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [25] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
*Anya Belz*

Main category: cs.CL

TL;DR: QRA++是一种定量评估方法，可以提高自然语言处理研究的可重复性，通过连续值评估和跨研究比较措施来评估实验相似性对可重复性的影响。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理领域，各个重现研究提供的个别数据点表明领域内的可重复性令人担忧。由于每个重现研究根据自己不明确的标准报告结论，因此很难解释、比较和学习其中的结论。

Method: 提出一种定量方法QRA++用于评估可重复性，能够在三个粒度水平上提供连续值的可重复性评估。

Result: 通过将QRA++应用于三个实验集，揭示了实验属性相似程度以及系统类型和评估方法对可重复性程度的影响。

Conclusion: QRA++提供了一种定量方法来进行可重复性评估，它能够在不同粒度水平上产生连续值的可重复性评估，并使用在不同研究中可直接比较的可重复性措施。

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [26] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
*Afifah Kashif,Heer Patel*

Main category: cs.CL

TL;DR: 研究发现大语言模型对某些国家和社会群体存在偏见，特别是对朝鲜国籍和精神残疾者。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨大语言模型对特定国家（如朝鲜）和有污名化的社会群体偏见的伦理影响。

Method: 通过结构化提示系列，评估模型对不同国籍和精神残疾情境的响应。

Result: 发现对朝鲜人表现出更大的负面偏见，尤其是在精神残疾方面。

Conclusion: 研究强调了设计大语言模型时需要更好地理解交叉身份的重要性。

Abstract: Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.

</details>


### [27] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
*Erin Palm,Astrit Manikantan,Mark E. Pepin,Herprit Mahal,Srikanth Subramanya Belwadi*

Main category: cs.CL

TL;DR: 医师使用AI工具进行记录，但尚无评估其质量的方法。我们比较了LLM生成的临床笔记与专家笔记，使用PDQI9工具评估其质量。结果显示，二者质量差异不显著，但支持使用PDQI9评估LLM笔记。


<details>
  <summary>Details</summary>
Motivation: 美国各地的医师开始使用生成型人工智能(AI)工具充当记录员以减轻记录临床会话的负担。然而，尚无既定方法来衡量AI记录员的质量。为解决这一差距，我们进行了研究。

Method: 我们开发了一项盲法研究，比较大型语言模型(LLM)生成的临床笔记与通过音频记录的临床会话得到的领域专家笔记的相对表现。临床专家使用PDQI9工具对专家撰写的黄金笔记和LLM撰写的环境笔记进行评估。来自5个医学专业的两位评估者对从97次患者访问中撰写的笔记进行评分。

Result: 在内科、骨科和妇产科评估者之间，我们发现了一致性较高的评分一致性(RWG大于0.7)，在儿科和心脏病学则为中等(RWG 0.5到0.7)至高的一致性。我们发现整体笔记质量略有差异，其中黄金笔记得分为4.25（满分5），环境笔记得分为4.20（p=0.04）。

Conclusion: 我们的研究结果支持使用PDQI9工具作为评估LLM撰写的临床笔记质量的实用方法，与人类撰写的笔记进行比较。

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [28] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
*Agam Shah,Siddhant Sukhani,Huzaifa Pardawala,Saketh Budideti,Riya Bhadani,Rudra Gopal,Siddhartha Somani,Michael Galarnyk,Soungmin Lee,Arnav Hiray,Akshar Ravichandran,Eric Kim,Pranav Aluru,Joshua Zhang,Sebastian Jaskowski,Veer Guda,Meghaj Tarte,Liqin Ye,Spencer Gosden,Rutwik Routu,Rachel Yuh,Sloka Chava,Sahasra Chava,Dylan Patrick Kelly,Aiden Chiang,Harsit Mittal,Sudheer Chava*

Main category: cs.CL

TL;DR: 引入了WCB数据集，标注了380,000个句子以开展多项任务。实验表明，跨银行的数据训练效果优于单银行数据。


<details>
  <summary>Details</summary>
Motivation: 误解中央银行的政策可能对弱势群体产生不成比例的影响，准确解读其通信内容至关重要，因此需要一个全面的数据集来帮助研究这些政策。

Method: 构建了一个包括380,000个句子的WCB数据集，采用双人标注、分歧解决和专家审查对每个句子进行标注，并定义了态度检测、时间分类和不确定性估计三个任务。使用七种预训练语言模型和九种大型语言模型进行基准测试。

Result: 实验验证了训练在多个银行数据上的模型会优于只在单一银行数据上进行训练的模型。同时，通过严格的人类评估和错误分析验证了框架的经济效用。

Conclusion: 通过分析多个中央银行的数据，训练的模型可以显著超过只通过单个银行数据训练出来的模型。

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [29] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/abs/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: 研究发现LLMs在简历评估中对性别和简历顺序有偏好，提示在自动化决策中需谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在职业候选者评估中的性别偏好问题，了解其是否在高风险决策中运用原则性推理。

Method: 利用22个LLM，通过提供职位描述和一对职配简历进行实验。每对简历在两种名字互换的情况下，每次提供两次，以观察性别偏好对候选人选择的影响。

Result: 女性名字的简历通常被选中，说明LLMs在评估中存在性别偏好。添加明确的性别字段后，这种偏好更加强烈。当使用性别中立的标识符时，偏好有所减少但仍存在。简历的顺序对候选选择有显著影响。

Conclusion: 研究发现，LLMs在评估求职者时存在性别偏好，并且对职位上呈现的顺序也有选择偏差。虽然女性名字的简历通常被倾向选择，但添加性别信息或使用性别中立的识别符可以改变这种结果。这表明在自动化决策中使用LLMs时需要谨慎。

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [30] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
*Yanhao Jia,Xinyi Wu,Qinglin Zhang,Yiran Qin,Luwei Xiao,Shuai Zhao*

Main category: cs.CL

TL;DR: 本文提出PBLBench，一个新基准，评估MLLMs的复杂推理能力。结果显示，即使最先进的模型在PBLBench上的准确率仅59%，显示出重大挑战。PBLBench旨在促进AI代理的发展，以减轻教师工作量并提高教育效率。


<details>
  <summary>Details</summary>
Motivation: 目前现有的基准无法提供自由形式的输出结构和严格的人类专家验证过程，这限制了它们在评估现实世界教育任务中的效力。此外，由于模型幻觉和不稳定性，缺少自动化流水线来帮助教师管理MLLMs的复杂职责。因此形成这个新基准（PBLBench），以评估与人类专家处理任务高度相似的模型任务的需求。

Method: 我们采用层次分析法（AHP），利用专家驱动的成对比较来推导结构化和加权的评估标准。这个过程为建立可靠的事实提供了基础，用于评估MLLMs/LLMs在PBLBench上的性能。

Result: 利用PBLBench评估了15个先进的MLLMs/LLMs，并发现即使是最先进的模型也仅达到59%的排名准确率，这突显了该基准所提出的重大挑战。

Conclusion: 我们提出了PBLBench，一个新的基准，旨在通过域特定知识和长上下文理解来评估复杂推理。这表明当前最先进的MLLMs/LLMs在PBLBench上的表现仅达到59%的排名准确率，展示了这个基准提出的重大挑战。我们相信，PBLBench将促进更强大的AI代理的发展，最终旨在减轻教师的工作量并提高教育生产力。

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [31] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
*Bernd Huber,Ghazal Fazelnia,Andreas Damianou,Sebastian Peleato,Max Lefarov,Praveen Ravichandran,Marco De Nadai,Mounia Lalmas-Roellke,Paul N. Bennett*

Main category: cs.CL

TL;DR: E2P efficiently personalizes LLMs using pre-computed embeddings without modifying the main model, proving effective and computationally light across multiple contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of personalizing LLMs by using existing user-specific information like embeddings without incurring the high costs of fine-tuning or extensive prompting.

Method: The method, Embedding-to-Prefix (E2P), injects pre-computed context embeddings into the LLM's hidden representation through a learned projection to a soft token prefix. This approach allows for personalization without modifying the backbone model.

Result: E2P is effective in maintaining contextual signals and performs well on publicly available datasets like Persona-Chat and PENS, as well as in large-scale settings such as music and podcast personalization, all with minimal computational cost.

Conclusion: E2P method provides a scalable and efficient solution for personalizing LLM-based systems by embedding user-specific context without costly model adaptations.

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [32] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
*Jinwoo Park,Seunggeun Cho,Dongsu Han*

Main category: cs.CL

TL;DR: SpecEdge, an edge-assisted inference framework, splits LLM workloads to utilize edge GPUs, enhancing cost efficiency and server throughput for scalable large language model serving.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to reduce the cost and resource intensity of serving large language models at scale by utilizing consumer-grade GPUs at the edge, which are often overlooked.

Method: The method involves splitting LLM workloads between edge and server GPUs using a speculative decoding scheme, proactive edge drafting, and pipeline-aware scheduling to improve cost efficiency and throughput.

Result: Experiments demonstrate that SpecEdge enhances overall cost efficiency by 1.91 times, achieves 2.22 times server throughput, and reduces inter-token latency by 11.24% compared to a server-only baseline.

Conclusion: SpecEdge introduces a scalable and cost-effective paradigm for serving large language models by efficiently utilizing edge and server resources.

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [33] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
*Ou Jiamin,Eikmans Emile,Buskens Vincent,Pankowska Paulina,Shan Yuli*

Main category: cs.CL

TL;DR: 研究探讨LLMs在经济信任游戏中超越自我利益的行为，发现ChatGPT-4在特定角色表现下超越人类信任和互惠水平。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何在社交互动上下文中应用其语言理解能力，尤其在经济信任游戏中展现出的社交偏好和互动推理能力。

Method: 将LLMs置于经济信任游戏中，通过模拟人类互动的场景，观察其在信任和互惠方面的表现。

Result: ChatGPT-4在无私或中立角色下表现出最高的信任和互惠水平，超过人类、Claude和Bard的表现。Claude和Bard则在不同情况下其信任和互惠水平有时超出，有时低于人类选择。赋予自私角色时，所有LLMs均表现出比人类更低的信任和互惠。

Conclusion: 在经济信任游戏中，LLMs展现出超越纯粹自我利益的行为，表现出信任和互惠，在最简单的一次性互动中模拟人类玩家的信任决策。在涉及信任偿还或多轮互动的场景中出现更大的人机偏差。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [34] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
*Linglong Qian,Zina Ibrahim*

Main category: cs.CL

TL;DR: This paper introduces METHOD, a transformer architecture for clinical sequence modeling in EHRs, outperforming the ETHOS model, capturing complex clinical data effectively.


<details>
  <summary>Details</summary>
Motivation: Applying transformer architectures to healthcare domains presents unique challenges due to patient timelines being characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships different from traditional language tasks.

Method: \METHOD integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing.

Result: Evaluations on the MIMIC-IV database demonstrate that \METHOD consistently outperforms the state-of-the-art \ETHOS model, particularly in predicting high-severity cases that require urgent clinical intervention. It exhibits stable performance across varying inference lengths, and better preserves clinical hierarchies and relationships between medical concepts.

Conclusion: \METHOD represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [35] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
*Fidaa khandaqji,Huthaifa I. Ashqar,Abdelrahem Atawnih*

Main category: cs.CL

TL;DR: 利用先进AI技术开发了PSL识别系统，通过创建41个手势类别的数据集和微调ViT模型，实现了97.59%的分类准确率，有助于提高听障学生的数学教育可及性。


<details>
  <summary>Details</summary>
Motivation: 根据听障学生数学教育难以获得的现状，通过开发一个精确的巴勒斯坦手语(PSL)识别系统来改善这种情况。

Method: 使用先进的人工智能技术，特别是微调的Vision Transformer (ViT) 模型进行手势分类，创建了一个包含41个数学手势类别的定制数据集。

Result: 模型达到了97.59%的准确率，有效识别数学符号，保证了高精度和可靠性。

Conclusion: 本研究开发的PSL识别系统有效地识别数学手势，展示了深度学习在开发智能教育工具中的重要性，能够为听障学生提供AI驱动的交互解决方案以增强数学理解。

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [36] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
*Luoxi Tang,Tharunya Sundar,Shuai Yang,Ankita Patra,Manohar Chippada,Giqi Zhao,Yi Li,Riteng Zhang,Tunan Zhao,Ting Yang,Yuqiao Meng,Weicheng Ma,Zhaohan Xi*

Main category: cs.CL

TL;DR: The study evaluates LLMs' capabilities in standardized test preparation using a benchmark called ESTBOOK, revealing insights and strategies for their use in educational contexts.


<details>
  <summary>Details</summary>
Motivation: To investigate LLMs' potential in supporting standardized test preparation and enhancing educational experiences.

Method: The study introduces ESTBOOK, a benchmark aggregating various standardized tests to systematically evaluate LLMs' accuracy and inference efficiency.

Result: The evaluation shows insights into LLMs' abilities and suggests strategies for improving their reliability in intelligent tutoring systems.

Conclusion: LLMs exhibit promising potential to enhance educational contexts, specifically in standardized test preparation.

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [37] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
*David Osei Opoku,Ming Sheng,Yong Zhang*

Main category: cs.CL

TL;DR: DO-RAG是一种整合知识图谱和向量检索的高效问答框架，能提高精准度及答案相关性。


<details>
  <summary>Details</summary>
Motivation: 提高领域专属问答系统的生成流畅性及基于结构化专家知识的高事实准确性。

Method: 集成多级知识图谱构建与语义向量检索的混合问答框架。

Result: 在数据库和电力领域的实验评估中，DO-RAG几乎达到完美的召回率和超过94%的答案相关性，比基准框架提高了最多33.38%。

Conclusion: DO-RAG提供了一种可靠的多领域高精度问答框架，具备可追溯性、适应性和性能效率。

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [38] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
*Van-Tinh Nguyen,Hoang-Duong Pham,Thanh-Hai To,Cong-Tuan Hung Do,Thi-Thu-Trang Dong,Vu-Trung Duong Le,Van-Phuc Hoang*

Main category: cs.CL

TL;DR: Medalyze是一个用于增强医学文本理解的AI应用，使用三个专门的FLAN-T5-Large模型，被证明在特定领域任务上性能优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 由于医学文本的复杂术语和特定语境，使得理解医学文本具有挑战性，因此设计了Medalyze应用以提高医学文本的理解能力。

Method: 使用三个特殊的FLAN-T5-Large模型，分别调整用于总结医疗报告、从病人医生对话中提取健康问题以及识别段落中的关键问题。

Result: 实验评估表明，该系统在领域特定任务中的总结性能优于GPT-4，基于BLEU、ROUGE-L、BERTScore和SpaCy Similarity等指标。

Conclusion: Medalyze provides a privacy-preserving and lightweight solution for improving information accessibility in healthcare.

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [39] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
*Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang*

Main category: cs.CL

TL;DR: SALMONN-omni significantly improves performance in human-machine speech interaction without codecs, using dynamic thinking in an LLM.


<details>
  <summary>Details</summary>
Motivation: Address error accumulation and challenges like barge-in and echo cancellation in full-duplex systems.

Method: A dynamic thinking mechanism within the LLM backbone, replacing audio codecs with standalone operation.

Result: 30% relative performance improvement over existing models with less training data, strong performance in complex scenarios, enhanced through reinforcement learning.

Conclusion: SALMONN-omni exhibits significant performance improvements and competitive behaviour compared to existing systems in full-duplex speech interaction.

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [40] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Main category: cs.CL

TL;DR: Mixture of Decoding (MoD) is proposed to mitigate hallucinations in LVLMs by adapting decoding strategies based on attention correctness, achieving superior results over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the persistent challenge of hallucinations in Large Vision-Language Models (LVLMs) to improve their performance across various visual tasks.

Method: The Mixture of Decoding (MoD) approach evaluates the correctness of the model's attention on image tokens by measuring the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens. Depending on this consistency, it adapts its decoding strategy either by amplifying critical information (when consistent) or suppressing misleading information (when inconsistent).

Result: MoD achieves significant improvements over existing decoding methods in mitigating hallucinations, as demonstrated by extensive experiments on multiple mainstream benchmarks.

Conclusion: Mixture of Decoding (MoD) significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in Large Vision-Language Models (LVLMs).

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [41] [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
*Yiduo Guo,Zhen Guo,Chuanwei Huang,Zi-Ang Wang,Zekai Zhang,Haofei Yu,Huishuai Zhang,Yikang Shen*

Main category: cs.CL

TL;DR: 提出了一种仅使用任务定义生成的合成数据进行RL微调的框架，成功减少了对人工数据注释的依赖，并显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于RL依赖大规模人工标注数据，限制了其广泛采用，因此提出Synthetic Data RL框架，仅使用任务定义生成的合成数据进行RL微调。

Method: 该方法首先从任务定义和检索文档生成问答对，然后根据模型解决问题的能力调整问题难度，并使用模型在样本中平均通过率选择问题进行RL训练。

Result: 在各个数据集上，Synthetic Data RL方法在有限的人类数据预算下超过了监督微调，几乎匹配使用全人类数据的RL，并显著提高了基础模型的性能。

Conclusion: Synthetic Data RL通过合成数据进行RL微调，在有限的人类数据预算下提供了与人工数据相当的性能，并显著提高基础模型的性能。

Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.

</details>


### [42] [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
*Valentina Carbonari,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.CL

TL;DR: The paper surveys the integration of LLMs in rare disease research, emphasizing current applications and future multimodal potentials, while addressing ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To explore the integration of LLMs in rare disease research and their potential in diagnostics, treatment, and patient care.

Method: Exploration of LLMs integration in rare disease analysis, review of foundational papers, and experimentation using multiple LLMs with structured questionnaires.

Result: Demonstrated application of LLMs in identifying medical information, simulating patient interaction, and formulating diagnoses, with discussion on challenges and ethical considerations.

Conclusion: LLMs are poised to evolve into truly multimodal platforms that integrate various data types, fostering better outcomes in rare disease research and clinical settings.

Abstract: Recent advances in artificial intelligence, particularly large language
models LLMs, have shown promising capabilities in transforming rare disease
research. This survey paper explores the integration of LLMs in the analysis of
rare diseases, highlighting significant strides and pivotal studies that
leverage textual data to uncover insights and patterns critical for diagnosis,
treatment, and patient care. While current research predominantly employs
textual data, the potential for multimodal data integration combining genetic,
imaging, and electronic health records stands as a promising frontier. We
review foundational papers that demonstrate the application of LLMs in
identifying and extracting relevant medical information, simulating intelligent
conversational agents for patient interaction, and enabling the formulation of
accurate and timely diagnoses. Furthermore, this paper discusses the challenges
and ethical considerations inherent in deploying LLMs, including data privacy,
model transparency, and the need for robust, inclusive data sets. As part of
this exploration, we present a section on experimentation that utilizes
multiple LLMs alongside structured questionnaires, specifically designed for
diagnostic purposes in the context of different diseases. We conclude with
future perspectives on the evolution of LLMs towards truly multimodal
platforms, which would integrate diverse data types to provide a more
comprehensive understanding of rare diseases, ultimately fostering better
outcomes in clinical settings.

</details>


### [43] [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
*Kristin Qi,Jiali Cheng,Youxiang Zhu,Hadi Amiri,Xiaohui Liang*

Main category: cs.CL

TL;DR: 本研究提出了一个框架，通过引入对比学习和图像模态，以及专家乘积策略，提高了多语言和多图片设置下的轻度认知障碍检测性能。


<details>
  <summary>Details</summary>
Motivation: 检测从图片描述中获取的轻度认知障碍在多语言和多图片设置中是至关重要但具有挑战性的。TAUKDIAL-2024挑战赛通过引入多语言发言者和多张图片来扩展这一范围，提出了新的分析挑战。

Method: 提出了一个包含三个组件的框架：(1) 通过监督对比学习增强判别表示学习；(2) 引入图像模态而不仅仅依赖于语音和文本模态；(3) 应用专家乘积(PoE)策略来减少虚假相关性和过拟合。

Result: 基于文本单模态基线，框架的MCI检测性能显著提升，未加权平均召回率（UAR）提高了7.1%（从68.1%到75.2%），F1分数提高了2.9%（从80.6%到83.5%）。特别是，对比学习组件在文本模态上提供了更大的提升。

Conclusion: 我们的框架在多语言和多图像的轻度认知障碍（MCI）检测中表现出色，尤其是在文本模态上取得了显著的提升。

Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet
challenging, especially in multilingual and multiple picture settings. Prior
work has primarily focused on English speakers describing a single picture
(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by
introducing multilingual speakers and multiple pictures, which presents new
challenges in analyzing picture-dependent content. To address these challenges,
we propose a framework with three components: (1) enhancing discriminative
representation learning via supervised contrastive learning, (2) involving
image modality rather than relying solely on speech and text modalities, and
(3) applying a Product of Experts (PoE) strategy to mitigate spurious
correlations and overfitting. Our framework improves MCI detection performance,
achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to
75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the
text unimodal baseline. Notably, the contrastive learning component yields
greater gains for the text modality compared to speech. These results highlight
our framework's effectiveness in multilingual and multi-picture MCI detection.

</details>


### [44] [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
*Jorge Paz-Ruza,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas,Carlos Eiras-Franco*

Main category: cs.CL

TL;DR: 该研究通过协同过滤方法预测用户在健康相关话题中的有害互动，获得超过80%的预测准确率，从而可以预防冲突用户与子社区的配对。


<details>
  <summary>Details</summary>
Motivation: 目前常用的检测、标记或移除有害评论的方法通常对平台和用户都适得其反，因此需要一种新的方法来预测并减少线上讨论中的用户有害行为。

Method: 应用基于协同过滤的机器学习方法，预测用户在健康相关在线讨论中的有害互动行为，特别是COVID相关讨论中。

Result: 实验结果表明，在预测用户与Reddit子社区之间的COVID相关讨论的有害行为方面，该方法的预测性能超过80%。

Conclusion: 通过提前预测用户之间可能发生的有害互动，可以更有效地减少有害评论的产生，而不是事后进行检测和移除。

Abstract: In health-related topics, user toxicity in online discussions frequently
becomes a source of social conflict or promotion of dangerous, unscientific
behaviour; common approaches for battling it include different forms of
detection, flagging and/or removal of existing toxic comments, which is often
counterproductive for platforms and users alike. In this work, we propose the
alternative of combatting user toxicity predictively, anticipating where a user
could interact toxically in health-related online discussions. Applying a
Collaborative Filtering-based Machine Learning methodology, we predict the
toxicity in COVID-related conversations between any user and subcommunity of
Reddit, surpassing 80% predictive performance in relevant metrics, and allowing
us to prevent the pairing of conflicting users and subcommunities.

</details>


### [45] [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
*Anandh C,Karthik Pandia Durai,Jeena Prakash,Manickavela Arumugam,Kadri Hacioglu,S. Pavankumar Dubagunta,Andreas Stolcke,Shankar Venkatesan,Aravind Ganapathiraju*

Main category: cs.CL

TL;DR: 提出改进的 ASR 端点检测方法，解决了传统流式传输中的延迟问题，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: T-ASR 模型普遍存在输出延迟，导致端点检测中出现错误或延迟，这影响了用户体验。为了解决这一问题，需要改进端点检测机制。

Method: 通过在每个单词末尾引入词尾标记和延迟惩罚，并使用辅助网络进行帧级语音活动检测，以解决端点检测中的延迟和错误问题。

Result: 实验表明，与传统的延迟惩罚方法相比，所提方法在端点检测的准确性和延迟方面具有优势。

Conclusion: 所提出的方法有效改善了流式传输中的端点检测问题，提升了用户体验。

Abstract: ASR endpointing (EP) plays a major role in delivering a good user experience
in products supporting human or artificial agents in human-human/machine
conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR
modelling technique preferred for streaming. A major limitation of T-ASR is
delayed emission of ASR outputs, which could lead to errors or delays in EP.
Inaccurate EP will cut the user off while speaking, returning incomplete
transcript while delays in EP will increase the perceived latency, degrading
the user experience. We propose methods to improve EP by addressing delayed
emission along with EP mistakes. To address the delayed emission problem, we
introduce an end-of-word token at the end of each word, along with a delay
penalty. The EP delay is addressed by obtaining a reliable frame-level speech
activity detection using an auxiliary network. We apply the proposed methods on
Switchboard conversational speech corpus and evaluate it against a delay
penalty method.

</details>


### [46] [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)
*Raphaël Sarfati,Haley Moller,Toni J. B. Liu,Nicolas Boullé,Christopher Earls*

Main category: cs.CL

TL;DR: 研究语言模型中嵌入物的风格几何结构，可用于作者归属和文学分析。


<details>
  <summary>Details</summary>
Motivation: 探讨整个提示的信息如何在变压器层的作用下浓缩为单一嵌入。

Method: 使用文学作品来分析语言模型的深度表示如何包含提示中无形而非事实的信息。

Result: 不同小说的简短摘录在潜在空间中独立分离，而来自同一作者的书籍集合比跨作者的更纠缠，表明嵌入物编码风格特征。

Conclusion: 语言模型的嵌入编码包含风格特征，并在风格的几何形态上显示出复杂的信息处理和压缩能力。

Abstract: Large language models use high-dimensional latent spaces to encode and
process textual information. Much work has investigated how the conceptual
content of words translates into geometrical relationships between their vector
representations. Fewer studies analyze how the cumulative information of an
entire prompt becomes condensed into individual embeddings under the action of
transformer layers. We use literary pieces to show that information about
intangible, rather than factual, aspects of the prompt are contained in deep
representations. We observe that short excerpts (10 - 100 tokens) from
different novels separate in the latent space independently from what
next-token prediction they converge towards. Ensembles from books from the same
authors are much more entangled than across authors, suggesting that embeddings
encode stylistic features. This geometry of style may have applications for
authorship attribution and literary analysis, but most importantly reveals the
sophistication of information processing and compression accomplished by
language models.

</details>


### [47] [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
*Anurag Mishra*

Main category: cs.CL

TL;DR: 研究解析了GPT模型在摘要任务中的适应机制，发现中间层的"摘要电路"，使用定向LoRA适配显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大多数关于大语言模型的机制可解释性研究集中在分类或生成任务，而不是摘要任务，因此本研究试图揭示GPT样模型在摘要任务中的适应机制。

Method: 使用预训练和微调模型的差异分析来量化注意力模式和内部激活的变化，识别模型结构中显著转变的层和注意力头。

Result: 中间层（特别是第2、3和5层）表现出最显著的变化，62%的注意力头减少了熵，表明信息选择的集中；定向LoRA适配在性能提升上优于标准LoRA微调。

Conclusion: 本研究揭示了GPT模型在摘要任务中存在特定的"摘要电路"，并可以通过定向LoRA适配来提升性能。

Abstract: Mechanistic interpretability research seeks to reveal the inner workings of
large language models, yet most work focuses on classification or generative
tasks rather than summarization. This paper presents an interpretability
framework for analyzing how GPT-like models adapt to summarization tasks. We
conduct differential analysis between pre-trained and fine-tuned models,
quantifying changes in attention patterns and internal activations. By
identifying specific layers and attention heads that undergo significant
transformation, we locate the "summarization circuit" within the model
architecture. Our findings reveal that middle layers (particularly 2, 3, and 5)
exhibit the most dramatic changes, with 62% of attention heads showing
decreased entropy, indicating a shift toward focused information selection. We
demonstrate that targeted LoRA adaptation of these identified circuits achieves
significant performance improvement over standard LoRA fine-tuning while
requiring fewer training epochs. This work bridges the gap between black-box
evaluation and mechanistic understanding, providing insights into how neural
networks perform information selection and compression during summarization.

</details>


### [48] [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
*Ruixiao Li,Fahao Chen,Peng Li*

Main category: cs.CL

TL;DR: 本文提出了LAPS-SD算法，通过有效调度，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 目前推理请求存在不确定执行时间的问题，使得有效调度系统中的请求变得困难。

Method: 提出了一种半预知请求调度算法LAPS-SD，通过动态调度请求来优化推理延迟。

Result: LAPS-SD减少了约39%的推理延迟。

Conclusion: LAPS-SD能够在不确定执行时间情况下有效地降低推理延迟。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
employing a small speculative model (SSM) to generate multiple candidate tokens
and verify them using the LLM in parallel. This technique has been widely
integrated into LLM inference serving systems. However, inference requests
typically exhibit uncertain execution time, which poses a significant challenge
of efficiently scheduling requests in these systems. Existing work estimates
execution time based solely on predicted output length, which could be
inaccurate because execution time depends on both output length and token
acceptance rate of verification by the LLM. In this paper, we propose a
semi-clairvoyant request scheduling algorithm called
Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a
number of inference requests, LAPS-SD can effectively minimize average
inference latency by adaptively scheduling requests according to their features
during decoding. When the token acceptance rate is dynamic and execution time
is difficult to estimate, LAPS-SD maintains multiple priority queues and allows
request execution preemption across different queues. Once the token acceptance
rate becomes stable, LAPS-SD can accurately estimate the execution time and
schedule requests accordingly. Extensive experiments show that LAPS-SD reduces
inference latency by approximately 39\% compared to state-of-the-art scheduling
methods.

</details>


### [49] [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
*Fuma Kurata,Mao Saeki,Masaki Eguchi,Shungo Suzuki,Hiroaki Takatsu,Yoichi Matsuyama*

Main category: cs.CL

TL;DR: 研究开发并验证了参与度和融洽度量表，成功捕捉了人类与对话代理在对话体验上的差异。


<details>
  <summary>Details</summary>
Motivation: 为评估多模态对话系统在外语学习中的用户体验质量，研究旨在开发和验证参与度和融洽度量表。

Method: 基于教育心理学、社会心理学和二语习得理论，设计了参与度和融洽度量表。研究通过74名日本英语学习者与受过训练的人类导师和对话代理进行角色扮演和讨论任务，并在每次对话任务后回答参与度和融洽度量表。采用Cronbach's α系数分析和一系列验证性因素分析研究量表的结构效度和设计项目的可靠性，然后比较与人类导师和对话代理的对话中的参与度和融洽度分数。

Result: 研究结果表明，设计的量表成功捕捉到了人类对话者与对话代理之间的对话体验质量的差异。

Conclusion: 研究成功地开发并验证了参与度和融洽度量表，这些量表能够评估使用多模态对话系统进行外语学习时的用户体验质量。在多视角下，这些量表能够区分人类对话者和对话代理之间的对话体验质量差异。

Abstract: This study aimed to develop and validate two scales of engagement and rapport
to evaluate the user experience quality with multimodal dialogue systems in the
context of foreign language learning. The scales were designed based on
theories of engagement in educational psychology, social psychology, and second
language acquisition.Seventy-four Japanese learners of English completed
roleplay and discussion tasks with trained human tutors and a dialog agent.
After each dialogic task was completed, they responded to the scales of
engagement and rapport. The validity and reliability of the scales were
investigated through two analyses. We first conducted analysis of Cronbach's
alpha coefficient and a series of confirmatory factor analyses to test the
structural validity of the scales and the reliability of our designed items. We
then compared the scores of engagement and rapport between the dialogue with
human tutors and the one with a dialogue agent. The results revealed that our
scales succeeded in capturing the difference in the dialogue experience quality
between the human interlocutors and the dialogue agent from multiple
perspectives.

</details>


### [50] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
*Haoyang Zhang,Hexin Liu,Xiangyu Zhang,Qiquan Zhang,Yuchen Hu,Junqi Zhao,Fei Tian,Xuerui Yang,Eng Siong Chng*

Main category: cs.CL

TL;DR: 研究帧率对语音标记影响，发现其对不同语言的影响不同，有助于优化自动语音识别等应用中帧率选择。


<details>
  <summary>Details</summary>
Motivation: 探索不同帧率如何影响语音标记化，以优化语音标记器的帧率选择。

Method: 编码不同帧率的语音并在语音识别任务中评估生成的语义标记。

Result: 研究发现帧率变化对每种语言的语音标记化有不同影响。

Conclusion: 帧率对语音标记的影响因语言而异，强调了帧率、音素密度和语言特定声学特征之间的相互作用。

Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.

</details>


### [51] [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
*Zenghao Duan,Zhiyi Yin,Zhichao Shi,Liang Pang,Shaoling Jing,Jiayi Wu,Yu Yan,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper proposes a new detoxification method for LLMs, GloSS, which surpasses previous methods by focusing on global toxic subspaces instead of layer-wise ones.


<details>
  <summary>Details</summary>
Motivation: To investigate the underlying mechanisms of toxicity generation in Large Language Models and propose an effective detoxification approach.

Method: This paper proposes GloSS (Global Toxic Subspace Suppression), a four-stage method that identifies and removes the global toxic subspace from the parameters of FFN.

Result: Experiments across a range of LLMs show that GloSS achieves state-of-the-art detoxification performance without requiring large-scale data or model retraining.

Conclusion: GloSS achieves state-of-the-art detoxification performance while preserving the models' general capabilities.

Abstract: This paper investigates the underlying mechanisms of toxicity generation in
Large Language Models (LLMs) and proposes an effective detoxification approach.
Prior work typically considers the Feed-Forward Network (FFN) as the main
source of toxicity, representing toxic regions as a set of toxic vectors or
layer-wise subspaces. However, our in-depth analysis reveals that the global
toxic subspace offers a more effective and comprehensive representation of
toxic region within the model. Building on this insight, we propose GloSS
(Global Toxic Subspace Suppression), a lightweight, four-stage method that
mitigates toxicity by identifying and removing the global toxic subspace from
the parameters of FFN. Experiments across a range of LLMs show that GloSS
achieves state-of-the-art detoxification performance while preserving the
models general capabilities, without requiring large-scale data or model
retraining.

</details>


### [52] [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
*Davide Picca*

Main category: cs.CL

TL;DR: 论文认为大型语言模型不应被视作认知系统，而应从符号学角度理解其在文化中的作用，强调其生成文本的能力而非思考，重新审视语言与人工系统在知识生产中的角色。


<details>
  <summary>Details</summary>
Motivation: 挑战将大型语言模型视为认知系统的趋势，旨在通过符号学框架提供更准确的理解，并避免拟人化。

Method: 通过理论分析及实际案例应用，展示LLMs如何作为符号代理，其输出可视为可解释的行为，开放给上下文协商及批判性反思。

Result: 展示LLMs在符号环境中的行动，强调其作为生成文本而非理解语言的工具，提供更具伦理意识的框架来研究与应用LLMs。

Conclusion: 该论文通过采用符号学视角，重新定义了大型语言模型（LLMs）在文化过程中所扮演的角色，强调其作为生成文本的工具，而非具有认知功能的系统。

Abstract: This paper challenges the prevailing tendency to frame Large Language Models
(LLMs) as cognitive systems, arguing instead for a semiotic perspective that
situates these models within the broader dynamics of sign manipulation and
meaning-making. Rather than assuming that LLMs understand language or simulate
human thought, we propose that their primary function is to recombine,
recontextualize, and circulate linguistic forms based on probabilistic
associations. By shifting from a cognitivist to a semiotic framework, we avoid
anthropomorphism and gain a more precise understanding of how LLMs participate
in cultural processes, not by thinking, but by generating texts that invite
interpretation. Through theoretical analysis and practical examples, the paper
demonstrates how LLMs function as semiotic agents whose outputs can be treated
as interpretive acts, open to contextual negotiation and critical reflection.
We explore applications in literature, philosophy, education, and cultural
production, emphasizing how LLMs can serve as tools for creativity, dialogue,
and critical inquiry. The semiotic paradigm foregrounds the situated,
contingent, and socially embedded nature of meaning, offering a more rigorous
and ethically aware framework for studying and using LLMs. Ultimately, this
approach reframes LLMs as technological participants in an ongoing ecology of
signs. They do not possess minds, but they alter how we read, write, and make
meaning, compelling us to reconsider the foundations of language,
interpretation, and the role of artificial systems in the production of
knowledge.

</details>


### [53] [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
*Abderrahman Skiredj,Ferdaous Azhari,Houdaifa Atou,Nouamane Tazi,Ismail Berrada*

Main category: cs.CL

TL;DR: 研究通过对开源大型语言模型进行达里亚语优化，显著提升其语言处理能力，并展示了绿色AI的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前开源的大型语言模型对摩洛哥阿拉伯语（达里亚语）支持不足，研究旨在提升达里亚语的流利度，同时保护语言模型的跨语言推理能力。

Method: 将三种紧凑型指令套件翻译为达里亚语，并保留部分原始英语指令，同时添加数学、编码和科学提示，然后通过LoRA调优Gemma模型进行训练。

Result: 通过LoRA调优，达里亚MMLU从32.8提升至42.7，并且添加推理密集的部分进一步提高到47.5；GemMaroc-27B模型在达里亚常识任务上表现优异且保留了强大的数学和推理能力。

Conclusion: 采用质量重于数量的对齐策略可以在保持跨语言推理能力的同时，实现流畅的达里亚语处理。这种方法不仅提升了达里亚语任务表现，还展示了绿色人工智能的潜力。

Abstract: Open-source large language models (LLMs) still marginalise Moroccan Arabic
(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters
or to sacrifice the very reasoning skills that make LLMs useful. We show that a
rigorously quality-over-quantity alignment strategy can surface fluent Darija
while safeguarding the backbone s cross-lingual reasoning at a sliver of the
usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6
K and TULU 50 K into Darija, preserve 20 of the English originals, and add
mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on
5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the
reasoning-dense TULU portion pushes it to 47.5 with no English regression.
Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which
matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,
scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc
retains Gemma-27B s strong maths and general-reasoning ability, showing only
minimal movement on GSM8K and English benchmarks. The entire model is trained
in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable
language technology. We release code, data and checkpoints to spur
Darija-centric applications in education, public services and everyday digital
interaction.

</details>


### [54] [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
*Ben Anson,Xi Wang,Laurence Aitchison*

Main category: cs.CL

TL;DR: 提出了一种在长上下文中具有尺度不变特性的注意力机制，在推理阶段使用，在验证损失及长上下文检索上表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发能够从短上下文训练推广到长上下文推理的注意力机制，以有效处理长上下文的问题。

Method: 在高斯假设下，我们进行注意力logits的简单位置依赖变换，以满足提出的尺度不变总注意力和注意力稀疏性条件。

Result: 验证损失方面取得明显优势，有效进行长上下文检索。

Conclusion: 实验表明，所提出的具有尺度不变总注意力和注意力稀疏性的注意力机制在从短上下文训练到长上下文验证中，在验证损失方面具有显著优势，并在长上下文检索中效果良好。

Abstract: One persistent challenge in LLM research is the development of attention
mechanisms that are able to generalise from training on shorter contexts to
inference on longer contexts. We propose two conditions that we expect all
effective long context attention mechanisms to have: scale-invariant total
attention, and scale-invariant attention sparsity. Under a Gaussian assumption,
we show that a simple position-dependent transformation of the attention logits
is sufficient for these conditions to hold. Experimentally we find that the
resulting scale-invariant attention scheme gives considerable benefits in terms
of validation loss when zero-shot generalising from training on short contexts
to validation on longer contexts, and is effective at long-context retrieval.

</details>


### [55] [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
*Yihong Wu,Liheng Ma,Muzhi Li,Jiaming Zhou,Jianye Hao,Ho-fung Leung,Irwin King,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本论文提出Mujica-MyGO，用于提升复杂问答任务的性能，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因缺乏事实知识而在问答任务中容易产生幻觉。尽管检索增强生成技术可以解决这些问题，但现有方法过于依赖上下文学习，并受限于LLM的基本推理能力。

Method: 提出了Mujica，一个用于复杂问答的多跳联合智能系统，包括一个用于问题分解的规划器和一个通过检索和推理解决问题的工作器。此外，引入了MyGO，一种用最大似然估计替代传统策略梯度更新的强化学习方法。

Result: 实证结果展示了Mujica-MyGO在多个数据集上提升多跳问答性能的有效性。

Conclusion: Mujica-MyGO有效提高了多跳问答任务的性能，提供了可扩展且资源高效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility, due to
the lack of factual knowledge, their application to Question Answering (QA)
tasks remains hindered by hallucination.
  While Retrieval-Augmented Generation mitigates these issues by integrating
external knowledge, existing approaches rely heavily on in-context learning,
whose performance is constrained by the fundamental reasoning capabilities of
LLMs.
  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex
Question Answering, comprising a planner that decomposes questions into a
directed acyclic graph of subquestions and a worker that resolves questions via
retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy
Gradient Optimization), a novel reinforcement learning method that replaces
traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by
sampling trajectories from an asymptotically optimal policy. MyGO eliminates
the need for gradient rescaling and reference models, ensuring stable and
efficient training.
  Empirical results across multiple datasets demonstrate the effectiveness of
Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a
scalable and resource-efficient solution for complex QA tasks.

</details>


### [56] [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
*Gordana Ispirova,Michael Sebek,Giulia Menichetti*

Main category: cs.CL

TL;DR: 本章节探讨了食品加工的演变及分类，分析机器学习及AI在食品信息学中的作用，引入了FoodProX模型及多模态AI以改善食品分类。


<details>
  <summary>Details</summary>
Motivation: 传统的分类框架如NOVA、Nutri-Score和SIGA存在主观性和复制性问题，这对于流行病学研究和公共政策形成障碍，因而需要新的计算方法来改善食品分类。

Method: 提出了一种使用随机森林模型的FoodProX算法，应用于营养成分数据以推断加工水平，并生成连续FPro评分，还利用了BERT和BioBERT等大型语言模型进行语义嵌入来预测任务。

Result: 使用Open Food Facts数据库进行案例研究，展示了多模态AI模型如何整合结构化与非结构化数据实现食品分类，并为公众健康提供新的食品加工评估途径。

Conclusion: 该章节认为多模态AI模型能够整合结构化和非结构化数据，实现大规模食品分类，为公共健康和研究中的食品加工评估提供了一种新范式。

Abstract: This chapter explores the evolution, classification, and health implications
of food processing, while emphasizing the transformative role of machine
learning, artificial intelligence (AI), and data science in advancing food
informatics. It begins with a historical overview and a critical review of
traditional classification frameworks such as NOVA, Nutri-Score, and SIGA,
highlighting their strengths and limitations, particularly the subjectivity and
reproducibility challenges that hinder epidemiological research and public
policy. To address these issues, the chapter presents novel computational
approaches, including FoodProX, a random forest model trained on nutrient
composition data to infer processing levels and generate a continuous FPro
score. It also explores how large language models like BERT and BioBERT can
semantically embed food descriptions and ingredient lists for predictive tasks,
even in the presence of missing data. A key contribution of the chapter is a
novel case study using the Open Food Facts database, showcasing how multimodal
AI models can integrate structured and unstructured data to classify foods at
scale, offering a new paradigm for food processing assessment in public health
and research.

</details>


### [57] [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)
*Md Rafi Ur Rashid,Vishnu Asutosh Dasu,Ye Wang,Gang Tan,Shagufta Mehnaz*

Main category: cs.CL

TL;DR: The paper introduces ASE, a novel framework enhancing LLM robustness against diverse attacks by simulating adversarial scenarios. It shows considerable improvement in security and natural interaction metrics.


<details>
  <summary>Details</summary>
Motivation: LLMs are prone to various safety risks like jailbreaks, toxic content, hallucinations, and bias, while existing defenses either address a single threat or resort to rigid rejection, which hampers user experience and does not generalize well across attacks.

Method: The paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that uses Chain-of-Thought (CoT) reasoning to guide LLMs through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before responding to user queries.

Result: Evaluation on four benchmarks with the latest LLMs shows ASE achieves near-zero jailbreak attack success rates, minimal toxicity, reduces rejections to less than 4%, and outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and 4-10x lower bias scores.

Conclusion: ASE sets a new paradigm for secure and natural human-AI interaction by transforming adversarial perception into an intrinsic cognitive process.

Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain
susceptible to a growing spectrum of safety risks, including jailbreaks, toxic
content, hallucinations, and bias. Existing defenses often address only a
single threat type or resort to rigid outright rejection, sacrificing user
experience and failing to generalize across diverse and novel attacks. This
paper introduces Adversarial Scenario Extrapolation (ASE), a novel
inference-time computation framework that leverages Chain-of-Thought (CoT)
reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides
the LLM through a self-generative process of contemplating potential
adversarial scenarios and formulating defensive strategies before generating a
response to the user query. Comprehensive evaluation on four adversarial
benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak
attack success rates and minimal toxicity, while slashing outright rejections
to <4%. ASE outperforms six state-of-the-art defenses in
robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and
4-10x lower bias scores. By transforming adversarial perception into an
intrinsic cognitive process, ASE sets a new paradigm for secure and natural
human-AI interaction.

</details>


### [58] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
*Prateek Verma,Mert Pilanci*

Main category: cs.CL

TL;DR: 训练文本LLM可以获得理解图像和音频的能力，实现跨模态应用，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 研究文本模型如何通过阅读开发出理解图像和音频的能力，推进对文本LLM学习内部电路的理解。

Method: 使用自回归LLM模型，通过文本令牌进行训练，并输入图像补丁、音频波形或令牌作为输入，输出分类管道中的嵌入或类别标签。

Result: 展示文本权重有助于音频分类（FSD-50K和GTZAN数据集）以及图像分类（CIFAR-10和Fashion-MNIST数据集）和图像补丁。

Conclusion: 通过激活文本模型内部强大的连接，可以利用这些模型进行各种应用，而不需要每次都从头开始训练模型。

Abstract: This paper presents a fascinating find: By training an auto-regressive LLM
model on text tokens, the text model inherently develops internally an ability
to understand images and audio, thereby developing the ability to see and hear
just by reading. Popular audio and visual LLM models fine-tune text LLM models
to give text output conditioned on images and audio embeddings. On the other
hand, our architecture takes in patches of images, audio waveforms or tokens as
input. It gives us the embeddings or category labels typical of a
classification pipeline. We show the generality of text weights in aiding audio
classification for datasets FSD-50K and GTZAN. Further, we show this working
for image classification on CIFAR-10 and Fashion-MNIST, as well on image
patches. This pushes the notion of text-LLMs learning powerful internal
circuits that can be utilized by activating necessary connections for various
applications rather than training models from scratch every single time.

</details>


### [59] [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)
*Kristine Ann M. Carandang,Jasper Meynard P. Araña,Ethan Robert A. Casin,Christopher P. Monterola,Daniel Stanley Y. Tan,Jesus Felix B. Valenzuela,Christian M. Alis*

Main category: cs.CL

TL;DR: 研究评估了12个语言模型在临床记录生成中语义一致性和正确性方面的表现，发现Meta的Llama 70B最可靠，建议本地部署以改善数据隐私合规和文档写作效率。


<details>
  <summary>Details</summary>
Motivation: 由于医疗提供者在准确记录和保护患者数据隐私方面的法律和伦理责任，需要评估由大型语言模型驱动的临床记录生成系统在真实临床流程中应用的可行性。

Method: 评估12个来自Anthropic、Meta、Mistral和OpenAI的开源及商业语言模型在临床记录生成中的可靠性，包括他们在字符串一致性、语义一致性和正确性（语义相似性）方面的表现。

Result: 所有模型组的语言模型在语义一致性方面稳定，大部分模型生成的记录接近专家编写的对应记录。

Conclusion: Meta的Llama 70B在生成临床记录方面最可靠，其次是Mistral的小模型。建议本地部署这些相对较小的开源模型，以确保数据隐私合规并提高医疗提供者在临床文档编写中的效率。

Abstract: Due to the legal and ethical responsibilities of healthcare providers (HCPs)
for accurate documentation and protection of patient data privacy, the natural
variability in the responses of large language models (LLMs) presents
challenges for incorporating clinical note generation (CNG) systems, driven by
LLMs, into real-world clinical processes. The complexity is further amplified
by the detailed nature of texts in CNG. To enhance the confidence of HCPs in
tools powered by LLMs, this study evaluates the reliability of 12 open-weight
and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms
of their ability to generate notes that are string equivalent (consistency
rate), have the same meaning (semantic consistency) and are correct (semantic
similarity), across several iterations using the same prompt. The results show
that (1) LLMs from all model families are stable, such that their responses are
semantically consistent despite being written in various ways, and (2) most of
the LLMs generated notes close to the corresponding notes made by experts.
Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small
model. With these findings, we recommend the local deployment of these
relatively smaller open-weight models for CNG to ensure compliance with data
privacy regulations, as well as to improve the efficiency of HCPs in clinical
documentation.

</details>


### [60] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
*Yanshu Li,Tian Yun,Jianjiang Yang,Pinyuan Feng,Jinfa Huang,Ruixiang Tang*

Main category: cs.CL

TL;DR: The paper presents TACO, a model that leverages task mapping for better multimodal ICL, showing improved performance across various tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: Address limitations in understanding how LVLMs exploit in-context sequences during inference and to improve multimodal ICL.

Method: Present TACO, a lightweight transformer-based model equipped with task-aware attention that dynamically configures in-context sequences, and inject task-mapping signals into the autoregressive decoding process.

Result: Experiments on five LVLMs and nine datasets show TACO's consistent superiority over baselines across diverse ICL tasks.

Conclusion: TACO consistently surpasses baselines across diverse ICL tasks, indicating that task mapping is a valuable perspective for interpreting and improving multimodal ICL.

Abstract: Multimodal in-context learning (ICL) has emerged as a key mechanism for
harnessing the capabilities of large vision-language models (LVLMs). However,
its effectiveness remains highly sensitive to the quality of input in-context
sequences, particularly for tasks involving complex reasoning or open-ended
generation. A major limitation is our limited understanding of how LVLMs
actually exploit these sequences during inference. To bridge this gap, we
systematically interpret multimodal ICL through the lens of task mapping, which
reveals how local and global relationships within and among demonstrations
guide model reasoning. Building on this insight, we present TACO, a lightweight
transformer-based model equipped with task-aware attention that dynamically
configures in-context sequences. By injecting task-mapping signals into the
autoregressive decoding process, TACO creates a bidirectional synergy between
sequence construction and task reasoning. Experiments on five LVLMs and nine
datasets demonstrate that TACO consistently surpasses baselines across diverse
ICL tasks. These results position task mapping as a valuable perspective for
interpreting and improving multimodal ICL.

</details>


### [61] [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)
*Xiaozhao Liu,Dinggang Shen,Xihui Liu*

Main category: cs.CL

TL;DR: 提出GLIM模型，通过语义总结和改进评估标准，成功解决EEG到文本生成中的幻觉问题，增强了语义解码的可靠性。


<details>
  <summary>Details</summary>
Motivation: 文章的动机是解决当前EEG到文本解码中普遍存在的幻觉问题，即生成文本是否真正反映了大脑中的语义激活，还是仅仅是生成模型的幻觉。作者认识到EEG和文本之间在信息容量上存在的根本不匹配问题，并将解码任务重新框定为核心意义的语义总结，而不是以前的逐字文本重建。

Method: 文章提出了一种新的生成语言检查模型（GLIM），该模型通过学习信息量大且可解释的EEG表示，改进了在异质和小规模数据条件下的语义基础。这些改进包括不依赖于教师强制的自然流畅的EEG基础句子生成，并支持超越文本相似性的健壮评估，通过EEG-文本检索和零样本语义分类进行跨情感类别、关系类型和语料库主题的评估。

Result: 在ZuCo数据集上的实验表明，GLIM模型能够在没有教师强制情况下，一致性生成流畅且基于EEG的句子。此外，该模型能够进行EEG-文本检索和零样本语义分类，在情感类别、关系类型和语料库主题上实现更稳健的评估。

Conclusion: 本文提出的GLIM模型为EEG到文本的解码提供了一种有效的新方法，通过引入语义总结和改进的评估标准，解决了生成模型中广泛存在的幻觉问题，并为大脑解码的可靠性评估奠定了基础。

Abstract: Pretrained generative models have opened new frontiers in brain decoding by
enabling the synthesis of realistic texts and images from non-invasive brain
recordings. However, the reliability of such outputs remains
questionable--whether they truly reflect semantic activation in the brain, or
are merely hallucinated by the powerful generative models. In this paper, we
focus on EEG-to-text decoding and address its hallucination issue through the
lens of posterior collapse. Acknowledging the underlying mismatch in
information capacity between EEG and text, we reframe the decoding task as
semantic summarization of core meanings rather than previously verbatim
reconstruction of stimulus texts. To this end, we propose the Generative
Language Inspection Model (GLIM), which emphasizes learning informative and
interpretable EEG representations to improve semantic grounding under
heterogeneous and small-scale data conditions. Experiments on the public ZuCo
dataset demonstrate that GLIM consistently generates fluent, EEG-grounded
sentences without teacher forcing. Moreover, it supports more robust evaluation
beyond text similarity, through EEG-text retrieval and zero-shot semantic
classification across sentiment categories, relation types, and corpus topics.
Together, our architecture and evaluation protocols lay the foundation for
reliable and scalable benchmarking in generative brain decoding.

</details>


### [62] [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
*Haoyan Yang,Runxue Bao,Cao Xiao,Jun Ma,Parminder Bhatia,Shangqian Gao,Taha Kass-Hout*

Main category: cs.CL

TL;DR: 本文提出了一种偏差检测机制RBD，不需要修改LLM评估器本身，即可提高其评估准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM评估器由于潜在偏见导致可靠性受损的问题。

Method: 引入Reasoning-based Bias Detector (RBD)作为插件模块，通过偏差检测与反馈驱动修正的迭代过程来操作，并设计完整的管道支持其发展。

Result: RBD在多种偏见类型上的实验结果显示出强大的有效性。具体来说，RBD-8B模型将评估准确性平均提高了18.5%，一致性提高了10.9%，超越了提示基准和微调评判者12.8%和17.2%。此外，其跨领域的偏见泛化能力和效率良好。

Conclusion: RBD具有显著的有效性和可扩展性，其能够在多个LLM评估器上提高评估准确性和一致性。

Abstract: LLM-as-a-Judge has emerged as a promising tool for automatically evaluating
generated outputs, but its reliability is often undermined by potential biases
in judgment. Existing efforts to mitigate these biases face key limitations:
in-context learning-based methods fail to address rooted biases due to the
evaluator's limited capacity for self-reflection, whereas fine-tuning is not
applicable to all evaluator types, especially closed-source models. To address
this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is
a plug-in module that identifies biased evaluations and generates structured
reasoning to guide evaluator self-correction. Rather than modifying the
evaluator itself, RBD operates externally and engages in an iterative process
of bias detection and feedback-driven revision. To support its development, we
design a complete pipeline consisting of biased dataset construction,
supervision collection, distilled reasoning-based fine-tuning of RBD, and
integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging
from 1.5B to 14B, and observe consistent performance improvements across all
scales. Experimental results on 4 bias types--verbosity, position, bandwagon,
and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong
effectiveness. For example, the RBD-8B model improves evaluation accuracy by an
average of 18.5% and consistency by 10.9%, and surpasses prompting-based
baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results
highlight RBD's effectiveness and scalability. Additional experiments further
demonstrate its strong generalization across biases and domains, as well as its
efficiency.

</details>


### [63] [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
*Santiago Acevedo,Andrea Mascaretti,Riccardo Rende,Matéo Mahaut,Marco Baroni,Alessandro Laio*

Main category: cs.CL

TL;DR: 提出了一种方法，通过定量分析深度神经网络在不同领域间的相似表示。发现语义信息在语言模型和视觉转换器内部层级和token之间的分布规律及其导致的因果不对称性。较大模型可提取更多信息，字幕表示可预测视觉表示并存在信息不对称性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对于语义相关数据会发展出相似的表示，本文旨在通过定量分析,深入探讨语义信息在跨领域数据（如图像与文本）中的表征形式及其编码过程。

Method: 我们提出一种方法，可以通过测量语义相关数据表示的相对信息含量来定量调查这一现象，并探究如何将其编码到大型语言模型（LLMs）和视觉转换器的多个token中。通过处理翻译句对，我们辨识出内在的“语义”层，以及更大的语言模型（如DeepSeek-V3）比较小的模型（如Llama3.1-8B）提取更多的通用信息。

Result: 在LLMs的语义层中，较大的模型能够提取更多的通用信息；语义信息不仅散布于多个token间，还展现了长距离以及过去到未来的因果不对称性。语义层内的字幕表示能够预测对应图像的视觉表示，并展示显著的信息不对称性。

Conclusion: 深度神经网络在处理语义相关的数据时能够发展出相似的表示，即便这些数据来自不同的领域。语义信息在大型语言模型与视觉转换器中散布于多个token之间，并展示出长距离的相关性以及过去到未来的因果不对称性。语义层不仅能够预测视觉图像的表示，还展示了图像与文本表示之间显著的、与模型相关的信息不对称性。

Abstract: Deep neural networks are known to develop similar representations for
semantically related data, even when they belong to different domains, such as
an image and its description, or the same text in different languages. We
present a method for quantitatively investigating this phenomenon by measuring
the relative information content of the representations of semantically related
data and probing how it is encoded into multiple tokens of large language
models (LLMs) and vision transformers. Looking first at how LLMs process pairs
of translated sentences, we identify inner ``semantic'' layers containing the
most language-transferable information. We find moreover that, on these layers,
a larger LLM (DeepSeek-V3) extracts significantly more general information than
a smaller one (Llama3.1-8B). Semantic information is spread across many tokens
and it is characterized by long-distance correlations between tokens and by a
causal left-to-right (i.e., past-future) asymmetry. We also identify layers
encoding semantic information within visual transformers. We show that caption
representations in the semantic layers of LLMs predict visual representations
of the corresponding images. We observe significant and model-dependent
information asymmetries between image and text representations.

</details>


### [64] [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)
*Pramit Bhattacharyya,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: BanglaByT5, a byte-level language model tailored for Bangla, shows superior performance in processing Bangla compared to traditional tokenizers and larger models.


<details>
  <summary>Details</summary>
Motivation: Traditional tokenizers fail to adequately capture the nuances of morphologically rich languages such as Bangla. A byte-level approach is introduced to address this gap and enhance language processing capabilities.

Method: Introduction of BanglaByT5, a byte-level encoder-decoder model, built on Google's ByT5 architecture and pre-trained using a 14GB corpus of Bangla literature and news articles.

Result: BanglaByT5 shows competitive performance in both zero-shot and supervised tasks, outperforming several larger multilingual models, proving effective for Bangla NLP.

Conclusion: Byte-level modeling, as demonstrated by BanglaByT5, is effective for morphologically rich languages like Bangla, providing a powerful tool for language processing tasks in diverse environments.

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing tasks. However, most LLM models use traditional
tokenizers like BPE and SentencePiece, which fail to capture the finer nuances
of a morphologically rich language like Bangla (Bengali). In this work, we
introduce BanglaByT5, the first byte-level encoder-decoder model explicitly
tailored for Bangla. Built upon a small variant of Googles ByT5 architecture,
BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality
literary and newspaper articles. Through zeroshot and supervised evaluations
across generative and classification tasks, BanglaByT5 demonstrates competitive
performance, surpassing several multilingual and larger models. Our findings
highlight the efficacy of byte-level modelling for morphologically rich
languages and highlight BanglaByT5 potential as a lightweight yet powerful tool
for Bangla NLP, particularly in both resource-constrained and scalable
environments.

</details>


### [65] [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
*Cécile Rousseau,Tobia Boschi,Giandomenico Cornacchia,Dhaval Salwala,Alessandra Pascale,Juan Bernabe Moreno*

Main category: cs.CL

TL;DR: SDForger是一个利用LLM生成高质量多变量时间序列的框架，能够从少量样本中高效生成合成时间序列，并在各种评估和任务中优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 在保障数据质量的同时，通过灵活高效的框架生成多变量时间序列，以应对对合成数据生成需求。

Method: 将时间序列信号转换为表格嵌入，然后编码为文本，用于微调自回归LLM。随后，通过采样新的文本嵌入并解码为合成时间序列。

Result: SDForger在各种数据集上表现优异，并即将开源。

Conclusion: SDForger在合成时间序列生成方面优于现有模型，在相似性评估和下游预测任务中表现出色。

Abstract: SDForger is a flexible and efficient framework for generating high-quality
multivariate time series using LLMs. Leveraging a compact data representation,
SDForger provides synthetic time series generation from a few samples and
low-computation fine-tuning of any autoregressive LLM. Specifically, the
framework transforms univariate and multivariate signals into tabular
embeddings, which are then encoded into text and used to fine-tune the LLM. At
inference, new textual embeddings are sampled and decoded into synthetic time
series that retain the original data's statistical properties and temporal
dynamics. Across a diverse range of datasets, SDForger outperforms existing
generative models in many scenarios, both in similarity-based evaluations and
downstream forecasting tasks. By enabling textual conditioning in the
generation process, SDForger paves the way for multimodal modeling and the
streamlined integration of time series with textual information. SDForger
source code will be open-sourced soon.

</details>


### [66] [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
*Tao Sun,Enhao Pan,Zhengkai Yang,Kaixin Sui,Jiajun Shi,Xianfu Cheng,Tongliang Li,Wenhao Huang,Ge Zhang,Jian Yang,Zhoujun Li*

Main category: cs.CL

TL;DR: Introduction of P2P, a multi-agent framework, to automate academic poster creation from papers, accompanied by new datasets and benchmarks for improved evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to automate the creation of academic posters, overcoming challenges like semantic richness and structural nuances, and provide standardized benchmarks for evaluation.

Method: P2P uses a multi-agent framework with three specialized agents for visual element processing, content generation, and final poster assembly, alongside checker modules. It also involves the creation of P2PInstruct, a large-scale instruction dataset, and P2PEval, a benchmark for evaluation.

Result: P2P generates high-quality, HTML-rendered academic posters and demonstrates strong potential for practical applications. Additionally, the newly established datasets and benchmarks aim to advance the field.

Conclusion: The P2P framework and its accompanying datasets/tools aim to streamline the process of generating and evaluating academic posters from papers, providing robust tools and benchmarks for future advancements.

Abstract: Academic posters are vital for scholarly communication, yet their manual
creation is time-consuming. However, automated academic poster generation faces
significant challenges in preserving intricate scientific details and achieving
effective visual-textual integration. Existing approaches often struggle with
semantic richness and structural nuances, and lack standardized benchmarks for
evaluating generated academic posters comprehensively. To address these
limitations, we introduce P2P, the first flexible, LLM-based multi-agent
framework that generates high-quality, HTML-rendered academic posters directly
from research papers, demonstrating strong potential for practical
applications. P2P employs three specialized agents-for visual element
processing, content generation, and final poster assembly-each integrated with
dedicated checker modules to enable iterative refinement and ensure output
quality. To foster advancements and rigorous evaluation in this domain, we
construct and release P2PInstruct, the first large-scale instruction dataset
comprising over 30,000 high-quality examples tailored for the academic
paper-to-poster generation task. Furthermore, we establish P2PEval, a
comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation
methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and
detailed, human-annotated checklists. Our contributions aim to streamline
research dissemination and provide the community with robust tools for
developing and evaluating next-generation poster generation systems.

</details>


### [67] [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)
*Yifei Liu,Yu Cui,Haibin Zhang*

Main category: cs.CL

TL;DR: 提出RRTL方法评估RLLMs工具学习安全性，发现其安全性优于传统LLMs，但存在欺骗性风险和多语言安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对RLLMs在工具学习中安全性的研究，因此有必要设计新的方法来评估其安全性能。

Method: 采用RRTL红队方法来评估RLLMs的工具学习安全性，包括识别欺骗性威胁和使用链式思考提示迫使工具调用。还建立了传统LLMs的基准。

Result: 七个主流RLLMs的评估表明，尽管其总体安全性能优于传统LLMs，但它们在工具使用披露和风险警示方面存在欺骗性风险，并在多语言安全性上仍有漏洞。

Conclusion: 本文提出的RRTL方法揭示了RLLMs在工具学习中的安全性问题，并提供了改进其安全性的建议。RLLMs比传统LLMs具有更好的安全性能，但仍存在显著的安全差异。

Abstract: While tool learning significantly enhances the capabilities of large language
models (LLMs), it also introduces substantial security risks. Prior research
has revealed various vulnerabilities in traditional LLMs during tool learning.
However, the safety of newly emerging reasoning LLMs (RLLMs), such as
DeepSeek-R1, in the context of tool learning remains underexplored. To bridge
this gap, we propose RRTL, a red teaming approach specifically designed to
evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the
identification of deceptive threats, which evaluates the model's behavior in
concealing the usage of unsafe tools and their potential risks; and (2) the use
of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also
includes a benchmark for traditional LLMs. We conduct a comprehensive
evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs
generally achieve stronger safety performance than traditional LLMs, yet
substantial safety disparities persist across models; (2) RLLMs can pose
serious deceptive risks by frequently failing to disclose tool usage and to
warn users of potential tool output risks; (3) CoT prompting reveals
multi-lingual safety vulnerabilities in RLLMs. Our work provides important
insights into enhancing the security of RLLMs in tool learning.

</details>


### [68] [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)
*Junlin Li,Guodong DU,Jing Li,Sim Kuan Goh,Wenya Wang,Yequan Wang,Fangming Liu,Ho-Kin Tang,Saleh Alharbi,Daojing He,Min Zhang*

Main category: cs.CL

TL;DR: MMER通过无训练的方法，重用和合并已存在的MLLMs以扩展其多模态能力并保留性能，结果显示其显著优于基准线。


<details>
  <summary>Details</summary>
Motivation: 针对当前多模态LLM训练方法资源密集且缺乏灵活性的缺点，该研究提出一种无训练的替代方案MMER，希望能实现多模态扩展和保留原始性能。

Method: MMER通过重用MLLM的多模态编码器并合并其LLM参数，同时比较原始和合并的LLM参数来生成二进制掩码。这些解耦的参数可以独立处理特定模态的输入，减少参数冲突并保留原始MLLM的保真度。

Result: 大量实验显示，与基准线相比，MMER在多模态能力上有显著提高，同时保留99%的原始性能，并显著减轻了灾难性遗忘。

Conclusion: MMER有效地扩展了LLMs的多模态能力，同时保留了99%的原始性能，大大减少了灾难性遗忘。

Abstract: Fine-tuning Large Language Models (LLMs) with multimodal encoders on
modality-specific data expands the modalities that LLMs can handle, leading to
the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies
on resource-intensive and inflexible fine-tuning from scratch with new
multimodal data. In this paper, we propose MMER (Multi-modality Expansion and
Retention), a training-free approach that integrates existing MLLMs for
effective multimodal expansion while retaining their original performance.
Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM
parameters. By comparing original and merged LLM parameters, MMER generates
binary masks to approximately separate LLM parameters for each modality. These
decoupled parameters can independently process modality-specific inputs,
reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can
also mitigate catastrophic forgetting by applying a similar process to MLLMs
fine-tuned on new tasks. Extensive experiments show significant improvements
over baselines, proving that MMER effectively expands LLMs' multimodal
capabilities while retaining 99% of the original performance, and also markedly
mitigates catastrophic forgetting.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 研究通过HistVis数据集评估了TTI系统在描绘历史时的表现，发现其在风格、时代和人口统计上存在不准确性。提出了一种评估方法以提高历史再现的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成图像(TTI)扩散模型在内容创作中变得越来越有影响力，人们对其社会和文化影响的关注也在增加。然而，尽管此前的研究主要考察了人口和文化偏见，但这些模型在准确再现历史背景方面的能力却仍未被充分探索。

Method: 研究提出了一种系统且可重现的方法，以评估TTI系统如何描绘不同历史时期。为此目的，引入了HistVis数据集——一个包含三种先进扩散模型使用精心设计的提示生成的30000幅合成图像的精选集合。研究评估了生成的图像在三个关键方面的表现：隐含风格关联、历史一致性和人口统计代表性。

Result: 研究发现，在历史主题的生成图像中存在系统性不准确性，TTI模型常通过未经声明的风格线索刻板化过去的时代，引入了现代背景下不应出现的物体，并未能反映出合理的种族和性别分布。

Conclusion: 通过这项工作，研究者揭示出当前TTI扩散模型在生成历史主题图像时存在的系统性不准确性。这些模型常常以未经声明的风格线索刻板化过去的时代，引入时代错置现象，并未能反映出合理的人口统计模式。该研究提供了一种可扩展的方法和基准，以评估生成图像中的历史再现，旨在推动构建更具历史准确性和文化对齐的TTI模型。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


### [70] [EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language](https://arxiv.org/abs/2505.17090)
*Phoebe Chua,Cathy Mengying Fang,Takehiko Ohkawa,Raja Kushalnagar,Suranga Nanayakkara,Pattie Maes*

Main category: cs.CV

TL;DR: EmoSign 数据集解决了手语情感识别研究的空白，为多模态情感识别提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 由于手语中的情感指标尚未得到充分理解，造成了批判性环境中的沟通障碍，故需建立这一 Emosign 数据集以解决这一问题。

Method: 通过包含情感和情绪标签的美国手语视频数据集，以及由专业解译经验的三位聋人手语用户进行的开放式情感提示描述注释；并提供情感分类基线模型。

Result: 为美国手语视频提供情感和情绪标签的全面数据集，并包含基线情感分类模型。

Conclusion: EmoSign 数据集填补了手语情感研究的空白，并为多模态情感识别模型能力设立了新基准。

Abstract: Unlike spoken languages where the use of prosodic features to convey emotion
is well studied, indicators of emotion in sign language remain poorly
understood, creating communication barriers in critical settings. Sign
languages present unique challenges as facial expressions and hand movements
simultaneously serve both grammatical and emotional functions. To address this
gap, we introduce EmoSign, the first sign video dataset containing sentiment
and emotion labels for 200 American Sign Language (ASL) videos. We also collect
open-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL
signers with professional interpretation experience. Alongside the annotations,
we include baseline models for sentiment and emotion classification. This
dataset not only addresses a critical gap in existing sign language research
but also establishes a new benchmark for understanding model capabilities in
multimodal emotion recognition for sign languages. The dataset is made
available at https://huggingface.co/datasets/catfang/emosign.

</details>


### [71] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
*Yanshu Li,JianJiang Yang,Bozheng Li,Ruixiang Tang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练即可应用于多模态学习的新方法CAMA，解决了注意力机制的内在问题。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态的ICL支持大量的实际应用，但其仍然不稳定，目前的研究主要集中在优化序列配置，而忽略了LVLM的内在机制。本文旨在通过理论分析发现标准注意力影响ICL表现的三个核心限制，并提出解决方法。

Method: 提出了一个名为Context-Aware Modulated Attention (CAMA)的方法，直接校准LVLM的注意力控制，这是一种简单而有效的即插即用方法。

Result: 在四个LVLM上进行评估，通过六个基准测试，证明了CAMA的有效性和普适性。

Conclusion: 本文提出的CAMA方法可以无缝应用于各种开源LVLM，评估结果表明其有效性和普适性。

Abstract: Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.

</details>


### [72] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/abs/2505.17127)
*Michal Golovanevsky,William Rudman,Michael Lepori,Amir Bar,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 研究引入Visual CounterFact数据集来测试多模态模型在直觉知识与视觉证据冲突中的决策，并提出PvP机制以控制输出，发现视觉证据往往最终占据主导。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨多模态大型语言模型在视觉问答任务中的推理是更多依赖于记忆的世界知识还是输入图像中的视觉信息。

Method: 引入Visual CounterFact数据集，包含视觉真实的反事实，以检验模型在直觉知识特性与视觉信息的冲突中如何做出决策。研究使用了激活层次的干预技术——PvP导向向量，以控制模型输出。

Result: 研究发现，在模型预测过程中，会首先反映出记忆的直觉知识，但在中后期层次则逐渐倾向于视觉证据。PvP机制能将92.5%的颜色预测和74.6%的尺寸预测从直觉转向反事实。

Conclusion: 研究表明，在多模态大型语言模型中，通过在输入层次的干预，可以在输出结果中更倾向于世界知识或视觉输入。PvP机制能够在大多数情况下成功实现这一点。

Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.

</details>


### [73] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
*Tanqiu Jiang,Jiacheng Liang,Rongyi Zhu,Jiawei Zhou,Fenglong Ma,Ting Wang*

Main category: cs.CV

TL;DR: 该研究开发了DTR，一种通过优化KV缓存减轻视觉-语言模型越狱攻击的推理时间防御机制。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型容易受到利用视觉文本交互的越狱攻击的影响。该研究旨在开发一种防御机制来缓解此类攻击。

Method: 采用了一种新的视觉模态引发的安全相关分布变化公式，通过动态调整视觉标记权重，最小化对抗性视觉输入的影响。

Result: DTR在攻击鲁棒性和对良性任务的性能方面均优于现有防御，成功应用KV缓存优化增强了多模态基础模型的安全性。

Conclusion: DTR通过优化模型的关键值(KV)缓存实现了推理时间防御，以减轻多模态越狱攻击的影响，同时保持模型的通用能力和推理效率。

Abstract: Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)

</details>


### [74] [A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data](https://arxiv.org/abs/2505.17201)
*Chaim Chai Elchik,Fatemeh Karimi Nejadasl,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.CV

TL;DR: 论文开发了一个多视图框架，利用立体视频提高水下鱼类的检测和跟踪精度，显示出比单视图方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 传统单视图MOT模型难以应对水下鱼类由于复杂3D运动和数据噪声带来的挑战，因此需要一种改进的跟踪方法。

Method: 通过适应最先进的单视图MOT模型FairMOT和YOLOv8，以及开发多视图框架来进行水下鱼类检测和跟踪。利用立体视频输入增强跟踪精度，并使用立体匹配技术生成3D输出。

Result: 开发的框架在检测鱼类实体方面具有47%的相对准确性，并能生成新的3D输出，以更好地理解鱼类运动和互动。

Conclusion: 本研究开发了一种多视图框架，通过立体视频输入提高跟踪精度和鱼类行为模式识别。集成和评估这些模型在水下鱼类视频数据集上的表现，显示出与单视图方法相比在精度和可靠性方面有显著改善。

Abstract: Multi-object tracking (MOT) in computer vision has made significant
advancements, yet tracking small fish in underwater environments presents
unique challenges due to complex 3D motions and data noise. Traditional
single-view MOT models often fall short in these settings. This thesis
addresses these challenges by adapting state-of-the-art single-view MOT models,
FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological
studies. The core contribution of this research is the development of a
multi-view framework that utilizes stereo video inputs to enhance tracking
accuracy and fish behavior pattern recognition. By integrating and evaluating
these models on underwater fish video datasets, the study aims to demonstrate
significant improvements in precision and reliability compared to single-view
approaches. The proposed framework detects fish entities with a relative
accuracy of 47% and employs stereo-matching techniques to produce a novel 3D
output, providing a more comprehensive understanding of fish movements and
interactions

</details>


### [75] [REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge](https://arxiv.org/abs/2505.17223)
*Siyang Song,Micol Spitale,Xiangyu Kong,Hengde Zhu,Cheng Luo,Cristina Palmero,German Barquero,Sergio Escalera,Michel Valstar,Mohamed Daoudi,Tobias Baur,Fabien Ringeval,Andrew Howes,Elisabeth Andre,Hatice Gunes*

Main category: cs.CV

TL;DR: REACT 2025挑战旨在开发ML模型生成人类穿戴式面部反应，提供了MARS数据集用于基准测试。


<details>
  <summary>Details</summary>
Motivation: 为应对人类互动中由讲话者行为引起的多样化面部反应，提出REACT 2025挑战，旨在开发能够生成多样性和现实性面部反应的模型。

Method: 提出了REACT 2025挑战，鼓励开发机器学习模型，并提供大规模的MARS数据集用于基准测试。

Result: 论文展示了两个子挑战：离线MAFRG和在线MAFRG，并报告了基线模型的性能，其代码已公开。

Conclusion: 本论文通过提出REACT 2025挑战，促进了机器学习模型的发展，以生成符合人类听者在双人互动中对刺激做出反应时的适当面部表情。

Abstract: In dyadic interactions, a broad spectrum of human facial reactions might be
appropriate for responding to each human speaker behaviour. Following the
successful organisation of the REACT 2023 and REACT 2024 challenges, we are
proposing the REACT 2025 challenge encouraging the development and benchmarking
of Machine Learning (ML) models that can be used to generate multiple
appropriate, diverse, realistic and synchronised human-style facial reactions
expressed by human listeners in response to an input stimulus (i.e.,
audio-visual behaviours expressed by their corresponding speakers). As a key of
the challenge, we provide challenge participants with the first natural and
large-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human
dyadic interactions containing a total of 2856 interaction sessions covering
five different topics. In addition, this paper also presents the challenge
guidelines and the performance of our baselines on the two proposed
sub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge
baseline code is publicly available at
https://github.com/reactmultimodalchallenge/baseline_react2025

</details>


### [76] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
*Omar Moured,Yufan Chen,Ruiping Liu,Simon Reiß,Philip Torr,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文介绍了CHAOS，一个用于评估多模态大型语言模型在图表扰动下鲁棒性的基准测试。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大型语言模型可以进行图表分析和可视化，现实应用中图表常常具有挑战性的或噪声特征，对‘异常图表’的解读仍然是一个重大挑战。

Method: CHAOS基准测试包括五种文本和十种视觉扰动，每种扰动分为三个级别（简单，中等，困难），这些级别灵感来自于人类评估的研究结果。测试包括13种最先进的MLLMs分为三组（一般模型、文档模型和专门针对图表的模型），根据其训练范围和数据进行分类。

Result: 实验和案例研究提供了关于模型在图表扰动方面的鲁棒性的重要见解，并旨在指导未来图表理解领域的研究。

Conclusion: 我们引入了CHAOS作为一个鲁棒性基准，用于系统地评估MLLMs在面对图表扰动时的表现。

Abstract: Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, "outlier charts" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.

</details>


### [77] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/abs/2505.17245)
*Ryota Yagi*

Main category: cs.CV

TL;DR: 研究提出了针对对象检测任务的数据集修剪方法，通过新的评分机制有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 在更复杂的计算机视觉任务中应用数据集修剪技术，如对象检测任务。

Method: 提出了一种新的评分方法--基于方差的预测评分 (VPS)，结合了IoU和置信度评分，识别特定于检测任务的信息训练样本。

Result: 在PASCAL VOC和MS COCO数据集上的实验表明，该方法在平均平均精度(mAP)方面优于先前的数据集修剪方法。还发现，注释数量和类分布变化会影响检测性能，但选择信息示例比数据集大小或平衡性更为关键。

Conclusion: 成功将数据集修剪技术应用到了对象检测领域，为复杂视觉任务中的数据集修剪奠定了基础。

Abstract: Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.

</details>


### [78] [ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation](https://arxiv.org/abs/2505.17256)
*Liang Shi,Yun Fu*

Main category: cs.CV

TL;DR: 我们提出一个无需训练的框架ExpertGen，实现高精度和多专家协作控制的文本到面部生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要额外的模块训练以处理特定控制，如身份或年龄，使其不灵活且资源密集。

Method: 我们提出ExpertGen，一个无需训练的框架，利用预训练的专家模型引导生成，通过潜在一致性模型确保每个扩散步骤的预测真实且符合分布。

Result: 专家模型可以高精度地引导生成过程，多个专家可以协作实现对不同面部特征的同时控制。

Conclusion: 我们的方法允许预训练的专家模型直接集成，并能够作为可控面部生成的即插即用组件。

Abstract: Recent advances in diffusion models have significantly improved text-to-face
generation, but achieving fine-grained control over facial features remains a
challenge. Existing methods often require training additional modules to handle
specific controls such as identity, attributes, or age, making them inflexible
and resource-intensive. We propose ExpertGen, a training-free framework that
leverages pre-trained expert models such as face recognition, facial attribute
recognition, and age estimation networks to guide generation with fine control.
Our approach uses a latent consistency model to ensure realistic and
in-distribution predictions at each diffusion step, enabling accurate guidance
signals to effectively steer the diffusion process. We show qualitatively and
quantitatively that expert models can guide the generation process with high
precision, and multiple experts can collaborate to enable simultaneous control
over diverse facial aspects. By allowing direct integration of off-the-shelf
expert models, our method transforms any such model into a plug-and-play
component for controllable face generation.

</details>


### [79] [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/abs/2505.17280)
*Pushkar Shukla,Aditya Chinchure,Emily Diana,Alexander Tolbert,Kartik Hosanagar,Vineeth N Balasubramanian,Leonid Sigal,Matthew Turk*

Main category: cs.CV

TL;DR: BiasConnect通过量化偏见之间的互动关系帮助TTI模型设计更公平的生成；InterMit在较少步骤下实现更低的偏见且图像质量更优。


<details>
  <summary>Details</summary>
Motivation: 理解偏见之间的相互关系是设计更公平的生成模型的关键，而量化这些影响具有挑战性。

Method: BiasConnect使用反事实干预分析不同偏见轴上的偏见互动结构，并估计调节一个偏见轴对另一个产生的影响；InterMit则通过用户定义的目标分布及优先权重来引导偏见缓解。

Result: BiasConnect的估算与观察到的偏见缓解后的结果呈现出强相关性（+0.65），而InterMit在降低偏见方面表现优异（0.33对比0.52），需要的缓解步骤更少（平均步骤2.38对比3.15），且生成图像质量优于传统技术。

Conclusion: BiasConnect和InterMit为TTI模型提供了有效的偏见量化和调整解决方案，展示了较低的偏见水平和更高的图像质量。

Abstract: The biases exhibited by text-to-image (TTI) models are often treated as
independent, though in reality, they may be deeply interrelated. Addressing
bias along one dimension - such as ethnicity or age - can inadvertently affect
another, like gender, either mitigating or exacerbating existing disparities.
Understanding these interdependencies is crucial for designing fairer
generative models, yet measuring such effects quantitatively remains a
challenge. To address this, we introduce BiasConnect, a novel tool for
analyzing and quantifying bias interactions in TTI models. BiasConnect uses
counterfactual interventions along different bias axes to reveal the underlying
structure of these interactions and estimates the effect of mitigating one bias
axis on another. These estimates show strong correlation (+0.65) with observed
post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an
intersectional bias mitigation algorithm guided by user-defined target
distributions and priority weights. InterMit achieves lower bias (0.33 vs.
0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields
superior image quality compared to traditional techniques. Although our
implementation is training-free, InterMit is modular and can be integrated with
many existing debiasing approaches for TTI models, making it a flexible and
extensible solution.

</details>


### [80] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/abs/2505.17311)
*Harim Kim,Yuhan Wang,Minkyu Ahn,Heeyoul Choi,Yuyin Zhou,Charmgil Hong*

Main category: cs.CV

TL;DR: 我们提出了一种多模态扩散框架Diff3M，通过整合X光片和电子健康记录，提高了异常检测能力，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的UAD模型仅依赖于影像特征，无法有效区分正常的解剖变异和病理异常。本研究旨在解决这一问题。

Method: 我们提出了一种新的图像-EHR跨注意力模块，将结构化临床背景纳入图像生成过程，并采用静态掩码策略来增强从异常到正常样貌的图像重建。

Result: Diff3M在CheXpert和MIMIC-CXR/IV评估中超越了现有的UAD方法，实现了最先进的性能。

Conclusion: Diff3M框架通过整合胸部X光片和结构化电子健康记录，提高了异常检测性能，并在CheXpert和MIMIC-CXR/IV评估中实现了领先的表现。

Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M

</details>


### [81] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
*Jiachen Jiang,Jinxin Zhou,Bo Peng,Xia Ning,Zhihui Zhu*

Main category: cs.CV

TL;DR: 提出补丁对齐训练以增强视觉和语言模型的对齐，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 改善视觉嵌入与大型语言模型之间的对齐，以提升多模态语言模型的能力。

Method: 首先调查了投影器在压缩视觉嵌入和与词嵌入对齐中的作用，提出了补丁对齐训练以增强补丁级对齐。

Result: 补丁对齐训练提高了补丁级对齐，实现了更强的压缩能力以及更高质量的视觉内容生成，并在多项任务中提升了模型性能。

Conclusion: 提出一种名为补丁对齐训练的方法，可以有效增强视觉补丁与语义词之间的对齐，从而提升多模态语言模型的性能。

Abstract: Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.

</details>


### [82] [Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens](https://arxiv.org/abs/2505.17317)
*Alyson East,Elizabeth G. Campolongo,Luke Meyers,S M Rayeed,Samuel Stevens,Iuliia Zarubiieva,Isadora E. Fluck,Jennifer C. Girón,Maximiliane Jousse,Scott Lowe,Kayla I Perry,Isabelle Betancourt,Noah Charney,Evan Donoso,Nathan Fox,Kim J. Landsbergen,Ekaterina Nepovinnykh,Michelle Ramirez,Parkash Singh,Khum Thapa-Magar,Matthew Thompson,Evan Waite,Tanya Berger-Wolf,Hilmar Lapp,Paula Mabee,Graham Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本文提出了十个关键考虑因素，以优化生物标本图像用于计算机视觉应用，并强调详细记录方法选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 弥合现有成像实践与自动化分析潜力之间的差距，优化生物标本图像以适应计算机视觉应用。

Method: 本文通过跨学科的合作，制定了十个相互关联的考虑因素，为生物标本图像在计算机视觉管道中的成功整合提供了框架。

Result: 生成支持自动性状提取、物种识别及新颖生态和进化分析的图像。

Conclusion: 成功的生物标本图像创建需要详细记录方法选择，以支持大规模的自动性状提取、物种识别及新颖的生态和进化分析。

Abstract: Biological collections house millions of specimens documenting Earth's
biodiversity, with digital images increasingly available through open-access
platforms. Most imaging protocols were developed for human visual
interpretation without considering computational analysis requirements. This
paper aims to bridge the gap between current imaging practices and the
potential for automated analysis by presenting key considerations for creating
biological specimen images optimized for computer vision applications. We
provide conceptual computer vision topics for context, addressing fundamental
concerns including model generalization, data leakage, and comprehensive
metadata documentation, and outline practical guidance on specimen imagine, and
data storage. These recommendations were synthesized through interdisciplinary
collaboration between taxonomists, collection managers, ecologists, and
computer scientists. Through this synthesis, we have identified ten
interconnected considerations that form a framework for successfully
integrating biological specimen images into computer vision pipelines. The key
elements include: (1) comprehensive metadata documentation, (2) standardized
specimen positioning, (3) consistent size and color calibration, (4) protocols
for handling multiple specimens in one image, (5) uniform background selection,
(6) controlled lighting, (7) appropriate resolution and magnification, (8)
optimal file formats, (9) robust data archiving strategies, and (10) accessible
data sharing practices. By implementing these recommendations, collection
managers, taxonomists, and biodiversity informaticians can generate images that
support automated trait extraction, species identification, and novel
ecological and evolutionary analyses at unprecedented scales. Successful
implementation lies in thorough documentation of methodological choices.

</details>


### [83] [Game-invariant Features Through Contrastive and Domain-adversarial Learning](https://arxiv.org/abs/2505.17328)
*Dylan Kline*

Main category: cs.CV

TL;DR: 我们的方法结合对比学习和域对抗训练，成功学习了能够泛化到不同游戏的视觉特征，实现更好的跨游戏迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统的游戏图像编码器往往过拟合于特定游戏的视觉风格，限制了在新游戏中的下游任务表现。我们的目标是学习能够在多种游戏中通用的视觉特征，改善模型的泛化能力。

Method: 我们的方法结合了对比学习和域对抗训练，通过一个对抗性的域分类器同时鼓励相似内容聚类并抑制游戏特定的视觉线索。这种方法产生的嵌入在不同的游戏中表现出良好的泛化能力。

Result: 通过在Bingsu游戏图像数据集上的实验，我们的方法在短时间的训练后，特征不再按游戏聚类，说明实现了特征不变性，可以提升跨游戏任务表现。

Conclusion: 我们的模型能够成功实现跨游戏的特征不变性，显示出改进跨游戏迁移的潜力，例如在尽可能少的微调下进行故障检测。

Abstract: Foundational game-image encoders often overfit to game-specific visual
styles, undermining performance on downstream tasks when applied to new games.
We present a method that combines contrastive learning and domain-adversarial
training to learn game-invariant visual features. By simultaneously encouraging
similar content to cluster and discouraging game-specific cues via an
adversarial domain classifier, our approach produces embeddings that generalize
across diverse games. Experiments on the Bingsu game-image dataset (10,000
screenshots from 10 games) demonstrate that after only a few training epochs,
our model's features no longer cluster by game, indicating successful
invariance and potential for improved cross-game transfer (e.g., glitch
detection) with minimal fine-tuning. This capability paves the way for more
generalizable game vision models that require little to no retraining on new
games.

</details>


### [84] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
*Amit Agarwal,Srikant Panda,Kulbhushan Pachauri*

Main category: cs.CV

TL;DR: FS-DAG是一种用于视觉丰富文档理解的模型架构，在少样本环境中具有高度的性能和适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决在少样本环境中视觉丰富文档理解的问题，同时应对OCR错误、拼写错误和领域变化等实际挑战。

Method: FS-DAG使用模块化框架将领域特定和语言/视觉特定的骨干结合起来，以适应不同文档类型，并通过少量数据进行适应。

Result: FS-DAG通过广泛实验展示了其在信息提取任务上的快速收敛速度和性能提升，与最先进方法相比性能显著。

Conclusion: FS-DAG在信息提取任务上表现突出，并展示了与现有最先进方法相比的显著改进。

Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag

</details>


### [85] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/abs/2505.17333)
*Xin You,Minghui Zhang,Hanxiao Zhang,Jie Yang,Nassir Navab*

Main category: cs.CV

TL;DR: 我们引入了一种新的图像到视频合成框架，并采用时间微分扩散模型来改善时间运动的建模，以模拟4D视频。这种方法在不同数据集上显示了优越的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法模拟时间运动，除非同时存在高剂量成像扫描的起始和结束帧。在术前数据采集阶段，轻微的患者运动可能导致呼吸周期内的动态背景，从而影响时间建模，因此需要解决此限制。

Method: 我们引入了一个图像到视频合成框架，并设计了一种时间微分扩散模型用于生成时间微分场，采用提示注意层和场增强层来促进微分场与I2V框架的交互，提高合成视频的时间变化准确性。

Result: 在ACDC心脏和4D肺部数据集上，我们的方法能够沿着内在运动轨迹模拟4D视频，在感知相似性和时间一致性方面与其他竞争方法相当。

Conclusion: 我们的研究引入了一个创新的图像到视频合成框架，并利用时间微分扩散模型成功地改善了临时差异场之间的交互，从而在数据集上展示了优越的4D视频模拟效果，展示了其在感知相似性和时间一致性方面的竞争力。

Abstract: Temporal modeling on regular respiration-induced motions is crucial to
image-guided clinical applications. Existing methods cannot simulate temporal
motions unless high-dose imaging scans including starting and ending frames
exist simultaneously. However, in the preoperative data acquisition stage, the
slight movement of patients may result in dynamic backgrounds between the first
and last frames in a respiratory period. This additional deviation can hardly
be removed by image registration, thus affecting the temporal modeling. To
address that limitation, we pioneeringly simulate the regular motion process
via the image-to-video (I2V) synthesis framework, which animates with the first
frame to forecast future frames of a given length. Besides, to promote the
temporal consistency of animated videos, we devise the Temporal Differential
Diffusion Model to generate temporal differential fields, which measure the
relative differential representations between adjacent frames. The prompt
attention layer is devised for fine-grained differential fields, and the field
augmented layer is adopted to better interact these fields with the I2V
framework, promoting more accurate temporal variation of synthesized videos.
Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach
simulates 4D videos along the intrinsic motion trajectory, rivaling other
competitive methods on perceptual similarity and temporal consistency. Codes
will be available soon.

</details>


### [86] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 本文研究TTI模型的历史时期表现，发现模型常刻板化历史风格，引入错位及人口偏差，提出方法改善准确性。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在内容创作中的影响力日益增加，人们开始关注其社会和文化影响。然而，这些模型能否准确地再现历史背景这一问题仍未被充分探讨。

Method: 引入HistVis数据集，由三种最先进的扩散模型根据精心设计的提示生成3万张合成图像，描绘了跨越不同历史时期的普遍人类活动。评估分为三个方面：隐含的风格关联、历史一致性及人口代表性。

Result: 实验结果揭示了在生成的历史主题图像中存在系统性错误，模型常通过不明确的风格线索刻板化过去的时代，引入时代错位，并未能反映合理的人口特征。

Conclusion: 本文通过引入HistVis数据集，提出了一种系统性和可复现的方法来评估TTI系统如何刻画不同的历史时期。实验发现，TTI模型在生成历史主题的图像时存在系统性错误，常通过不明确的风格线索刻板化过去的时代，引入时代错位，并未能反映合理的人口特征。通过提供一个可扩展的方法和基准，这项工作为建立更具历史准确性和文化一致性的TTI模型提供了初步研究。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


### [87] [EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language](https://arxiv.org/abs/2505.17090)
*Phoebe Chua,Cathy Mengying Fang,Takehiko Ohkawa,Raja Kushalnagar,Suranga Nanayakkara,Pattie Maes*

Main category: cs.CV

TL;DR: EmoSign是第一个包含情感与情绪标注的ASL数据集，填补了手语情感识别领域的空白，并设立了新的研究基准。


<details>
  <summary>Details</summary>
Motivation: 尽管在口语中表达情感的韵律特征研究较多，手语中的情感指示仍然不太明朗，特别是在关键场合中导致沟通障碍。该研究旨在填补这一空白。

Method: 该研究引入了EmoSign数据集，其中包含了200个ASL视频的情感和情绪标注，并收集了情绪线索的开放式描述。标注过程由3位具有专业翻译经验的听障ASL手语人士完成，并提供了情感和情绪分类的基准模型。

Result: 研究提供的EmoSign数据集成为现有手语研究的重要补充，并为手语多模态情感识别模型能力的理解建立了新的基准。

Conclusion: 该研究通过提供情感和情绪标注的ASL视频数据集EmoSign，填补了手语情感识别领域的一个关键空白，并为多模态情感识别设立了新的基准。

Abstract: Unlike spoken languages where the use of prosodic features to convey emotion
is well studied, indicators of emotion in sign language remain poorly
understood, creating communication barriers in critical settings. Sign
languages present unique challenges as facial expressions and hand movements
simultaneously serve both grammatical and emotional functions. To address this
gap, we introduce EmoSign, the first sign video dataset containing sentiment
and emotion labels for 200 American Sign Language (ASL) videos. We also collect
open-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL
signers with professional interpretation experience. Alongside the annotations,
we include baseline models for sentiment and emotion classification. This
dataset not only addresses a critical gap in existing sign language research
but also establishes a new benchmark for understanding model capabilities in
multimodal emotion recognition for sign languages. The dataset is made
available at https://huggingface.co/datasets/catfang/emosign.

</details>


### [88] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
*Yanshu Li,JianJiang Yang,Bozheng Li,Ruixiang Tang*

Main category: cs.CV

TL;DR: 研究提出了CAMA，一种改进LVLMs注意力机制的方法，解决多模态ICL的不稳定性，并在多个基准测试中证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态ICL在LVLMs中不稳定性的问题，同时为LVLM内部机制提供一个理论分析。

Method: 提出了一种名为CAMA的新的注意力机制，直接校准LVLMs的注意力logit。

Result: CAMA在四个LVLMs的六个基准测试上进行了评价，证明其有效性和通用性。

Conclusion: 研究提出了一种新的注意力机制CAMA，可以改进LVLMs在多模态情况下的不稳定性能。

Abstract: Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.

</details>


### [89] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/abs/2505.17127)
*Michal Golovanevsky,William Rudman,Michael Lepori,Amir Bar,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 研究多模态大模型的推理依赖，通过Visual CounterFact数据集发现视觉输入在中后期占据主导，并提出PvP引导向量有效控制模型输出。


<details>
  <summary>Details</summary>
Motivation: 为了研究多模态大模型在推理时是依赖于记忆的世界知识还是输入图像中的视觉信息。

Method: 提出并使用Visual CounterFact数据集，分析模型预测的变化，并引入Pixels Versus Priors (PvP) 引导向量进行激活级干预以控制模型输出。

Result: PvP引导向量平均成功将92.5%的颜色预测和74.6%的大小预测从记忆的先验知识转向视觉输入的反事实。

Conclusion: 在多模态模型中，视觉输入在评估过程中最终覆盖了先验知识，同时PvP引导向量能够有效控制模型的输出，使之偏向于世界知识或视觉输入。

Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.

</details>


### [90] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
*Tanqiu Jiang,Jiacheng Liang,Rongyi Zhu,Jiawei Zhou,Fenglong Ma,Ting Wang*

Main category: cs.CV

TL;DR: The paper introduces DTR, a novel defense optimizing KV caches to protect vision-language models from jailbreak attacks, outperforming existing defenses in robustness and performance.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of large vision-language models to jailbreak attacks that exploit visual-textual interactions, the paper aims to enhance safety without relying on curated safety-specific data or costly image-to-text conversion.

Method: The paper proposes DTR, a defense mechanism that optimizes the model's key-value caches to mitigate multimodal jailbreak attacks by dynamically adjusting visual token weights.

Result: DTR outperforms existing defenses in terms of attack robustness and benign task performance, demonstrating its effectiveness through extensive evaluation across various models and attack benchmarks.

Conclusion: DTR significantly improves the robustness of vision-language models against jailbreak attacks while maintaining performance on benign tasks, marking a novel application of KV cache optimization for safety.

Abstract: Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)

</details>


### [91] [A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data](https://arxiv.org/abs/2505.17201)
*Chaim Chai Elchik,Fatemeh Karimi Nejadasl,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.CV

TL;DR: 该研究通过使用多视角框架改进了单视图MOT模型，提高了水下鱼类检测和跟踪的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于复杂的3D运动和数据噪声，传统的单视图MOT模型在跟踪水下环境中的鱼时往往不足，该研究旨在解决这些挑战。

Method: 研究通过改进最先进的单视图MOT模型FairMOT和YOLOv8来适应生态研究中水下鱼类的检测和跟踪。

Result: 提出的框架在检测鱼类实体时具有相对47%的精确度，并通过立体匹配技术生成新的3D输出，更全面地理解鱼类运动和互动。

Conclusion: 该研究提出了一种多视角框架，通过使用立体视频输入提高跟踪精度和鱼类行为模式识别，与单视图方法相比，在精确性和可靠性方面表现出显著改善。

Abstract: Multi-object tracking (MOT) in computer vision has made significant
advancements, yet tracking small fish in underwater environments presents
unique challenges due to complex 3D motions and data noise. Traditional
single-view MOT models often fall short in these settings. This thesis
addresses these challenges by adapting state-of-the-art single-view MOT models,
FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological
studies. The core contribution of this research is the development of a
multi-view framework that utilizes stereo video inputs to enhance tracking
accuracy and fish behavior pattern recognition. By integrating and evaluating
these models on underwater fish video datasets, the study aims to demonstrate
significant improvements in precision and reliability compared to single-view
approaches. The proposed framework detects fish entities with a relative
accuracy of 47% and employs stereo-matching techniques to produce a novel 3D
output, providing a more comprehensive understanding of fish movements and
interactions

</details>


### [92] [REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge](https://arxiv.org/abs/2505.17223)
*Siyang Song,Micol Spitale,Xiangyu Kong,Hengde Zhu,Cheng Luo,Cristina Palmero,German Barquero,Sergio Escalera,Michel Valstar,Mohamed Daoudi,Tobias Baur,Fabien Ringeval,Andrew Howes,Elisabeth Andre,Hatice Gunes*

Main category: cs.CV

TL;DR: REACT 2025挑战促进ML模型生成多样和同步的人类面部反应，新数据集MARS用于多模态基准测试。


<details>
  <summary>Details</summary>
Motivation: 在灵活多样的人类交互中，对每种行为的回应都有许多可能的面部反应，鼓励开发能多样化响应的机器学习模型。

Method: 提供一个新的自然大规模多模态数据集(MARS)，包括137次人类互动，涵盖2856个会话，通过线下和在线MAFRG的基线性能进行基准测试。

Result: 提供了基线代码，展示了离线和在线MAFRG子挑战的基线性能。

Conclusion: 本文提出了REACT 2025挑战，推动机器学习模型的发展，用于生成多样化、真实且同步的人类面部反应，以响应输入刺激。

Abstract: In dyadic interactions, a broad spectrum of human facial reactions might be
appropriate for responding to each human speaker behaviour. Following the
successful organisation of the REACT 2023 and REACT 2024 challenges, we are
proposing the REACT 2025 challenge encouraging the development and benchmarking
of Machine Learning (ML) models that can be used to generate multiple
appropriate, diverse, realistic and synchronised human-style facial reactions
expressed by human listeners in response to an input stimulus (i.e.,
audio-visual behaviours expressed by their corresponding speakers). As a key of
the challenge, we provide challenge participants with the first natural and
large-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human
dyadic interactions containing a total of 2856 interaction sessions covering
five different topics. In addition, this paper also presents the challenge
guidelines and the performance of our baselines on the two proposed
sub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge
baseline code is publicly available at
https://github.com/reactmultimodalchallenge/baseline_react2025

</details>


### [93] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
*Omar Moured,Yufan Chen,Ruiping Liu,Simon Reiß,Philip Torr,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 研究引入 CHAOS 基准来评估多模态大型语言模型在异常图表扰动下的稳健性，发现了模型在面对不同扰动时的表现差异，并为未来研究提供了指导。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，图表通常具有挑战性或噪声特征，尤其是“异常图表”对多模态大型语言模型提出了巨大挑战。本研究旨在通过 CHAOS 基准系统地评估这些模型的稳健性。

Method: 本研究介绍了一种称为 CHAOS 的稳健性基准，用于系统地评估多模态大型语言模型在图表扰动情况下的表现。CHAOS 包含 5 种文本和 10 种视觉扰动，每种扰动均有 3 个不同的严重等级。

Result: 通过大量实验和案例研究，分析得出关于模型在图表扰动下的关键稳健性见解。这些见解旨在指导未来图表理解领域的研究。

Conclusion: 通过综合分析，研究表明当面对图表扰动时，现代多模态大型语言模型（MLLMs）的稳健性差异显著。研究提供的基准有助于未来图表理解领域相关研究。

Abstract: Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, "outlier charts" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.

</details>


### [94] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/abs/2505.17245)
*Ryota Yagi*

Main category: cs.CV

TL;DR: The paper extends dataset pruning to object detection, proposes a novel scoring method, and shows improved performance over previous methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Dataset pruning has shown promise in efficient machine learning but its application to complex computer vision tasks, like object detection, is underexplored. This paper aims to extend pruning techniques from image classification to object detection.

Method: We proposed tailored solutions for dataset pruning in object detection, including a novel scoring method named Variance-based Prediction Score (VPS) that uses Intersection over Union (IoU) and confidence scores.

Result: Our approach consistently outperforms prior dataset pruning methods in terms of mean Average Precision (mAP) on PASCAL VOC and MS COCO datasets.

Conclusion: Our work bridges dataset pruning and object detection, paving the way for dataset pruning in complex vision tasks.

Abstract: Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.

</details>


### [95] [ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation](https://arxiv.org/abs/2505.17256)
*Liang Shi,Yun Fu*

Main category: cs.CV

TL;DR: ExpertGen框架利用预训练专家模型实现无训练的可控人脸生成，通过潜在一致性模型实现精确引导，并支持多专家协作控制多个面部特征。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在面部生成中缺乏细粒度的特征控制，现有方法需要对特定控制模块进行额外训练，缺乏灵活性并资源密集。

Method: 采用了一种潜在一致性模型，以确保在每个扩散步骤中实现真实且符合分布的预测，从而实现对生成过程的精确引导。通过利用预训练的专家模型如人脸识别、面部属性识别和年龄估计网络进行指导。

Result: 实验结果表明，专家模型能够以高精度引导生成过程，并且多个专家可以协作，实现对多个面部特征的同时控制。

Conclusion: 提出的ExpertGen框架能够显著提高面部生成的精确度，通过整合离线专家模型实现无训练的可控人脸生成。

Abstract: Recent advances in diffusion models have significantly improved text-to-face
generation, but achieving fine-grained control over facial features remains a
challenge. Existing methods often require training additional modules to handle
specific controls such as identity, attributes, or age, making them inflexible
and resource-intensive. We propose ExpertGen, a training-free framework that
leverages pre-trained expert models such as face recognition, facial attribute
recognition, and age estimation networks to guide generation with fine control.
Our approach uses a latent consistency model to ensure realistic and
in-distribution predictions at each diffusion step, enabling accurate guidance
signals to effectively steer the diffusion process. We show qualitatively and
quantitatively that expert models can guide the generation process with high
precision, and multiple experts can collaborate to enable simultaneous control
over diverse facial aspects. By allowing direct integration of off-the-shelf
expert models, our method transforms any such model into a plug-and-play
component for controllable face generation.

</details>


### [96] [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/abs/2505.17280)
*Pushkar Shukla,Aditya Chinchure,Emily Diana,Alexander Tolbert,Kartik Hosanagar,Vineeth N Balasubramanian,Leonid Sigal,Matthew Turk*

Main category: cs.CV

TL;DR: 研究提出BiasConnect工具分析TTI模型中的偏差交互，InterMit算法有效减偏并提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 理解偏差之间的相互作用是设计更公平的生成模型的关键，因此需要一种方法来定量分析这些影响。

Method: 引入BiasConnect工具进行反事实干预分析偏差交互，提出InterMit算法通过用户定义的目标分布和优先权重来指导偏差缓解。

Result: InterMit实现了更低的偏差（0.33与0.52相比较），所需缓解步数更少（平均步数2.38 vs. 3.15），并提供比传统技术更高质量的图像。

Conclusion: InterMit能有效降低偏差，同时提高图像质量。BiasConnect能够估算偏差交互的影响，并与解决偏差后的结果表现出强相关性。

Abstract: The biases exhibited by text-to-image (TTI) models are often treated as
independent, though in reality, they may be deeply interrelated. Addressing
bias along one dimension - such as ethnicity or age - can inadvertently affect
another, like gender, either mitigating or exacerbating existing disparities.
Understanding these interdependencies is crucial for designing fairer
generative models, yet measuring such effects quantitatively remains a
challenge. To address this, we introduce BiasConnect, a novel tool for
analyzing and quantifying bias interactions in TTI models. BiasConnect uses
counterfactual interventions along different bias axes to reveal the underlying
structure of these interactions and estimates the effect of mitigating one bias
axis on another. These estimates show strong correlation (+0.65) with observed
post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an
intersectional bias mitigation algorithm guided by user-defined target
distributions and priority weights. InterMit achieves lower bias (0.33 vs.
0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields
superior image quality compared to traditional techniques. Although our
implementation is training-free, InterMit is modular and can be integrated with
many existing debiasing approaches for TTI models, making it a flexible and
extensible solution.

</details>


### [97] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/abs/2505.17311)
*Harim Kim,Yuhan Wang,Minkyu Ahn,Heeyoul Choi,Yuyin Zhou,Charmgil Hong*

Main category: cs.CV

TL;DR: 此论文提出了一种结合X光影像和电子健康记录的多模态扩散框架，提高了无监督异常检测的性能，尤其在区分正常与异常特征时效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的无监督异常检测模型仅依赖于成像特征，限制了其区分正常解剖变异和病理异常的能力。为此，作者提出了一个多模态的框架以改善异常检测性能。

Method: 提出了一种新的图像-EHR交叉注意模块，将结构化临床上下文结合到图像生成过程中。此外，开发了一种静态遮蔽策略，以增强从异常中重建正常图像的能力。

Result: 在CheXpert和MIMIC-CXR/IV数据集上的广泛评估显示，Diff3M超越了现有的无监督异常检测方法，实现了最先进的性能。

Conclusion: 此研究提出了Diff3M，一个多模态扩散框架，通过结合胸部X射线和电子健康记录进行增强的异常检测。通过整合临床背景信息与影像生成，提升模型在识别正常与异常特征上的能力。评估结果显示，该方法在CheXpert和MIMIC-CXR/IV数据集上实现了最先进的性能，优于现有的无监督异常检测方法。

Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M

</details>


### [98] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
*Jiachen Jiang,Jinxin Zhou,Bo Peng,Xia Ning,Zhihui Zhu*

Main category: cs.CV

TL;DR: 研究提出了一种改进视觉与语言模型对齐的方法，显著提升了多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了改善多模态大语言模型的能力，特别是利用预训练视觉编码器和大语言模型的方法，需要更好地对齐视觉嵌入与大语言模型。此论文旨在探讨如何改善这种对齐。

Method: 研究中提出了“patch-aligned training”方法，通过这一方法增强视觉碎片与语义词的对齐，从而提升模型性能。

Result: 该方法使得多模态大语言模型在指称表达定位任务上提升了16%，在问答任务上提升4%，在现代指令跟随基准上提升3%。

Conclusion: 通过调整视觉嵌入与词嵌入的对齐方式，尤其是采用“patch-aligned training”方法，可以显著提高多模态大语言模型的性能。

Abstract: Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.

</details>


### [99] [Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens](https://arxiv.org/abs/2505.17317)
*Alyson East,Elizabeth G. Campolongo,Luke Meyers,S M Rayeed,Samuel Stevens,Iuliia Zarubiieva,Isadora E. Fluck,Jennifer C. Girón,Maximiliane Jousse,Scott Lowe,Kayla I Perry,Isabelle Betancourt,Noah Charney,Evan Donoso,Nathan Fox,Kim J. Landsbergen,Ekaterina Nepovinnykh,Michelle Ramirez,Parkash Singh,Khum Thapa-Magar,Matthew Thompson,Evan Waite,Tanya Berger-Wolf,Hilmar Lapp,Paula Mabee,Graham Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 此论文提出了十项建议，以优化生物标本图像的计算机视觉应用，促进自动化特征提取和物种识别等分析。


<details>
  <summary>Details</summary>
Motivation: 为缩小当前成像实践与自动化分析潜力之间的差距，提出生物标本图像的关键考虑因素，以优化计算机视觉应用。因为大多数成像协议是为人类视觉解读而开发的，没有考虑到计算分析的需求。

Method: 通过跨学科的合作（包括分类学家、收藏管理者、生态学家和计算机科学家），进行了建议的综合，提出了创建适合计算机视觉应用的生物标本图像的关键考虑因素。

Result: 制定了包含十个相互关联的考虑因素的框架，以成功将生物标本图像整合到计算机视觉管道中。

Conclusion: 通过实施这些建议，收藏管理者、分类学家和生物多样性信息学家可以生成支持自动特征提取、物种识别和前所未有规模的新生态和进化分析的图像。成功实施的关键在于对方法选择进行彻底记录。

Abstract: Biological collections house millions of specimens documenting Earth's
biodiversity, with digital images increasingly available through open-access
platforms. Most imaging protocols were developed for human visual
interpretation without considering computational analysis requirements. This
paper aims to bridge the gap between current imaging practices and the
potential for automated analysis by presenting key considerations for creating
biological specimen images optimized for computer vision applications. We
provide conceptual computer vision topics for context, addressing fundamental
concerns including model generalization, data leakage, and comprehensive
metadata documentation, and outline practical guidance on specimen imagine, and
data storage. These recommendations were synthesized through interdisciplinary
collaboration between taxonomists, collection managers, ecologists, and
computer scientists. Through this synthesis, we have identified ten
interconnected considerations that form a framework for successfully
integrating biological specimen images into computer vision pipelines. The key
elements include: (1) comprehensive metadata documentation, (2) standardized
specimen positioning, (3) consistent size and color calibration, (4) protocols
for handling multiple specimens in one image, (5) uniform background selection,
(6) controlled lighting, (7) appropriate resolution and magnification, (8)
optimal file formats, (9) robust data archiving strategies, and (10) accessible
data sharing practices. By implementing these recommendations, collection
managers, taxonomists, and biodiversity informaticians can generate images that
support automated trait extraction, species identification, and novel
ecological and evolutionary analyses at unprecedented scales. Successful
implementation lies in thorough documentation of methodological choices.

</details>


### [100] [Game-invariant Features Through Contrastive and Domain-adversarial Learning](https://arxiv.org/abs/2505.17328)
*Dylan Kline*

Main category: cs.CV

TL;DR: 提出了一种结合对比学习和域对抗训练的方法，成功实现了游戏图像的视觉特征不变性，有望开发无需重新训练即可泛化于新游戏的视觉模型。


<details>
  <summary>Details</summary>
Motivation: 基础的游戏图像编码器常常因过度拟合于游戏特定的视觉风格而在应用于新游戏时表现不佳。

Method: 我们提出了一种结合对比学习与域对抗训练的方法以学习游戏不变的视觉特征。通过同时鼓励相似内容聚类并通过对抗域分类器来抑制游戏特定线索，我们的方法产生了可以在不同游戏间泛化的嵌入。

Result: 实验使用了Bingsu游戏图像数据集（来自10个游戏的10,000张截图），展示仅需几个训练周期后，我们模型的特征不再按游戏聚类，表明成功地实现了不变量性并有潜力通过最小的微调来改善跨游戏迁移（例如，故障检测）。

Conclusion: 这种能力为开发更具普遍性的游戏视觉模型铺平了道路，这些模型在新游戏上几乎不需要重新训练。

Abstract: Foundational game-image encoders often overfit to game-specific visual
styles, undermining performance on downstream tasks when applied to new games.
We present a method that combines contrastive learning and domain-adversarial
training to learn game-invariant visual features. By simultaneously encouraging
similar content to cluster and discouraging game-specific cues via an
adversarial domain classifier, our approach produces embeddings that generalize
across diverse games. Experiments on the Bingsu game-image dataset (10,000
screenshots from 10 games) demonstrate that after only a few training epochs,
our model's features no longer cluster by game, indicating successful
invariance and potential for improved cross-game transfer (e.g., glitch
detection) with minimal fine-tuning. This capability paves the way for more
generalizable game vision models that require little to no retraining on new
games.

</details>


### [101] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
*Amit Agarwal,Srikant Panda,Kulbhushan Pachauri*

Main category: cs.CV

TL;DR: FS-DAG is a scalable and efficient model for document understanding in few-shot settings, improving upon state-of-the-art performance with less than 90M parameters. It handles OCR errors, misspellings, and domain shifts effectively in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Current models for visually rich document understanding require large datasets and computational resources, which can be challenging in real-world settings. The goal is to develop a model that is both efficient and capable of handling practical challenges like OCR errors, misspellings, and domain shifts in few-shot settings.

Method: The proposed model, FS-DAG, employs a scalable architecture that uses domain-specific and language/vision specific backbones within a modular framework. This allows the model to adapt to diverse document types with minimal data.

Result: FS-DAG performs robustly in Information Extraction tasks under practical conditions. The model is lightweight with less than 90 million parameters and shows significant improvements in convergence speed and performance under few-shot learning conditions.

Conclusion: FS-DAG shows significant improvements in convergence speed and performance in information extraction tasks compared to current state-of-the-art methods, demonstrating that smaller and more efficient models can be developed without compromising performance.

Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag

</details>


### [102] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/abs/2505.17333)
*Xin You,Minghui Zhang,Hanxiao Zhang,Jie Yang,Nassir Navab*

Main category: cs.CV

TL;DR: 这项研究通过图像到视频的框架来模拟呼吸引起的规律性运动，解决了现有方法在时间建模上的缺陷，并在实验中取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法在没有高剂量扫描的情况下模拟时间运动，而患者轻微的移动可能导致动态背景，从而影响时间建模。

Method: 图像到视频（I2V）合成框架；时间微分扩散模型；用于生成时间微分场和相邻帧之间的相对微分表示。

Result: 通过在ACDC心脏和4D肺部数据集上的大量实验结果，表明所提出的方法能够按照内在运动轨迹模拟4D视频，在感知相似性和时间一致性方面与其他竞争方法相比表现优异。

Conclusion: 研究表明，通过图像到视频的合成框架来模拟呼吸过程中的规律性运动是可行的。其方法在保持图像的时间一致性方面优于现有技术。

Abstract: Temporal modeling on regular respiration-induced motions is crucial to
image-guided clinical applications. Existing methods cannot simulate temporal
motions unless high-dose imaging scans including starting and ending frames
exist simultaneously. However, in the preoperative data acquisition stage, the
slight movement of patients may result in dynamic backgrounds between the first
and last frames in a respiratory period. This additional deviation can hardly
be removed by image registration, thus affecting the temporal modeling. To
address that limitation, we pioneeringly simulate the regular motion process
via the image-to-video (I2V) synthesis framework, which animates with the first
frame to forecast future frames of a given length. Besides, to promote the
temporal consistency of animated videos, we devise the Temporal Differential
Diffusion Model to generate temporal differential fields, which measure the
relative differential representations between adjacent frames. The prompt
attention layer is devised for fine-grained differential fields, and the field
augmented layer is adopted to better interact these fields with the I2V
framework, promoting more accurate temporal variation of synthesized videos.
Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach
simulates 4D videos along the intrinsic motion trajectory, rivaling other
competitive methods on perceptual similarity and temporal consistency. Codes
will be available soon.

</details>


### [103] [Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering](https://arxiv.org/abs/2505.17338)
*Zhongpai Gao,Meng Zheng,Benjamin Planche,Anwesa Choudhuri,Terrence Chen,Ziyan Wu*

Main category: cs.CV

TL;DR: Render-FM is a new model for real-time CT scan rendering, offering high visual fidelity and rapid processing to aid clinical workflows.


<details>
  <summary>Details</summary>
Motivation: Current high-fidelity neural rendering techniques require time-consuming optimization per scene, hindering their clinical applicability. There's a need for a method that is both efficient and generalizable.

Method: Render-FM uses an encoder-decoder architecture to regress 6D Gaussian Splatting (6DGS) parameters directly from CT volumes, leveraging large-scale pre-training on diverse medical data.

Result: Render-FM provides high-quality, real-time 3D visualizations with significantly reduced preparation time, from nearly an hour to seconds, facilitating real-time applications in medical imaging.

Conclusion: Render-FM achieves comparable or superior visual fidelity to specialized methods with drastically reduced preparation time, facilitating real-time integration into clinical workflows.

Abstract: Volumetric rendering of Computed Tomography (CT) scans is crucial for
visualizing complex 3D anatomical structures in medical imaging. Current
high-fidelity approaches, especially neural rendering techniques, require
time-consuming per-scene optimization, limiting clinical applicability due to
computational demands and poor generalizability. We propose Render-FM, a novel
foundation model for direct, real-time volumetric rendering of CT scans.
Render-FM employs an encoder-decoder architecture that directly regresses 6D
Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan
optimization through large-scale pre-training on diverse medical data. By
integrating robust feature extraction with the expressive power of 6DGS, our
approach efficiently generates high-quality, real-time interactive 3D
visualizations across diverse clinical CT data. Experiments demonstrate that
Render-FM achieves visual fidelity comparable or superior to specialized
per-scan methods while drastically reducing preparation time from nearly an
hour to seconds for a single inference step. This advancement enables seamless
integration into real-time surgical planning and diagnostic workflows. The
project page is: https://gaozhongpai.github.io/renderfm/.

</details>


### [104] [Ocular Authentication: Fusion of Gaze and Periocular Modalities](https://arxiv.org/abs/2505.17343)
*Dillon Lohr,Michael J. Proulx,Mehedi Hasan Raju,Oleg V. Komogortsev*

Main category: cs.CV

TL;DR: 研究眼动和眼周图像的融合，提出多模态身份验证系统，显著提高验证性能。


<details>
  <summary>Details</summary>
Motivation: 单独的眼动和眼周图像在用户身份验证方面表现良好，但其组合在一个统一的注视估计算法中尚未进行规模化探索。

Method: 采用先进机器学习架构来捕捉身份验证表示，并融合眼动和眼周图像两种模态。

Result: 多模态方法在所有场景中都明显优于单模态系统，超越FIDO基准，证明了其在规模上的有效性。

Conclusion: 多模态身份验证系统提供了比单模态系统更优越的性能，超越了FIDO基准。

Abstract: This paper investigates the feasibility of fusing two eye-centric
authentication modalities-eye movements and periocular images-within a
calibration-free authentication system. While each modality has independently
shown promise for user authentication, their combination within a unified
gaze-estimation pipeline has not been thoroughly explored at scale. In this
report, we propose a multimodal authentication system and evaluate it using a
large-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)
signal quality equivalent to a consumer-facing virtual reality (VR) device. Our
results show that the multimodal approach consistently outperforms both
unimodal systems across all scenarios, surpassing the FIDO benchmark. The
integration of a state-of-the-art machine learning architecture contributed
significantly to the overall authentication performance at scale, driven by the
model's ability to capture authentication representations and the complementary
discriminative characteristics of the fused modalities.

</details>


### [105] [Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey](https://arxiv.org/abs/2505.17352)
*Preeti Lamba,Kiran Ravish,Ankita Kushwaha,Pawan Kumar*

Main category: cs.CV

TL;DR: 研究提出通过强化学习和奖励建模来对齐扩散模型，并识别未来的五个研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型与人类偏好和安全约束对齐的关键挑战。

Method: 调查最近的进展，包括从人类和AI反馈进行强化学习、直接偏好优化和可微分奖励方法。

Result: 提出了五个未来研究方向，并详细阐述了每个方向的研究计划。

Conclusion: 该论文提出了通过强化学习（RL）和奖励建模来实现扩散模型与人类偏好和安全标准的对齐。

Abstract: Diffusion models have emerged as leading generative models for images and
other modalities, but aligning their outputs with human preferences and safety
constraints remains a critical challenge. This thesis proposal investigates
methods to align diffusion models using reinforcement learning (RL) and reward
modeling. We survey recent advances in fine-tuning text-to-image diffusion
models with human feedback, including reinforcement learning from human and AI
feedback, direct preference optimization, and differentiable reward approaches.
We classify these methods based on the type of feedback (human, automated,
binary or ranked preferences), the fine-tuning technique (policy gradient,
reward-weighted likelihood, direct backpropagation, etc.), and their efficiency
and safety outcomes. We compare key algorithms and frameworks, highlighting how
they improve alignment with user intent or safety standards, and discuss
inter-relationships such as how newer methods build on or diverge from earlier
ones. Based on the survey, we identify five promising research directions for
the next two years: (1) multi-objective alignment with combined rewards, (2)
efficient human feedback usage and active learning, (3) robust safety alignment
against adversarial inputs, (4) continual and online alignment of diffusion
models, and (5) interpretable and trustworthy reward modeling for generative
images. Each direction is elaborated with its problem statement, challenges,
related work, and a proposed research plan. The proposal is organized as a
comprehensive document with literature review, comparative tables of methods,
and detailed research plans, aiming to contribute new insights and techniques
for safer and value-aligned diffusion-based generative AI.

</details>


### [106] [Dual Ascent Diffusion for Inverse Problems](https://arxiv.org/abs/2505.17353)
*Minseo Kim,Axel Levy,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 新方法利用双上升优化框架与扩散模型解决MAP问题，提高图像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MAP或后验采样方法由于不同的计算近似而导致样本不准确或次优。论文旨在通过新方法提高图像恢复问题的质量和鲁棒性。

Method: 使用双上升优化框架解决最大后验（MAP）问题，并结合扩散模型先验以提升解决方案的准确性和效率。

Result: 该框架在图像恢复问题中的各项质量度量上优于现有技术，具备更强的噪声鲁棒性和更快的计算速度，并能更忠实地估计观察到的解决方案。

Conclusion: 论文提出了一种新的双上升优化框架，以解决使用扩散模型先验的MAP问题，该方法较现有技术在图像质量、噪声鲁棒性、速度和解决方案忠实性方面表现更优。

Abstract: Ill-posed inverse problems are fundamental in many domains, ranging from
astrophysics to medical imaging. Emerging diffusion models provide a powerful
prior for solving these problems. Existing maximum-a-posteriori (MAP) or
posterior sampling approaches, however, rely on different computational
approximations, leading to inaccurate or suboptimal samples. To address this
issue, we introduce a new approach to solving MAP problems with diffusion model
priors using a dual ascent optimization framework. Our framework achieves
better image quality as measured by various metrics for image restoration
problems, it is more robust to high levels of measurement noise, it is faster,
and it estimates solutions that represent the observations more faithfully than
the state of the art.

</details>


### [107] [Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues](https://arxiv.org/abs/2505.17358)
*Chinmay Talegaonkar,Nikhil Gandudi Suresh,Zachary Novack,Yash Belhe,Priyanka Nagasamudra,Nicholas Antipa*

Main category: cs.CV

TL;DR: 通过在推理时使用散焦模糊线索，提升了Marigold在零样本单目深度估计中的性能，特别是在分布外数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的单目度量深度估计方法在零样本泛化上已经有显著进展，但在分布外数据集上性能下降严重。本研究针对这一局限提出了改进方法。

Method: 在推理时注入散焦模糊线索，并通过捕捉不同光圈下的图像视角，优化Marigold的尺度参数和噪声潜变量，从而恢复度量深度。

Result: 与现有的零样本单目深度估计方法相比，在自收集的真实数据集上展示了定量和定性的改进。

Conclusion: 提出了一种基于Marigold的零样本、尺度不变的单目深度估计方法，通过在推理时注入散焦模糊线索来实现该方法。

Abstract: Recent monocular metric depth estimation (MMDE) methods have made notable
progress towards zero-shot generalization. However, they still exhibit a
significant performance drop on out-of-distribution datasets. We address this
limitation by injecting defocus blur cues at inference time into Marigold, a
\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular
depth estimation (MDE). Our method effectively turns Marigold into a metric
depth predictor in a training-free manner. To incorporate defocus cues, we
capture two images with a small and a large aperture from the same viewpoint.
To recover metric depth, we then optimize the metric depth scaling parameters
and the noise latents of Marigold at inference time using gradients from a loss
function based on the defocus-blur image formation model. We compare our method
against existing state-of-the-art zero-shot MMDE methods on a self-collected
real dataset, showing quantitative and qualitative improvements.

</details>


### [108] [Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches](https://arxiv.org/abs/2505.17363)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.CV

TL;DR: 该研究评估了四种深度学习模型在IoT botnet检测中的性能，其中VAE-MLP和ViT-MLP在多类分类任务中表现优异，而所有模型在二元分类任务中均达到高精准度。


<details>
  <summary>Details</summary>
Motivation: 由于IoT botnet攻击的指数增长，研究人员探索了多种先进技术以增强IoT安全，包括降维和攻击检测。

Method: 本研究评估了四种先进的深度学习架构：VAE编码器与MLP、VAE编码器与GCN、VAE编码器与GAT、ViT编码器与MLP，使用IoT基准数据集N-BaIoT进行评估。

Result: 在二元分类任务中，所有模型在各项指标上均超过99.93%。而在多类分类任务中，GNN模型的性能显著低于VAE-MLP和ViT-MLP，精确度分别为86.42%、89.46%、99.72%和98.38%。

Conclusion: 研究表明，在IoT botnet检测中，深度学习架构具有显著效果，特别是在二元分类任务中，所有模型表现出色。然而，在多类分类任务中，GNN模型的性能不如VAE-MLP和ViT-MLP。

Abstract: Due to the exponential rise in IoT-based botnet attacks, researchers have
explored various advanced techniques for both dimensionality reduction and
attack detection to enhance IoT security. Among these, Variational Autoencoders
(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including
Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have
garnered significant research attention in the domain of attack detection. This
study evaluates the effectiveness of four state-of-the-art deep learning
architectures for IoT botnet detection: a VAE encoder with a Multi-Layer
Perceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT
encoder with an MLP. The evaluation is conducted on a widely studied IoT
benchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.
For the binary classification task, all models achieved over 99.93% in
accuracy, recall, precision, and F1-score, with no notable differences in
performance. In contrast, for the multiclass classification task, GNN-based
models showed significantly lower performance compared to VAE-MLP and ViT-MLP,
with accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,
VAE-MLP, and ViT-MLP, respectively.

</details>


### [109] [Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture](https://arxiv.org/abs/2505.17364)
*Apar Pokhrel,Gia Dao*

Main category: cs.CV

TL;DR: 此研究比较了几种集成于YOLOv8的骨干网络架构，以优化停车位检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 停车位占用检测是智能停车管理系统发展的关键组成部分。

Method: 对集成了YOLOv8的定制骨干架构进行全面的比较分析。具体来说，我们在PKLot数据集上评估了ResNet-18、VGG16、EfficientNetV2、Ghost等不同的骨干网络，比较其检测准确性和计算效率。

Result: 实验结果凸显了每种架构的优点和权衡，为选择合适的停车占用模型提供了洞察。

Conclusion: 传统的目标检测方法如YOLOv8能快速且准确地检测停车场中的车辆，但在车辆部分可见、小型车辆以及光照条件差的情况下表现不佳。通过综合比较分析，我们找到了各个架构在检测准确性和计算效率之间的权衡。

Abstract: Parking space occupancy detection is a critical component in the development
of intelligent parking management systems. Traditional object detection
approaches, such as YOLOv8, provide fast and accurate vehicle detection across
parking lots but can struggle with borderline cases, such as partially visible
vehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In
this work, we perform a comprehensive comparative analysis of customized
backbone architectures integrated with YOLOv8. Specifically, we evaluate
various backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot
dataset in terms of detection accuracy and computational efficiency.
Experimental results highlight each architecture's strengths and trade-offs,
providing insight into selecting suitable models for parking occupancy.

</details>


### [110] [EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion](https://arxiv.org/abs/2505.17367)
*Zichuan Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为EVM-Fusion的新架构，用于多器官医学图像分类。通过多路径设计和新颖的神经算法融合机制，此架构在多个医学图像数据集上表现出优异的性能和良好的可解释性，测试准确率达到99.75%。该技术展示了其在可信赖的医学诊断应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类对于临床决策至关重要，但对准确性、可解释性和普遍性的要求仍然是挑战。为解决这些问题，本文引入了一种新的架构和机制，以增强医学图像分类的性能和信赖度。

Method: 本文提出了一种名为EVM-Fusion的解释性Vision Mamba架构，采用了一种新颖的神经算法融合（NAF）机制用于多器官医学图像分类。EVM-Fusion通过多路径设计，利用DenseNet和U-Net为基础的路径，并结合Vision Mamba（Vim）模块，与传统的特征路径并行运行。这些多样化的特征通过一个两阶段的融合过程动态整合：跨模态注意力后，进行迭代NAF块，从而学习一种自适应的融合算法。同时，通过路径特定的空间注意力、Vim Δ 值图、传统特征的SE注意力以及跨模态注意力权重，内置了内在的可解释性。

Result: 通过在一个多样的9类多器官医学图像数据集上进行实验，EVM-Fusion表现出强大的分类性能，达到了99.75%的测试准确率，并提供了对其决策过程的多方面洞察，突显了其在医学诊断中作为值得信赖的人工智能的潜力。

Conclusion: 该研究强调了EVM-Fusion在多器官医学图像分类中的有效性和可解释性，证明了其在医学诊断中作为一个可信赖的人工智能解决方案的潜力。并且，通过其强大的分类性能和对决策过程的深入理解，提升了医学图像分类的可信度和实用性。

Abstract: Medical image classification is critical for clinical decision-making, yet
demands for accuracy, interpretability, and generalizability remain
challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba
architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for
multi-organ medical image classification. EVM-Fusion leverages a multipath
design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)
modules, operate in parallel with a traditional feature pathway. These diverse
features are dynamically integrated via a two-stage fusion process: cross-modal
attention followed by the iterative NAF block, which learns an adaptive fusion
algorithm. Intrinsic explainability is embedded through path-specific spatial
attention, Vim {\Delta}-value maps, traditional feature SE-attention, and
cross-modal attention weights. Experiments on a diverse 9-class multi-organ
medical image dataset demonstrate EVM-Fusion's strong classification
performance, achieving 99.75% test accuracy and provide multi-faceted insights
into its decision-making process, highlighting its potential for trustworthy AI
in medical diagnostics.

</details>


### [111] [Dual-sensing driving detection model](https://arxiv.org/abs/2505.17392)
*Leon C. C. K,Zeng Hui*

Main category: cs.CV

TL;DR: 提出了一种结合计算机视觉和生理信号分析的新型双模态驾驶员疲劳检测方法，突破单模态限制，系统表现优于传统方法并已验证其实际适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态方法存在局限性，本研究旨在利用两种感知模式的互补优势突破这些局限。

Method: 本文采用计算机视觉和生理信号分析相结合的新型双模态感知方法。创新架构结合实时面部特征分析与生理信号处理，并采用先进的融合策略以实现稳健的疲劳检测。

Result: 实验表明该方法在受控环境和真实场景中表现优于传统方法，能高效运行于现有硬件，准确性和可靠性高。系统经过大量驾驶情境测试验证，具有减少疲劳相关事故的巨大潜力。

Conclusion: 该研究提供了一种更可靠、成本较低、更加人性化的驾驶员疲劳检测解决方案。

Abstract: In this paper, a novel dual-sensing driver fatigue detection method combining
computer vision and physiological signal analysis is proposed. The system
exploits the complementary advantages of the two sensing modalities and breaks
through the limitations of existing single-modality methods. We introduce an
innovative architecture that combines real-time facial feature analysis with
physiological signal processing, combined with advanced fusion strategies, for
robust fatigue detection. The system is designed to run efficiently on existing
hardware while maintaining high accuracy and reliability. Through comprehensive
experiments, we demonstrate that our method outperforms traditional methods in
both controlled environments and real-world conditions, while maintaining high
accuracy. The practical applicability of the system has been verified through
extensive tests in various driving scenarios and shows great potential in
reducing fatigue-related accidents. This study contributes to the field by
providing a more reliable, cost-effective, and humane solution for driver
fatigue detection.

</details>


### [112] [Wildfire Detection Using Vision Transformer with the Wildfire Dataset](https://arxiv.org/abs/2505.17395)
*Gowtham Raj Vuppari,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Main category: cs.CV

TL;DR: This paper explores using Vision Transformers (ViTs) to improve wildfire detection in the US despite challenges in data acquisition and computational costs.


<details>
  <summary>Details</summary>
Motivation: The increasing frequency and intensity of wildfires in the US, especially in areas like California, highlight the critical need for advanced detection techniques to mitigate damage and save lives.

Method: The paper utilizes Vision Transformers (ViTs) for early wildfire detection, employing a dataset of high-resolution images categorized into 'fire' and 'nofire'. Images are resized to 224 x 224 pixels, converted into tensor format for processing, and normalized using ImageNet statistics to train the model.

Result: The study presents a methodology for using Vision Transformers (ViTs) to enhance early wildfire detection through processing complex image data. However, challenges such as limited real-time data availability and computational expenses remain significant.

Conclusion: Deep learning models, particularly Vision Transformers (ViTs), have the potential to significantly enhance the early detection of wildfires by processing complex image data with high accuracy. However, challenges such as obtaining high-quality, real-time data and computational costs in training these models need to be addressed for effectiveness.

Abstract: The critical need for sophisticated detection techniques has been highlighted
by the rising frequency and intensity of wildfires in the US, especially in
California. In 2023, wildfires caused 130 deaths nationwide, the highest since
1990. In January 2025, Los Angeles wildfires which included the Palisades and
Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused
loss of human lives. The devastation underscores the urgent need for effective
detection and prevention strategies. Deep learning models, such as Vision
Transformers (ViTs), can enhance early detection by processing complex image
data with high accuracy. However, wildfire detection faces challenges,
including the availability of high-quality, real-time data. Wildfires often
occur in remote areas with limited sensor coverage, and environmental factors
like smoke and cloud cover can hinder detection. Additionally, training deep
learning models is computationally expensive, and issues like false
positives/negatives and scaling remain concerns. Integrating detection systems
with real-time alert mechanisms also poses difficulties. In this work, we used
the wildfire dataset consisting of 10.74 GB high-resolution images categorized
into 'fire' and 'nofire' classes is used for training the ViT model. To prepare
the data, images are resized to 224 x 224 pixels, converted into tensor format,
and normalized using ImageNet statistics.

</details>


### [113] [Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention](https://arxiv.org/abs/2505.17412)
*Shuang Wu,Youtian Lin,Feihu Zhang,Yifei Zeng,Yikang Yang,Yajie Bao,Jiachen Qian,Siyu Zhu,Philip Torr,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 本文提出Direct3D S2框架，通过空间稀疏注意机制，显著提升了稀疏体积数据的处理效率，实现了3D生成的质量和效率的提升，并可以用较少的资源进行大规模训练。


<details>
  <summary>Details</summary>
Motivation: 生成高分辨率3D形状的计算和内存挑战。现有方法需要昂贵的计算资源，促进更高效的3D生成方法的需求。

Method: 我们引入了空间稀疏注意机制，它极大地提高了稀疏体积数据上的扩散变压器计算效率。此外，我们的框架包含一个变分自编码器，在输入、潜在和输出阶段保持一致的稀疏体积格式。

Result: 通过使用空间稀疏注意机制，我们的模型在正向传播和反向传播上实现了3.9倍和9.6倍的速度提升。同时能够使用8个GPU进行1024分辨率的训练，相较传统需要至少32个GPU进行256分辨率的训练。

Conclusion: 本文提出一种基于稀疏体积的可扩展3D生成框架Direct3D S2，可以以显著降低的训练成本实现卓越的输出质量。通过实验，Direct3D S2不仅在生成质量和效率方面超越了最先进方法，而且实现了使用只有8个GPU进行1024分辨率训练的能力。

Abstract: Generating high resolution 3D shapes using volumetric representations such as
Signed Distance Functions presents substantial computational and memory
challenges. We introduce Direct3D S2, a scalable 3D generation framework based
on sparse volumes that achieves superior output quality with dramatically
reduced training costs. Our key innovation is the Spatial Sparse Attention
mechanism, which greatly enhances the efficiency of Diffusion Transformer
computations on sparse volumetric data. SSA allows the model to effectively
process large token sets within sparse volumes, significantly reducing
computational overhead and achieving a 3.9x speedup in the forward pass and a
9.6x speedup in the backward pass. Our framework also includes a variational
autoencoder that maintains a consistent sparse volumetric format across input,
latent, and output stages. Compared to previous methods with heterogeneous
representations in 3D VAE, this unified design significantly improves training
efficiency and stability. Our model is trained on public available datasets,
and experiments demonstrate that Direct3D S2 not only surpasses
state-of-the-art methods in generation quality and efficiency, but also enables
training at 1024 resolution using only 8 GPUs, a task typically requiring at
least 32 GPUs for volumetric representations at 256 resolution, thus making
gigascale 3D generation both practical and accessible. Project page:
https://nju3dv.github.io/projects/Direct3D-S2/.

</details>


### [114] [VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR](https://arxiv.org/abs/2505.17423)
*Shenghui Chen,Po-han Li,Sandeep Chichali,Ufuk Topcu*

Main category: cs.CV

TL;DR: VIBE方法无需人工标注，能有效评估视频生成摘要的实用性和对齐度，显著提高任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型（VLMs）生成的输出往往冗长且重复，影响任务表现。需要一种能够生成简洁摘要的方法，以减轻人类监督任务中的认知负担和节省时间。

Method: 提出了一种视频到文本信息瓶颈评估方法（VIBE），通过两个指标（与视觉内容的对齐度和信息实用性）对VLM输出进行无标注评分。

Result: 通过对LearningPaper24、SUTD-TrafficQA和LongVideoBench进行人类研究，VIBE摘要在提高任务准确性和减少响应时间方面表现突出，与天真VLM摘要或原始视频相比，准确性提高了61.23%，响应时间减少了75.77%。

Conclusion: VIBE选出的摘要显著提高了任务准确性和减少了响应时间，表明其有效性。

Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still
require human supervision. For example, tasks like traffic officers reviewing
hour-long dashcam footage or researchers screening conference videos can
benefit from concise summaries that reduce cognitive load and save time. Yet
current vision-language models (VLMs) often produce verbose, redundant outputs
that hinder task performance. Existing video caption evaluation depends on
costly human annotations and overlooks the summaries' utility in downstream
tasks. We address these gaps with Video-to-text Information Bottleneck
Evaluation (VIBE), an annotation-free method that scores VLM outputs using two
metrics: grounding (how well the summary aligns with visual content) and
utility (how informative it is for the task). VIBE selects from randomly
sampled VLM outputs by ranking them according to the two scores to support
effective human decision-making. Human studies on LearningPaper24,
SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE
consistently improve performance-boosting task accuracy by up to 61.23% and
reducing response time by 75.77% compared to naive VLM summaries or raw video.

</details>


### [115] [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
*Wei Jie Yeo,Rui Mao,Moloud Abdar,Erik Cambria,Ranjan Satapathy*

Main category: cs.CV

TL;DR: 本文提出了一种名为LTC的对比框架，通过识别并切除虚假注意力头来缓解CLIP中的错误关联问题，并提高分类性能，尤其是在偏见基准上的表现显著提升。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在无监督情境下表现出色，但易于学习目标变量与混淆因素之间的虚假关联，需要一种方法来纠正这些虚假关联。

Method: 我们提出了一种对比框架LTC，通过机制洞察识别Vision Transformers中的虚假注意力头，并通过有针对性的切除加以缓解。

Result: LTC成功识别并校正了虚假注意力头，同时利用通过正交投影引入判别特征提高了分类性能。

Conclusion: LTC显著提高了在存在背景和性别偏差的基准上的分类表现，特别是最差组的准确率提升超过50%。

Abstract: Multimodal models like CLIP have gained significant attention due to their
remarkable zero-shot performance across various tasks. However, studies have
revealed that CLIP can inadvertently learn spurious associations between target
variables and confounding factors. To address this, we introduce
\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies
spurious attention heads in Vision Transformers via mechanistic insights and
mitigates them through targeted ablation. Furthermore, LTC identifies salient,
task-relevant attention heads, enabling the integration of discriminative
features through orthogonal projection to improve classification performance.
We evaluate LTC on benchmarks with inherent background and gender biases,
achieving over a $>50\%$ gain in worst-group accuracy compared to non-training
post-hoc baselines. Additionally, we visualize the representation of selected
heads and find that the presented interpretation corroborates our contrastive
mechanism for identifying both spurious and salient attention heads. Code
available at https://github.com/wj210/CLIP_LTC.

</details>


### [116] [Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision](https://arxiv.org/abs/2505.17437)
*Yuanshao Zhu,James Jianqiao Yu,Xiangyu Zhao,Xiao Han,Qidong Liu,Xuetao Wei,Yuxuan Liang*

Main category: cs.CV

TL;DR: OmniTraj是一个将多种模态整合到统一系统中的轨迹检索框架，支持高效、灵活的查询，并在大规模数据上效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备和数据收集技术的普及，轨迹数据呈指数增长，对时空数据挖掘尤其是高效准确的轨迹检索带来了重大挑战。目前的轨迹检索方法存在诸多限制，如缺乏对大规模数据的效率、条件查询支持不足以及依赖轨迹相似性度量，因此需要新的解决方案。

Method: OmniTraj框架设计了专用的编码器用于每种模态，将其嵌入并融合到共享表示空间中，支持基于任何单一模态或其组合的查询。

Result: 在两个真实世界的数据集上的大量实验表明，OmniTraj在处理大规模数据、提供灵活的多模态查询以及支持下游任务和应用方面表现出色。

Conclusion: OmniTraj能够有效处理大规模数据，并支持灵活的多模态查询，同时支持下游任务和应用。

Abstract: The widespread adoption of mobile devices and data collection technologies
has led to an exponential increase in trajectory data, presenting significant
challenges in spatio-temporal data mining, particularly for efficient and
accurate trajectory retrieval. However, existing methods for trajectory
retrieval face notable limitations, including inefficiencies in large-scale
data, lack of support for condition-based queries, and reliance on trajectory
similarity measures. To address the above challenges, we propose OmniTraj, a
generalized and flexible omni-semantic trajectory retrieval framework that
integrates four complementary modalities or semantics -- raw trajectories,
topology, road segments, and regions -- into a unified system. Unlike
traditional approaches that are limited to computing and processing
trajectories as a single modality, OmniTraj designs dedicated encoders for each
modality, which are embedded and fused into a shared representation space. This
design enables OmniTraj to support accurate and flexible queries based on any
individual modality or combination thereof, overcoming the rigidity of
traditional similarity-based methods. Extensive experiments on two real-world
datasets demonstrate the effectiveness of OmniTraj in handling large-scale
data, providing flexible, multi-modality queries, and supporting downstream
tasks and applications.

</details>


### [117] [VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models](https://arxiv.org/abs/2505.17440)
*Hefei Mei,Zirui Wang,Shen You,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 论文提出了一种有效且计算开销低的视觉编码器攻击(VEAttack)，通过扰动图像标记生成对抗样本，显著削弱LVLM的多种任务性能，并揭示了LVLM攻击的若干关键观察。


<details>
  <summary>Details</summary>
Motivation: 现有的有效攻击集中在任务特定的白盒设置中，但这些方法在LVLMs背景下受限，因为LVLMs旨在处理多种下游任务，且需要昂贵的全模型梯度计算。本文通过攻击视觉编码器来解决这一问题。

Method: VEAttack通过最小化干净和扰动视觉特征之间的余弦相似性来生成对抗样本，并提出优化图像标记而非分类标记的方式进行图像扰动。

Result: 在图像字幕任务上性能下降94.5%和在视觉问题回答任务上性能下降75.7%。此外，揭示了一些与LVLM攻击/防御相关的关键观察：1）LLM的隐藏层变化，2）标记注意力差异，3）传递攻击中的莫比乌斯带现象，4）对攻击步骤低敏感性。

Conclusion: VEAttack对LVLM的视觉编码器进行攻击，不需要访问后续的大型语言模型、任务信息和标签，从而显著降低计算开销并消除传统白盒攻击在LVLM中的任务和标签依赖性。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities in multimodal understanding and generation, yet their
vulnerability to adversarial attacks raises significant robustness concerns.
While existing effective attacks always focus on task-specific white-box
settings, these approaches are limited in the context of LVLMs, which are
designed for diverse downstream tasks and require expensive full-model gradient
computations. Motivated by the pivotal role and wide adoption of the vision
encoder in LVLMs, we propose a simple yet effective Vision Encoder Attack
(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we
propose to generate adversarial examples by minimizing the cosine similarity
between the clean and perturbed visual features, without accessing the
following large language models, task information, and labels. It significantly
reduces the computational overhead while eliminating the task and label
dependence of traditional white-box attacks in LVLMs. To make this simple
attack effective, we propose to perturb images by optimizing image tokens
instead of the classification token. We provide both empirical and theoretical
evidence that VEAttack can easily generalize to various tasks. VEAttack has
achieved a performance degradation of 94.5% on image caption task and 75.7% on
visual question answering task. We also reveal some key observations to provide
insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token
attention differential, 3) M\"obius band in transfer attack, 4) low sensitivity
to attack steps. The code is available at
https://github.com/hfmei/VEAttack-LVLM

</details>


### [118] [Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds](https://arxiv.org/abs/2505.17442)
*Hao Jing,Anhong Wang,Yifan Zhang,Donghan Bu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了一种基于反射预测知识蒸馏的框架，通过反射率重建和知识蒸馏提高低比特率压缩点云检测的准确度和鲁棒性，实验结果显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前压缩传输系统在反射率编码上的负担和信息丢失导致的检测鲁棒性差的问题。

Method: 本文提出了一种基于反射预测知识蒸馏(RPKD)的3D目标检测框架，压缩点云坐标，舍弃反射率，利用几何反射预测模块重建反射率，并通过教师检测器进行反射率知识蒸馏和检测知识蒸馏，提高学生检测器的鲁棒性。

Result: 实验结果表明，在KITTI数据集低码率（2.146 Bpp）下，RPKD-PV方法的mAP达到73.6，超过现有的PV-RCNN基线检测方法。

Conclusion: 本文提出了一种基于反射预测知识蒸馏(RPKD)的3D目标检测框架，通过反射预测模块重建丢弃的反射率，提高了低比特率压缩点云的检测准确性，同时提高了学生检测器的鲁棒性，实验结果证明在不同码率下压缩点云的检测精度都有显著提升。

Abstract: Regarding intelligent transportation systems for vehicle networking,
low-bitrate transmission via lossy point cloud compression is vital for
facilitating real-time collaborative perception among vehicles with restricted
bandwidth. In existing compression transmission systems, the sender lossily
compresses point coordinates and reflectance to generate a transmission code
stream, which faces transmission burdens from reflectance encoding and limited
detection robustness due to information loss. To address these issues, this
paper proposes a 3D object detection framework with reflectance
prediction-based knowledge distillation (RPKD). We compress point coordinates
while discarding reflectance during low-bitrate transmission, and feed the
decoded non-reflectance compressed point clouds into a student detector. The
discarded reflectance is then reconstructed by a geometry-based reflectance
prediction (RP) module within the student detector for precise detection. A
teacher detector with the same structure as student detector is designed for
performing reflectance knowledge distillation (RKD) and detection knowledge
distillation (DKD) from raw to compressed point clouds. Our RPKD framework
jointly trains detectors on both raw and compressed point clouds to improve the
student detector's robustness. Experimental results on the KITTI dataset and
Waymo Open Dataset demonstrate that our method can boost detection accuracy for
compressed point clouds across multiple code rates. Notably, at a low code rate
of 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of
73.6, outperforming existing detection methods with the PV-RCNN baseline.

</details>


### [119] [PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints](https://arxiv.org/abs/2505.17445)
*Inpyo Song,Hyemin Hwang,Jangwon Lee*

Main category: cs.CV

TL;DR: 引入PawPrint数据集，通过结合深度学习和经典特征，提供更可靠的宠物足迹识别方法，改善传统识别手段的不足，并为未来的研究和应用提供新方向。


<details>
  <summary>Details</summary>
Motivation: 随着宠物拥有率的上升以及宠物走失和被偷事件的频发，现有的识别方法存在诸多限制，因此亟需开发更有效的宠物识别与追踪方法。

Method: 本研究对现代深度神经网络（如CNN，Transformers）和经典局部特征进行了全面的基准测试，以识别狗和猫的足迹。

Result: 研究结果表明，不同方法在基质复杂性和数据可获得性方面各有优劣。结合学习的全局表示和局部描述符的方法有望在不同的实际条件下提高可靠性。

Conclusion: 本研究通过引入PawPrint和PawPrint+数据集为宠物和野生动物提供了一种非侵入的识别和监测方法，这不仅提高了可靠性，还为今后的相关研究指明了方向。

Abstract: In the United States, as of 2023, pet ownership has reached 66% of households
and continues to rise annually. This trend underscores the critical need for
effective pet identification and monitoring methods, particularly as nearly 10
million cats and dogs are reported stolen or lost each year. However,
traditional methods for finding lost animals like GPS tags or ID photos have
limitations-they can be removed, face signal issues, and depend on someone
finding and reporting the pet. To address these limitations, we introduce
PawPrint and PawPrint+, the first publicly available datasets focused on
individual-level footprint identification for dogs and cats. Through
comprehensive benchmarking of both modern deep neural networks (e.g., CNN,
Transformers) and classical local features, we observe varying advantages and
drawbacks depending on substrate complexity and data availability. These
insights suggest future directions for combining learned global representations
with local descriptors to enhance reliability across diverse, real-world
conditions. As this approach provides a non-invasive alternative to traditional
ID tags, we anticipate promising applications in ethical pet management and
wildlife conservation efforts.

</details>


### [120] [Real-time Traffic Accident Anticipation with Feature Reuse](https://arxiv.org/abs/2505.17449)
*Inpyo Song,Jangwon Lee*

Main category: cs.CV

TL;DR: RARE通过轻量级框架减少延迟并提高预测准确性，实现了实时事故预测，适合安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 预测潜在交通事故以提高自动驾驶的安全性和实时性，但现有方法的计算量较大，难以部署。

Method: 引入RARE框架，利用单个预训练对象检测器的中间特征，使用注意力得分排序损失优先关注相关对象，减少额外特征提取步骤。

Result: RARE在DAD和CCD基准测试中实现了4-8倍加速，达到每帧13.6毫秒（73.3 FPS）的延迟，并获得了最先进的平均精度。

Conclusion: RARE提供的实时事故预测框架在减少延迟和提高准确性上表现出色，突出其在安全关键应用中的潜力。

Abstract: This paper addresses the problem of anticipating traffic accidents, which
aims to forecast potential accidents before they happen. Real-time anticipation
is crucial for safe autonomous driving, yet most methods rely on
computationally heavy modules like optical flow and intermediate feature
extractors, making real-world deployment challenging. In this paper, we thus
introduce RARE (Real-time Accident anticipation with Reused Embeddings), a
lightweight framework that capitalizes on intermediate features from a single
pre-trained object detector. By eliminating additional feature-extraction
pipelines, RARE significantly reduces latency. Furthermore, we introduce a
novel Attention Score Ranking Loss, which prioritizes higher attention on
accident-related objects over non-relevant ones. This loss enhances both
accuracy and interpretability. RARE demonstrates a 4-8 times speedup over
existing approaches on the DAD and CCD benchmarks, achieving a latency of
13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced
complexity, it attains state-of-the-art Average Precision and reliably
anticipates imminent collisions in real time. These results highlight RARE's
potential for safety-critical applications where timely and explainable
anticipation is essential.

</details>


### [121] [Graph Mamba for Efficient Whole Slide Image Understanding](https://arxiv.org/abs/2505.17457)
*Jiaxuan Lu,Junyan Shi,Yuhui Lin,Fang Yan,Yue Gao,Shaoting Zhang,Xiaosong Wang*

Main category: cs.CV

TL;DR: WSI-GMamba框架结合GNNs和Mamba的优势，提高了WSI分析的准确性和计算效率，降低了计算成本，实现了可扩展的大规模分析。


<details>
  <summary>Details</summary>
Motivation: 由于WSIs在组织病理学中分辨率高、体积大以及复杂的竖排关系，使得大规模医学图像分析面临巨大挑战。现有的多实例学习（MIL）方法，比如图神经网络（GNNs）和基于Transformer的模型，存在可扩展性和计算成本方面的限制，因此提出了WSI-GMamba框架以解决这些问题。

Method: WSI-GMamba框架结合了轻量级GNNs的关系模型和Mamba的计算效率。GMamba模块通过双向状态空间模型（Bi-SSM）集成消息传递、图扫描与压缩，以及特征聚合，达到Transformer级别性能，同时减少7倍的FLOPs。

Result: WSI-GMamba框架通过结合轻量级GNNs和Mamba的优势，实现了一个高性能、可扩展的解决方案，适合于大规模WSI分析。该框架在幻灯片级分类中提供了高准确性和高计算效率。

Conclusion: WSI-GMamba框架提供了一种可扩展的解决方案，用于大规模WSI分析，在提供高准确性和计算效率的同时实现幻灯片级分类。

Abstract: Whole Slide Images (WSIs) in histopathology present a significant challenge
for large-scale medical image analysis due to their high resolution, large
size, and complex tile relationships. Existing Multiple Instance Learning (MIL)
methods, such as Graph Neural Networks (GNNs) and Transformer-based models,
face limitations in scalability and computational cost. To bridge this gap, we
propose the WSI-GMamba framework, which synergistically combines the relational
modeling strengths of GNNs with the efficiency of Mamba, the State Space Model
designed for sequence learning. The proposed GMamba block integrates Message
Passing, Graph Scanning & Flattening, and feature aggregation via a
Bidirectional State Space Model (Bi-SSM), achieving Transformer-level
performance with 7* fewer FLOPs. By leveraging the complementary strengths of
lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable
solution for large-scale WSI analysis, offering both high accuracy and
computational efficiency for slide-level classification.

</details>


### [122] [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
*Kazuki Hayashi,Shintaro Ozaki,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CV

TL;DR: 研究表明，LVLMs能解释但无法模拟色觉障碍患者的颜色感知，呼吁开发更包容的多模态系统。


<details>
  <summary>Details</summary>
Motivation: 探讨LVLMs在处理人类视觉和语言感知多样性时的能力，尤其是颜色感知方面的个体差异。

Method: 使用Ishihara Test进行实验，评估LVLMs是否能够解释和模拟色觉障碍患者的视觉体验。

Result: LVLMs能够用自然语言解释色觉异常，但无法模拟这些人在图像任务中的颜色感知。

Conclusion: 需要开发能够考虑颜色感知多样性的多模态系统，以促进关于感知包容性和公平性的更广泛讨论。

Abstract: Large-scale Vision Language Models (LVLMs) are increasingly being applied to
a wide range of real-world multimodal applications, involving complex visual
and linguistic reasoning. As these models become more integrated into practical
use, they are expected to handle complex aspects of human interaction. Among
these, color perception is a fundamental yet highly variable aspect of visual
understanding. It differs across individuals due to biological factors such as
Color Vision Deficiencies (CVDs), as well as differences in culture and
language. Despite its importance, perceptual diversity has received limited
attention. In our study, we evaluate LVLMs' ability to account for individual
level perceptual variation using the Ishihara Test, a widely used method for
detecting CVDs. Our results show that LVLMs can explain CVDs in natural
language, but they cannot simulate how people with CVDs perceive color in image
based tasks. These findings highlight the need for multimodal systems that can
account for color perceptual diversity and support broader discussions on
perceptual inclusiveness and fairness in multimodal AI.

</details>


### [123] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
*Jiangning Zhu,Yuxing Zhou,Zheng Wang,Juntao Yao,Yima Gu,Yuhui Yuan,Shixia Liu*

Main category: cs.CV

TL;DR: OrionBench是一个用于开发精确检测模型的基准工具，支持图表和人类可识别对象的检测，提高视觉语言模型的图表理解性能。


<details>
  <summary>Details</summary>
Motivation: 图表在科学、商业、和交流中扮演重要角色，提高视觉语言模型处理图表的能力至关重要，而现有模型在信息图元素的视觉定位上存在局限。

Method: 通过结合模型循环和编程方法来生成标注，并利用这些标注来开发检测模型。

Result: OrionBench包含大量真实和合成的信息图及标注，通过构建思维盒子方案、比较现有模型和应用于文档布局与UI元素检测三个方面证明其效用。

Conclusion: OrionBench提升了图表和人类可识别对象的检测能力，从而提高了视觉语言模型的图表理解性能。

Abstract: Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.

</details>


### [124] [PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation](https://arxiv.org/abs/2505.17475)
*Uyoung Jeong,Jonathan Freer,Seungryul Baek,Hyung Jin Chang,Kwang In Kim*

Main category: cs.CV

TL;DR: 推出了PoseBH框架，通过非参数化关键点原型和自监督机制解决姿态估计中的多数据集训练问题，并在多个数据集上取得成效。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法解决姿态估计中骨架异质性带来的挑战，特别是在数据集合并和多头监督方面的复杂性。

Method: 引入非参数化关键点原型和跨类型自监督机制，通过统一的嵌入空间学习关键点，从而实现各类骨架类型的无缝整合。

Result: PoseBH兼具提高能力和对多种数据集的适应能力，代码在GitHub上可用。

Conclusion: PoseBH显著提升了多种人体和动物姿态数据集的泛化能力，同时在标准人类姿态基准上保持了表现。

Abstract: We study multi-dataset training (MDT) for pose estimation, where skeletal
heterogeneity presents a unique challenge that existing methods have yet to
address. In traditional domains, \eg regression and classification, MDT
typically relies on dataset merging or multi-head supervision. However, the
diversity of skeleton types and limited cross-dataset supervision complicate
integration in pose estimation. To address these challenges, we introduce
PoseBH, a new MDT framework that tackles keypoint heterogeneity and limited
supervision through two key techniques. First, we propose nonparametric
keypoint prototypes that learn within a unified embedding space, enabling
seamless integration across skeleton types. Second, we develop a cross-type
self-supervision mechanism that aligns keypoint predictions with keypoint
embedding prototypes, providing supervision without relying on teacher-student
models or additional augmentations. PoseBH substantially improves
generalization across whole-body and animal pose datasets, including
COCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard
human pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint
embeddings transfer effectively to hand shape estimation (InterHand2.6M) and
human body shape estimation (3DPW). The code for PoseBH is available at:
https://github.com/uyoung-jeong/PoseBH.

</details>


### [125] [The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts](https://arxiv.org/abs/2505.17476)
*Yuchen Zhang,Yaxiong Wang,Yujiao Wu,Lianwei Wu,Li Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种应对MLLM驱动的多模态虚假信息的对抗性管道，通过构建合成数据集和新的框架，解决了当前方法中低估欺骗风险和不真实失调的局限性，实验结果显示出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要面临两个基本局限性：（1）低估了MLLM驱动的欺骗风险；（2）依赖于不具有语义一致性的人工失调内容。为了全面解决这些问题，提出了新的对抗性管道。

Method: 本文引入了一种新的对抗性管道和框架，通过构建MLLM驱动的合成多模态（MDSM）数据集，然后利用人工制品感知编码策略和操作导向推理框架（AMD），针对MLLM生成的多模态虚假信息进行检测。

Result: 综合实验验证了本文提出的框架作为一种检测MLLM驱动的多模态欺骗的统一架构，具有优越的泛化能力。

Conclusion: 本文提出了一种新的对抗性管道，利用多模态大型语言模型（MLLMs）来生成高风险的虚假信息，并通过构建MLLM驱动的合成多模态数据集（MDSM）及人工制品感知编码策略和操作导向推理框架（AMD）来检测多模态虚假信息，这一方案验证了其强大的泛化能力。

Abstract: The detection and grounding of multimedia manipulation has emerged as a
critical challenge in combating AI-generated disinformation. While existing
methods have made progress in recent years, we identify two fundamental
limitations in current approaches: (1) Underestimation of MLLM-driven deception
risk: prevailing techniques primarily address rule-based text manipulations,
yet fail to account for sophisticated misinformation synthesized by multimodal
large language models (MLLMs) that can dynamically generate semantically
coherent, contextually plausible yet deceptive narratives conditioned on
manipulated images; (2) Unrealistic misalignment artifacts: currently focused
scenarios rely on artificially misaligned content that lacks semantic
coherence, rendering them easily detectable. To address these gaps
holistically, we propose a new adversarial pipeline that leverages MLLMs to
generate high-risk disinformation. Our approach begins with constructing the
MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered
using state-of-the-art editing techniques and then paired with MLLM-generated
deceptive texts that maintain semantic consistency with the visual
manipulations. Building upon this foundation, we present the Artifact-aware
Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:
Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,
to tame MLLMs for the MDSM problem. Comprehensive experiments validate our
framework's superior generalization capabilities as a unified architecture for
detecting MLLM-powered multimodal deceptions.

</details>


### [126] [Research on Defect Detection Method of Motor Control Board Based on Image Processing](https://arxiv.org/abs/2505.17493)
*Jingde Huang,Zhangyu Huang,Chenyu Li,Jiantong Liu*

Main category: cs.CV

TL;DR: 该论文研究了电机控制板的缺陷检测技术，使用图像处理方法提高了检测的准确性和效率，准确率达到99%以上，可用于生产线上的在线检测。


<details>
  <summary>Details</summary>
Motivation: 研究电机控制板的缺陷检测技术是提高电机控制板质量控制水平的重要手段。

Method: 研究了电机控制板数字图像的处理方法，分析了影响图像特征提取的噪声抑制方法，建立了检测电机控制板缺陷特征提取及色差识别的特定模型，优化了缺陷图像搜索算法，并进行了对比实验。

Result: 电机控制板缺陷检测模型的准确率超过99%，能够实现生产线上大量电机控制板的及时图像处理，达到高效缺陷检测。

Conclusion: 缺陷检测方法不仅可以用于电机控制板缺陷的在线检测，还可以为集成电路板缺陷处理提供解决方案。

Abstract: The motor control board has various defects such as inconsistent color
differences, incorrect plug-in positions, solder short circuits, and more.
These defects directly affect the performance and stability of the motor
control board, thereby having a negative impact on product quality. Therefore,
studying the defect detection technology of the motor control board is an
important means to improve the quality control level of the motor control
board. Firstly, the processing methods of digital images about the motor
control board were studied, and the noise suppression methods that affect image
feature extraction were analyzed. Secondly, a specific model for defect feature
extraction and color difference recognition of the tested motor control board
was established, and qualified or defective products were determined based on
feature thresholds. Thirdly, the search algorithm for defective images was
optimized. Finally, comparative experiments were conducted on the typical motor
control board, and the experimental results demonstrate that the accuracy of
the motor control board defect detection model-based on image processing
established in this paper reached over 99%. It is suitable for timely image
processing of large quantities of motor control boards on the production line,
and achieved efficient defect detection. The defect detection method can not
only be used for online detection of the motor control board defects, but also
provide solutions for the integrated circuit board defect processing for the
industry.

</details>


### [127] [RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition](https://arxiv.org/abs/2505.17501)
*Yuehan Jin,Xiaoqing Liu,Yiyuan Yang,Zhiwen Yu,Tong Zhang,Kaixiang Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RoHyDR的框架，它通过结合扩散生成器和对抗学习，解决了不完整多模态情感识别中的模态缺失问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对现实中噪声或传感器故障导致的数据缺失或损坏，对应的情感识别面临不完整多模态情感识别(IMER)的挑战，因此引入一个处理缺失模态的恢复机制。

Method: 提出了一种新的框架RoHyDR，采用扩散生成器和对抗学习进行无模态恢复，并结合多阶段优化策略来增强稳定性和训练效率。

Result: RoHyDR在广泛使用的两大多模态情感识别数据集上进行的综合实验表明，其性能优于最先进的IMER方法，在不同缺失模态场景下实现了稳健的识别性能。

Conclusion: 通过综合实验，RoHyDR显示出在处理不同缺失模态场景下的优越性能，相比现有最优IMER方法，在鲁棒性识别表现上都有所提升。

Abstract: Multimodal emotion recognition analyzes emotions by combining data from
multiple sources. However, real-world noise or sensor failures often cause
missing or corrupted data, creating the Incomplete Multimodal Emotion
Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion
Recovery (RoHyDR), a novel framework that performs missing-modality recovery at
unimodal, multimodal, feature, and semantic levels. For unimodal representation
recovery of missing modalities, RoHyDR exploits a diffusion-based generator to
generate distribution-consistent and semantically aligned representations from
Gaussian noise, using available modalities as conditioning. For multimodal
fusion recovery, we introduce adversarial learning to produce a realistic fused
multimodal representation and recover missing semantic content. We further
propose a multi-stage optimization strategy that enhances training stability
and efficiency. In contrast to previous work, the hybrid diffusion and
adversarial learning-based recovery mechanism in RoHyDR allows recovery of
missing information in both unimodal representation and multimodal fusion, at
both feature and semantic levels, effectively mitigating performance
degradation caused by suboptimal optimization. Comprehensive experiments
conducted on two widely used multimodal emotion recognition benchmarks
demonstrate that our proposed method outperforms state-of-the-art IMER methods,
achieving robust recognition performance under various missing-modality
scenarios. Our code will be made publicly available upon acceptance.

</details>


### [128] [Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning](https://arxiv.org/abs/2505.17509)
*Shiji Zhao,Qihui Zhu,Shukun Xiong,Shouwei Ruan,Yize Fan,Ranjie Duan,Qing Guo,Xingxing Wei*

Main category: cs.CV

TL;DR: 为增强视觉语言模型（VLMs）在对抗样本下的鲁棒性，提出Adversarial Mixture Prompt Tuning (AMPT)方法，通过混合学习提示提高泛化，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 增强大型预训练视觉语言模型（VLMs）对抗对抗样本的鲁棒性，因为这些模型对对抗样本高度敏感，可能带来安全风险。

Method: 提出了一种名为Adversarial Mixture Prompt Tuning (AMPT)的对抗性调整方法，通过增加学习提示的数量而不是延长提示长度，提高对抗性多样攻击的泛化能力，并通过条件权重路由器根据输入对抗性图像预测多个学习提示的混合权重。

Result: 我们的方法在11个数据集的不同实验设置下实现了比最先进方法更好的对抗性鲁棒性。

Conclusion: 通过AMPT方法，VLMs在面对不同对抗性攻击时的泛化能力得到了增强，实验结果表明其对抗性鲁棒性优于最先进的方法。

Abstract: Large pre-trained Vision Language Models (VLMs) have excellent generalization
capabilities but are highly susceptible to adversarial examples, presenting
potential security risks. To improve the robustness of VLMs against adversarial
examples, adversarial prompt tuning methods are proposed to align the text
feature with the adversarial image feature without changing model parameters.
However, when facing various adversarial attacks, a single learnable text
prompt has insufficient generalization to align well with all adversarial image
features, which finally leads to the overfitting phenomenon. To address the
above challenge, in this paper, we empirically find that increasing the number
of learned prompts can bring more robustness improvement than a longer prompt.
Then we propose an adversarial tuning method named Adversarial Mixture Prompt
Tuning (AMPT) to enhance the generalization towards various adversarial attacks
for VLMs. AMPT aims to learn mixture text prompts to obtain more robust text
features. To further enhance the adaptability, we propose a conditional weight
router based on the input adversarial image to predict the mixture weights of
multiple learned prompts, which helps obtain sample-specific aggregated text
features aligning with different adversarial image features. A series of
experiments show that our method can achieve better adversarial robustness than
state-of-the-art methods on 11 datasets under different experimental settings.

</details>


### [129] [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://arxiv.org/abs/2505.17529)
*Yeongjae Cho,Keonwoo Kim,Taebaek Hwang,Sungzoon Cho*

Main category: cs.CV

TL;DR: 提出了Ensemble Decoding方法，以解决LVLMs对象幻觉问题，实验表明其效果显著。


<details>
  <summary>Details</summary>
Motivation: LVLMs在图像描述和视觉问答任务中应用广泛，但仍存在对象幻觉问题，以至于生成的描述错误地反映视觉内容。现有方法遇到可扩展性挑战并依赖额外模块，因此需要新的解决策略。

Method: 提出了一种称为Ensemble Decoding（ED）的新策略，通过将输入图像分解为子图像并通过注意力图分配权重来组合logit分布。此外，引入了ED自适应合理性约束来校准logit分布，并开发了FastED变体以满足速度关键的应用需求。

Result: 广泛的实验表明，我们提出的方法在解决幻觉问题时达到了最先进的性能，证实了方法的有效性。

Conclusion: 我们提出的Ensemble Decoding（ED）方法在解决LVLMs的对象幻觉问题上有效，并在幻觉基准测试中实现了最新的性能水平。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly expanded their utility in tasks like image captioning and visual
question answering. However, they still struggle with object hallucination,
where models generate descriptions that inaccurately reflect the visual content
by including nonexistent objects or misrepresenting existing ones. While
previous methods, such as data augmentation and training-free approaches,
strive to tackle this issue, they still encounter scalability challenges and
often depend on additional external modules. In this work, we propose Ensemble
Decoding (ED), a novel strategy that splits the input image into sub-images and
combines logit distributions by assigning weights through the attention map.
Furthermore, we introduce ED adaptive plausibility constraint to calibrate
logit distribution and FastED, a variant designed for speed-critical
applications. Extensive experiments across hallucination benchmarks demonstrate
that our proposed method achieves state-of-the-art performance, validating the
effectiveness of our approach.

</details>
